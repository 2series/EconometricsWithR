<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Using R for Introduction to Econometrics</title>
  <meta name="description" content="Using R for Introduction to Econometrics">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Using R for Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Using R for Introduction to Econometrics" />
  
  
  

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2017-10-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="a-review-of-statistics-using-r.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="hideOutput.js"></script>

<script src="js/d3.v3.min.js"></script> 
<script src="js/leastsquares.js"></script>
<script src="js/d3functions.js"></script>
<script src="d3.slider.js"></script>
<script src="slrm.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">URFITE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i>Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#expected-values-mean-and-variance"><i class="fa fa-check"></i>Expected Values, Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li><a href="probability-theory.html#thetdist">The Student <span class="math inline">\(t\)</span> Distribution</a></li>
<li><a href="probability-theory.html#the-f-distribution">The <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#random-sampling-and-the-distribution-of-sample-averages"><i class="fa fa-check"></i>Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#estimation-of-the-population-mean"><i class="fa fa-check"></i>Estimation of the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#properties-of-the-population-mean"><i class="fa fa-check"></i>Properties of the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i>Hypothesis Tests Concerning the Population Mean</a><ul>
<li><a href="a-review-of-statistics-using-r.html#p-value"><span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="a-review-of-statistics-using-r.html#calculating-the-p-value-when-sigma_y-is-known">Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#sample-variance-sample-standard-deviation-and-standard-error"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li><a href="a-review-of-statistics-using-r.html#calculating-the-p-value-when-sigma_y-is-unknown">Calculating the <span class="math inline">\(p\)</span>-value When <span class="math inline">\(\sigma_Y\)</span> is Unknown</a></li>
<li><a href="a-review-of-statistics-using-r.html#the-t-statistic">The <span class="math inline">\(t\)</span>-statistic</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i>Confidence intervals for the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#comparing-means-from-different-populations"><i class="fa fa-check"></i>Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#an-application-to-the-gender-gap-of-earnings"><i class="fa fa-check"></i>An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i>Scatterplots, Sample Covariance and Sample Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i>Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i>Measures of fit</a><ul>
<li><a href="lrwor.html#the-r2">The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#standard-error-of-the-regression"><i class="fa fa-check"></i>Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-least-squares-assumptions"><i class="fa fa-check"></i>The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption #1: The Error Term has Conditional Mean of Zero</a></li>
<li><a href="lrwor.html#assumption-2-all-x_i-y_i-are-independently-and-identically-distributed">Assumption #2: All <span class="math inline">\((X_i, Y_i)\)</span> are Independently and Identically Distributed</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption #3: Large outliers are unlikely</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i>The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-1"><i class="fa fa-check"></i>R Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-2"><i class="fa fa-check"></i>R Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-3"><i class="fa fa-check"></i>R Simulation Study 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#testing-two-sided-hypotheses-concerning-beta_1">Testing Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#confidence-intervals-for-regression-coefficients"><i class="fa fa-check"></i>Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#r-simulation-study-5.1"><i class="fa fa-check"></i>R Simulation Study 5.1</a></li>
</ul></li>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#regression-when-x-is-a-binary-variable">Regression when <span class="math inline">\(X\)</span> is a Binary Variable</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#heteroskedasticity-and-homoskedasticity"><i class="fa fa-check"></i>Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#the-gauss-markov-theorem"><i class="fa fa-check"></i>The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#r-simulation-study-blue-estimator"><i class="fa fa-check"></i>R Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small">Using the <span class="math inline">\(t\)</span>-Statistic in Regression When the Sample Size Is Small</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using R for Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-theory" class="section level1">
<h1><span class="header-section-number">2</span> Probability Theory</h1>
<p>This chapter reviews some basic concepts of probability theory and demonstrates how they can be applied in R.</p>
<p>Most of the statistical functionalities in R’s standard version are collected in the <tt>stats</tt> package. It provides simple functions which compute descriptive measures and faciliate calculus involving a variety of probability distributions but also holds more sophisticated routines that e.g. enable the user to estimate a large number of models based on the same data or help to conduct extensive simulation studies. Execute <code>library(help = &quot;stats&quot;)</code> in the console to view the documentation and a complete list of all functions gathered in <code>stats</code>.</p>
<p>In what follows, we lay our focus on (some of) the probability distributions that are handled by R and show how to use the relevant functions to solve simple problems. Thereby we will repeat some core concepts of probability theory. Among other things, You will learn how to draw random numbers, how to compute densities, probabilities, quantiles and alike. As we shall see, it is very convenient to rely on these routines, especially when writing Your own functions.</p>
<div id="random-variables-and-probability-distributions" class="section level2 unnumbered">
<h2>Random Variables and Probability Distributions</h2>
<p>For a start, let us briefly review some basic concepts in probability.</p>
<ul>
<li>The mutually exclusive results of a random process are called the <em>outcomes</em>. ‘Mutually exclusive’ means that only one of the possible outcomes is observed.</li>
<li>We refer to the <em>probability</em> of an outcome as the proportion of the time that the outcome occurs in the long run, that is if the experiment is repeated very often.</li>
<li>The set of all possible outcomes of a random variable is called the <em>sample space</em>.</li>
<li>An <em>event</em> is a subset of the sample space and consists of one or more outcomes.</li>
</ul>
<p>These indeas are unified in the concept of a <em>random variable</em> which is a numerical summary of random outcomes. Random variables can be <em>discrete</em> or <em>continuous</em>.</p>
<ul>
<li>Discrete random variables have discrete outcomes, e.g. <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</li>
<li>A continuous random variable takes on a continuum of possible values.</li>
</ul>
<div id="probability-distributions-of-discrete-random-variables" class="section level3 unnumbered">
<h3>Probability Distributions of Discrete Random Variables</h3>
<p>A typical example for a discrete random variable <span class="math inline">\(D\)</span> is the result of a die roll: in terms of a random experiment this is nothing but randomly selecting a sample of size <span class="math inline">\(1\)</span> from a set of numbers which are mutually exclusive outcomes. Here, the sample space is <span class="math inline">\(\{1,2,3,4,5,6\}\)</span> and we can think of many different events, e.g. ‘the observed outcome lies between <span class="math inline">\(2\)</span> and <span class="math inline">\(5\)</span>’.</p>
<p>A basic function to draw random samples from a specified set of elements is the the function <code>sample()</code>, see <code>?sample</code>. We can use it to simulate the random outcome of a die roll. Let’s role the dice!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">1</span>) </code></pre></div>
<pre><code>## [1] 4</code></pre>
<p>The probability distribution of a discrete random variable is the list of all possible values of the variable and thier probabilities which sum to <span class="math inline">\(1\)</span>. The cumulative probability distribution function states the probability that the random variable is less than or equal to a particular value.</p>
<p>For the die roll, this is straightforward to set up</p>
<table>
<thead>
<tr class="header">
<th>Outcome</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Probability distribution</td>
<td>1/6</td>
<td>1/6</td>
<td>1/6</td>
<td>1/6</td>
<td>1/6</td>
<td>1/6</td>
</tr>
<tr class="even">
<td>Cumulative probability distribution</td>
<td>1/6</td>
<td>2/6</td>
<td>3/6</td>
<td>4/6</td>
<td>5/6</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>We can easily plot both functions using R. Since the probability equals <span class="math inline">\(1/6\)</span> for each outcome, we set up the vector <code>probability</code> by using the <code>rep()</code> function which replicates a given value a specified number of times.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate the vector of probabilities </span>
probability &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">6</span>,<span class="dv">6</span>) 

<span class="co"># plot the probabilites </span>
<span class="kw">plot</span>(probability, <span class="dt">xlab =</span> <span class="st">&quot;outcomes&quot;</span>, 
     <span class="dt">main =</span> <span class="st">&quot;Probability Distribution&quot;</span>
     ) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>For the cumulative probability distribution we need the cumulative probabilities i.e. we need the cumulative sums of the vector <code>probability</code>. These sums can be computed using <code>cumsum()</code>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#generate the vector of cumulative probabilities </span>
cum_probability &lt;-<span class="st"> </span><span class="kw">cumsum</span>(probability) 

<span class="co"># plot the probabilites </span>
<span class="kw">plot</span>(cum_probability, 
     <span class="dt">xlab =</span> <span class="st">&quot;outcomes&quot;</span>, 
     <span class="dt">main =</span> <span class="st">&quot;Cumulative Probability Distribution&quot;</span>
     ) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="bernoulli-trials" class="section level3 unnumbered">
<h3>Bernoulli Trials</h3>
<p>The set of elements <code>sample()</code> draws from does not have to consist of numbers only. We might as well simulate coin tossing with outcomes <span class="math inline">\(H\)</span> (head) and <span class="math inline">\(T\)</span> (tail).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;H&quot;</span>,<span class="st">&quot;T&quot;</span>),<span class="dv">1</span>) </code></pre></div>
<pre><code>## [1] &quot;T&quot;</code></pre>
<p>The result of a coin toss is a Bernoulli distributed random variable i.e. a variable with to possible distinct outcomes.</p>
<p>Imagine you are about to toss a coin <span class="math inline">\(10\)</span> times in a row and wonder how likely it is to end up with a sequence of outcomes like</p>
<p><span class="math display">\[ H \, H \, T \, T \,T \,H \,T \,T \, H \, H .\]</span></p>
<p>This is a typical example of a Bernoulli experiment as it consists of <span class="math inline">\(n=10\)</span> Bernoulli trials that are independent of each other and we are interested in the likelihood of observing <span class="math inline">\(k=5\)</span> successes <span class="math inline">\(H\)</span> that occur with probability <span class="math inline">\(p=0.5\)</span> (assuming a fair coin) in each trial.</p>
<p>It is a well known result that the number of successes <span class="math inline">\(k\)</span> follows a binomial distribution</p>
<p><span class="math display">\[ k \sim B(n,p). \]</span></p>
<p>The probability of observing <span class="math inline">\(k\)</span> successes in the experiment <span class="math inline">\(B(n,p)\)</span> is hence given by</p>
<p><span class="math display">\[f(k)=P(k)=\begin{pmatrix}n\\ k \end{pmatrix} \cdot p^k \cdot
q^{n-k}=\frac{n!}{k!(n-k)!} \cdot p^k \cdot q^{n-k}\]</span></p>
<p>where <span class="math inline">\(\begin{pmatrix}n\\ k \end{pmatrix}\)</span> is a binomial coefficient.</p>
<p>In R, we can solve the problem stated above by means of the function <code>dbinom()</code> which calculates the probability of the binomial distribution for parameters <code>x</code>, <code>size</code>, and <code>prob</code>, see <code>?binom</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">5</span>,
       <span class="dt">size =</span> <span class="dv">10</span>,
       <span class="dt">prob =</span> <span class="fl">0.5</span>
       ) </code></pre></div>
<pre><code>## [1] 0.2460938</code></pre>
<p>We conclude that the probability of observing Head <span class="math inline">\(k=5\)</span> times when tossing the coin <span class="math inline">\(n=10\)</span> times is about <span class="math inline">\(24.6\%\)</span>.</p>
<p>Now assume we are interested in <span class="math inline">\(P(4 \leq k \leq 7)\)</span> i.e. the probability of observing <span class="math inline">\(4\)</span>, <span class="math inline">\(5\)</span>, <span class="math inline">\(6\)</span> or <span class="math inline">\(7\)</span> successes for <span class="math inline">\(B(10,0.5)\)</span>. This is easily computed by providing a vector as the <code>x</code> argument in our call of <code>dbinom()</code> and summing up using <code>sum()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(
  <span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">4</span><span class="op">:</span><span class="dv">7</span>, 
         <span class="dt">size =</span> <span class="dv">10</span>, 
         <span class="dt">prob =</span> <span class="fl">0.5</span>
         )
  )</code></pre></div>
<pre><code>## [1] 0.7734375</code></pre>
<p>The Probability distribution of a discrete random variable is nothing but a list of all possible outcomes that can occur and their respective probabilities. In our coin tossing example, we face <span class="math inline">\(11\)</span> possible outcomes for <span class="math inline">\(k\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set up vector of possible outcomes</span>
k &lt;-<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">10</span></code></pre></div>
<p>To visualize the probability distribution function of <span class="math inline">\(k\)</span> we may therefore simply call</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># assign probabilities</span>
probability &lt;-<span class="st"> </span><span class="kw">dbinom</span>(<span class="dt">x =</span> k,
                      <span class="dt">size =</span> <span class="dv">10</span>, 
                      <span class="dt">prob =</span> <span class="fl">0.5</span>
                      )

<span class="co"># plot outcomes against probabilities</span>
<span class="kw">plot</span>(<span class="dt">x =</span> k, 
     <span class="dt">y =</span> probability,
     <span class="dt">main =</span> <span class="st">&quot;Probability Distribution Function&quot;</span>) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>In a similar fashion we may plot the cumulative distribution function of <span class="math inline">\(k\)</span> by executing the following code chunk:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prob &lt;-<span class="st"> </span><span class="kw">cumsum</span>(
              <span class="kw">dbinom</span>(<span class="dt">x =</span><span class="dv">0</span><span class="op">:</span><span class="dv">10</span>, 
                     <span class="dt">size =</span> <span class="dv">10</span>, 
                     <span class="dt">prob =</span> <span class="fl">0.5</span>
                     )
              )

k &lt;-<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">10</span> 
<span class="kw">plot</span>(<span class="dt">x =</span> k, 
     <span class="dt">y =</span> prob,
     <span class="dt">main =</span> <span class="st">&quot;Cumulative Distribution Function&quot;</span>) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="expected-values-mean-and-variance" class="section level3 unnumbered">
<h3>Expected Values, Mean and Variance</h3>
<p>The expected value of a random variable is the long-run average value of the random variable over many repeated trials. For a discrete random variable, the expected value is computed as a weighted average of its possible outcomes whereby the weights are the related probabilities. This is formalized in Key Concept 2.1.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.1
</h3>
<h3 class="left">
Expected Value and the Mean
</h3>
<p>
<p>Suppose the random variable <span class="math inline">\(Y\)</span> takes on <span class="math inline">\(k\)</span> possible values, <span class="math inline">\(y_1, \dots, y_k\)</span>, where <span class="math inline">\(y_1\)</span> denotes the first value, <span class="math inline">\(y_2\)</span> denotes the second value, and so forth, and that the probability that <span class="math inline">\(Y\)</span> takes on <span class="math inline">\(y_1\)</span> is <span class="math inline">\(p_1\)</span>, the probability that <span class="math inline">\(Y\)</span> takes on <span class="math inline">\(y_2\)</span> is <span class="math inline">\(p_2\)</span> and so forth. The expected value of <span class="math inline">\(Y\)</span>, <span class="math inline">\(E(Y)\)</span> is defined as</p>
<p><span class="math display">\[ E(Y) = y_1 p_1 + y_2 p_2 + \cdots + y_k p_k = \sum_{i=1}^k y_i p_i \]</span></p>
where the notation <span class="math inline">\(\sum_{i=1}^k y_i p_i\)</span> means “the sum of <span class="math inline">\(y_i\)</span> <span class="math inline">\(p_i\)</span> for <span class="math inline">\(i\)</span> running from <span class="math inline">\(1\)</span> to <span class="math inline">\(k\)</span>”. The expected value of <span class="math inline">\(Y\)</span> is also called the mean of <span class="math inline">\(Y\)</span> or the expectation of <span class="math inline">\(Y\)</span> and is denoted by <span class="math inline">\(\mu_y\)</span>.
</p>
</div>
<p>In the dice example, the random variable, <span class="math inline">\(D\)</span> say, takes on <span class="math inline">\(6\)</span> possible values <span class="math inline">\(d_1 = 1, d_2 = 2, \dots, d_6 = 6\)</span>. Assuming a fair dice, each of the <span class="math inline">\(6\)</span> outcomes occurs with a probability of <span class="math inline">\(1/6\)</span>. It is therefore easy to calculate the exact value of <span class="math inline">\(E(D)\)</span> by hand:</p>
<p><span class="math display">\[ E(D) = 1/6 \sum_{i=1}^6 d_i = 3.5 \]</span></p>
<p>Here, this is simply the average of the natural numbers from <span class="math inline">\(1\)</span> to <span class="math inline">\(6\)</span> since all wights <span class="math inline">\(p_i\)</span> are <span class="math inline">\(1/6\)</span>. Convince Yourself that this can be easily calculated using the function <code>mean()</code> which computes the arithmetic mean of a numeric vector.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>)</code></pre></div>
<pre><code>## [1] 3.5</code></pre>
<p>An example of sampling with replacement is rolling a dice three times in a row.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># rolling a dice three times in a row</span>
<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">3</span>, <span class="dt">replace =</span> T)</code></pre></div>
<pre><code>## [1] 2 3 4</code></pre>
<p>Of course we could also consider a much bigger number of trials, <span class="math inline">\(10000\)</span> say. Doing so, it would be pointless to simply print the results to the console: by default R displays up to <span class="math inline">\(1000\)</span> entries of large vectors and omitts the remainder (give it a go). Eyeballing the numbers does not reveal too much. Instead let us calculate the sample average of the outcomes using <code>mean()</code> and see if the result comes close to the expected value <span class="math inline">\(E(D)=3.5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># compute the sample mean of 10000 die rolls</span>
<span class="kw">mean</span>(
    <span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, 
           <span class="dv">10000</span>, 
           <span class="dt">replace =</span> T
           )
    )</code></pre></div>
<pre><code>## [1] 3.5039</code></pre>
<p>We find the sample mean to be fairly close to the expected value. (ref to WLLN)</p>
<p>Other frequently encountered measures are the variance and the standard deviation. Both are measures of the <em>dispersion</em> of a random variable.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.2
</h3>
<h3 class="left">
Variance and Standard Deviation
</h3>
<p>
The Variance of the discrete <em>random variable</em> <span class="math inline">\(Y\)</span>, denoted <span class="math inline">\(\sigma^2_Y\)</span>, is <span class="math display">\[ \sigma^2_Y = \text{Var}(Y) = E\left[(Y-\mu_y)^2\right] = \sum_{i=1}^k (y_i - \mu_y)^2 p_i \]</span> The standard deviation of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\sigma_Y\)</span>, the square root of the variance. The units of the standard deviation are the same as the units of <span class="math inline">\(Y\)</span>.
</p>
</div>
<p>The variance as defined in Key Concept 2.2 <em>is not</em> implemented as a function in R. Instead we have the function <code>var()</code> which computes the <em>sample variance</em></p>
<p><span class="math display">\[ s^2_Y = \frac{1}{n-1} \sum_{i=1}^n (y_i - \overline{y})^2. \]</span></p>
<p>Remember that <span class="math inline">\(s^2_Y\)</span> is different from the so called <em>population variance</em> of <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[ \text{Var}(Y) = \frac{1}{N} \sum_{i=1}^N (y_i - \mu_Y)^2, \]</span></p>
<p>since it measures how the data is dispersed around the sample average <span class="math inline">\(\overline{y}\)</span> instead of the population mean <span class="math inline">\(\mu_Y\)</span>. This becomes clear when we look at our dice rolling example. For <span class="math inline">\(D\)</span> we have</p>
<p><span class="math display">\[ \text{Var}(D) = 1/6 \sum_{i=1}^6 (d_i - 3.5)^2 = 2.92  \]</span> which is obviously different from the result of <span class="math inline">\(s^2\)</span> as computed by <code>var()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>)</code></pre></div>
<pre><code>## [1] 3.5</code></pre>
</div>
</div>
<div id="probability-distributions-of-continuous-random-variables" class="section level2 unnumbered">
<h2>Probability Distributions of Continuous Random Variables</h2>
<p>Since a continuous random variable takes on a continuum of possible values, we cannot use the concept of a probability distribution as used for discrete random variables. Instead, the probability distribution of a continuous random variable is summarized by its <em>probability density function</em> (PDF).</p>
<p>The cumulative probability distribution function (CDF) for a continuous random variable is defined just as in the discrete case. Hence, the cumulative probability distribution of a continuous random variables states the probability that the random variable is less than or equal to a particular value.</p>
<p>For completeness, we present revisions of Key Concepts 2.1 and 2.2 for the continuous case.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.3
</h3>
<h3 class="left">
Probabilities, Expected Value and Variance of a Continuous Random Variable
</h3>
<p>
<p>Let <span class="math inline">\(f_Y(y)\)</span> denote the probability density function of <span class="math inline">\(Y\)</span>. Because probabilities cannot be negative, we have <span class="math inline">\(f_Y\geq 0\)</span> for all <span class="math inline">\(y\)</span>. The Probability that <span class="math inline">\(Y\)</span> falls between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, <span class="math inline">\(a &lt; b\)</span> is <span class="math display">\[ P(a \leq Y \leq b) = \int_a^b f_Y(y) \mathrm{d}y. \]</span> We further have that <span class="math inline">\(P(-\infty \leq Y \leq \infty) = 1\)</span> and therefore <span class="math inline">\(\int_{-\infty}^{\infty} f_Y(y) \mathrm{d}y = 1\)</span>.</p>
<p>As for the discrete case, the expected value of <span class="math inline">\(Y\)</span> is the probability weighted average of its values. Due to continuity, we use intergrals instead of sums.</p>
<p>The expected value of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[ E(Y) =  \mu_Y = \int y f_Y(y) \mathrm{d}y. \]</span></p>
<p>The variance is the expected value of <span class="math inline">\((Y - \mu_Y)^2\)</span>. We thus have</p>
<p><span class="math display">\[ \text{Var}(Y) =  \sigma_Y^2 = \int (y - \mu_Y)^2 f_Y(y) \mathrm{d}y. \]</span></p>
</p>
</div>
<p>Let us discuss an example:</p>
<p>Consider the continuous random variable <span class="math inline">\(X\)</span> with propability density function</p>
<p><span class="math display">\[ f_X(x) = \frac{3}{x^4}, x&gt;1. \]</span></p>
<ul>
<li>We can show analytically that the integral of <span class="math inline">\(f_X(x)\)</span> over the real line equals <span class="math inline">\(1\)</span>.</li>
</ul>
<span class="math display">\[\begin{align}
 \int f_X(x) \mathrm{d}x =&amp;  \int_{1}^{\infty} \frac{3}{x^4} \mathrm{d}x \\
  =&amp; \lim_{t \rightarrow \infty} \int_{1}^{t} \frac{3}{x^4} \mathrm{d}x \\
  =&amp; \lim_{t \rightarrow \infty}  -x^{-3} \rvert_{x=1}^t \\
  =&amp; -\left(\lim_{t \rightarrow \infty}\frac{1}{t^3} - 1\right) \\
  =&amp; 1
\end{align}\]</span>
<ul>
<li>The expectation of <span class="math inline">\(X\)</span> can be computed as follows:</li>
</ul>
<span class="math display">\[\begin{align}
 E(X) = \int x \cdot f_X(x) \mathrm{d}x =&amp;  \int_{1}^{\infty} x \cdot \frac{3}{x^4} \mathrm{d}x \\
  =&amp; - \frac{3}{2} x^{-2} \rvert_{x=1}^{\infty} \\
  =&amp; -\frac{3}{2} \left( \lim_{t \rightarrow \infty} \frac{1}{t^2} - 1 \right) \\
  =&amp; \frac{3}{2}
\end{align}\]</span>
<ul>
<li>Note that the variance of <span class="math inline">\(X\)</span> can be expressed as <span class="math inline">\(\text{Var}(X) = E(X^2) - E(X)^2\)</span>. Since <span class="math inline">\(E(X)\)</span> has been computed in the previous step, we seek <span class="math inline">\(E(X^2)\)</span>:</li>
</ul>
<span class="math display">\[\begin{align}
 E(X^2)= \int x^2 \cdot f_X(x) \mathrm{d}x =&amp;  \int_{1}^{\infty} x^2 \cdot \frac{3}{x^4} \mathrm{d}x \\
  =&amp; -3 x^{-1} \rvert_{x=1}^{\infty} \\
  =&amp; -3 \left( \lim_{t \rightarrow \infty} \frac{1}{t} - 1 \right) \\
  =&amp; 3
\end{align}\]</span>
<p>So we have shown that the area under the curve equals one, that the expectation is <span class="math inline">\(E(X)=\frac{3}{2} \ \)</span> and we found the variance to be <span class="math inline">\(\text{Var}(X) = \frac{3}{4}\)</span>. However, this was quite tedious and, as we shall see soon, an analytic approach is not applicable for some probability density functions e.g. if integrals have no closed form solutions.</p>
<p>Luckily, R enables us to find the results derived above in an instant. The tool we use for this is the function <code>integrate()</code>. First, we have to define the functions we want to calculate integrals for as R functions, i.e. the PDF <span class="math inline">\(f_X(x)\)</span> as well as the expressions <span class="math inline">\(x\cdot f_X(x)\)</span> and <span class="math inline">\(x^2\cdot f_X(x)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define functions</span>
f &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="dv">3</span><span class="op">/</span>x<span class="op">^</span><span class="dv">4</span>
g &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">*</span><span class="kw">f</span>(x)
h &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span><span class="op">*</span><span class="kw">f</span>(x)</code></pre></div>
<p>Next, we use <code>integrate()</code> and set lower and upper limits of integration to <span class="math inline">\(1\)</span> and <span class="math inline">\(\infty\)</span> using arguments <code>lower</code> and <code>upper</code>. By default, <code>integrate()</code> prints the result along with an estimate of the calculation error to the console. However, the outcome is not a numeric value one can do further calculation with readily. In order to get only a numeric value of the integral, we need to use the <code>$</code> operator in conjunction with <code>value</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate area under curve</span>
AUC &lt;-<span class="st"> </span><span class="kw">integrate</span>(f, 
                 <span class="dt">lower =</span> <span class="dv">1</span>, 
                 <span class="dt">upper =</span> <span class="ot">Inf</span>
                 )
AUC </code></pre></div>
<pre><code>## 1 with absolute error &lt; 1.1e-14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate E(X)</span>
EX &lt;-<span class="st"> </span><span class="kw">integrate</span>(g,
                <span class="dt">lower =</span> <span class="dv">1</span>,
                <span class="dt">upper =</span> <span class="ot">Inf</span>)
EX</code></pre></div>
<pre><code>## 1.5 with absolute error &lt; 1.7e-14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate Var(X)</span>
VarX &lt;-<span class="st"> </span><span class="kw">integrate</span>(h,
                  <span class="dt">lower =</span> <span class="dv">1</span>,
                  <span class="dt">upper =</span> <span class="ot">Inf</span>
                  )<span class="op">$</span>value <span class="op">-</span><span class="st"> </span>EX<span class="op">$</span>value<span class="op">^</span><span class="dv">2</span> 
VarX</code></pre></div>
<pre><code>## [1] 0.75</code></pre>
<p>Although there is a wide variety of distributions, the ones most often encountered in econometrics are the normal, chi-squared, Student <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> distributions. Therefore we will discuss some core R functions that allow to do calculations involving densities, probabilities and quantiles of these distributions.</p>
<p>Every probability distribution that R handles has four basic functions whose names consist of a prefix followed by a root name. As an example, take the normal distribution. The root name of all four functions associated with the normal distribution is <tt>norm</tt>. The four prefixes are</p>
<ul>
<li><tt>d</tt> for “density” - probability function / probability density function</li>
<li><tt>p</tt> for “probability” - cumulative distribution function</li>
<li><tt>q</tt> for “quantile” - quantile function (inverse cumulative distribution function)</li>
<li><tt>r</tt> for “random” - random number generator</li>
</ul>
<p>Thus, for the normal distribution we have the R functions <code>dnorm()</code>, <code>pnorm()</code>, <code>qnorm()</code> and <code>rnorm()</code>.</p>
<div id="the-normal-distribution" class="section level3 unnumbered">
<h3>The Normal Distribution</h3>
<p>The probably most important probability distribution considered here is the normal distribution. This is not least due to the special role of the standard normal distribution and the Central Limit Theorem which is treated shortly during the course of this section. Distributions of the normal family have a familiar symmetric, bell-shaped probability density. A normal distribution is characterized by its mean <span class="math inline">\(\mu\)</span> and its standard deviation <span class="math inline">\(\sigma\)</span> what is concisely expressed by <span class="math inline">\(N(\mu,\sigma^2)\)</span>. The normal distribution has the PDF</p>
<p><span class="math display">\[ f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x - μ)^2/(2 σ^2)}. \]</span></p>
<p>For the standard normal distribution we have <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>. Standard normal variates are often denoted <span class="math inline">\(Z\)</span>. Usually, the standard normal PDF is denoted by <span class="math inline">\(\phi\)</span> and the standard normal CDF is denoted by <span class="math inline">\(\Phi\)</span>. Hence,</p>
<p><span class="math display">\[ \phi(c) = \Phi&#39;(c) \ \ , \ \ \Phi(c) = P(Z \leq c) \ \ , \ \ Z \sim N(0,1).
\]</span> In R, we can conveniently obtain density values of normal distributions using the function <code>dnorm()</code>. Let us draw a plot of the standard normal density function using <code>curve()</code> and <code>dnorm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw a plot of the N(0,1) pdf</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x),
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>),
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, 
      <span class="dt">main =</span> <span class="st">&quot;Standard Normal Density Function&quot;</span>
      ) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can obtain the density at different positions by passing a vector of quantiles to <code>dnorm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute denstiy at x=-1.96, x=0 and x=1.96</span>
<span class="kw">dnorm</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">1.96</span>, <span class="dv">0</span>, <span class="fl">1.96</span>))</code></pre></div>
<pre><code>## [1] 0.05844094 0.39894228 0.05844094</code></pre>
<p>Similary as for the PDF, we can plot the standard normal CDF using <code>curve()</code> and <code>pnorm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the standard normal CDF</span>
<span class="kw">curve</span>(<span class="kw">pnorm</span>(x), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), 
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, 
      <span class="dt">main =</span> <span class="st">&quot;Standard Normal Cumulative Distribution Function&quot;</span>
      )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can also use R to calculate the probability of events associated with a standard normal random variate.</p>
<p>Let us say we are interested in <span class="math inline">\(P(Z \leq 1.337)\)</span>. For some general continuous random variable <span class="math inline">\(Z\)</span> on <span class="math inline">\([-\infty,\infty]\)</span> with density function <span class="math inline">\(g(x)\)</span> we would have to determine <span class="math inline">\(G(x)\)</span>, the antiderivative of <span class="math inline">\(g(x)\)</span> since</p>
<p><span class="math display">\[ P(Z \leq 1,337 ) = G(1,337) = \int_{-\infty}^{1,337} g(x) \mathrm{d}x.  \]</span></p>
<p>However, if <span class="math inline">\(Z \sim N(0,1)\)</span>, we have <span class="math inline">\(g(x)=\phi(x)\)</span> so there is no analytic solution to the integral above and it is cumbersome to come up with an approximation. However, we may circumvent this using R in different ways. <br> The first approach makes use of the function <code>integrate()</code> which allows to solve one-dimensional integration problems using a numerical method. For this, we first define the function we want to compute the integral of as a R function <code>f</code>. In our example, <code>f</code> needs to be the standard normal density function and hence takes a single argument <code>x</code>. Following the definition of <span class="math inline">\(\phi(x)\)</span> we define <code>f</code> as</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the standard normal PDF as a R function</span>
f &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="dv">1</span><span class="op">/</span>(<span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi)) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>)
}</code></pre></div>
<p>Let us check if this function enables us to compute standard normal density values by passing it a vector of quantiles.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define vector of quantiles</span>
quants &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.96</span>,<span class="dv">0</span>,<span class="fl">1.96</span>)

<span class="co"># compute density values</span>
<span class="kw">f</span>(quants)</code></pre></div>
<pre><code>## [1] 0.05844094 0.39894228 0.05844094</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compare to results produced by dnorm()</span>
<span class="kw">f</span>(quants) <span class="op">==</span><span class="st"> </span><span class="kw">dnorm</span>(quants)</code></pre></div>
<pre><code>## [1] TRUE TRUE TRUE</code></pre>
<p>Notice that the results produces by <code>f()</code> are indeed equivalent to those given by <code>dnorm()</code>.</p>
<p>Next, we call <code>integrate()</code> on <code>f()</code> and further specify the arguments <code>lower</code> and <code>upper</code>, the lower and upper limits of integration.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># integrate f()</span>
<span class="kw">integrate</span>(f, 
          <span class="dt">lower =</span> <span class="op">-</span><span class="ot">Inf</span>, 
          <span class="dt">upper =</span> <span class="fl">1.337</span>
          )</code></pre></div>
<pre><code>## 0.9093887 with absolute error &lt; 1.7e-07</code></pre>
<p>We find that the probability of observing <span class="math inline">\(Z \leq 1,337\)</span> is about <span class="math inline">\(0.9094\%\)</span>.</p>
<p>A second and much more convenient way is to use the function <code>pnorm()</code> which also allows calculus involving the standard normal cumulative distribution function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute a probability using pnorm()</span>
<span class="kw">pnorm</span>(<span class="fl">1.337</span>)</code></pre></div>
<pre><code>## [1] 0.9093887</code></pre>
<p>The result matches the outcome of the approach using <code>ìntegrate()</code>.</p>
<p>Let us discuss some further examples:</p>
<p>A commonly known result is that <span class="math inline">\(95\%\)</span> probability mass of a standard normal lies in the intervall <span class="math inline">\([-1.96, 1.96]\)</span>, that is in a distance of about <span class="math inline">\(2\)</span> standard deviations to the mean. We can easily confirm this by calculating</p>
<p><span class="math display">\[ P(-1.96 \leq Z \leq 1.96) = 1-2\times P(Z \leq -1.96) \]</span> due to symmetry of the standard normal PDF. Thanks to R, we can abondon the table of the standard normal CDF again and instead solve this by means of the function <code>pnorm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the probability</span>
<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">pnorm</span>(<span class="op">-</span><span class="fl">1.96</span>)) </code></pre></div>
<pre><code>## [1] 0.9500042</code></pre>
<p>Now consider a random variable <span class="math inline">\(Y\)</span> with <span class="math inline">\(Y \sim N(5,25)\)</span>. As You should already know from Your statistics courses it is not possible to make any statement of probability without prior standardizing as shown in Key Concept 2.4.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.4
</h3>
<h3 class="left">
Computing Probabilities Involving Normal Random Variables
</h3>
<p>
<p>Suppose <span class="math inline">\(Y\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[Y
\sim N(\mu, \sigma^2)\]</span> Then <span class="math inline">\(Y\)</span> is standardized by substracting its mean and dividing by its standard deviation: <span class="math display">\[ Z = \frac{Y -\mu}{\sigma} \]</span> Let <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> denote two numbers whereby <span class="math inline">\(c_1 &lt; c_2\)</span> and further <span class="math inline">\(d_1 = (c_1 - \mu) / \sigma\)</span> and <span class="math inline">\(d_2 = (c_2 - \mu)/\sigma\)</span>. Then</p>
<span class="math display">\[\begin{align} 
P(Y \leq c_2) =&amp; \, P(Z \leq d_2) = \Phi(d_2) \\ 
P(Y \geq c_1) =&amp; \, P(Z \geq d_1) = 1 - \Phi(d_1) \\ 
P(c_1 \leq Y \leq c_2) =&amp; \, P(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1) 
\end{align}\]</span>
</p>
</div>
<p>R functions that handle the normal distribution can perform this standardization. If we are interested in <span class="math inline">\(P(3 \leq Y \leq 4)\)</span> we can use <code>pnorm()</code> and adjust for a mean and/or a standard deviation that deviate from <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma = 1\)</span> by specifying the arguments <code>mean</code> and <code>sd</code> accordingly. <strong>Attention</strong>: <code>pnorm()</code> requires the argument <code>sd</code> which is the standard deviation, not the variance!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="dv">4</span>, <span class="dt">mean =</span> <span class="dv">5</span>, <span class="dt">sd =</span> <span class="dv">5</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">3</span>, <span class="dt">mean =</span> <span class="dv">5</span>, <span class="dt">sd =</span> <span class="dv">5</span>) </code></pre></div>
<pre><code>## [1] 0.07616203</code></pre>
<p>An extension of the normal distribution in a univariate setting is the multivariate normal distribution. The PDF of two random normal variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by</p>
<span class="math display" id="eq:bivnorm">\[\begin{align}
g_{X,Y}(x,y) =&amp; \, \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho_{XY}^2}} \\ 
\cdot &amp; \, \exp \left\{ \frac{1}{-2(1-\rho_{XY}^2)} \left[ \left( \frac{x-\mu_x}{\sigma_x} \right)^2 - 2\rho_{XY}\left( \frac{x-\mu_X}{\sigma_X} \right)\left( \frac{y-\mu_Y}{\sigma_Y} \right) + \left( \frac{y-\mu_Y}{\sigma_Y} \right)^2 \right]  \right\}. \tag{2.1}
\end{align}\]</span>
<p>Equation <a href="probability-theory.html#eq:bivnorm">(2.1)</a> contains the bivariate normal PDF. Admittedly, it is hard to gain insights from this complicated expression. Instead, let us consider the special case where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated standard normal random variables with density functions <span class="math inline">\(f_X(x)\)</span> and <span class="math inline">\(f_Y(y)\)</span> and we assume that they have a joint normal distribution. We then have the parameters <span class="math inline">\(\sigma_X = \sigma_Y = 1\)</span>, <span class="math inline">\(\mu_X=\mu_Y=0\)</span> (due to marginal standard normality) and <span class="math inline">\(\rho_{XY}=0\)</span> (due to uncorrelatedness). The joint probability density function of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> then becomes</p>
<p><span class="math display">\[ g_{X,Y}(x,y) = f_X(x) f_Y(y) = \frac{1}{2\pi} \cdot \exp \left\{ -\frac{1}{2} \left[x^2 + y^2 \right]  \right\}, \tag{2.2}  \]</span></p>
<p>the PDF of the bivariate standard normal distribution. The next plot provides an interactive three dimensional plot of (<a href="#mjx-eqn-2.2">2.2</a>). By moving the mouse curser over the plot You can see that the density is rotationally invariant.</p>
<center>
<iframe scrolling="no" seamless="seamless" style="border:none" src="https://plot.ly/~mca_unidue/22.embed?width=550&amp;height=550?showlink=false/800/1200" width="600" height="400">
</iframe>
<center>
<p><a name="chisquare"></a></p>
</div>
<div id="the-chi-squared-distribution" class="section level3 unnumbered">
<h3>The Chi-Squared Distribution</h3>
<p>Another distribution relevant in econometric day-to-day work is the chi-squared distribution. It is often needed when testing special types of hypotheses frequently ecountered when dealing with regression models.</p>
<p>The sum of <span class="math inline">\(M\)</span> squared independent standard normal distributed random variables follows a chi-squared distribution with <span class="math inline">\(M\)</span> degrees of freedom.</p>
<p><span class="math display">\[ Z_1^2 + \dots + Z_M^2 = \sum_{m=1}^M Z_m^2 \sim \chi^2_M \ \ \text{with} \ \ Z_m \overset{i.i.d.}{\sim} N(0,1) \label{eq:chisq}\]</span></p>
<p>A <span class="math inline">\(\chi^2\)</span> distributed random variable with <span class="math inline">\(M\)</span> degrees of freedom has expectation <span class="math inline">\(M\)</span>, mode at <span class="math inline">\(M-2\)</span> for <span class="math inline">\(n \geq 2\)</span> and variance <span class="math inline">\(2 \cdot M\)</span>.</p>
<p>For example, if we have</p>
<p><span class="math display">\[ Z_1,Z_2,Z_3 \overset{i.i.d.}{\sim} N(0,1) \]</span></p>
<p>it holds that</p>
<p><span class="math display">\[ Z_1^2+Z_2^2+Z_3^3 \sim \chi^2_3. \tag{2.3} \]</span> By means of the code below, we can display the PDF and the CDF of a <span class="math inline">\(\chi^2_3\)</span> random variable in a single plot. This is achieved by setting the argument <code>add = TRUE</code> in the second call of <code>curve()</code>. Further we adjust limits of both axes using <code>xlim</code> and <code>ylim</code> and choose different colors to make both functions better distinguishable. The plot is completed by adding a legend with help of the function <code>legend()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the PDF</span>
<span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df=</span><span class="dv">3</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>), 
      <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), 
      <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, 
      <span class="dt">main=</span><span class="st">&quot;p.d.f. and c.d.f of Chi-Squared Distribution, m = 3&quot;</span>
      )

<span class="co"># add the CDF to the plot</span>
<span class="kw">curve</span>(<span class="kw">pchisq</span>(x, <span class="dt">df=</span><span class="dv">3</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>), 
      <span class="dt">add =</span> <span class="ot">TRUE</span>, 
      <span class="dt">col=</span><span class="st">&quot;red&quot;</span>
      )

<span class="co"># add a legend to the plot</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, 
       <span class="kw">c</span>(<span class="st">&quot;PDF&quot;</span>,<span class="st">&quot;CDF&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), 
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-27-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice that, since the outcomes of a <span class="math inline">\(\chi^2_M\)</span> distributed random variable are always positive, the domain of the related PDF and CDF is <span class="math inline">\(\mathbb{R}_{\geq0}\)</span>.</p>
<p>As expectation and variance depend (solely) on the degrees of freedom, the distribution’s shape changes drastically if we vary the number of squared standard normals that are summed up. This relation is often depicted by overlaying densities for different <span class="math inline">\(M\)</span>, see e.g. the <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">Wikipedia Article</a>.</p>
<p>Of course, one can easily reproduce such a plot using R. Again we start by plotting the density of the <span class="math inline">\(\chi_1^2\)</span> distribution on the intervall <span class="math inline">\([0,15]\)</span> with <code>curve()</code>. In the next step, we loop over degrees of freedom <span class="math inline">\(m=2,...,7\)</span> and add a density curve for each <span class="math inline">\(m\)</span> to the plot. We also adjust the line color for each iteration of the loop by setting <code>col = m</code>. At last, we add a legend that displays degrees of freedom and the associated colors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the density for m=1</span>
<span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df=</span><span class="dv">1</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>), 
      <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, 
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, 
      <span class="dt">main=</span><span class="st">&quot;Chi-Square Distributed Random Variables&quot;</span>
      )

<span class="co"># add densities for m=2,...,7 to the plot using a for loop </span>
<span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">7</span>) {
  <span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df =</span> m),
        <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>), 
        <span class="dt">add =</span> T, 
        <span class="dt">col =</span> m
        )
}

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="kw">as.character</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>), 
       <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span> , 
       <span class="dt">lty =</span> <span class="dv">1</span>, 
       <span class="dt">title =</span> <span class="st">&quot;D.f.&quot;</span>
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-28-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It is evident that increasing the degrees of freedom shifts the distribution to the right (the modus becomes larger) and increases its dispersion (the distribution’s variance grows).</p>
</div>
<div id="thetdist" class="section level3 unnumbered">
<h3>The Student <span class="math inline">\(t\)</span> Distribution</h3>
<p><a name="tdist"></a></p>
<p>Let <span class="math inline">\(Z\)</span> be a standard normal variate, <span class="math inline">\(W\)</span> a random variable that follows a <span class="math inline">\(\chi^2_M\)</span> distribution with <span class="math inline">\(M\)</span> degrees of freedom and further assume that <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> are independently distributed. Then it holds that</p>
<p><span class="math display">\[ \frac{Z}{\sqrt{W/M}} =:X \sim t_M \]</span> and we say that <span class="math inline">\(X\)</span> follows a student <span class="math inline">\(t\)</span> distribution (or simple <span class="math inline">\(t\)</span> distribution) with <span class="math inline">\(M\)</span> degrees of freedom.</p>
<p>As for the <span class="math inline">\(\chi^2_M\)</span> distribution, a <span class="math inline">\(t\)</span> distribution depends on the degrees of freedom <span class="math inline">\(M\)</span>. <span class="math inline">\(t\)</span> distributions are symmetric, bell-shaped and look very similar to a normal distribution, especially when <span class="math inline">\(M\)</span> is large. This is not a coincidence: for a sufficient large <span class="math inline">\(M\)</span>, a <span class="math inline">\(t_M\)</span> distribution can be approximated by the standard normal distribution. This approximation works reasonably well for <span class="math inline">\(M\geq 30\)</span>. As we will show later by means of a small simulation study, the <span class="math inline">\(t_{\infty}\)</span> distribution <em>is</em> the standard normal distribution.</p>
<p>A <span class="math inline">\(t_M\)</span> distributed random variable has an expectation value if <span class="math inline">\(M&gt;1\)</span> and a variance if <span class="math inline">\(n&gt;2\)</span>.</p>
<span class="math display">\[\begin{align}
  E(X) =&amp; 0 \ , \ M&gt;1 \\
  \text{Var}(X) =&amp; \frac{M}{M-2} \ , \ M&gt;2
\end{align}\]</span>
<p>Let us graph some <span class="math inline">\(t\)</span> distributions with different <span class="math inline">\(M\)</span> and compare them with the standard normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the standard normal density</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), 
      <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, 
      <span class="dt">lty=</span><span class="dv">2</span>, 
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, 
      <span class="dt">main=</span><span class="st">&quot;Theoretical Densities of t-Distributions&quot;</span>
      )

<span class="co"># plot the t density for m=2</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df=</span><span class="dv">2</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), 
      <span class="dt">col=</span><span class="dv">2</span>, 
      <span class="dt">add =</span> T
      )

<span class="co"># plot the t density for m=4</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df=</span><span class="dv">4</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), 
      <span class="dt">col=</span><span class="dv">3</span>, 
      <span class="dt">add=</span>T
      )

<span class="co"># plot the t density for m=25</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df=</span><span class="dv">25</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), 
      <span class="dt">col=</span><span class="dv">4</span>, 
      <span class="dt">add=</span>T
      )

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="kw">c</span>(<span class="st">&quot;N(0,1)&quot;</span>,<span class="st">&quot;M=2&quot;</span>,<span class="st">&quot;M=4&quot;</span>,<span class="st">&quot;M=25&quot;</span>), 
       <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, 
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot indicates what has been claimed in the previous paragraph: as the degrees of freedom increase, the shape of the <span class="math inline">\(t\)</span> distribution comes closer to that of a standard normal bell. Already for <span class="math inline">\(M=25\)</span> we find little difference to the dashed line which belongs to the standard normal density curve. If <span class="math inline">\(M\)</span> is small, we find the distribution to have slightly havier tails than a standard normal, i.e. it has a “fatter” bell shape.</p>
</div>
<div id="the-f-distribution" class="section level3 unnumbered">
<h3>The <span class="math inline">\(F\)</span> Distribution</h3>
<p>Another ratio of random variables important to econometricians is the ratio of two indpendently <span class="math inline">\(\chi^2\)</span> distributed random variables that are divided by their degrees of freedom. Such a quantity follows a <span class="math inline">\(F\)</span> distribution with numerator degrees of freedom <span class="math inline">\(M\)</span> and denominator degrees of freedom <span class="math inline">\(n\)</span>, denoted <span class="math inline">\(F_{M,n}\)</span>. The distribution was first derived by George Snedecor but was named in honor of <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Sir Ronald Fisher</a>.</p>
<p><span class="math display">\[ \frac{W/M}{V/n} \sim F_{M,n} \ \ \text{with} \ \ W \sim \chi^2_M \ \ , \ \ V \sim \chi^2_n \]</span></p>
<p>By definition, the domain of both PDF and CDF of a <span class="math inline">\(F_{M,n}\)</span> distributed random variable is <span class="math inline">\(\mathbb{R}_{\geq0}\)</span>.</p>
<p>Say we have a <span class="math inline">\(F\)</span> distributed random variable <span class="math inline">\(Y\)</span> with numerator degrees of freedom <span class="math inline">\(3\)</span> and denominator degrees of freedom <span class="math inline">\(14\)</span> and are interested in <span class="math inline">\(P(Y \geq 2)\)</span>. This can be computed with help of the function <code>pf()</code>. By setting the argument <code>lower.tail</code> to <code>TRUE</code> we ensure that R computes <span class="math inline">\(1- P(Y \leq 2)\)</span>, i.e. the probability mass in the tail right of <span class="math inline">\(2\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pf</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">13</span>, <span class="dt">lower.tail =</span> F)</code></pre></div>
<pre><code>## [1] 0.1638271</code></pre>
<p>We can visualize this probability by drawing a line plot of the related density function and adding a color shading with <code>polygon()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define coordinate vectors for vertices of the polygon</span>
x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="fl">0.01</span>), <span class="dv">10</span>)
y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">df</span>(<span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="fl">0.01</span>), <span class="dv">3</span>, <span class="dv">14</span>), <span class="dv">0</span>)

<span class="co"># draw density of F_{3, 14}</span>
<span class="kw">curve</span>(<span class="kw">df</span>(x ,<span class="dv">3</span> ,<span class="dv">14</span>), 
      <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.8</span>), 
      <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>), 
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>,
      <span class="dt">main =</span> <span class="st">&quot;Density Function&quot;</span>
      )

<span class="co"># draw the polygon</span>
<span class="kw">polygon</span>(x, y, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The <span class="math inline">\(F\)</span> distribution is related to many other distributions. An important special case encountered in econometrics arises if the denominator degrees of freedom are large such that the <span class="math inline">\(F_{M,n}\)</span> distribution can be approximated by the <span class="math inline">\(F_{M,\infty}\)</span> distribution which turns out to be simply the distribution of a <span class="math inline">\(\chi^2_M\)</span> random variable divided by its degrees of freedom <span class="math inline">\(M\)</span>.</p>
<p><span class="math display">\[ W/M \sim F_{M,\infty} \ \ , \ \ W \sim \chi^2_M \]</span></p>
</div>
</div>
<div id="random-sampling-and-the-distribution-of-sample-averages" class="section level2 unnumbered">
<h2>Random Sampling and the Distribution of Sample Averages</h2>
<p>To clarify the basic idea of random sampling, let us jump back to the die rolling example:</p>
<p>Suppose we are rolling the dice <span class="math inline">\(n\)</span> times. This means we are interested in the outcomes of <span class="math inline">\(n\)</span> random processes <span class="math inline">\(Y_i, \ i=1,...,n\)</span> which are characterized by the same distribution. Since these outcomes are selected randomly, they are <em>random variables</em> themselves and their realisations will differ each time we draw a sample, i.e. each time we roll the dice <span class="math inline">\(n\)</span> times. Furthermore, each observation is randomly drawn from the same population, that is the numbers from <span class="math inline">\(1\)</span> to <span class="math inline">\(6\)</span>, and their individual distribution is the same. Hence we say that <span class="math inline">\(Y_1,\dots,Y_n\)</span> are identically distributed.<br> Moreover, we know that the value of any of the <span class="math inline">\(Y_i\)</span> does not provide any information on the remainder of the sample. In our example, rolling a six as the first observation in our sample does not alter the distributions of <span class="math inline">\(Y_2,\dots,Y_n\)</span>: all numbers are equally likely to occur. This means that all <span class="math inline">\(Y_i\)</span> are also independently distributed. Thus, we say that <span class="math inline">\(Y_1,\dots,Y_n\)</span> are independently and identically distributed (<em>i.i.d</em>). The dice example is the simplest sampling scheme used in statistics. That is why it is called <em>simple random sampling</em>. This concept is condensed in Key Concept 2.5.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.5
</h3>
<h3 class="left">
Simple Random Sampling and i.i.d. Random Variables
</h3>
<p>
<p>In simple random sampling, <span class="math inline">\(n\)</span> objects are drawn at random from a population. Each object is equally likely to end up in the sample. We denote the value of the random variable <span class="math inline">\(Y\)</span> for the <span class="math inline">\(i^{th}\)</span> randomly drawn object as <span class="math inline">\(Y_i\)</span>. Since all objects are equally likely to be drawn and the distribution of <span class="math inline">\(Y_i\)</span> is the same for all <span class="math inline">\(i\)</span>, the <span class="math inline">\(Y_i, \dots, Y_n\)</span> are independently and identically distributed (i.i.d.). This means the distribution of <span class="math inline">\(Y_i\)</span> is the same for all <span class="math inline">\(i=1,\dots,n\)</span> and <span class="math inline">\(Y_1\)</span> is distributed independently of <span class="math inline">\(Y_2, \dots, Y_n\)</span> <span class="math inline">\(Y_2\)</span> is distributed independently of <span class="math inline">\(Y_1, Y_3, \dots, Y_n\)</span> and so forth.</p>
</p>
</div>
<p>What happens if we consider functions of the sample data? Consider the example of rolling a dice two times in a row once again. A sample now consists of two independent random draws from the set <span class="math inline">\(\{1,2,3,4,5,6\}\)</span>. In view of the aforementioned, it is apparent that any function of these two random variables is also random, e.g. their sum. Convince Yourself by executing the code below several times.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">2</span>, <span class="dt">replace =</span> T))</code></pre></div>
<pre><code>## [1] 6</code></pre>
<p>Clearly this sum, let us call it <span class="math inline">\(S\)</span>, is a random variable as it depends on randomly drawn summands. For this example, we can completely enumerate all outcomes and hence write down the theoretical probability distribution of our function of the sample data, <span class="math inline">\(S\)</span>:</p>
<p>We face <span class="math inline">\(6^2=36\)</span> possible pairs. Those pairs are</p>
<span class="math display">\[\begin{align}
  &amp;(1,1)    (1,2)   (1,3)   (1,4)   (1,5)   (1,6) \\ 
  &amp;(2,1)    (2,2)   (2,3)   (2,4)   (2,5)   (2,6) \\ 
  &amp;(3,1)    (3,2)   (3,3)   (3,4)   (3,5)   (3,6) \\ 
  &amp;(4,1)    (4,2)   (4,3)   (4,4)   (4,5)   (4,6) \\ 
  &amp;(5,1)    (5,2)   (5,3)   (5,4)   (5,5)   (5,6) \\ 
  &amp;(6,1)    (6,2)   (6,3)   (6,4)   (6,5)   (6,6)
\end{align}\]</span>
<p>Thus, possible outcomes for <span class="math inline">\(S\)</span> are</p>
<p><span class="math display">\[ \left\{ 2,3,4,5,6,7,8,9,10,11,12 \right\} . \]</span> Enumeration of outcomes yields</p>
<span class="math display">\[\begin{align}
  P(S) = 
  \begin{cases} 
    1/36, \ &amp; S = 2 \\ 
    2/36, \ &amp; S = 3 \\
    3/36, \ &amp; S = 4 \\
    4/36, \ &amp; S = 5 \\
    5/36, \ &amp; S = 6 \\
    6/36, \ &amp; S = 7 \\
    5/36, \ &amp; S = 8 \\
    4/36, \ &amp; S = 9 \\
    3/36, \ &amp; S = 10 \\
    2/36, \ &amp; S = 11 \\
    1/36, \ &amp; S = 12
  \end{cases}
\end{align}\]</span>
<p>We can also compute <span class="math inline">\(E(S)\)</span> and <span class="math inline">\(\text{Var}(S)\)</span> as stated in Key Concept 2.1 and Key Concept 2.2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Vector of outcomes</span>
S &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">:</span><span class="dv">12</span>

<span class="co"># Vector of probabilities</span>
PS &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,<span class="dv">5</span><span class="op">:</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">36</span>

<span class="co"># Expectation of S</span>
ES &lt;-<span class="st"> </span>S <span class="op">%*%</span><span class="st"> </span>PS; ES</code></pre></div>
<pre><code>##      [,1]
## [1,]    7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Variance of S</span>
VarS &lt;-<span class="st"> </span>(S <span class="op">-</span><span class="st"> </span><span class="kw">c</span>(ES))<span class="op">^</span><span class="dv">2</span> <span class="op">%*%</span><span class="st"> </span>PS; VarS</code></pre></div>
<pre><code>##          [,1]
## [1,] 5.833333</code></pre>
<p>(The <code>%*%</code> operator is used to compute the scalar product of two vectors.)</p>
<p>So the distribution of <span class="math inline">\(S\)</span> is known. It is also evident that its distribution differs considerably from the marginal distribution, i.e. the distribution of a single die roll’s outcome, <span class="math inline">\(D\)</span> . Let us visualize this using barplots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># divide the plotting area in one row with two columns</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))

<span class="co"># plot the distribution of S</span>
<span class="kw">names</span>(PS) &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">:</span><span class="dv">12</span>

<span class="kw">barplot</span>(PS, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.2</span>), 
        <span class="dt">xlab =</span> <span class="st">&quot;S&quot;</span>, 
        <span class="dt">ylab =</span><span class="st">&quot;Probability&quot;</span>, 
        <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>, 
        <span class="dt">space=</span><span class="dv">0</span>, 
        <span class="dt">main=</span><span class="st">&quot;Sum of Two Die Rolls&quot;</span>
        )

<span class="co"># plot the distribution of D </span>
probability &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">6</span>,<span class="dv">6</span>)
<span class="kw">names</span>(probability) &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">6</span>

<span class="kw">barplot</span>(probability, 
        <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.2</span>), 
        <span class="dt">xlab =</span> <span class="st">&quot;D&quot;</span>, 
        <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>, 
        <span class="dt">space =</span> <span class="dv">0</span>, 
        <span class="dt">main =</span> <span class="st">&quot;Outcome of a single Die Roll&quot;</span>
        )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-34-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Many econometric procedures deal with averages of sampled data. It is almost always assumed that observations are drawn randomly from a larger, unkown population. As demonstrated for the sample function <span class="math inline">\(S\)</span>, computing an average of a random sample also has the effect to make the average a random variable itself. This random variable in turn has a probability distribution which is called the sampling distribution. Knowledge about the sampling distribution of an average is therefore crucial for understanding the performance of econometric procedures.</p>
<p>The <em>sample average</em> of a sample of <span class="math inline">\(n\)</span> observations <span class="math inline">\(Y_1, \dots, Y_n\)</span> is</p>
<p><span class="math display">\[ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i = \frac{1}{n} (Y_1 + Y_2 + \cdots + Y_n). \]</span> <span class="math inline">\(\overline{Y}\)</span> is also called the sample mean.</p>
<div id="mean-and-variance-of-the-sample-mean" class="section level3 unnumbered">
<h3>Mean and Variance of the Sample Mean</h3>
<p>Denote <span class="math inline">\(\mu_Y\)</span> and <span class="math inline">\(\sigma_Y^2\)</span> the mean and the variance of the <span class="math inline">\(Y_i\)</span> and suppose that all observations <span class="math inline">\(Y_1,\dots,Y_n\)</span> are i.i.d. such that in particular mean and variance are the same for all <span class="math inline">\(i=1,\dots,n\)</span>. Then we have that</p>
<p><span class="math display">\[ E(\overline{Y}) = E\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) = \frac{1}{n} E\left(\sum_{i=1}^n Y_i\right) = \frac{1}{n} \sum_{i=1}^n E\left(Y_i\right) = \frac{1}{n} \cdot n \cdot \mu_Y = \mu_Y    \]</span> and</p>
<span class="math display">\[\begin{align}
  \text{Var}(\overline{Y}) =&amp; \text{Var}\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) \\
  =&amp; \frac{1}{n^2} \sum_{i=1}^n \text{Var}(Y_i) + \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1, j\neq i}^n \text{cov}(Y_i,Y_j) \\
  =&amp; \frac{\sigma^2_Y}{n} \\
  =&amp; \sigma_{\overline{Y}}^2.
\end{align}\]</span>
<p>Note that the second summand vanishes since <span class="math inline">\(\text{cov}(Y_i,Y_j)=0\)</span> for <span class="math inline">\(i\neq j\)</span> due to independence of the observations.</p>
<p>Consequently, the standard deviation of the sample mean is given by</p>
<p><span class="math display">\[ \sigma_{\overline{Y}} = \frac{\sigma_Y}{\sqrt{n}}. \]</span></p>
<p>It is worthwhile to mention that these results hold irrespective of the underlying distribution of the <span class="math inline">\(Y_i\)</span>.</p>
<div id="the-sampling-distribution-of-overliney-when-y-is-normally-distributed" class="section level4 unnumbered">
<h4>The Sampling Distribution of <span class="math inline">\(\overline{Y}\)</span> when <span class="math inline">\(Y\)</span> Is Normally Distributed</h4>
<p>If the <span class="math inline">\(Y_1,\dots,Y_n\)</span> are i.i.d. draws from a normal distribution with mean <span class="math inline">\(\mu_Y\)</span> and variance <span class="math inline">\(\sigma_Y^2\)</span>, Key Concept XXX states that the following holds for their sample average <span class="math inline">\(\overline{Y}\)</span>:</p>
<p><span class="math display">\[ \overline{Y} \sim N(\mu_y, \sigma_Y^2/n) \tag{2.4} \]</span></p>
<p>For example, if a sample <span class="math inline">\(Y_i\)</span> with <span class="math inline">\(i=1,\dots,10\)</span> is drawn from a standard normal distribution with mean <span class="math inline">\(\mu_Y = 0\)</span> and variance <span class="math inline">\(\sigma_Y^2=1\)</span> we have</p>
<p><span class="math display">\[ \overline{Y} \sim N(0,0.1).\]</span></p>
<p>We can use R’s random number generation facilities to verifiy this result. The basic idea is to simulate outcomes of the true distribution of <span class="math inline">\(\overline{Y}\)</span> by repeatedly drawing random samples of 10 observation from the <span class="math inline">\(N(0,1)\)</span> distribution and computing their respective averages. If we do this for a large number of repititions, the simulated dataset of averages should quite accurately reflect the theoretical distribution of <span class="math inline">\(\overline{Y}\)</span> if the theoretical result holds.</p>
<p>The approach sketched above is an example of what is commonly known as <em>Monte Carlo Simulation</em> or <em>Monte Carlo Experiment</em>. To perform this simulation in R, we proceed as follows:</p>
<ol style="list-style-type: decimal">
<li>Choose a sample size <code>n</code> and the number of samples to be drawn <code>reps</code>.</li>
<li>Use the function <code>replicate()</code> in conjunction with <code>rnorm()</code> to draw <code>n</code> observations from the standard normal distribution <code>rep</code> times. <strong>Note</strong>: the outcome of <code>replicate()</code> is a matrix with dimensions <code>n</code> <span class="math inline">\(\times\)</span> <code>rep</code>. It contains the drawn samples as <em>columns</em>.</li>
<li>Compute sample means using <code>colMeans()</code>. This function computes the mean of each column i.e. of each sample and returns a vector.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Set sample size and number of samples</span>
n &lt;-<span class="st"> </span><span class="dv">10</span>
reps &lt;-<span class="st"> </span><span class="dv">10000</span>

<span class="co"># Perform random sampling</span>
samples &lt;-<span class="st"> </span><span class="kw">replicate</span>(reps, <span class="kw">rnorm</span>(n)) <span class="co"># 10 x 10000 sample matrix</span>

<span class="co"># Compute sample means</span>
sample.avgs &lt;-<span class="st"> </span><span class="kw">colMeans</span>(samples)</code></pre></div>
<p>After performing these steps we end up with a vector of sample averages. You can check the vector property of <code>sample.avgs</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Check that sample.avgs is a vector</span>
<span class="kw">is.vector</span>(sample.avgs) </code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print the first few entries to the console</span>
<span class="kw">head</span>(sample.avgs)</code></pre></div>
<pre><code>## [1] -0.12406767 -0.10649421 -0.01033423 -0.39905236 -0.41897968 -0.90883537</code></pre>
<p>A straightforward approach to examine the distribution of univariate numerical data is to plot it as a histogram and compare it to some known or assumed distribution. This comparison can be done with help of a suitable statistical test or by simply eyeballing some graphical representations of these distributions. For our simulated sample averages, we will do the latter by means of the functions <code>hist()</code> and <code>curve()</code>.</p>
<p>By default, <code>hist()</code> will give us a frequency histogram i.e. a bar chart where observations are grouped into ranges, also called bins. The ordinate reports the number of observations falling into each of the bins. Instead, we want it to report density estimates for comparison purposes. This is achieved by setting the argument <code>freq = FALSE</code>. The number of bins is adjusted by the argument <code>breaks</code>.</p>
<p>Using <code>curve()</code>, we overlay the histogram with a red line which represents the theoretical density of a <span class="math inline">\(N(0, 0.1)\)</span> distributed random variable. Remember to use the argument <code>add = TRUE</code> to add the curve to the current plot. Otherwise R will open a new graphic device and discard the histogram plot!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the density histogram</span>
<span class="kw">hist</span>(sample.avgs, 
     <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">1.4</span>), 
     <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span> , 
     <span class="dt">freq =</span> F, 
     <span class="dt">breaks =</span> <span class="dv">20</span>
     )

<span class="co"># overlay the theoretical distribution of sample averages on top of the histogram</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x, <span class="dt">sd =</span> <span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(n)), 
      <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd=</span><span class="st">&quot;2&quot;</span>, 
      <span class="dt">add=</span>T
      )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-37-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>From inspection of the plot we can tell that the distribution of <span class="math inline">\(\overline{Y}\)</span> is indeed very close to that of a <span class="math inline">\(N(0, 0.1)\)</span> distributed random variable so that evidence obained from the Monte Carlo Simulation supports the theoretical claim.</p>
<p>Let us discuss another example where using simple random sampling in a simulation setup helps to verify a well known result. As discussed before, the <a href="#chisquare">Chi-squared</a> distribution with <span class="math inline">\(m\)</span> degrees of freedom arises as the distribution of the sum of <span class="math inline">\(m\)</span> independent squared standard normal distributed random variables.</p>
<p>To visualize the claim stated in equation (<a href="#mjx-eqn-2.3">2.3</a>), we proceed similarly as in the example before:</p>
<ol style="list-style-type: decimal">
<li>Choose the degrees of freedom <code>DF</code> and the number of samples to be drawn <code>reps</code>.</li>
<li>Draw <code>reps</code> random samples of size <code>DF</code> from the standard normal distribution using <code>replicate()</code>.</li>
<li>For each sample, by squaring the outcomes and summing them up columnwise. Store the results</li>
</ol>
<p>Again, we produce a density estimate for the distribution underlying our simulated data using a density histogram and overlay it with a line graph of the theoretical density function of the <span class="math inline">\(\chi^2_3\)</span> distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Number of repititions</span>
reps &lt;-<span class="st"> </span><span class="dv">10000</span>

<span class="co"># Set degrees of freedom of a chi-Square Distribution</span>
DF &lt;-<span class="st"> </span><span class="dv">3</span> 

<span class="co"># Sample 10000 column vectors à 3 N(0,1) R.V.S</span>
Z &lt;-<span class="st"> </span><span class="kw">replicate</span>(reps, <span class="kw">rnorm</span>(DF)) 

<span class="co"># Column sums of squares</span>
X &lt;-<span class="st"> </span><span class="kw">colSums</span>(Z<span class="op">^</span><span class="dv">2</span>)

<span class="co"># Histogram of column sums of squares</span>
<span class="kw">hist</span>(X, 
     <span class="dt">freq =</span> F, 
     <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">breaks =</span> <span class="dv">40</span>, 
     <span class="dt">ylab=</span><span class="st">&quot;Density&quot;</span>, 
     <span class="dt">main=</span><span class="st">&quot;&quot;</span>
     )

<span class="co"># Add theoretical density</span>
<span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df =</span> DF), 
      <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>, 
      <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, 
      <span class="dt">add =</span> T
      )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="large-sample-approximations-to-sampling-distributions" class="section level3 unnumbered">
<h3>Large Sample Approximations to Sampling Distributions</h3>
<p>Sampling distributions as considered in the last section play an important role in the development of econometric methods. In general, there are two different approaches in characterizing sampling distributions: an “exact” approach and an “approximate” approach.</p>
<p>The exact approach aims to find a general formula for the sampling distribution that holds for any sample size <span class="math inline">\(n\)</span>. We call this the <em>exact distribution</em> or <em>finite sample distribution</em>. In the previous examples of die rolling and normal variates, we have dealt with functionals of random variables whose sample distributions are <em>excactly known</em> in the sense that we can write them down as analytical expressions and do calculations. <br> However, this is not always possible. For <span class="math inline">\(\overline{Y}\)</span>, result (<a href="#mjx-eqn-2.4">2.4</a>) tells us that normality of the <span class="math inline">\(Y_i\)</span> implies normality of <span class="math inline">\(\overline{Y}\)</span> (we demonstrated this for the special case of <span class="math inline">\(Y_i \overset{i.i.d.}{\sim} N(0,1)\)</span> with <span class="math inline">\(n=10\)</span> using a simulation study that involves random sampling). Unfortunately, the <em>exact</em> distribution of <span class="math inline">\(\overline{Y}\)</span> is unknown, very hard to derive or even untractable if we drop the assumption of <span class="math inline">\(Y\)</span> having a normal distribution. <br></p>
<p>Therefore, as can be guessed from its name, the “approximate” approach aims to find an approximation to the sampling distribution wherby it is required that the sample size <span class="math inline">\(n\)</span> is large. A distribution that is used as a large-sample approximation to the sampling distribution is also called the <em>asymptotic distribution</em>. This is due to the fact that the asymptotic distribution <em>is</em> the sampling distribution for <span class="math inline">\(n \rightarrow \infty\)</span> i.e. the approximation becomes exact if the sample size goes to infinity. However, there are cases where the difference between the sampling distribution and the asymptotic distribution is negligible for moderate or even small samples sizes so that approximations work very good.</p>
<p>In this section we will discuss two well known results that are used to approximate sampling distributions and thus consitute key tools in econoemtric theory: the <em>law of large numbers</em> and the <em>central limit theorem</em>.<br> The law of large numbers states that in large samples, <span class="math inline">\(\overline{Y}\)</span> is close to <span class="math inline">\(\mu_Y\)</span> with high probability. The central limit theorem says that the sampling distribution of the standardized sample average, that is <span class="math inline">\((\overline{Y} - \mu_Y)/\sigma_{\overline{Y}}\)</span> is asymptotically normal distributed. It is particualarly interesting that both results do not depend on the distribution of <span class="math inline">\(Y\)</span>. In other words, beeing unable to describe the complicated sampling distribution of <span class="math inline">\(\overline{Y}\)</span> if <span class="math inline">\(Y\)</span> is not normal, approximations of the latter using the central limit theorem simplify the development and applicability of econometric procedures enormously. This is a key component underlying the theory of statistical inference for regression models. Both results are summarized in Key Concept 2.6 and Key Concept 2.7.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.6
</h3>
<h3 class="left">
Convergence in Probability, Consistency and the Law of Large Numbers
</h3>
<p>
<p>The sample average <span class="math inline">\(\overline{Y}\)</span> converges in probability to <span class="math inline">\(\mu_Y\)</span> — we say that <span class="math inline">\(\overline{Y}\)</span> is <em>consistent</em> for <span class="math inline">\(\mu_Y\)</span> — if the probability that <span class="math inline">\(\overline{Y}\)</span> is in the range <span class="math inline">\((\mu_Y - \epsilon)\)</span> to <span class="math inline">\((\mu_Y + \epsilon)\)</span> becomes arbitrarly close to <span class="math inline">\(1\)</span> as <span class="math inline">\(n\)</span> increases for any constant <span class="math inline">\(\epsilon &gt; 0\)</span>. We write this short as</p>
<p><span class="math display">\[ \overline{Y} \xrightarrow[]{p} \mu_Y. \]</span></p>
<p>Consider the independently and identically distributed random variables <span class="math inline">\(Y_i, i=1,\dots,n\)</span> with expectation <span class="math inline">\(E(Y_i)=\mu_Y\)</span> and variance <span class="math inline">\(\text{Var}(Y_i)=\sigma^2_Y\)</span>. Under the condition that <span class="math inline">\(\sigma^2_Y&lt; \infty\)</span>, that is large outliers are unlikely, the law of large numbers states</p>
<p><span class="math display">\[ \overline{Y} \xrightarrow[]{p} \mu_Y. \]</span></p>
</p>
</div>
<p>The core statement of the law of large numbers is that under quite general conditions, the probability of obtaining a sample average <span class="math inline">\(\overline{Y}\)</span> that is close to <span class="math inline">\(\mu_Y\)</span> is high if we have a large sample size.</p>
<p>Consider the example of repeatedly tossing a coin where <span class="math inline">\(Y_i\)</span> is the result of the <span class="math inline">\(i^{th}\)</span> coin toss. <span class="math inline">\(Y_i\)</span> is a Bernoulli distributed random variable with</p>
<p><span class="math display">\[ P(Y_i) = \begin{cases} p, &amp; Y_i = 1 \\ 1-p, &amp; Y_i = 0 \end{cases} \]</span> where <span class="math inline">\(p = 0.5\)</span> as we assume a fair coin. It is straightforward to show that</p>
<p><span class="math display">\[ \mu_Y = p = 0.5. \]</span> Say <span class="math inline">\(p\)</span> is the probabiliy of observing head and denote <span class="math inline">\(R_n\)</span> the proportion of heads in the first <span class="math inline">\(n\)</span> tosses,</p>
<p><span class="math display">\[ R_n = \frac{1}{n} \sum_{i=1}^n Y_i. \tag{2.5}\]</span></p>
<p>According to the law of large numbers, the observed proportion of heads converges in probability to <span class="math inline">\(\mu_Y = 0.5\)</span>, the probability of tossing head in a single coin toss,</p>
<p><span class="math display">\[ R_n \xrightarrow[]{p} \mu_Y=0.5 \ \ \text{as} \ \ n \rightarrow \infty.  \]</span> The following application simulates <span class="math inline">\(1000\)</span> coin tosses with a fair coin and computes the fraction of heads observed for each additional toss interactively. The result is a random path that, as stated by the law of large numbers, shows a tendency to approach the vaule of <span class="math inline">\(0.5\)</span> as <span class="math inline">\(n\)</span> grows.</p>
<p>We can use R to compute and illustrate such paths by simulation. The procedure is as follows:</p>
<ol style="list-style-type: decimal">
<li>Sample <code>N</code> observations from the Bernoulli distribution e.g. using <code>sample()</code>.</li>
<li>Calculate the proportion of heads <span class="math inline">\(R_n\)</span> as in (<a href="#mjx-eqn-2.5">2.5</a>). A way to achieve this is to call <code>cumsum()</code> on the vector of observations <code>Y</code> to obtain its cumulative sum and then divide by the respective number of observations.</li>
</ol>
<p>We continue by plotting the path and also add a dashed line for the benchmark <span class="math inline">\(R_n = p = 0.5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># Set number of coin tosses and simulate</span>
N &lt;-<span class="st"> </span><span class="dv">30000</span>
Y &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, N, <span class="dt">replace =</span>T)

<span class="co"># Calculate R_n for 1:N</span>
S &lt;-<span class="st"> </span><span class="kw">cumsum</span>(Y)
R &lt;-<span class="st"> </span>S<span class="op">/</span>(<span class="dv">1</span><span class="op">:</span>N)

<span class="co"># Plot the path.</span>
<span class="kw">plot</span>(R, 
     <span class="dt">ylim=</span><span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.7</span>), 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;n&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;R_n&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Converging Share of Heads in Repeated Coin Tossing&quot;</span>
     )

<span class="co"># Add a dashed line for R_n = 0.5</span>
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>,N), 
      <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>), 
      <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>, 
      <span class="dt">lty =</span> <span class="dv">2</span>, 
      <span class="dt">lwd =</span> <span class="dv">1</span>
      )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-39-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>There are several things to be said about this plot.</p>
<ul>
<li><p>The blue graph shows the observed proportion of heads when tossing a coin <span class="math inline">\(n\)</span> times.</p></li>
<li><p>Since the <span class="math inline">\(Y_i\)</span> are radnom variables, <span class="math inline">\(R_n\)</span> is a random variate, too. The path depicted is only one of many possible realisations of <span class="math inline">\(R_n\)</span> as it is determined by the <span class="math inline">\(30000\)</span> observations sampled from the Bernoulli distribution. Thus, the code chunk above produces a differnt path every time You execute it (try this!).</p></li>
<li><p>If the number of coin tosses <span class="math inline">\(n\)</span> is small, we observe the proportion of heads to be anything but close to its theoretical value, <span class="math inline">\(\mu_Y = 0.5\)</span>. However, as more and more observation are included in the sample we find that the path stabilizes in neighbourhood of <span class="math inline">\(0.5\)</span>.<br> This is the message to take away: the average of multiple trials shows a clear tendency to converge to its expected value as the sample size increases, just as claimed by the law of large numbers.</p></li>
</ul>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.6
</h3>
<h3 class="left">
The Central Limit Theorem
</h3>
<p>
<p>Suppose that <span class="math inline">\(Y_1,\dots,Y_n\)</span> are independently and identically distributed random variables with expectation <span class="math inline">\(E(Y_i)=\mu_Y\)</span> and variance <span class="math inline">\(\text{Var}(Y_i)=\sigma^2_Y\)</span>, <span class="math inline">\(0&lt;\sigma^2_Y&lt;\infty\)</span>.<br> The central limit theorem states that, if the sample size <span class="math inline">\(n\)</span> goes to infinity, the distribution of the scaled (by <span class="math inline">\(\sqrt{n}\)</span>) standardized sample average <span class="math display">\[ \frac{\overline{Y} - \mu_Y}{\sigma_{\overline{Y}}} = \frac{\overline{Y} - \mu_Y}{\sigma_Y/\sqrt{n}} \ \]</span> becomes arbitrarily well approximated by the standard normal distribution.</p>
</p>
</div>
<p>According to the central limit theorem, the distribution of the sample mean <span class="math inline">\(\overline{Y}\)</span> of the bernoulli distributed random variables <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(i=1,...,n\)</span> is well approximated by the normal distribution with parameters <span class="math inline">\(\mu_Y=p=0.5\)</span> and <span class="math inline">\(\sigma^2_{\overline{Y}} = p(1-p) = 0.25\)</span> for large <span class="math inline">\(n\)</span>. Consequently, for the standardized sample mean we conclude that the ratio</p>
<p><span class="math display">\[ \frac{\overline{Y} - 0.5}{0.5/\sqrt{n}} \tag{2.6}\]</span> should be well approximated by the standard normal distribution <span class="math inline">\(N(0,1)\)</span>. We employ another simulation study to demonstrate this graphically. The idea is as follows.</p>
<p>Draw a large number of random samples, <span class="math inline">\(10000\)</span> say, of size <span class="math inline">\(n\)</span> from the Bernoulli distribution and compute the sample averages. Standardize the averages as shown in (<a href="#mjx-eqn-2.6">2.6</a>). Next, visualize the distribution of the generated standardized sample averages by means of a density histogram and compare to the standard normal distribution. Repeat this for different sample sizes <span class="math inline">\(n\)</span> to see how increasing the sample size <span class="math inline">\(n\)</span> impacts the simulated distribution of the averages.</p>
<p>In R, we realized this as follows:</p>
<ol style="list-style-type: decimal">
<li><p>We start by defining that the next four subsequently generated figures shall be drawn in a <span class="math inline">\(2\times2\)</span> array such that they can be easily compared. This is done by calling <code>par(mfrow = c(2, 2))</code> before the figures are generated.</p></li>
<li><p>We define the number of repetitions <code>reps</code> as <span class="math inline">\(10000\)</span> and create a vector of sample sizes named <code>sample.sizes</code>. We consider samples of sizes <span class="math inline">\(2\)</span>, <span class="math inline">\(10\)</span>, <span class="math inline">\(50\)</span> and <span class="math inline">\(100\)</span>.</p></li>
<li><p>Next, we combine two <code>for()</code> loops to simulate the data and plot the distributions. The inner loop generates <span class="math inline">\(10000\)</span> random samples, each consisting of <code>n</code> observations that are drawn from the bernoulli distribution, and computes the standardized averages. The outer loop executes the inner loop for the different sample sizes <code>n</code> and produces a plot for each iteration.</p></li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Subdivide the plot panel into a 2-by-2 array</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))

<span class="co"># Set number of repetitions and the sample sizes</span>
reps &lt;-<span class="st"> </span><span class="dv">10000</span>
sample.sizes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>)

<span class="co"># outer loop (loop over the sample sizes)</span>
  <span class="cf">for</span> (n <span class="cf">in</span> sample.sizes) {
    
    samplemean &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,reps) <span class="co">#initialize the vector of sample menas</span>
    stdsamplemean &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,reps) <span class="co">#initialize the vector of standardized sample menas</span>

<span class="co"># inner loop (loop over repetitions)   </span>
    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps) {
      x &lt;-<span class="st"> </span><span class="kw">rbinom</span>(n,<span class="dv">1</span>,<span class="fl">0.5</span>)
      samplemean[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(x)
      stdsamplemean[i] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(n)<span class="op">*</span>(<span class="kw">mean</span>(x)<span class="op">-</span><span class="fl">0.5</span>)<span class="op">/</span><span class="fl">0.5</span>
    }
    
<span class="co"># plot the histogram and overlay it with the N(0,1) density for every iteration    </span>
    <span class="kw">hist</span>(stdsamplemean, 
         <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
         <span class="dt">breaks =</span> <span class="dv">40</span>, 
         <span class="dt">freq =</span> <span class="ot">FALSE</span>, 
         <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), 
         <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>), 
         <span class="dt">xlab =</span> <span class="kw">paste</span>(<span class="st">&quot;n =&quot;</span>, n), 
         <span class="dt">main =</span> <span class="st">&quot;&quot;</span>
         )
    
    <span class="kw">curve</span>(<span class="kw">dnorm</span>(x), 
          <span class="dt">lwd =</span> <span class="dv">2</span>, 
          <span class="dt">col=</span><span class="st">&quot;darkred&quot;</span>, 
          <span class="dt">add =</span> <span class="ot">TRUE</span>
          )
  }  </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-40-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that the simulated sampling distribution of the standardized average tends to deviate strongly from the standard normal distribution if the sample size is small, e.g. for <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=10\)</span>. However as <span class="math inline">\(n\)</span> grows, the histograms are approaching the bell shape of a standard normal and we can be confident that the approximation works quite well as seen for <span class="math inline">\(n=100\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-review-of-statistics-using-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-ch2.Rmd",
"text": "Edit"
},
"download": ["URFITE.pdf", "URFITE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
