<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Using R for Introduction to Econometrics</title>
  <meta name="description" content="Using R for Introduction to Econometrics">
  <meta name="generator" content="bookdown 0.7.1 and GitBook 2.6.7">

  <meta property="og:title" content="Using R for Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Using R for Introduction to Econometrics" />
  
  
  

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-03-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="internal-and-external-validity.html">
<link rel="next" href="internal-and-external-validity-when-the-regression-is-used-for-forecasting.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="MathJax-2.7.2/MathJax.js?config=default"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>

<!-- datacamp-light-latest -->

<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>


<!-- <script src="js/d3.v3.min.js"></script>
<script src="js/leastsquares.js"></script>
<script src="js/d3functions.js"></script>
<script src="d3.slider.js"></script>
<script src="slrm.js"></script> -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">URFITE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#expected-values-mean-and-variance"><i class="fa fa-check"></i>Expected Values, Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html"><i class="fa fa-check"></i><b>2.2</b> Probability Distributions of Continuous Random Variables</a><ul>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li><a href="probability-distributions-of-continuous-random-variables.html#thetdist">The Student <span class="math inline">\(t\)</span> Distribution</a></li>
<li><a href="probability-distributions-of-continuous-random-variables.html#the-f-distribution">The <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html"><i class="fa fa-check"></i><b>2.3</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="estimation-of-the-population-mean.html"><a href="estimation-of-the-population-mean.html"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-the-sample-mean.html"><a href="properties-of-the-sample-mean.html"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#p-value"><span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-sigma_y-is-known">Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#sample-variance-sample-standard-deviation-and-standard-error"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-sigma_y-is-unknown">Calculating the <span class="math inline">\(p\)</span>-value When <span class="math inline">\(\sigma_Y\)</span> is Unknown</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#the-t-statistic">The <span class="math inline">\(t\)</span>-statistic</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confidence-intervals-for-the-population-mean.html"><a href="confidence-intervals-for-the-population-mean.html"><i class="fa fa-check"></i><b>3.4</b> Confidence intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="comparing-means-from-different-populations.html"><a href="comparing-means-from-different-populations.html"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="an-application-to-the-gender-gap-of-earnings.html"><a href="an-application-to-the-gender-gap-of-earnings.html"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="scatterplots-sample-covariance-and-sample-correlation.html"><a href="scatterplots-sample-covariance-and-sample-correlation.html"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="estimating-the-coefficients-of-the-linear-regression-model.html"><a href="estimating-the-coefficients-of-the-linear-regression-model.html"><i class="fa fa-check"></i><b>4.1</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="estimating-the-coefficients-of-the-linear-regression-model.html"><a href="estimating-the-coefficients-of-the-linear-regression-model.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="measures-of-fit.html"><a href="measures-of-fit.html"><i class="fa fa-check"></i><b>4.2</b> Measures of Fit</a><ul>
<li><a href="measures-of-fit.html#the-r2">The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#standard-error-of-the-regression"><i class="fa fa-check"></i>Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html"><i class="fa fa-check"></i><b>4.3</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption #1: The Error Term has Conditional Mean of Zero</a></li>
<li><a href="the-least-squares-assumptions.html#assumption-2-all-x_i-y_i-are-independently-and-identically-distributed">Assumption #2: All <span class="math inline">\((X_i, Y_i)\)</span> are Independently and Identically Distributed</a></li>
<li class="chapter" data-level="" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption #3: Large outliers are unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tsdotoe.html"><a href="tsdotoe.html"><i class="fa fa-check"></i><b>4.4</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-1"><i class="fa fa-check"></i>R Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-2"><i class="fa fa-check"></i>R Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-3"><i class="fa fa-check"></i>R Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="testing-two-sided-hypotheses-concerning-beta-1.html"><a href="testing-two-sided-hypotheses-concerning-beta-1.html"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="5.2" data-path="confidence-intervals-for-regression-coefficients.html"><a href="confidence-intervals-for-regression-coefficients.html"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="confidence-intervals-for-regression-coefficients.html"><a href="confidence-intervals-for-regression-coefficients.html#r-simulation-study-5.1"><i class="fa fa-check"></i>R Simulation Study 5.1</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regression-when-x-is-a-binary-variable.html"><a href="regression-when-x-is-a-binary-variable.html"><i class="fa fa-check"></i><b>5.3</b> Regression when <span class="math inline">\(X\)</span> is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html#r-simulation-study-blue-estimator"><i class="fa fa-check"></i>R Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><a href="using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><i class="fa fa-check"></i><b>5.6</b> Using the <span class="math inline">\(t\)</span>-Statistic in Regression When the Sample Size Is Small</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="the-multiple-regression-model.html"><a href="the-multiple-regression-model.html"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="measures-of-fit-in-multiple-regression.html"><a href="measures-of-fit-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="the-distribution-of-the-ols-estimators-in-multiple-regression.html"><a href="the-distribution-of-the-ols-estimators-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><a href="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="an-application-to-test-scores-and-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="an-application-to-test-scores-and-the-student-teacher-ratio.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="joint-hypothesis-testing-using-the-f-statistic.html"><a href="joint-hypothesis-testing-using-the-f-statistic.html"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the <span class="math inline">\(F\)</span>-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="confidence-sets-for-multiple-coefficients.html"><a href="confidence-sets-for-multiple-coefficients.html"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="model-specification-for-multiple-regression.html"><a href="model-specification-for-multiple-regression.html"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="model-specification-for-multiple-regression.html"><a href="model-specification-for-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="analysis-of-the-test-score-data-set.html"><a href="analysis-of-the-test-score-data-set.html"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="a-general-strategy-for-modeling-nonlinear-regression-functions.html"><a href="a-general-strategy-for-modeling-nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modeling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="interactions-between-independent-variables.html"><a href="interactions-between-independent-variables.html"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><a href="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="assessing-studies-based-on-multiple-regression.html"><a href="assessing-studies-based-on-multiple-regression.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="internal-and-external-validity.html"><a href="internal-and-external-validity.html"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="threats-to-internal-validity-of-multiple-regression-analysis.html"><a href="threats-to-internal-validity-of-multiple-regression-analysis.html"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><a href="internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity When the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="example-test-scores-and-class-size.html"><a href="example-test-scores-and-class-size.html"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regression-with-panel-data.html"><a href="regression-with-panel-data.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="PDWTTP.html"><a href="PDWTTP.html"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and Afer” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="regression-with-time-fixed-effects.html"><a href="regression-with-time-fixed-effects.html"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="the-fixed-effects-regression-assumptions-and-standard-errors-for-fixed-effects-regression.html"><a href="the-fixed-effects-regression-assumptions-and-standard-errors-for-fixed-effects-regression.html"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="drunk-driving-laws-and-traffic-deaths.html"><a href="drunk-driving-laws-and-traffic-deaths.html"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="regression-with-a-binary-dependent-variable.html"><a href="regression-with-a-binary-dependent-variable.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="binary-dependent-variables-and-the-linear-probability-model.html"><a href="binary-dependent-variables-and-the-linear-probability-model.html"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="probit-and-logit-regression.html"><a href="probit-and-logit-regression.html"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="probit-and-logit-regression.html"><a href="probit-and-logit-regression.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="probit-and-logit-regression.html"><a href="probit-and-logit-regression.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="estimation-and-inference-in-the-logit-and-probit-models.html"><a href="estimation-and-inference-in-the-logit-and-probit-models.html"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="application-to-the-boston-hmda-data.html"><a href="application-to-the-boston-hmda-data.html"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="instrumental-variables-regression.html"><a href="instrumental-variables-regression.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="TIVEWASRAASI.html"><a href="TIVEWASRAASI.html"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="TGIVRM.html"><a href="TGIVRM.html"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="checking-instrument-validity.html"><a href="checking-instrument-validity.html"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="application-to-the-demand-for-cigarettes-2.html"><a href="application-to-the-demand-for-cigarettes-2.html"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="where-do-valid-instruments-come-from.html"><a href="where-do-valid-instruments-come-from.html"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="experiments-and-quasi-experiments.html"><a href="experiments-and-quasi-experiments.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="potential-outcomes-causal-effects-and-idealized-experiments.html"><a href="potential-outcomes-causal-effects-and-idealized-experiments.html"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="threats-to-validity-of-experiments.html"><a href="threats-to-validity-of-experiments.html"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="experimental-estimates-of-the-effect-of-class-size-reductions.html"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="experimental-estimates-of-the-effect-of-class-size-reductions.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="experimental-estimates-of-the-effect-of-class-size-reductions.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="quasi-experiments.html"><a href="quasi-experiments.html"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="quasi-experiments.html"><a href="quasi-experiments.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="quasi-experiments.html"><a href="quasi-experiments.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="introduction-to-time-series-regression-and-forecasting.html"><a href="introduction-to-time-series-regression-and-forecasting.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="using-regression-models-for-forecasting.html"><a href="using-regression-models-for-forecasting.html"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="time-series-data-and-serial-correlation.html"><a href="time-series-data-and-serial-correlation.html"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="time-series-data-and-serial-correlation.html"><a href="time-series-data-and-serial-correlation.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="autoregressions.html"><a href="autoregressions.html"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="autoregressions.html#the-pth-order-autoregressive-model">The <span class="math inline">\(p^{th}\)</span>-Order Autoregressive Model</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="can-you-beat-the-market-part-i.html"><a href="can-you-beat-the-market-part-i.html"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="additional-predictors-and-the-adl-model.html"><a href="additional-predictors-and-the-adl-model.html"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="additional-predictors-and-the-adl-model.html"><a href="additional-predictors-and-the-adl-model.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="lag-length-selection-using-information-criteria.html"><a href="lag-length-selection-using-information-criteria.html"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="nonstationarity-i-trends.html"><a href="nonstationarity-i-trends.html"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="nonstationarity-ii-breaks.html"><a href="nonstationarity-ii-breaks.html"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="can-you-beat-the-market-part-ii.html"><a href="can-you-beat-the-market-part-ii.html"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? Part II</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="estimation-of-dynamic-causal-effects.html"><a href="estimation-of-dynamic-causal-effects.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="the-orange-juice-data.html"><a href="the-orange-juice-data.html"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="dynamic-causal-effects.html"><a href="dynamic-causal-effects.html"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="dynamic-multipliers-and-cumulative-dynamic-multipliers.html"><a href="dynamic-multipliers-and-cumulative-dynamic-multipliers.html"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="hac-standard-errors.html"><a href="hac-standard-errors.html"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors.html"><a href="estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors.html"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="orange-juice-prices-and-cold-weather.html"><a href="orange-juice-prices-and-cold-weather.html"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using R for Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="threats-to-internal-validity-of-multiple-regression-analysis" class="section level2">
<h2><span class="header-section-number">9.2</span> Threats to Internal Validity of Multiple Regression Analysis</h2>
<p>This section lists five sources that cause the OLS estimator in (multiple) regression models to be biased and inconsistent for the causal effect of interest and discusses possible remedies. Note that all five sources arise from violation of the first least squares assumption in Key Concept 6.4.</p>
<p>This sections treats:</p>
<ul>
<li><p>Omitted variable Bias</p></li>
<li><p>Misspecification of the functional form</p></li>
<li><p>Measurement errors</p></li>
<li><p>Missing data and sample selection</p></li>
<li><p>Simultaneous causality bias</p></li>
</ul>
<p>Beside these threats for consistency of the coefficient estimation, we will also briefly discuss sources of inconsistent estimation of OLS standard errors.</p>
<div id="omitted-variable-bias-1" class="section level4 unnumbered">
<h4>Omitted Variable Bias</h4>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.2
</h3>
<h3 class="left">
Omitted Variable Bias: Should I include More Variables in My Regression?
</h3>
<p>Inclusion of additional variables reduces the risk of omitted variable bias but may increase the variance of the estimator of the coefficient of interest.</p>
<p>We present some guidelines that help deciding whether to include an additional variable:</p>
<ol style="list-style-type: decimal">
<li><p>Specify the coefficient(s) of interest.</p></li>
<li><p>Identify the most important potential sources of omitted variable bias by using knowlegde available <em>before</em> estimating the model. You should end up with a base specification and a set of regressors that are questionable.</p></li>
<li><p>Use different model specifactions to test whether questionable regressors have coefficients different from zero.</p></li>
<li><p>Use tables to provide “full disclosure” of your results i.e. present different model specifications that do both support your argument and enable the reader to see the effect of including questionable regressors.</p></li>
</ol>
</div>
<p>By now you should be aware of omitted variable bias and its consequences. Key Concept 9.2 gives some guidelines on how to proceed if there are control variables that possibly allow to reduce an omitted variable bias. If including additional variables to mitigate the bias is not an option because there are no adequate controls, there are different approaches to solve the problem:</p>
<ol style="list-style-type: decimal">
<li><p>Usage of Panel data (discussed in Chapter 10)</p></li>
<li><p>Usage of Instrumental variables regression (discussed in Chapter 12)</p></li>
<li><p>Usage of a randomized control experiment (discussed in Chapter 13)</p></li>
</ol>
</div>
<div id="misspecification-of-the-functional-form-of-the-regression-function" class="section level4 unnumbered">
<h4>Misspecification of the Functional Form of the Regression Function</h4>
<p>If the population regression function is nonlinear but the regression model is linear, we say that the functional form of the regression function is misspecified. This leads to a bias of the OLS estimator.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.3
</h3>
<h3 class="left">
Functional Form Misspecification
</h3>
<p>We say a regression suffers from misspecification of the functional form when the functional form of the estimated regression model differs from the functional form of the population regression function. Functional form misspecification leads to biased and inconsistent coefficient estimators. A way to detect functional form misspecification is to plot the estimated regression function and the data. This may also be helpful to choose the correct functional form.</p>
</div>
<p>It is easy to come up with an example case of misspecification of the functional form:</p>
<p>Consider the case where the population regression function is <span class="math display">\[ Y_i = - X_i^2 \]</span> but the estimated model is <span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + u_i \]</span> so that the regression function is misspecified.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">3</span>)

<span class="co"># generate data set</span>
X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)
Y &lt;-<span class="st"> </span>X<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)

<span class="co"># plot the data</span>
<span class="kw">plot</span>(X, Y, 
     <span class="dt">main =</span> <span class="st">&quot;Misspecification of Functional Form&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>
     )

<span class="co"># estimate and plot the regression function</span>
ms_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)
ms_mod</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Coefficients:
## (Intercept)            X  
##     8.11363     -0.04684</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">abline</span>(ms_mod, 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>,
       <span class="dt">lwd =</span> <span class="dv">2</span>
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-197-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It is evident that regression errors are relatively small for observations close to <span class="math inline">\(X=-3\)</span> and <span class="math inline">\(X=3\)</span> but increase for <span class="math inline">\(X\)</span> values closer to zero and even more for regressor values beyond <span class="math inline">\(-4\)</span> and <span class="math inline">\(4\)</span>. Consequences are drastic: the intercept is estimated to be <span class="math inline">\(8.1\)</span> and for the slope parameter we obtain an estimate close to zero which is obviously wrong. This issue does not dissapear as the number of observations is inreased because OLS is biased <em>and</em> inconsistent due to the misspecification of the regression function.</p>
</div>
<div id="measurement-error-and-errors-in-variables-bias" class="section level4 unnumbered">
<h4>Measurement Error and Errors-in-Variables Bias</h4>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.4
</h3>
<h3 class="left">
Errors-in-Variable Bias
</h3>
<p>When independent variables are measure imprecisely, we speak of Errors-in-variables bias. This bias does not disappear if the sample size is large. If the measurement error has mean zero and is independent of the affected variable, the OLS estimator of the respective coefficient is biased towards zero.</p>
</div>
Suppose you are measuring data on the single regressor <span class="math inline">\(X_i\)</span> incorrectly so that there is a measurement error and you observe <span class="math inline">\(\overset{\sim}{X}_i\)</span> instead of <span class="math inline">\(X_i\)</span>. Then, instead of estimating the population the regression model <span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + u_i \]</span> you end up estimating
<span class="math display">\[\begin{align*}
  Y_i =&amp; \, \beta_0 + \beta_1 \overset{\sim}{X}_i + \underbrace{\beta_1 (X_i - \overset{\sim}{X}_i) + u_i}_{=v_i} \\
  Y_i =&amp; \, \beta_0 + \beta_1 \overset{\sim}{X}_i + v_i
\end{align*}\]</span>
<p>where <span class="math inline">\(\overset{\sim}{X}_i\)</span> and the error term <span class="math inline">\(v_i\)</span> are correlated. Thus OLS would be biased and inconsitent for the true <span class="math inline">\(\beta_1\)</span> in this example.</p>
<p>One can show that direction and strength of the bias depend on the correlation between the observed regressor, <span class="math inline">\(\overset{\sim}{X}_i\)</span>, and the measurement error, <span class="math inline">\((X_i - \overset{\sim}{X}_i)\)</span>. The correlation depends on the type of the measurement error.</p>
<p>The classical measurement error model assumes that the error, <span class="math inline">\(w_i\)</span>, has zero mean and that the error is uncorrelated with the variable, <span class="math inline">\(X_i\)</span>, and the error term of the poulation regression model, <span class="math inline">\(u_i\)</span>:</p>
<span class="math display">\[\begin{equation}
  \overset{\sim}{X}_i = X_i + w_i \ \ , \ \ corr(w_i,u_i)=0 \ \ , \ \ corr(w_i,X_i)=0 
\end{equation}\]</span>
Then it holds that
<span class="math display" id="eq:cmembias">\[\begin{equation}
  \widehat{\beta}_1 \xrightarrow{p}{\frac{\sigma_{X}^2}{\sigma_{X}^2 + \sigma_{w}^2}} \tag{9.1}
\end{equation}\]</span>
<p>Which implies inconsistency as <span class="math inline">\(\sigma_{X}^2, \sigma_{w}^2 &gt; 0\)</span> such that the fraction in <a href="threats-to-internal-validity-of-multiple-regression-analysis.html#eq:cmembias">(9.1)</a> is smaller than <span class="math inline">\(1\)</span>. Note that, there are two extreme cases: first, if there is no measurement error, <span class="math inline">\(\sigma_{w}^2=0\)</span> such that <span class="math inline">\(\widehat{\beta}_1 \xrightarrow{p}{\beta_1}\)</span>. Second, if <span class="math inline">\(\sigma_{w}^2 \gg \sigma_{X}^2\)</span> we have <span class="math inline">\(\widehat{\beta}_1 \xrightarrow{p}{0}\)</span>. This is the case if the measurement error is so large that there essentially is no information on <span class="math inline">\(X\)</span>.</p>
<p>The most obvious way to deal with errors-in-variables bias is to use an accurately measured <span class="math inline">\(X\)</span>. If this not possible, instrumental variables regression can be an option. One might also deal with the issue by using a mathematical model of the measurement error and correct the estimates. For example, if it is plausible that the classical measurement error model applies and if there is information that can be used to estimate the ratio in equation <a href="threats-to-internal-validity-of-multiple-regression-analysis.html#eq:cmembias">(9.1)</a>, one could compute an estimate that corrects for the downwoard bias.</p>
For example, consider two bivariate normal distributed random variables <span class="math inline">\(X,Y\)</span>. It is a well known result, that the conditional expectation function of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> has the form
<span class="math display" id="eq:bnormexpfn">\[\begin{align*}
  E(Y\vert X) = E(Y) + \rho_{X,Y} \frac{\sigma_{Y}}{\sigma_{X}}\left[X-E(X)\right]. \tag{9.2} 
\end{align*}\]</span>
Thus for
<span class="math display" id="eq:bvnormd">\[\begin{align*}
  (X, Y) \sim \mathcal{N}\left[\begin{pmatrix}50\\ 100\end{pmatrix},\begin{pmatrix}10 &amp; 5 \\ 5 &amp; 10 \end{pmatrix}\right] \tag{9.3}
\end{align*}\]</span>
according to <a href="threats-to-internal-validity-of-multiple-regression-analysis.html#eq:bnormexpfn">(9.2)</a>, the population regression function is
<span class="math display">\[\begin{align*}
  Y_i =&amp; \, 100 + 0.5 (X - 50) \\
      =&amp; \, 75 + 0.5 X.
\end{align*}\]</span>
<p>Suppose you gather data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> but that can you only measure <span class="math inline">\(\overset{\sim}{X_i} = X_i + w_i\)</span> with <span class="math inline">\(w_i \overset{i.i.d.}{\sim} \mathcal{N}(0,\sqrt{10})\)</span>. Since the <span class="math inline">\(w_i\)</span> are purely random, there is no correlation between the <span class="math inline">\(X_i\)</span> and the <span class="math inline">\(w_i\)</span> so that we have a case of the classical measurement error model. We can illustrate this using <tt>R</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># load the mvtnorm package and simulate data</span>
<span class="kw">library</span>(mvtnorm)
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="kw">rmvnorm</span>(<span class="dv">1000</span>, <span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">100</span>), 
          <span class="dt">sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">10</span>))
          )
  )

<span class="co"># set columns names</span>
<span class="kw">colnames</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>,<span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p>We now estimate a simple linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> using this sample data and run the same regression again but this time add i.i.d. <span class="math inline">\(\mathcal{N}(0,\sqrt{10})\)</span> errors to <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the model (without measurement error)</span>
nme_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> dat)

<span class="co"># estimate the model (measurement error in X)</span>
dat<span class="op">$</span>X &lt;-<span class="st"> </span>dat<span class="op">$</span>X <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">sd =</span> <span class="kw">sqrt</span>(<span class="dv">10</span>))
me_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> dat)

<span class="co"># print estimated coefficients to console</span>
nme_mod<span class="op">$</span>coefficients</code></pre></div>
<pre><code>## (Intercept)           X 
##  76.3002047   0.4755264</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">me_mod<span class="op">$</span>coefficients</code></pre></div>
<pre><code>## (Intercept)           X 
##   87.276004    0.255212</code></pre>
<p>We visualize the estimation results and compare with the population regression function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot sample data</span>
<span class="kw">plot</span>(dat<span class="op">$</span>X, dat<span class="op">$</span>Y, 
     <span class="dt">pch=</span><span class="dv">20</span>, 
     <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>
     )

<span class="co"># add population regression function</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(<span class="dv">75</span>,<span class="fl">0.5</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>
      )

<span class="co"># add estimated regression functions</span>
<span class="kw">abline</span>(nme_mod, 
       <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>
       )
<span class="kw">abline</span>(me_mod, 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>
       )

<span class="co"># add legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">lty =</span> <span class="dv">1</span>, 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;purple&quot;</span>, <span class="st">&quot;darkred&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Population&quot;</span>, <span class="st">&quot;No Errors&quot;</span>, <span class="st">&quot;Errors&quot;</span>)
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-200-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice that in the situation without measurement error, the estimated regression function is close to the population regression function. Things are different when we use the error afflicted data on <span class="math inline">\(X\)</span>: both the estimate for the intercept and the estimate for the coefficient on <span class="math inline">\(X\)</span> differ considerably from results obtained using the “clean” data on <span class="math inline">\(X\)</span>. In particular <span class="math inline">\(\widehat{\beta}_1 = 0.255\)</span>. This is evidence for the downward bias. We are in the comfortable situation to know <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma^2_w\)</span>. This allows us to correct for bias using <a href="threats-to-internal-validity-of-multiple-regression-analysis.html#eq:cmembias">(9.1)</a>. Using this information we obtain a biased-corrected estimate <span class="math display">\[\frac{\sigma_X^2 + \sigma_w^2}{\sigma_X^2} \widehat{\beta}_1 = \frac{10+10}{10} 0.255 = 0.51\]</span> which is fairly close to the true coefficient in the population regression function, <span class="math inline">\(\beta_1=0.5\)</span>.</p>
</div>
<div id="missing-data-and-sample-selection" class="section level4 unnumbered">
<h4>Missing Data and Sample Selection</h4>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.5
</h3>
<h3 class="left">
Sample Selection Bias
</h3>
<p>When the sampling process influences the availability of data and when there is a relation of this sampling process to the dependend variable that goes beyond the dependence on the regressors, we say that there is a sample selection bias. This bias is due to correlation between one ore more regressors and the error term. This implies both bias and inconsistency of the OLS estimator.</p>
</div>
<p>There are three cases of Sample selection but only one of which poses a threat to internal validity of a regression study. The three cases are:</p>
<ol style="list-style-type: decimal">
<li><p>Data are missing at random.</p></li>
<li><p>Data are missing based on the value of a regressor.</p></li>
<li><p>Data are missing due to a selection process that is releated to the dependent variable.</p></li>
</ol>
<p>Let us jump back to the example of variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> distributed as stated in equation <a href="threats-to-internal-validity-of-multiple-regression-analysis.html#eq:bvnormd">(9.3)</a> and illustrate all three cases using <tt>R</tt>.</p>
<p>If data are missing at random, this is nothing but loosing observations. For example, loosing <span class="math inline">\(50\%\)</span> of sample would be the same as never having seen the (randomly chosen) half of the sample observed. This does not introduce an estimation bias.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># simulate data</span>
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="kw">rmvnorm</span>(<span class="dv">1000</span>, <span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">100</span>), 
          <span class="dt">sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">10</span>))
          )
  )

<span class="kw">colnames</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>,<span class="st">&quot;Y&quot;</span>)

<span class="co"># mark 500 randomly selected observations</span>
id &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>, <span class="dt">size =</span> <span class="dv">500</span>)

<span class="kw">plot</span>(dat<span class="op">$</span>X[<span class="op">-</span>id], 
     dat<span class="op">$</span>Y[<span class="op">-</span>id], 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="kw">points</span>(dat<span class="op">$</span>X[id], 
       dat<span class="op">$</span>Y[id], 
       <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>, 
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add population regression function</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(<span class="dv">75</span>,<span class="fl">0.5</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>
      )

<span class="co"># add estimated regression function for full sample</span>
<span class="kw">abline</span>(nme_mod)

<span class="co"># estimate model case 1, add regression line</span>
dat &lt;-<span class="st"> </span>dat[<span class="op">-</span>id,]

c1_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(dat<span class="op">$</span>Y <span class="op">~</span><span class="st"> </span>dat<span class="op">$</span>X, <span class="dt">data =</span> dat)
<span class="kw">abline</span>(c1_mod, <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>)

<span class="co"># add legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">lty =</span> <span class="dv">1</span>, 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;purple&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Population&quot;</span>, <span class="st">&quot;Full sample&quot;</span>, <span class="st">&quot;500 obs. randomly selected&quot;</span>)
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-201-1.png" width="672" style="display: block; margin: auto;" /> The gray dots are the <span class="math inline">\(500\)</span> discarded observations. The estimation results when using the remaining observations deviate only marginally from the results obtained using the full sample.</p>
<p>Selecting data randomly based on the value of a regressor has also the effect of reducing the sample size and does not introduce estimation bias. We will now drop observations with <span class="math inline">\(X &gt; 45\)</span>, reestimate the model and compare.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># simulate data</span>
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="kw">rmvnorm</span>(<span class="dv">1000</span>, <span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">100</span>), 
          <span class="dt">sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">10</span>))
          )
  )

<span class="kw">colnames</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>,<span class="st">&quot;Y&quot;</span>)

<span class="co"># mark observations</span>
id &lt;-<span class="st"> </span>dat<span class="op">$</span>X <span class="op">&gt;=</span><span class="st"> </span><span class="dv">45</span>

<span class="kw">plot</span>(dat<span class="op">$</span>X[<span class="op">-</span>id], 
     dat<span class="op">$</span>Y[<span class="op">-</span>id], 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="kw">points</span>(dat<span class="op">$</span>X[id], 
       dat<span class="op">$</span>Y[id], 
       <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>, 
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add population regression function</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(<span class="dv">75</span>,<span class="fl">0.5</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>
      )

<span class="co"># add estimated regression function for full sample</span>
<span class="kw">abline</span>(nme_mod)

<span class="co"># estimate model case 1, add regression line</span>
dat &lt;-<span class="st"> </span>dat[<span class="op">-</span>id,]

c2_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(dat<span class="op">$</span>Y <span class="op">~</span><span class="st"> </span>dat<span class="op">$</span>X, <span class="dt">data =</span> dat)
<span class="kw">abline</span>(c2_mod, <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>)

<span class="co"># add legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">lty =</span> <span class="dv">1</span>, 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;purple&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Population&quot;</span>, <span class="st">&quot;Full sample&quot;</span>, <span class="st">&quot;Obs. with X &lt;= 45&quot;</span>)
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-202-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Note that although we dropped more than <span class="math inline">\(90\%\)</span> of all observations, the estimated regression function is very close to the line estimated based on the full sample.</p>
<p>In the third case we face sample selection bias. We can illustrate this by using the selection procedure <code>dat[which(dat$X &lt;= 55 &amp; dat$Y &gt;= 100)]</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># simulate data</span>
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="kw">rmvnorm</span>(<span class="dv">1000</span>, <span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">100</span>), 
          <span class="dt">sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">10</span>))
          )
  )

<span class="kw">colnames</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>,<span class="st">&quot;Y&quot;</span>)

<span class="co"># mark observations</span>
id &lt;-<span class="st"> </span><span class="kw">which</span>(dat<span class="op">$</span>X <span class="op">&lt;=</span><span class="st"> </span><span class="dv">55</span> <span class="op">&amp;</span><span class="st"> </span>dat<span class="op">$</span>Y <span class="op">&gt;=</span><span class="st"> </span><span class="dv">100</span>)

<span class="kw">plot</span>(dat<span class="op">$</span>X[<span class="op">-</span>id], 
       dat<span class="op">$</span>Y[<span class="op">-</span>id], 
       <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>, 
       <span class="dt">pch =</span> <span class="dv">20</span>,
       <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>,
       <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="kw">points</span>(dat<span class="op">$</span>X[id], 
     dat<span class="op">$</span>Y[id], 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>
     )

<span class="co"># add population regression function</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(<span class="dv">75</span>,<span class="fl">0.5</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>
      )

<span class="co"># add estimated regression function for full sample</span>
<span class="kw">abline</span>(nme_mod)

<span class="co"># estimate model case 1, add regression line</span>
dat &lt;-<span class="st"> </span>dat[id,]

c3_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(dat<span class="op">$</span>Y <span class="op">~</span><span class="st"> </span>dat<span class="op">$</span>X, <span class="dt">data =</span> dat)
<span class="kw">abline</span>(c3_mod, <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>)

<span class="co"># add legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">lty =</span> <span class="dv">1</span>, 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;purple&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Population&quot;</span>, <span class="st">&quot;Full sample&quot;</span>, <span class="st">&quot;X &lt;= 55 &amp; Y &gt;= 100&quot;</span>)
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-203-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that the selection process leads to biased estimation results. There are methods that allow to correct for sample selection bias. However, these methods are beyond the scope of the book and therefore not considered here. The concept of sample selection bias is summarized in Key Concept 9.5.</p>
</div>
<div id="simultaneous-causality" class="section level4 unnumbered">
<h4>Simultaneous Causality</h4>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.6
</h3>
<h3 class="left">
Simultaneous Causality Bias
</h3>
<p>So far we have assumed that the changes in the independent variable <span class="math inline">\(X\)</span> are responsible for changes in the dependent variable <span class="math inline">\(Y\)</span>. When the reverse is also true, say that there is simultaneous causality between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This reverse causality leads to correlaltion between <span class="math inline">\(X\)</span> and the error in the population regression of interest such that the coefficient on <span class="math inline">\(X\)</span> is estimated with a bias.</p>
</div>
<p>Suppose we are interested in estimating the effect of a <span class="math inline">\(20\%\)</span> increase of cigarettes prices on ciggarettes consumption in the United States using a multiple regression model. This may be investigated using the dataset <code>CigarettesSW</code> which is part of the <code>AER</code> package. <code>CigarettesSW</code> is a panel data set on cigarette consumption for all 48 continental US States from 1985-1995 and provides data on economic indicators and average local prices, taxes and per capita pack consumption.</p>
<p>After loading the dataset, we pick observations for the year 1995 and plot logarithmns of average federal and local excise taxes, <code>tax</code>, against pack consumtion, <code>packs</code>, and estimate a simple linear regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load data</span>
<span class="kw">library</span>(AER)
<span class="kw">data</span>(<span class="st">&quot;CigarettesSW&quot;</span>)
c1995 &lt;-<span class="st"> </span><span class="kw">subset</span>(CigarettesSW, year <span class="op">==</span><span class="st"> &quot;1995&quot;</span>)

<span class="co"># estimate model</span>
cigcon_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(packs) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(price), <span class="dt">data =</span> c1995)
cigcon_mod</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(packs) ~ log(price), data = c1995)
## 
## Coefficients:
## (Intercept)   log(price)  
##      10.850       -1.213</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot estimated regression line and data</span>
<span class="kw">plot</span>(<span class="kw">log</span>(c1995<span class="op">$</span>price), <span class="kw">log</span>(c1995<span class="op">$</span>packs),
     <span class="dt">xlab =</span> <span class="st">&quot;ln(Price)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;ln(Consumption)&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Demand for Cigarettes&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>
     )

<span class="kw">abline</span>(cigcon_mod, 
       <span class="dt">col=</span><span class="st">&quot;darkred&quot;</span>, 
       <span class="dt">lwd=</span><span class="fl">1.5</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-204-1.png" width="672" /></p>
<p>Remember from chapter 8 that, due to the log-log specification, in the population regression the coefficient on the logarithm of price is interpreted as the price elasticity of consumption. The estimated coefficient suggest that a <span class="math inline">\(1\%\)</span> increase in cigarettes prices reduces cigarette consumption by about <span class="math inline">\(1.2\%\)</span>, on average. Have we estimated a demand curve? The answer is no: this is a classic example of simultaneous causality (see Key Concept 9.6) since the observations are market equilibria which are determined by both changes in supply and changes in demand such that the price is correlated with the error term and the OLS estimator is biased. We can neither estimate a demand nor a supply curve consistently using this approach.</p>
<p>We will return to this issue (and the dataset) in chapter 12 which presents instrumental variables regression, an approach that allows consistent estimation when there is simultaneous causality.</p>
</div>
<div id="sources-of-inconsistency-of-ols-standard-errors" class="section level4">
<h4><span class="header-section-number">9.2.0.1</span> Sources of Inconsistency of OLS Standard Errors</h4>
<p>There are two central threats to computation of consistent OLS standard errors:</p>
<ol style="list-style-type: decimal">
<li><p>Heteroskedasticity: implications of heteroskedasticiy have been discussed in chapter 5. Heteroskedasticity robust standard errors as computed by the function <code>vcovHC()</code> from the package <code>sandwich</code> produce valid standard errors unter heteroskedasticity.</p></li>
<li><p>Serial correlation: if the population regression error is correlated across observations, we speak of serial correlation. This happens often in applications were repeated observations are used like in panel data studies. As for heteroskedasticity, the <code>vcovHC()</code> package can be used to obtain valid standard errors when there is serial correlation.</p></li>
</ol>
<p>Inconsistently computed standard errors will produce invalid hypothesis tests and wrong confidence intervals. E.g. when testing the null hypothesis that some model coefficient is zero, we cannot trust the outcome anymore because the test may fail to have the significance level of <span class="math inline">\(5\%\)</span> due to a wrongly computed standard error.</p>
<p>Key Concept 9.7 summarizes all discussed threats to internal validity.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.7
</h3>
<h3 class="left">
Threats to Internal Validity of a Regression Study
</h3>
<p>The five primary threats to internal validity of a multiple regression study are:</p>
<ol style="list-style-type: decimal">
<li><p>Omitted variables</p></li>
<li><p>Misspecefication of functional form</p></li>
<li><p>Errors in variables (due to measurement error in regressors)</p></li>
<li><p>Sample selection</p></li>
<li><p>Simultaneous causality</p></li>
</ol>
<p>All these threats lead to failure of the first least squares assumption <span class="math display">\[E(u_i\vert X_{1i},\dots ,X_{ki}) \neq 0\]</span> so that the OLS estimator is biased <em>and</em> inconsistent.</p>
<p>Furthermore, if one does not adjust for heteroskedasticity <em>and</em>/<em>or</em> serial correlation, if present, incorrect standard errors may be a threat to internal validity of the study.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="internal-and-external-validity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="internal-and-external-validity-when-the-regression-is-used-for-forecasting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-ch9.Rmd",
"text": "Edit"
},
"download": ["URFITE.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
