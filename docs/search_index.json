[
["introduction-to-time-series-regression-and-forecasting.html", "14 Introduction to Time Series Regression and Forecasting", " 14 Introduction to Time Series Regression and Forecasting library(dynlm) library(stargazer) Time series data is data that is collected for a single entitity over time time. This is different from cross-section data where data on multiple entities at the same point in time. Time series data allows estimation of the effect on \\(Y\\) of a change in \\(X\\) over time. This is what econometricians call a dynamic causal effect. Let us go back to the application to cigarette consumption of Chapter 12 where we where interest in estimating the effect on cigarette demand of a price increase caused by a raise of the general sales tax. One might use time series data to assess the effect of causal effect of tax hike on smoking both initially and in subsequent periods. Another application of time series data is forecasting. For example, weather services use time series models to predict tomorrow’s average temperatur using todays average termperature of today and the past, or, to motivate an economic example, the central bank is interested in forecasting next month’s unemployment rate. The remainder of the book deals with the econometric techniques for the analysis of time series data and application of the latter to problems of forecasting and estimation of dynamic causal effects. This section covers the basic concepts presented in Chapter 14 of the book, explains how to visualize time series data and demonstrates how to estimate simple autoregressive models where the regressors are past values of the dependent variable. In this context we will also discuss stationarity, an important property which has far-reaching consequences since it determines whether the past of a series has any power in explaining the series’ future. All empirical applications use data on U.S. macroeconomic indicators like Gross Domestic Product (GDP) and the unemployment rate. "],
["using-regression-models-for-forecasting.html", "14.1 Using Regression Models for Forecasting", " 14.1 Using Regression Models for Forecasting What is the difference between estimating models for assessment of causal effects and forecasting? Consider again the simple example of estimating the casual effect on test scores of the student-teacher ratio from Chapter 4. library(AER) data(CASchools) CASchools$STR &lt;- CASchools$students/CASchools$teachers CASchools$score &lt;- (CASchools$read + CASchools$math)/2 mod &lt;- lm(score ~ STR, data = CASchools) mod ## ## Call: ## lm(formula = score ~ STR, data = CASchools) ## ## Coefficients: ## (Intercept) STR ## 698.93 -2.28 As has been stressed in Chapter 6, the estimate of the coefficient on the student-teacher ratio does not have causal interpretation due to omitted variable bias. However, in terms deciding which schooling to send her child to, it might nevertheless be appealig for a parent to use mod for forecasting test scores in schooling districts where no public data about on scores are available. As an example, assume that the average class in a district has \\(25\\) students. There is no such thing like a perfect forecast but the following one-liner might be helpful for the parent to decide. predict(mod, newdata = data.frame(&quot;STR&quot; = 25)) ## 1 ## 641.9377 In a time series context, the parent could use data on present and past years test scores to predict to forecast next years test scores — a typical application for an autoregressive model. "],
["time-series-data-and-serial-correlation.html", "14.2 Time Series Data and Serial Correlation", " 14.2 Time Series Data and Serial Correlation GDP measures the productivity of an economy. It is commonly defined as the value of goods services produced over a given time period. The data set us_macro_quarterly.xlsx is provided by the authors and can be downloaded here. It provides data on quarterly data on US real (i.e. inflation adjusted) GDP from years 1947 to 2004. As before, a good point to start with in pre-estimation is plotting the data. The packages PerformanceAnalytics and quantmod provide some very convenient functions for plotting and computing with time series data. We also load the package xlsx to read the data into R. # attach the packages library(xlsx) library(PerformanceAnalytics) library(quantmod) We begin by importing the data set. # load US macroeconomic data USMacroSWQ &lt;- read.xlsx2(&quot;Data/us_macro_quarterly.xlsx&quot;, sheetIndex = 1, header = T, colClasses = c(&quot;character&quot;, rep(&quot;numeric&quot;, 9)), stringsAsFactors = F) When dealing with time series data in R it is preferable to work with time-series objects that keep track of the frequency of the data and are extensible. In what follows we will use xts objects, see ?xts. Since the data In USMacroSWQ are in quarterly frequence we convert the first column to yearqtr format before generating the xts object GDP. The function Delt() from the package Quantmod computes growth rates. Note that, since the data is quarterly, we anualize the quarterly changes to obtain anual growth rates as \\[Rate_{annual} = (1+Rate_{quarterly})^4-1\\]. # GDP series as xts object GDP &lt;- xts(USMacroSWQ$GDPC96, as.yearqtr(USMacroSWQ$X., format = &quot;%Y:0%q&quot;))[&quot;1960::2013&quot;] # GDP growth series as xts object GDPGrowth &lt;- xts(400 * log(GDP/lag(GDP))) We use chart.TimeSeries() from the package PerformanceAnalytics to generate plots of GDP and GDPGrowth. The following code chunks reproduce figure 14.1 of the book. # reproduce figure 14.1 (a) of the book chart.TimeSeries(log(GDP), date.format = &quot;%Y&quot;, minor.ticks = F, col = &quot;steelblue&quot;, period.areas = c( &quot;1960::1961&quot;,&quot;1970&quot;,&quot;1974::1975&quot;, &quot;1980&quot;,&quot;1981::1982&quot;,&quot;1990::1991&quot;,&quot;2001&quot;,&quot;2007::2009&quot; ), ylab = &quot;Logarithm&quot;, main = &quot;US Quarterly Real GDP&quot; ) # reproduce figure 14.1 (b) of the book chart.TimeSeries(GDPGrowth, date.format = &quot;%Y&quot;, minor.ticks = F, col = &quot;steelblue&quot;, period.areas = c( &quot;1960::1961&quot;,&quot;1970&quot;,&quot;1974::1975&quot;, &quot;1980&quot;,&quot;1981::1982&quot;,&quot;1990::1991&quot;,&quot;2001&quot;,&quot;2007::2009&quot; ), ylab = &quot;Percent at an annual rate&quot;, main = &quot;Growth Rates in US Real GDP&quot; ) Notation, Lags, Differences, Logarithms and Growth Rates If observations of a variable \\(Y\\) are recorded over time, we denote \\(Y_t\\) the value observed at time \\(t\\). The period between two sequential observations \\(Y_t\\) and \\(Y_{t-1}\\) is a unit of time: hours, days, weeks, months, quarters, years and so on. Key Concept 14.1 introduces the essential terminology and notation for time series data we will use. Key Concept 14.1 Lags, First Differences, Logarithms and Growth Rates Previous values of a time series are called lags. The first lag of \\(Y_t\\) is \\(Y_{t-1}\\). The \\(j^th\\) lag of \\(Y_t\\) is \\(Y_{t-j}\\). In R, lags of univariate or multivariate time series are conveniently computed by lag(), see ?lag Sometimes we need to work with a differenced series. The first difference of a series is \\(\\Delta Y_{t} = Y_t - Y_{t-1}\\) — the difference between periods \\(t\\) and \\(t-1\\). If Y is a time series, the first lag is computed as Y-lag(Y). It may be convenient to work with the first difference in logarithms of a series. We denote this by \\(\\Delta \\log(Y_t) = \\log(Y_t) - \\log(Y_{t-1})\\). For a time series Y, this obtained using log(Y/lag(Y)). \\(100 \\Delta Y_t\\) is an approximation for the percentage change between \\(Y_t\\) and \\(Y_{t-1}\\). The definitions made in Key Concept 14.1 are useful because of two properties that are common to many economic time series: Exponential growth: some economic series grow approximately exponentially such that taking logarithm of the series makes them approximately linear. The standard deviation of many economic time series is approximately proportional to their level. Therefore, the standard deviation of the logarithm of such a series is approximately constant. Furthermore, it is common to report rates of growth in macroeconomic series which is why -differences are often used. Table 14.1 of the book present quaterly U.S. GDP, its logarithm, the anualized growth rate and the first lag of the anualized growth rate series for the period 2012:Q1 - 2013:Q1. The following simple function can be used to compute these quantities for time series series. # compute logarithms, annual growth rates and 1st lag of growth rates quants &lt;- function(series) { s &lt;- series return( data.frame(&quot;Lead&quot; = s, &quot;Logarithm&quot; = log(s), &quot;AnnualGrowthRate&quot; = 400 * log(s/lag(s)), &quot;1stLagAnnualGrowthRate&quot; = lag(400 * log(s/lag(s))) ) ) } Notice that the annual growth rate is computed using the approximation \\[ Annual Growth Y_t = 400 \\cdot \\left[\\log(Y_t) - \\log(Y_{t-1})\\right] \\] discussed in Key Concept 14.1. We call quants() on observations for the period 2011 Q3 - 2013 Q1. quants(GDP[&quot;2011-07::2013-01&quot;]) ## Lead Logarithm AnnualGrowthRate X1stLagAnnualGrowthRate ## 2011 Q3 15062.14 9.619940 NA NA ## 2011 Q4 15242.14 9.631819 4.7518062 NA ## 2012 Q1 15381.56 9.640925 3.6422231 4.7518062 ## 2012 Q2 15427.67 9.643918 1.1972004 3.6422231 ## 2012 Q3 15533.99 9.650785 2.7470216 1.1972004 ## 2012 Q4 15539.63 9.651149 0.1452808 2.7470216 ## 2013 Q1 15583.95 9.653997 1.1392015 0.1452808 Autocorrelation Observations of a time series are typically correlated. This type correlation is called autocorrelation or serial correlation. Key Concept 14.2 summarizes the concepts of population autocovariance and population autocorrelation and shows how to compute their sample equivalents. Key Concept 14.2 Autocorrelation and Autocovariance The covariance between \\(Y_t\\) and its \\(j^{th}\\) lag, \\(Y_{t-j}\\), is called the \\(j^{th}\\) autocovariance of the series \\(Y_t\\). The \\(j^{th}\\) autocorrelation coefficient, also called the serial correlation coefficient, measures the correlation between \\(Y_t\\) and \\(Y_{t-j}\\). We thus have \\[\\begin{align*} j^{th} \\text{autocovariance} =&amp; \\, cov(Y_t,Y_{t-j}) \\\\ j^{th} \\text{autocorrelation} = \\rho_j =&amp; \\, corr(Y_t,Y_{t-j}) = \\frac{cov(Y_t,Y_{t-j)}}{\\sqrt{var(Y_t)var{Y_{t-j}}}}. \\end{align*}\\] Population Autocovariance and population autocorrelation can be estimated by \\(\\widehat{cov(Y_t,Y_{t-j})}\\), the sample autocovariance, and \\(\\widehat{\\rho}_j\\), the sample autocorrelation. \\[\\begin{align*} \\widehat{cov(Y_t,Y_{t-j})} =&amp; \\, \\frac{1}{T} \\sum_{t=j+1}^T (Y_t - \\overline{Y}_{j+1:T})(Y_{t-j} - \\overline{Y}_{1:T-j}) \\\\ \\widehat{\\rho}_j =&amp; \\, \\frac{\\widehat{cov(Y_t,Y_{t-j})}}{\\widehat{var(Y_t)}}. \\end{align*}\\] In R the function acf() from the package stats computes the sample autocovariance or the sample autocorrelation function. Using acf() it is straightforward to compute the first four sample autocorrelations of the series GDPGrowth. acf(na.omit(GDPGrowth), lag.max = 4, plot = F) ## ## Autocorrelations of series &#39;na.omit(GDPGrowth)&#39;, by lag ## ## 0.00 0.25 0.50 0.75 1.00 ## 1.000 0.352 0.273 0.114 0.106 This is evidence that there is mild positive autocorrelation in the growth of GDP: if GDP grows faster than average in one period, there is a tendency that it grows faster than average in the following period. Other Examples of Economic Time Series Figure 14.2 of the book presents four plots. The U.S. unemployment rate, the U.S. Dollar / British Pound exchange rate, The logarithm of the Janapese industrial production index as well as daily changes in the Whilshire 5000 stock price index, a financial time series. The next code chunck reproduces the plots of the three macroenomic series and adds percentage changes in the daily values of the New York Stock Exchange Composite index as a fourth one (the data set, NYSESW comes with the AER package). # define series as xts objects USUnemp &lt;- xts(USMacroSWQ$UNRATE, as.yearqtr(USMacroSWQ$X., format = &quot;%Y:0%q&quot;))[&quot;1960::2013&quot;] DollarPoundFX &lt;- xts(USMacroSWQ$EXUSUK, as.yearqtr(USMacroSWQ$X., format = &quot;%Y:0%q&quot;))[&quot;1960::2013&quot;] JPIndProd &lt;- xts(log(USMacroSWQ$JAPAN_IP), as.yearqtr(USMacroSWQ$X., format = &quot;%Y:0%q&quot;))[&quot;1960::2013&quot;] data(&quot;NYSESW&quot;) NYSESW &lt;- xts(Delt(NYSESW)) par(mfrow = c(2, 2)) chart.TimeSeries(USUnemp, date.format = &quot;%Y&quot;, minor.ticks = F, col = &quot;steelblue&quot;, ylab = &quot;Percent&quot;, main = &quot;US Unemployment Rate&quot; ) chart.TimeSeries(DollarPoundFX, date.format = &quot;%Y&quot;, minor.ticks = F, col = &quot;steelblue&quot;, ylab = &quot;Dollar per pound&quot;, main = &quot;U.S. Dollar / B. Pound Exchange Rate&quot; ) chart.TimeSeries(JPIndProd, date.format = &quot;%Y&quot;, minor.ticks = F, col = &quot;steelblue&quot;, ylab = &quot;Logarithm&quot;, main = &quot;Japanese Industrial Production&quot; ) chart.TimeSeries(NYSESW, date.format = &quot;%Y&quot;, minor.ticks = F, col = &quot;steelblue&quot;, ylab = &quot;Percent per Day&quot;, main = &quot;New York Stock Exchange Composite Index&quot; ) Note that the series show quite different characteristics. The unemployment rate increases during recessions and declines in times of economic recoveries and growth. The Dollar/Pund exchange rates shows a deterministic patern until the end of the Bretton Woods system. Japans industrial production exhibits an upward trend and decreasing growth. Daily changes in the New York Stock Exchange composite index seems to be random around the zero line. Sample autocorrelations confirm this conjecture. acf(na.omit(NYSESW), plot = F, lag.max = 10) ## ## Autocorrelations of series &#39;na.omit(NYSESW)&#39;, by lag ## ## 0 1 2 3 4 5 6 7 8 9 ## 1.000 0.040 -0.016 -0.023 0.000 -0.036 -0.027 -0.059 0.013 0.017 ## 10 ## 0.004 The first 10 sample autocorrelation coefficients are very close to zero. Further evidence can be found by looking at the plot generated by acf() by default. acf(na.omit(NYSESW), main = &quot;Sample Autocorrelation for NYSESW Data&quot;) The blue dashed lines represent an approximate \\(95\\%\\) confidence interval if there was no serial correlation. If a sample autocorrelation lies beyond these bands, it is statistically different from zero. For most lags we see that the sample autocorrelation does not exceed the confidence bands and there are only a few that lie marginally beyond the limits. Furthermore, note that NYSWSW series shows a pattern which econometricians call volatility clustering: there are periods of high and periods of low variance. This is common for many financial time series. "],
["autoregressions.html", "14.3 Autoregressions", " 14.3 Autoregressions Growth forecasts are important for many economic entitites. For example, the production industry relies forecasts of GDP growth published by the central bank when deciding on future budgets and production plans. Autoregressive models are heavily used in economic forecasting. An autoregressive model relates a time series variable to its past values. Autoregressive models are heavily used in forecasting. This chapter discusses the basic ideas of autoregressions models, shows how they are estimated and discusses an application to forecasting of GDP growth using R. 14.3.0.1 The First-Order Autoregressive Model It is intuitive that the immediate past of a time series should have power to predict its near future. The simplest autoregressive model uses only the most recent outcome of the time series observed to predict future value. Such a model is called a first-order autoregressive model which is often abbriviated AR(1) where the 1 indicates that the order is one. \\[\\begin{align*} Y_t = \\beta_0 + \\beta_1 Y_{t-1} + u_t \\end{align*}\\] is the AR(1) population model of a time series \\(Y_t\\). For the GDP growth series, an autoregressive model of order one uses only the information on GDP growth observed in the last quarter to predict a future growth rate. The first-order autoregression model of the GDP growth rate can be estimated by computing OLS estimates in the regression of \\(GDPGR_t\\) on \\(GDPGR_{t-1}\\), \\[\\begin{align*} \\widehat{GDPGR}_t = \\beta_0 + \\beta_1 GDPGR_{t-1} + u_t. \\tag{14.1} \\end{align*}\\] Following the book we use data from 1962 to 2012 to estimate (14.1). This is easily done with the function ar.ols() from package stats. # Subset data GDPGRSub &lt;- GDPGrowth[&quot;1962::2012&quot;] # estimate the model ar.ols(GDPGRSub, order.max = 1, demean = F, intercept = T) ## ## Call: ## ar.ols(x = GDPGRSub, order.max = 1, demean = F, intercept = T) ## ## Coefficients: ## 1 ## 0.3384 ## ## Intercept: 1.995 (0.2993) ## ## Order selected 1 sigma^2 estimated as 9.886 We can check that the computations done by ar.ols() are the same as done by lm(). # length of data set N &lt;-length(GDPGRSub) GDPGR_leads &lt;- as.numeric(GDPGRSub[-1]) GDPGR_lags &lt;- as.numeric(GDPGRSub[-N]) # estimate the model armod &lt;- lm(GDPGR_leads ~ GDPGR_lags) armod ## ## Call: ## lm(formula = GDPGR_leads ~ GDPGR_lags) ## ## Coefficients: ## (Intercept) GDPGR_lags ## 1.9950 0.3384 As usual, we may use coeftest() to obtain a robust summary on the estimated regression coefficients. # robust summary coeftest(armod, vcov. = vcovHC(armod, type = &quot;HC0&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.994986 0.349539 5.7075 4.070e-08 *** ## GDPGR_lags 0.338436 0.075812 4.4641 1.339e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Thus the estimated model is \\[\\begin{align*} \\widehat{GDPGR}_t = \\underset{(0.350)}{1.995} + \\underset{(0.076)}{0.338} GDPGR_{t-1} \\tag{14.2}. \\end{align*}\\] Notice that we omit the first observation for \\(GDPGR_{1962 \\ Q1}\\) from the dependent variable vector since \\(GDPGR_{1962 \\ Q1 - 1} = GDPGR_{1961 \\ Q4}\\), is not included in the sample. Similarly, the last observation, \\(GDPGR_{2012 \\ Q4}\\), is excluded from the predictor vector since the data set does not include \\(GDPGR_{2012 \\ Q4 + 1} = GDPGR_{2013 \\ Q1}\\). Put differently, when estimation the model, one observation is lost because of the time series structure of the data. Forecasts and Forecast Errors Suppose a random variable \\(Y_t\\) follows an AR(1) model with an intercept and that you have an OLS estimate of the model on the basis of observations for \\(T\\) periods. Then you may use the AR(1) model to obtain \\(Y_{T+1\\vert T}\\), a forecast for \\(Y_{T+1}\\) where \\[\\begin{align*} \\widehat{Y}_{T+1\\vert T} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 Y_T. \\end{align*}\\] The forecast error is \\[\\begin{align*} \\text{Forecast error} = Y_{T+1} - \\widehat{Y}_{T+1\\vert T}. \\end{align*}\\] Forecasts versus predicted values Note that forecasted values of \\(Y_t\\) are not what we refer to as OLS predicted values of \\(Y_t\\). Also, the forecast error is not an OLS residual. Forecasts and forecast errors are obtained using out-of-sample values while predicted values and residuals are computed for actually in-sample values that were actually observed and used in estimation of the model. The root mean squared forecast error (RMSFE) measures the typical size of the forecast error and is defined as \\[\\begin{align*} RMSFE = \\sqrt{E\\left[\\left(Y_{T+1} - \\widehat{Y}_{T+1\\vert T}\\right)^2\\right]}. \\end{align*}\\] Application to GDP Growth Using (14.2), the estimated AR(1) model of GDP growth, we may perform the forecast for GDP growth in 2013:Q1 (remember that the model was estimated using data for periods 1962:Q1 - 2012:Q4 so 2013:Q1 is an out of sample period). This is done by plugging \\(GDPGR_{2012:Q4} \\approx 0.15\\) in (14.2), \\[\\begin{align*} \\widehat{GDPGR}_{2013:Q1} = 1.995 + 0.348 \\cdot 0.15 = 2.047. \\end{align*}\\] forecast() from the forecast package has some useful features for forecasting of time series data. library(forecast) # GDP growth rate 2012:Q4 new &lt;- data.frame(&quot;GDPGR_lags&quot; = GDPGR_leads[N-1]) # predict GDP growth rate 2013:Q1 forecast(armod, newdata = new) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1 2.044155 -2.036225 6.124534 -4.213414 8.301723 Using forecast() we obtain the same point forecast of about 2.0, along with \\(80\\%\\) and \\(95\\%\\) forecast intervals. We conclude that our AR(1) model forecasts GDP growth to be \\(2\\%\\) in 2013:Q1. How accurate is this forecast? First, notice that the forecast error is pretty big: \\(GDPGR_{2013:Q1} \\approx 1.1\\%\\) while our forecast is \\(2\\%\\). Second, by calling summary() on armod we find that the model explains only little of the variation in the growth rate of GDP and the \\(SER\\) is about \\(3.16\\). Leaving aside forecast uncertainty due to estimation of \\(\\beta_0\\) and \\(\\beta_1\\), the \\(RMSFE\\) must be at least \\(3.16\\%\\) which is pretty inaccuarate. # compute the forecast error forecast(armod, newdata = new)$mean - GDPGrowth[&quot;2013&quot;][1] ## [,1] ## 2013 Q1 0.9049532 # R^2 summary(armod)$r.squared ## [1] 0.1149576 # SER summary(armod)$sigma ## [1] 3.15979 The \\(p^{th}\\)-Order Autoregressive Model For forecasting GPD growth in period \\(t\\), the AR(\\(1\\)) model (14.2) disregards any information in the past of the series that is more distant than one period. An AR(\\(p\\)) model incorporates the information of \\(p\\) lags of the series. The idea is explained in Key Concept 14.3. Key Concept 14.3 Autoregressions An AR(\\(p\\)) model assumes that a time series \\(Y_t\\) can be represented by a linear function of \\(p\\) of its lagged values. We say that \\[\\begin{align*} Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_p Y_{t-p} + u_t. \\end{align*}\\] is an autoregressive model of order \\(p\\) where \\(E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots)=0\\). Following the book, we estimate an AR(\\(2\\)) model of the GDP growth series from 1962:Q1 to 2012:Q4. # estimate AR2 model GDPGR_AR2 &lt;- dynlm(ts(GDPGR_leads) ~ L(ts(GDPGR_leads)) + L(ts(GDPGR_leads), 2)) coeftest(GDPGR_AR2, vcov. = sandwich) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.631747 0.402023 4.0588 7.096e-05 *** ## L(ts(GDPGR_leads)) 0.277787 0.079250 3.5052 0.0005643 *** ## L(ts(GDPGR_leads), 2) 0.179269 0.079951 2.2422 0.0260560 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The estimation yields \\[\\begin{align} \\widehat{GDPGR}_t = \\underset{(0.40)}{1.63} + \\underset{(0.08)}{0.28} GDPGR_{t-1} + \\underset{(0.08)}{0.18} GDPGR_{t-1}. \\tag{14.3} \\end{align}\\] We see that the coefficient on the second lag is significantly different from zero. Note that the fit improves slightly: \\(\\overline{R^2}\\) grows from \\(0.11\\) for the AR(\\(1\\)) model to about \\(0.14\\) and the \\(SER\\) reduces to \\(3.13\\). # R^2 summary(GDPGR_AR2)$r.squared ## [1] 0.1425484 # SER summary(GDPGR_AR2)$sigma ## [1] 3.132122 We may use the AR(\\(2\\)) model to obtain a forecast for GDP growth in 2013:Q1 in the same manner as for the AR(1) model. # AR(2) forecast of GDP growth in 2013:Q1 &quot;Forecast&quot; &lt;- c(&quot;2013:Q1&quot; = coef(GDPGR_AR2) %*% c(1, GDPGR_leads[N-1], GDPGR_leads[N-2])) This leads to a forecast error of roughly \\(-1\\%\\). # compute AR(2) forecast error GDPGrowth[&quot;2013&quot;][1] - Forecast ## [,1] ## 2013 Q1 -1.025358 "],
["can-you-beat-the-market-part-i.html", "14.4 Can You Beat the Market? (Part I)", " 14.4 Can You Beat the Market? (Part I) The thoery of efficient capital markets states stock prices embody all currently available information. If this hypothesis holds, it should not be possible to estimate a useful model for forecasting future stock returns using publicly available information on past returns: if it was possible to forecast the market, traders would be able to make arbitrage, e.g. by relying on an AR(\\(2\\)) model, they would use information that is not already priced-in which would be not consistent with the theory. This idea is presented in the Box Can You Beat the Market? (Part I) on page 582 of the book. This section reproduces the estimation results. We start by importing monthly data from 1931:1 to 2002:12 on excess returns of a broad-based index of stock prices, the CRSP value-weighted index. The data are provided by the authors of the book as an excel sheet which can be downloaded here. # read in data on stock returns SReturns &lt;- read.xlsx2(&quot;Data/Stock_Returns_1931_2002.xlsx&quot;, sheetIndex = 1, colIndex = 1:4, header = T, colClasses = rep(&quot;numeric&quot;, 4), stringsAsFactors = F) We continue by converting the data to an object of class ts. # convert to ts object StockReturns &lt;- ts(SReturns[, 3:4], start = c(1931, 1), end = c(2002, 12), frequency = 12) Next, we estimate AR(\\(1\\)), AR(\\(2\\)) and AR(\\(4\\)) models of excess returns from 1960:1 to 2002:12. # AR(1) SR_AR1 &lt;- dynlm(ExReturn ~ L(ExReturn), data = StockReturns, start = c(1960,1), end = c(2002,12)) # AR(2) SR_AR2 &lt;- dynlm(ExReturn ~ L(ExReturn) + L(ExReturn, 2), data = StockReturns, start = c(1960,1), end = c(2002,12)) # AR(4) SR_AR4 &lt;- dynlm(ExReturn ~ L(ExReturn) + L(ExReturn, 2) + L(ExReturn, 3) + L(ExReturn, 4), data = StockReturns, start = c(1960,1), end = c(2002,12)) After computing robust standard errors, we gather the results in a stargazer table. rob_se &lt;- list( sqrt(diag(sandwich(SR_AR1))), sqrt(diag(sandwich(SR_AR2))), sqrt(diag(sandwich(SR_AR4))) ) stargazer(SR_AR1, SR_AR2, SR_AR4, title = &quot;Autoregressive Models of Monthly Excess Stock Returns&quot;, header = FALSE, model.numbers = F, omit.table.layout = &quot;n&quot;, digits = 2, column.labels = c(&quot;AR(1)&quot;, &quot;AR(2)&quot;, &quot;AR(4)&quot;), dep.var.caption = &quot;Excess returns on the CSRP value-weighted index&quot;, dep.var.labels.include = FALSE, covariate.labels = c(&quot;$excess return_{t-1}$&quot;, &quot;$excess return_{t-2}$&quot;, &quot;$excess return_{t-3}$&quot;, &quot;$excess return_{t-4}$&quot;, &quot;Intercept&quot;), se = rob_se, omit.stat = c(&quot;rsq&quot;) ) Autoregressive Models of Monthly Excess Stock Returns Excess returns on the CSRP value-weighted index AR(1)AR(2)AR(4) excess returnt-10.050.050.05 (0.05)(0.05)(0.05) excess returnt-2-0.05-0.05 (0.05)(0.05) excess returnt-30.01 (0.05) excess returnt-4-0.02 (0.05) Intercept0.310.33*0.33 (0.20)(0.20)(0.20) Observations516516516 Adjusted R20.0010.001-0.002 Residual Std. Error4.33 (df = 514)4.33 (df = 513)4.34 (df = 511) F Statistic1.31 (df = 1; 514)1.37 (df = 2; 513)0.72 (df = 4; 511) The results presented above are consistent with the hypothesis of efficient financial markets: there are no statistically significant coefficients in any of estimated models and the the hypotheses that the respective set lags has no power in explaining today’s returns cannot be rejected. Notice also that \\(\\overline{R^2}\\) is almost zero in all models and even negative for the AR(\\(4\\)) model. This suggests that none of the models is useful for forecasting stock returns. "],
["additional-predictors-and-the-adl-model.html", "14.5 Additional Predictors and The ADL Model", " 14.5 Additional Predictors and The ADL Model Instead of only using the dependent variable’s lags as predictors, an autoregressive distributed lag (ADL) model uses also other lags of other variables for forecasting. The general ADL model is summarized in Key Concept 14.4 Key Concept 14.4 The Autoregressive Distributed Lag Model An ADL(\\(p\\),\\(q\\)) model assumes that a time series \\(Y_t\\) can be represented by a linear function of \\(p\\) of its lagged values and \\(q\\) lags of \\(X_t\\), another time series. We say that \\[\\begin{align*} Y_t =&amp; \\, \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_p Y_{t-p} \\\\ +&amp; \\, \\delta_1 X_{t-1} + \\delta_2 X_{t-2} + \\dots + \\delta_q X_{t-q} X_{t-q} + u_t. \\end{align*}\\] is an autoregressive distributed lag model with \\(p\\) lags of \\(Y_t\\) and \\(q\\) lags of \\(X_t\\) where \\(E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots, X_{t-1}, X_{t-2}, \\dots)=0\\). Forecasting GDP Growth Using the Term Spread Interest on long-term and short term treasury bonds are closly linked to the macroeconomic development. While interest rates on both types of bonds have the same long-run tendencies, they behave quite differently in the short run. The difference in interest rates of two bonds with distinct matuarity is called the term spread. The following code chuncks reproduce figure 14.3 of the book which displays interest rates of 10-year U.S. Treasury bonds and 3 month U.S. Treasury bills from 1960 to 2012. # 3 months Treasury bills interest rate TB3MS &lt;- xts(USMacroSWQ$TB3MS, as.yearqtr(USMacroSWQ$X., format = &quot;%Y:0%q&quot;))[&quot;1960::2012&quot;] # 10 years Treasury bonds interest rate TB10YS &lt;- xts(USMacroSWQ$GS10, as.yearqtr(USMacroSWQ$X., format = &quot;%Y:0%q&quot;))[&quot;1960::2012&quot;] # term spread TSpread &lt;- TB10YS - TB3MS # reproduce figure 14.2 (a) of the book chart.TimeSeries(merge(TB3MS,TB10YS), date.format = &quot;%Y&quot;, minor.ticks = F, col = c(&quot;black&quot;, &quot;steelblue&quot;), lty = c(2,1), period.areas = c( &quot;1960::1961&quot;,&quot;1970&quot;,&quot;1974::1975&quot;, &quot;1980&quot;,&quot;1981::1982&quot;,&quot;1990::1991&quot;,&quot;2001&quot;,&quot;2007::2009&quot; ), ylab = &quot;Percent per annum&quot;, main = &quot;Interest Rates&quot;, legend.loc = &quot;topright&quot; ) # reproduce figure 14.2 (b) of the book chart.TimeSeries(TSpread, date.format = &quot;%Y&quot;, minor.ticks = F, col = &quot;steelblue&quot;, period.areas = c( &quot;1960::1961&quot;,&quot;1970&quot;,&quot;1974::1975&quot;, &quot;1980&quot;,&quot;1981::1982&quot;,&quot;1990::1991&quot;,&quot;2001&quot;,&quot;2007::2009&quot; ), ylab = &quot;Percent per annum&quot;, main = &quot;Term Spread&quot; ) Notice that before recessions, the gap between interests on long-term bonds and short term bills narrows and consequently the term spread declines drastically towards zero or even falls below zero in times of economic stress. This information might be used to improve forecasts of future GDP growth. We check this by estimating an ADL(\\(2\\),\\(1\\)) model and an ADL(\\(2\\),\\(2\\)) model of the GDP growth rate using lags of GDP growth and lags of the term spread as regressors and use both models for forecasting the GDP growth in 2013:Q1. # convert series to ts objects GDPGrowth_ts &lt;- ts(GDPGrowth, start = c(1960, 1), end = c(2013, 4), frequency = 4) TSpread_ts &lt;- ts(TSpread, start = c(1960, 1), end = c(2012, 4), frequency = 4) # join both series ADLdata &lt;- ts.union(GDPGrowth_ts, TSpread_ts) # estimate the ADL(2,1) model of GDP growth GDPGR_ADL21 &lt;- dynlm(GDPGrowth_ts ~ L(GDPGrowth_ts) + L(GDPGrowth_ts, 2) + L(TSpread_ts), start = c(1962, 1), end = c(2012, 4)) coeftest(GDPGR_ADL21, vcov. = sandwich) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.954990 0.486976 1.9611 0.051260 . ## L(GDPGrowth_ts) 0.267729 0.082562 3.2428 0.001387 ** ## L(GDPGrowth_ts, 2) 0.192370 0.077683 2.4763 0.014104 * ## L(TSpread_ts) 0.444047 0.182637 2.4313 0.015925 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The estimated equation of the ADL(\\(2\\), \\(1\\)) model is \\[\\begin{align} \\widehat{GDPGR}_t = \\underset{(0.49)}{0.96} + \\underset{(0.08)}{0.26} GDPGR_{t-1} + \\underset{(0.08)}{0.19} GDPGR_{t-2} + \\underset{(0.18)}{0.44} TSpread_{t-1} \\tag{14.4} \\end{align}\\] Notice that all coefficients are significant at the level of \\(5\\%\\). # 2012:Q3 / 2012:Q4 data on GDP growth and term spread t &lt;- window(ADLdata, c(2012, 3), c(2012, 4)) # ADL(2,1) GDP growth forecast for 2013:Q1 ADL21_forecast &lt;- coef(GDPGR_ADL21) %*% c(1, t[2,1], t[1,1], t[2,2]) ADL21_forecast ## [,1] ## [1,] 2.241689 # Forecast error window(GDPGrowth_ts, c(2013,1), c(2013,1)) - ADL21_forecast ## Qtr1 ## 2013 -1.102487 Model (14.4) predicts the GDP growth in 2013:Q1 to be \\(2.24\\%\\) which leads to a forecast error of \\(-1.10\\%\\). We estimate the ADL(\\(2\\),\\(2\\)) specification to see whether adding additional information on past term spread improves the forecast. # estimate the ADL(2,2) model of GDP growth GDPGR_ADL22 &lt;- dynlm(GDPGrowth_ts ~ L(GDPGrowth_ts) + L(GDPGrowth_ts, 2) + L(TSpread_ts) + L(TSpread_ts, 2), start = c(1962, 1), end = c(2012, 4)) coeftest(GDPGR_ADL22, vcov. = sandwich) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.967967 0.472470 2.0487 0.041800 * ## L(GDPGrowth_ts) 0.243175 0.077836 3.1242 0.002049 ** ## L(GDPGrowth_ts, 2) 0.177070 0.077027 2.2988 0.022555 * ## L(TSpread_ts) -0.139554 0.422162 -0.3306 0.741317 ## L(TSpread_ts, 2) 0.656347 0.429802 1.5271 0.128326 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For the ADL(\\(2\\),\\(2\\)) model we obtain \\[\\begin{align} \\widehat{GDPGR}_t =&amp; \\, \\underset{(0.47)}{0.98} + \\underset{(0.08)}{0.24} GDPGR_{t-1} \\\\ +&amp; \\, \\underset{(0.08)}{0.18} GDPGR_{t-2} + \\underset{(0.42)}{-0.14} TSpread_{t-1} + \\underset{(0.43)}{0.66} TSpread_{t-2}. \\tag{14.5} \\end{align}\\] The coefficients on both lags of the term spread are not significant at the \\(10\\%\\) level. # ADL(2,2) GDP growth forecast for 2013:Q1 ADL22_forecast &lt;- coef(GDPGR_ADL22) %*% c(1, t[2,1], t[1,1], t[2,2], t[1,2]) ADL22_forecast ## [,1] ## [1,] 2.274407 # Forecast error window(GDPGrowth_ts, c(2013,1), c(2013,1)) - ADL22_forecast ## Qtr1 ## 2013 -1.135206 The ADL(\\(2\\),\\(2\\)) forecast of GDP growth in 2013:Q1 is \\(2.27\\%\\) which imples a forecast error of \\(1.14\\%\\). Do the ADL models (14.4) and (14.5) improve upon the simple AR(\\(2\\)) model (14.3) in terms of forecasting GPD growth in 2013:Q1? The answer is yes: while \\(SER\\) and \\(\\overline{R}^2\\) improve only slightly, an \\(F\\)-test on the term spread coefficients in (14.5) provides evidence that the model does better in explaining GPD growth than the AR(\\(2\\)) model as the hypothesis that both coefficients are zero cannot be rejected at the significance level of \\(5\\%\\). # compare adj. R2 c( &quot;Adj.R2 AR(2)&quot; = summary(GDPGR_AR2)$r.squared, &quot;Adj.R2 ADL(2,1)&quot; = summary(GDPGR_ADL21)$r.squared, &quot;Adj.R2 ADL(2,2)&quot; = summary(GDPGR_ADL22)$r.squared ) ## Adj.R2 AR(2) Adj.R2 ADL(2,1) Adj.R2 ADL(2,2) ## 0.1425484 0.1743996 0.1855245 # compare SER c( &quot;SER AR(2)&quot; = summary(GDPGR_AR2)$sigma, &quot;SER ADL(2,1)&quot; = summary(GDPGR_ADL21)$sigma, &quot;SER ADL(2,2)&quot; = summary(GDPGR_ADL22)$sigma ) ## SER AR(2) SER ADL(2,1) SER ADL(2,2) ## 3.132122 3.070760 3.057655 # F-test on coefficients of term spread linearHypothesis(GDPGR_ADL22, c(&quot;L(TSpread_ts)=0&quot;,&quot;L(TSpread_ts, 2)=0&quot;), vcov. = sandwich ) ## Linear hypothesis test ## ## Hypothesis: ## L(TSpread_ts) = 0 ## L(TSpread_ts, 2) = 0 ## ## Model 1: restricted model ## Model 2: GDPGrowth_ts ~ L(GDPGrowth_ts) + L(GDPGrowth_ts, 2) + L(TSpread_ts) + ## L(TSpread_ts, 2) ## ## Note: Coefficient covariance matrix supplied. ## ## Res.Df Df F Pr(&gt;F) ## 1 201 ## 2 199 2 4.4344 0.01306 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Stationarity In general, forecasts an be improved by using mutliple predictors — just as in cross-sectional regression. For time series regressions to yield reliable models, the assumption of stationarity must be fulfilled. Key Concept 14.5 explains what stationarity is. Key Concept 14.5 Stationarity We say that a time series \\(Y_t\\) is stationary if its probability distribution is time independent, that is the joint distribution \\(Y_{s+1}, Y_{s+2},\\dots,Y_{s+T}\\) does not change as \\(s\\) varies, regardless of \\(T\\). Similarly, we say that two time series \\(X_t\\) and \\(Y_t\\) are jointly stationary if the joint distribution of \\((X_{s+1},Y_{s+1}, X_{s+2},Y_{s+2} \\dots, X_{s+T}Y_{s+T})\\) does not depend on \\(s\\), regardless of \\(T\\). In a probabilistic sense, stationarity means that information about how a time series evolves in the future is inherent to its past. If this is not the case, we cannot use the past of a series as a reliable guideline for its future. Time Series Regression with Multiple Predictors The concept of stationarity is a key assumption in the general time series regression model with multiple predictors. Key Concept 14.6 elaborates the model and its assumptions. Key Concept 14.6 Time Series Regression with Multiple Predictors The general time series regression model extends the ADL such that multiple regressors and their lags are included. It uses \\(p\\) lags of the dependent variable and \\(q_l\\) lags of \\(l\\) additional predictors where \\(l=1,\\dots,k\\): \\[\\begin{equation} \\begin{aligned} Y_t =&amp; \\, \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_{p} Y_{t-p} \\\\ +&amp; \\, \\delta_{11} X_{1,t-1} + \\delta_{12} X_{1,t-2} + \\dots + \\delta_{1q} X_{1,t-q} \\\\ +&amp; \\, \\dots \\\\ +&amp; \\, \\delta_{k1} X_{k,t-1} + \\delta_{k2} X_{k,t-2} + \\dots + \\delta_{kq} X_{k,t-q} \\\\ +&amp; \\, u_t \\end{aligned} \\end{equation}\\] For estimation we make the following assumptions: The error term \\(u_t\\) has conditional mean zero given all regressors and their lags: \\[E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots, X_{1,t-1}, X_{1,t-2} \\dots, X_{k,t-1}, X_{k,t-2}, \\dots)\\] This assumption is an extension of the conditional mean zero assumption used for AR and ADL models and guarantees that the gernal time series regression model stated above gives the best forecast of \\(Y_t\\) given its lags, the additional regressors \\(X_{1,t},\\dots,X_{k,t}\\) and their lags. The \\(i.i.d\\) assumption for cross-sectional data is not (entirely) meaningful for time series data. We replace it by the following one with two parts: The \\((Y_{t}, X_{1,t}, \\dots, X_{k,t})\\) have a stationary distribution (the “identically distributed” part of the i.i.d. assumption for cross-setional data). If this does not hold, forecasts may be biased and infernce can be strongly misleading. \\((Y_{t}, X_{1,t}, \\dots, X_{k,t})\\) and \\((Y_{t-j}, X_{1,t-j}, \\dots, X_{k,t-j})\\) become independent as \\(j\\) gets large (the “idependly” distributed part of the i.i.d. assumption for cross-sectional data). This assumption is also called weak dependence. It ensures that, in large samples, the WLLN and the central limit theorem hold. Large outliers are unlikely: \\(E(X_{1,t}^4), E(X_{2,t}^4), \\dots, E(X_{k,t}^4)\\) and \\(E(Y_t^4)\\) have nonzero, finite fourth moments. No perfect multicollinearity. Since many economic time series appear to be nonstationary, assumption 2 of Key Concept 14.6 is a crucial one in applied macroeconomics and finance which is why statistical test for stationarity / nonstationarity have been developed. Chapters 14.6 and 14.7 are devoted to this topic. Statistical inference and the Granger causality test If a \\(X\\) is a useful predictor for \\(Y\\), in a regression of \\(Y_t\\) on lags of its own and lags of \\(X_t\\), not all of the coefficients on the lags on \\(X_t\\) are zero. This concept is called “Granger causality” and is an interesting hypothesis to test. Key Concept 14.7 summarizes the idea. Key Concept 14.7 Granger Causality Tests The Granger causality test (Granger, 1969) is an \\(F\\)-test of the null hypothesis that all lags of a variable \\(X\\) included in a time series regression model do not have predictive power for \\(Y_t\\). The Granger causality test does not test whether \\(X\\) actually causes \\(Y\\) but whether the included lags have are informative in terms of predicting \\(Y\\). Notice that we have already performed a Granger causality test on the coefficients of term spread in (14.5), the ADL(\\(2\\),\\(2\\)) model of GDP growth and concluded that at least on of the first two lags of term spread has predictive power for GDP growth. Forecast Uncertainty and Forecast Intervals In general, it is a good practice to report a measure of the uncertainty inherent the estimation. Uncertainty is particulary of interest when forecasting a time series. For example, consider a simple ADL\\((1,1)\\) model \\[\\begin{align*} Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\delta_1 X_{t-1} + u_t \\end{align*}\\] where \\(u_t\\) is a homoskedastic error term. Then, the forecast error is \\[\\begin{align*} Y_{T+1} - \\widehat{Y}_{T+1\\vert T} = u_{T+1} - \\left[(\\widehat{\\beta}_0 - \\beta_0) + (\\widehat{\\beta}_1 - \\beta_1) Y_T + (\\widehat{\\delta_1} - \\delta_1) X_t \\right] \\end{align*}\\] The mean squared forecast error (MSFE) and the RMFSE are \\[\\begin{align*} MFSE =&amp; \\, E\\left[(Y_{T+1} - \\widehat{Y}_{T+1\\vert T})^2 \\right] \\\\ =&amp; \\, \\sigma_u^2 + var\\left[ (\\widehat{\\beta}_0 - \\beta_0) + (\\widehat{\\beta}_1 - \\beta_1) Y_T + (\\widehat{\\delta_1} - \\delta_1) X_t \\right], \\\\ RMFSE =&amp; \\, \\sqrt{\\sigma_u^2 + var\\left[ (\\widehat{\\beta}_0 - \\beta_0) + (\\widehat{\\beta}_1 - \\beta_1) Y_T + (\\widehat{\\delta_1} - \\delta_1) X_t \\right]}. \\end{align*}\\] A \\(95\\%\\) forecast interval is an interval that covers the true value of \\(Y_{T+1}\\) in \\(95\\%\\) of repeated applications. Note that there is a major difference in computing a confidence interval and a forecast interval: when computing a confidence interval of a point estimate we use large sample approximations that are justified by the central limit theorem and thus are valid for a large range of error term distributions. For computation of a forecast interval of \\(Y_{T+1}\\), however, we must make an additional assumption about the distribution of \\(u_{T+1}\\), the error term in period \\(T+1\\). Assuming that \\(u_{T+1}\\) is normally distributed on can construct a \\(95\\%\\) forecast interval for \\(Y_{T+1}\\) using \\(SE(Y_{T+1} - \\widehat{Y}_{T+1\\vert T})\\), an estimate of the RMSFE: \\[\\begin{align*} \\widehat{Y}_{T+1\\vert T} \\pm 1.96 \\cdot SE(Y_{T+1} - \\widehat{Y}_{T+1\\vert T}) \\end{align*}\\] Of course the computation gets more complicated when the error term is heteroskedastic or if we are interested in computing a forecast interval for \\(T+s, s&gt;1\\). In some applications it is useful to report multiple forecast intervals for each step in range of subsequents periods, see the Box The River of Blood on p. 592 of the book. Such forecast ranges can be visualized in a so-called fan chart. We will not replicate the fan chart presented in Figure 14.2 of book because the underlying model is by far more compley than the simple AR and ADL models treated here. Instead, in the example below we use simulated time series data and estimate an ADL(\\(1\\),\\(1\\)) model which is than used for forecasting the subsequent 25 future outcomes of the series. We choose level = seq(5,99,10) in the call of forecast() such that forecast intervals with levels \\(5\\%,15\\%,\\dots,95%\\) are computed for each point forecast of the series. # simulate time series set.seed(1234) Z &lt;- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200) X &lt;- arima.sim(list(order = c(1, 0, 0), ar = 0.2), n = 200) Y &lt;- 0.7 * Z + 0.7 * X + rnorm(100) # estimate an ADL(1,1) model using arima, see ?arima model &lt;- arima(Y, order = c(2, 0, 0)) # compute points forecasts and prediction intervals for next 25 periods fc &lt;- forecast(model, h = 25, level = seq(5, 99, 10)) # plot fan chart plot(fc, main = &quot;Forecast Fan Chart for ADL(2,2) Model of Simulated Data&quot;, showgap = F, fcol = &quot;red&quot;, flty = 2) The dashed red line shows point forecasts of the series for the next 25 periods based on an \\(ADL(1,1)\\) model and the shaded areas represent the prediction intervals. The degree of shading indicates the level of the prediction interval. The darkest of the blue bands displays the \\(5\\%\\) forecast intervals and the color fades towards grey as the level of the intervals increases. "],
["lag-length-selection-using-information-criteria.html", "14.6 Lag Length Selection Using Information Criteria", " 14.6 Lag Length Selection Using Information Criteria The selection of lag lengths in AR and ADL models can be partially governed by economic theory. However, there are statistical methods that are helpful to determine how many lags should be included as regressors. In general, to many lags inflate the standard errors of coefficient estimates and thus imply an increase in the forecast error while omitting lags that should be included in the model may result in estimation bias. The order of an AR model can be determined using two approaches \\(F\\)-test approach: Estimate an AR(\\(p\\)) model and test significance of the largest lag(s). If the test rejects, drop the respective lag(s) from the model. This approach has the tendency to produce models where the order is too large: in a significance test we always face the risk of rejecting a true null hypothesis! Relying on an information criterion: The circumvent the issue of producing too large models, one may choose the lag order that minimizes one of the following two information criterea: The Bayes information criterion (BIC): \\[BIC(p) = \\log\\left(\\frac{SSR(p)}{T}\\right) + (p + 1) \\frac{\\log(T)}{T}\\] The Akaike information criterion (AIC): \\[AIC(p) = \\log\\left(\\frac{SSR(p)}{T}\\right) + (p + 1) \\frac{2}{T}\\] Both criteria are estimators of the optimal lag length \\(p\\). The lag order \\(\\widehat{p}\\) that minimizes the respective criterion is called the BIC estimate or the AIC estimate of the optimal model order. The basic idea of both criteria is that the \\(SSR\\) decreases as additional lags are added to the model such that the first addend decreases wheres the second addend increases as the lag order grows. One can show that the the \\(BIC\\) is a consistent estimator of the true lag order while the AIC is not which is due to the differing factors in the second addend. Nevertheless, both estimators are used in practice where the \\(AIC\\) is sometimes used as an alternative when the \\(BIC\\) yields a model with too few lags. dynlm() does not compute informatin criteria by default. We will therefore write a short function that reports the \\(BIC\\) (and also the chosen lag order \\(p\\) and \\(R^2\\)) for objects of class dynlm. # compute BIC for AR models BIC &lt;- function(model) { ssr &lt;- sum(model$residuals^2) t &lt;- length(model$residuals) npar &lt;- length(model$coef) return( round( c( &quot;p&quot; = npar - 1, &quot;BIC&quot; = log(ssr/t) + npar * log(t)/t, &quot;R2&quot; = summary(model)$r.squared ), 4 ) ) } Table 14.3 of the book presents a breakdown of how the \\(BIC\\) is computed for AR(\\(p\\)) models of GDP growth with order \\(p=1,\\dots,6\\). The final result can be easily reproduced using sapply() and the function BIC() defined above. # Apply BIC() to an intercept only model of GDP growth BIC( dynlm(ts(GDPGR_leads) ~ 1) ) ## p BIC R2 ## 0.0000 2.4394 0.0000 # loop BIC over models of different orders order &lt;- 1:6 BICs &lt;- sapply(order, function(x) &quot;AR&quot; = BIC( dynlm(ts(GDPGR_leads) ~ L(ts(GDPGR_leads), 1:x)) ) ) BICs ## [,1] [,2] [,3] [,4] [,5] [,6] ## p 1.0000 2.0000 3.0000 4.0000 5.0000 6.0000 ## BIC 2.3486 2.3475 2.3774 2.4034 2.4188 2.4429 ## R2 0.1143 0.1425 0.1434 0.1478 0.1604 0.1591 Note that increasing the lag order increases \\(R^2\\) becaused \\(SSR\\) decreases as additional lags are added to the model but according to the \\(BIC\\), we should decide for the AR(\\(2\\)) model instead of the AR(\\(6\\)) model. It helps to decide wether the decrease in \\(SSR\\) is enough to justify adding another regressor. If we would have to compare a bigger set of models, a convenient way to select the model with the lowest \\(BIC\\) is using the function which.min() # select the AR model with the smallest BIC BICs[, which.min(BICs[2, ])] ## p BIC R2 ## 2.0000 2.3475 0.1425 The \\(BIC\\) may also be used to select lag lengths in time series regression models with multiple predictors. In a model with \\(K\\) coefficients, including the intercept, we have \\[\\begin{align*} BIC(K) = \\log\\left(\\frac{SSR(K)}{T}\\right) + K \\frac{\\log(T)}{T}. \\end{align*}\\] Notice that choosing the optimal model according to the \\(BIC\\) can be computationally demaning because there may be many different combinations of lag lengths when there are more multiple predictors. To motivate an example, we estimate ADL(\\(p\\),\\(q\\)) models of GDP growth where, as above, the additional variable is the term spread between short-term and long-term bonds. We impose the restriction that \\(p=q_1=\\dots=q_k\\) so that only \\(p_{max}\\) models (\\(p=1,\\dots,p_{max}\\)) need to be estimated. In the example below we choose \\(p_{max} = 12\\). # loop BIC over ADL models order &lt;- 1:12 BICs &lt;- sapply(order, function(x) BIC( dynlm(GDPGrowth_ts ~ L(GDPGrowth_ts, 1:x) + L(TSpread_ts, 1:x), start = c(1962, 1), end = c(2012, 4)) ) ) BICs ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## p 2.0000 4.0000 6.0000 8.0000 10.0000 12.0000 14.0000 16.0000 18.0000 ## BIC 2.3411 2.3408 2.3813 2.4181 2.4568 2.5048 2.5539 2.6029 2.6182 ## R2 0.1417 0.1855 0.1950 0.2072 0.2178 0.2211 0.2234 0.2253 0.2581 ## [,10] [,11] [,12] ## p 20.0000 22.0000 24.0000 ## BIC 2.6646 2.7205 2.7664 ## R2 0.2678 0.2702 0.2803 Notice that from the definition of BIC(), for ADL models with \\(p=q\\) it follows that p reports the number of estimated coefficients excluding the intercept. Tus the lag order is obtained by deviding p by 2. # select the ADL model with the smallest BIC BICs[, which.min(BICs[2, ])] ## p BIC R2 ## 4.0000 2.3408 0.1855 The \\(BIC\\) is in favor of the ADL(\\(2\\),\\(2\\)) model (14.5) we have estimated before. "],
["nonstationarity-i-trends.html", "14.7 Nonstationarity I: Trends", " 14.7 Nonstationarity I: Trends If a series is nonstationary, conventional hypothesis tests, confidence intervals and forecasts can be strongly misleading. The assumption of stationarity is violated if a series exhibits trends or breaks and the resulting complications in an econometric analysis depend on the specific type of nonstationarity. This section focuses on time series that exhibit trends. A series is said to exhibit a trend if it fluctuates around a persistent long-term movement. One distinguishes between deterministic and stochastic trends. We say that a trend is deterministic if it is a nonrandom function of time. A trend is said to be stochastic if it is a random function of time. A careful look at the figures we have produced in Chapter 14.2 reveals that many economic time series show a trending behaviour that is probably best modeled by stochastic trends. This is why the book focuses on the treatment of stochastic trends. The Random Walk Model of a Trend The simplest way to model a time series \\(Y_t\\) that has stochastic trend is the random walk \\[\\begin{align*} Y_t = Y_{t-1} + u_t \\tag{14.6} \\end{align*}\\] where the \\(u_t\\) are i.i.d. errors with \\(E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots) = 0\\). Note that \\[\\begin{align*} E(Y_t\\vert Y_{t-1}, Y_{t-2}\\dots) =&amp; \\, E(Y_{t-1}\\vert Y_{t-1}, Y_{t-2}\\dots) + E(u_t\\vert Y_{t-1}, Y_{t-2}\\dots) \\\\ =&amp; \\, Y_{t-1} \\end{align*}\\] so the best forecast for \\(Y_t\\), todays value of \\(Y\\), is \\(Y_{t-1}\\), the observation made yesterday so the difference between \\(Y_t\\) and \\(Y_{t-1}\\) is unpredictable. One can shows that the path followed by \\(Y_t\\) consists of random steps \\(u_t\\), hence it is called a random walk. Assume that \\(Y_0\\), the starting of a random walk is \\(0\\). Another way to write out (14.6) is \\[\\begin{align*} Y_0 =&amp; \\, 0 \\\\ Y_1 =&amp; \\, 0 + u_1 \\\\ Y_2 =&amp; \\, 0 + u_1 + u_2 \\\\ \\vdots &amp; \\, \\\\ Y_t =&amp; \\, \\sum_{i=1}^t u_i. \\end{align*}\\] Therefore we have that \\[\\begin{align*} var(Y_t) =&amp; \\, var(u_1 + u_2 + \\dots + u_t) \\\\ =&amp; \\, t \\sigma_u^2. \\end{align*}\\] Thus the variance of a random walk depends on time which violates the assumption presented in Key Concept 14.5: a random walk is nonstationary. Obviously, (14.6) is a special case of an AR(\\(1\\)) model where the \\(\\beta_1 = 1\\). One can show that a time series that follows an AR(\\(1\\)) model is stationary if \\(\\lvert\\beta_1\\rvert &lt; 1\\). In a general AR(\\(p\\)) model, stationarity is linked to the roots of the polynomial \\[1-\\beta_1 z - \\beta_2 z^2 - \\beta_3 z^3 - \\dots - \\beta_p z^p.\\] If all roots are greater than \\(1\\) in absolute value, the AR(\\(p\\)) series is stationary. If at least one root equals \\(1\\), the AR(\\(p\\)) is said to have a unit root and thus has a stochastic trend. It is straightforward to simulate random walks in R using arima.sim(). The function matplot() is covenient for simple plots of the columns of a matrix. # simulate and plot random walks starting at 0 set.seed(1) RWs &lt;- ts( replicate(n = 4, arima.sim(model = list(order = c(0, 1 ,0)), n = 100) ) ) matplot(RWs, type =&quot;l&quot;, col = c(&quot;steelblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;, &quot;orange&quot;), lty = 1, lwd = 2, main = &quot;Four Random Walks&quot;, xlab = &quot;Time&quot;, ylab = &quot;Value&quot; ) Adding a constant to (14.6) yields \\[\\begin{align*} Y_t = \\beta_0 + Y_{t-1} + u_t \\tag{14.7}, \\end{align*}\\] a random walk with drift. This allows to model the tendency of a series to move in one direction or the other. If \\(\\beta_0\\) is positive, the series drifts upwards and it follows a downward trend if \\(\\beta_0\\) is negative. # simulate and plot random walks with drift starting at 0 set.seed(1) RWsd &lt;- ts( replicate(n = 4, arima.sim(model = list(order = c(0,1,0)), n = 100) ) ) matplot(RWsd, type=&quot;l&quot;, col = c(&quot;steelblue&quot;,&quot;darkgreen&quot;,&quot;darkred&quot;,&quot;orange&quot;), lty = 1, lwd = 2, main = &quot;Four Random Walks with Drift&quot;, xlab = &quot;Time&quot;, ylab = &quot;Value&quot; ) Problems Caused by Stochastic Trends OLS estimation of the coefficients on regressors that have a stochastic trend is problematic because the distribution of the estimator and its \\(t\\)-statistic is nonnormal, even asymptotically. This has various consequences: Downward bias of autoregressive coefficients If \\(Y_t\\) is a random walk, the coefficient \\(\\beta_1\\) can be consistently estimated by OLS but the estimator is biased toward zero. This bias is roughly \\(E(\\widehat{\\beta}_1) = 1 - 5.3/T\\) which is substantial for sample sizes encountered in macroeconomics. This estimation bias causes forecasts of \\(Y_t\\) to perform worse than a pure random walk model. Nonnormally distributed \\(t\\)-statistics The nonnormal distributions of OLS estimates of the coefficient on a stochastic regressors translates to a nonnormal distributions of its \\(t\\)-statistic so that normal critical values are invalid and therefore usual confidence intervals and hypothesis tests are invalid, too, and the true distribution of the \\(t\\)-statistic cannot be readily determined. Spurious Regression When a time series that exhibits a stochastic trend is regressed on another time series that does have a stochastic trend too, the estimated relationship may appear highly significant. This is what econometricians call a spurious relationship. As an example for spurious regression, consider again the green and the red random walks that we have simulated above. We know that there is no relationship between both series: they are purely random and independent of each other. # spurious relationship matplot(RWs[,c(2,3)], lty = 1, lwd = 2, type = &quot;l&quot;, col = c(&quot;darkgreen&quot;,&quot;darkred&quot;), xlab = &quot;Time&quot;, ylab = &quot;&quot;, main = &quot;A Spurious Relationship&quot; ) Imagine we did not have this information and instead conject that the green series is useful for predicting the red series and thus end up estimating the ADL(\\(0\\),\\(1\\)) model \\[\\begin{align*} Red_t = \\beta_0 + \\beta_1 Green_{t-1} + u_t. \\end{align*}\\] # estimate spurious AR model summary( dynlm(RWs[,2] ~ L(RWs[,3])) )$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.459488 0.3635104 -9.516889 1.354156e-15 ## L(RWs[, 3]) 1.047195 0.1450874 7.217687 1.135828e-10 The result is obviously spurious: the coefficient on \\(Green_{t-1}\\) is estimated to be about \\(1\\) and the \\(p\\)-value of \\(1.14 \\cdot 10^{-10}\\) of the corresponding \\(t\\)-test indicates that the coefficient is highly significant while its true value is in fact zero. As an empirical example, consider the U.S. unemployment rate and the Japanese industrial production. Both series show an upward trending behaviour from the mod-1960s through the early 1980s. # Plot U.S. unemployment rate &amp; Japanese industrial production chart.TimeSeries(merge(USUnemp, JPIndProd), date.format = &quot;%Y&quot;, minor.ticks = F, col = c(&quot;darkred&quot;, &quot;steelblue&quot;), main = &quot;Spurious Regression: Macroeconomic Time series&quot;, legend.loc = &quot;topleft&quot;, ylab = &quot;&quot; ) # Estimate regression using data from 1962 to 1985 SR_Unemp1 &lt;- dynlm(ts(USUnemp[&quot;1962::1985&quot;]) ~ ts(JPIndProd[&quot;1962::1985&quot;])) coeftest(SR_Unemp1, vcov = sandwich) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.37452 1.12041 -2.1193 0.0367 * ## ts(JPIndProd[&quot;1962::1985&quot;]) 2.22057 0.29233 7.5961 2.227e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A simple regression of the U.S. unemployment rate on Japanese industrial production using data from 1962 to 1985 yields \\[\\begin{align*} \\widehat{U.S. UR}_t = \\underset{(1.12)}{-2.37} + \\underset{(0.29)}{2.22} \\log(JapaneseIP)_t. \\tag{14.8} \\end{align*}\\] This appears to be a significant relationship: the \\(t\\)-statistic of the coefficient on \\(\\log(JapaneseIP)_t\\) is bigger than 7. # Estimate regression using data from 1986 to 2012 SR_Unemp2 &lt;- dynlm(ts(USUnemp[&quot;1986::2012&quot;]) ~ ts(JPIndProd[&quot;1986::2012&quot;])) coeftest(SR_Unemp2, vcov = sandwich) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.7763 5.4066 7.7270 6.596e-12 *** ## ts(JPIndProd[&quot;1986::2012&quot;]) -7.7771 1.1714 -6.6391 1.386e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 By estimating the same model, this time with data from 1986 to 2012, we obtain \\[\\begin{align*} \\widehat{U.S. UR}_t = \\underset{(5.41)}{41.78} + \\underset{(1.17)}{-7.78} \\log(JapaneseIP)_t \\tag{14.9} \\end{align*}\\] which is suprisingly quite different from (14.8) which indicates a moderate postive relationship, in contrast to the strong negative (14.9). This phenomenon can be attributed to stochastic trends in the series. Since, there is no economic reasoning that relates both trends, both regressions are spurious. Testing for a Unit AR Root A formal test for a stochastic trend has been proposed by Dickey and Fuller (1979) and is therefore termend the Dickey-Fuller test. As discussed above, a time series that follows an AR(\\(1\\)) model with \\(\\beta_1 = 1\\) has a stochastic trend. Thus, the testing problem is \\[\\begin{align*} H_0: \\beta_1 = 1 \\ \\ \\ \\text{vs.} \\ \\ \\ H_1: \\beta_1 &lt; 1. \\end{align*}\\] so the null hypothesis is that the AR(\\(1\\)) has a unit root and the alternative hypothesis is that it is stationary. One often rewrites the AR(\\(1\\)) by substracting \\(Y_{t-1}\\) on both sides: \\[\\begin{align*} Y_t = \\beta_0 + \\beta_1 Y_{t-1} + u_t \\ \\ \\Leftrightarrow \\ \\ \\Delta Y_t = \\beta_0 + \\delta_1 Y_{t-1} + \\end{align*}\\] where \\(\\delta_1 = \\beta_1 - 1\\). The testing problem then becomes \\[\\begin{align*} H_0: \\delta_1 = 0 \\ \\ \\ \\text{vs.} \\ \\ \\ H_1: \\beta_1 &lt; 0 \\end{align*}\\] which is convenient since the corresponding test statistic by default reported by relevant R functions.1 The Dickey-Fuller test can also be applied in an AR(\\(p\\)) model. The Augmented Dickey-Fuller (ADF) test is summarized in Key Concept 14.8. Key Concept 14.8 The ADF Test for a Unit Root Consider the regression \\[\\begin{align*} \\Delta Y_t = \\beta_0 + \\delta Y_{t-1} + \\gamma_1 \\Delta_1 Y_{t-1} + \\gamma_2 \\Delta Y_{t-2} + \\dots + \\gamma_p \\Delta Y_{t-p} + u_t. \\tag{14.10} \\end{align*}\\] The ADF test for a unit autoregressive root test the hypothesis \\(H_0: \\delta = 0\\) (stochastic trend) against the one-sided alternative \\(H_1: \\delta &lt; 0\\) (stationarity) using the usual OLS \\(t\\)-statistic. If it is assumed that \\(Y_t\\) is stationary around a deterministic linear time trend, the model is augmented by the regressor \\(t\\), that is \\(Y_t\\) becomes \\[\\begin{align*} \\Delta Y_t = \\beta_0 + at + \\delta Y_{t-1} + \\gamma_1 \\Delta_1 Y_{t-1} + \\gamma_2 \\Delta Y_{t-2} + \\dots + \\gamma_p \\Delta Y_{t-p} + u_t \\tag{14.11} \\end{align*}\\] where again \\(H_0: \\delta = 0\\) is tested against \\(H_1: \\delta &lt; 0\\). The optimal lag length \\(p\\) can be estimated using information criterea. Notice that in the regression (14.10), \\(p=0\\) (that is no lags of \\(\\Delta Y_t\\) are used as regressors) corresponds to a simple AR(\\(1\\)). Under the null hypothesis, the \\(t\\)-statistic for \\(H_0: \\delta = 0\\) does not have a normal distribution. The cricital values can only be obtained from simulation and differ for regressions (14.10) and (14.11) since the distribution of the ADF test statistic is sensitive to whether a deterministic time trend is included or not. Critical Values for the ADF Statistic Key Concept 14.8 states that the critical values for the ADF test in the regressions (14.10) and (14.11) can only be determined using simulation. The idea of the simulation study is to simulate a large number of ADF test test statistics and use them to estimate quantiles of their distribution. This sections shows how this is feasible whithin R. First, consider an AR(\\(1\\)) model with drift. The procedure is as follows: Simulate \\(N\\) random walks with \\(n\\) observations using the data generating process \\[\\begin{align*} Y_t =&amp; \\, \\beta_0 + \\beta_1 Y_{t-1} + u_t, \\end{align*}\\] \\(t=1,\\dots,n\\) where \\(N\\) and \\(n\\) are large numbers. For each random walk, estimate the regression \\[\\begin{align*} \\Delta Y_t =&amp; \\, \\beta_0 + \\beta_1 Y_{t-1} + u_t, \\end{align*}\\] compute ADF test statistic. Save all \\(N\\) test statistics in a vector. Estimate quantiles of the distribution of the ADF test statistics using the \\(N\\) test statistics obtained from the simulation. For the case with drift and linear time trend we replace the data generating process by \\[\\begin{align*} Y_t =&amp; \\, \\beta_0 + \\alpha t + \\beta_1 Y_{t-1} + u_t \\end{align*}\\] and estimate \\[\\begin{align*} \\Delta Y_t =&amp; \\, \\beta_0 + \\alpha t + \\beta_1 Y_{t-1} + u_t, \\end{align*}\\] Loosely speaking, the precision of the estimated quantiles depends on two factors: \\(n\\), the length of the underlying series and \\(N\\), the number of test statistics used. Since we are interested in estimating quantiles of the asymptotic distribution (the Dickey-Fuller distribution) of the ADF test statistic so both using many observations and large number of simulated test statistics will increase the precision of the estimated quantiles. We choose \\(n=N=1000\\) as the computational burden grows quickly with \\(n\\) and \\(N\\). # repititions N &lt;- 1000 # observations n &lt;- 1000 # define drift an trend drift &lt;- 0.5 trend &lt;- 1:n # simulate N random walks with drift RWD &lt;- ts(replicate(n = N, drift + arima.sim(model = list(order = c(0,1,0)), n = n - 1) ) ) # compute ADF test statistics and store them in &#39;ADFD&#39; ADFD &lt;- numeric(N) for(i in 1:ncol(RWD)) { ADFD[i] &lt;- summary( dynlm(diff(RWD[,i],1) ~ L(RWD[,i],1)) )$coef[2,3] } # simulate N random walks with drift + trend RWDT &lt;- ts(replicate(n = N, trend + drift + arima.sim(model = list(order = c(0,1,0)), n = n - 1) ) ) # compute ADF test statistics and store them in &#39;ADFDT&#39; ADFDT &lt;- numeric(N) for(i in 1:ncol(RWDT)) { ADFDT[i] &lt;- summary( dynlm(diff(RWDT[,i],1) ~ L(RWDT[,i],1) + trend(RWDT[,i], scale = F)) )$coef[2,3] } # estimate quantiles for ADF regression with drift round(quantile(ADFD, c(0.1,0.05,0.01)),2) ## 10% 5% 1% ## -2.62 -2.83 -3.39 # estimate quantiles for ADF regression with drift + trend round(quantile(ADFDT, c(0.1,0.05,0.01)),2) ## 10% 5% 1% ## -3.11 -3.43 -3.97 The estimated quantiles are close to the large sample critical values of the ADF test statistic reported in Table 14.4 of the book. Table 14.1: Large Sample Critical Values of ADF Test Deterministic Regressors 10% 5% 1% Intercept only -2.57 -2.86 -3.43 Intercept and time trend -3.12 -3.41 -3.96 The results show that using standard normal critical values might be fatal: the 5% critical value of the standard normal distribution is \\(-1.64\\) but for the Dickey-Fuller distributions the estimated critical values are \\(-2.87\\) (drift) and \\(-3.43\\) (drift and linear time trend). This implies that a true null hypothesis (the series has a stochastic trend) would be rejected far to often if the inappropriate normal critical values were used. We may use the simulated test statistics for a graphical comparison of the standard normal density and (estimates of) both Dickey-Fuller densities. # plot standard normal density curve(dnorm(x), from = -6, to = 3, ylim = c(0,0.6), lty = 2, ylab = &quot;Density&quot;, xlab = &quot;t-Statistic&quot;, main = &quot;Distributions of ADF Test Statistics&quot;, col = &quot;darkred&quot;, lwd = 2) # plot density estimates of both Dickey-Fuller distributions lines(density(ADFD), lwd = 2, col = &quot;darkgreen&quot;) lines(density(ADFDT), lwd = 2, col = &quot;blue&quot;) # add a legend legend(&quot;topleft&quot;, c(&quot;N(0,1)&quot;,&quot;Drift&quot;,&quot;Drift+Trend&quot;), col = c(&quot;darkred&quot;,&quot;darkgreen&quot;,&quot;blue&quot;), lty = c(2,1,1), lwd = 2 ) The deviations from the standard normal distribution are significant: both Dickey-Fuller distributions are skewed to the left and have a havier left tail. Does U.S. GDP Have a Unit Root? As an empirical example, we use the ADF test to assess whether there is a stochastic trend in U.S. GDP using the regression \\[\\begin{align*} \\Delta\\log(GDP_t) = \\beta_0 + \\alpha t + \\beta_1 \\log(GDP_{t-1}) + \\beta_2 \\Delta \\log(GDP_{t-1}) + \\beta_3 \\Delta \\log(GDP_{t-2}) + u_t. \\end{align*}\\] # log GDP series LogGDP &lt;- ts(log(GDP[&quot;1962::2012&quot;])) # estimate model coeftest( dynlm(diff(LogGDP) ~ trend(LogGDP, scale = F) + L(LogGDP) + diff(L(LogGDP)) + diff(L(LogGDP),2)) ) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.27877045 0.11793233 2.3638 0.019066 * ## trend(LogGDP, scale = F) 0.00023818 0.00011090 2.1476 0.032970 * ## L(LogGDP) -0.03332452 0.01441436 -2.3119 0.021822 * ## diff(L(LogGDP)) 0.08317976 0.11295542 0.7364 0.462371 ## diff(L(LogGDP), 2) 0.18763384 0.07055574 2.6594 0.008476 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The estimation yields \\[\\begin{align*} \\Delta\\log(GDP_t) =&amp; \\, \\underset{(0.118)}{0.28} + \\underset{(0.0001)}{0.0002} t + \\underset{(0.014)}{-0.033} \\log(GDP_{t-1}) \\\\ &amp; \\, + \\underset{(0.113)}{0.083} \\Delta \\log(GDP_{t-1}) + \\underset{(0.071)}{0.188} \\Delta \\log(GDP_{t-2}) + u_t, \\end{align*}\\] so the ADF test statistic is \\(t=-0.033/0.014 = - 2.35\\). The corresponding \\(5\\%\\) critical value from table 14.1 is \\(-3.41\\) so we cannot reject the hypothesis that \\(\\log(GDP=\\)has a stochastic trend in favor of the alternative that it is stationary around a deterministic linear time trend. The \\(t\\)-statistic of the Dickey-Fuller test is computed using homoskedasticity-only standard errors since under the null hypothesis, the usual \\(t\\)-statistic is robust to heteroskedasticity.↩ "],
["nonstationarity-ii-breaks.html", "14.8 Nonstationarity II: Breaks", " 14.8 Nonstationarity II: Breaks "]
]
