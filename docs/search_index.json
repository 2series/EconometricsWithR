[
["hypothesis-tests-and-confidence-intervals-in-multiple-regression.html", "7 Hypothesis Tests and Confidence Intervals in Multiple Regression", " 7 Hypothesis Tests and Confidence Intervals in Multiple Regression This chapter discusses methods that allow to quantify the sampling uncertainty inherent to the OLS estimator of the coefficients in multiple regression models. The basis for this are hypothesis tests and confidence intervals which, just as for the simple linear regression model, can be computed using basic R functions. We will also tackle the issue of testing joint hypotheses on these coefficients. "],
["hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html", "7.1 Hypothesis Tests and Confidence Intervals for a Single Coefficient", " 7.1 Hypothesis Tests and Confidence Intervals for a Single Coefficient First, we will discuss how to compute standard errors, how to test hypotheses and how to construct confidence intervals for a single regression coefficient \\(\\beta_j\\) in a multiple regression model. The basic idea is summarized in Key Concept 7.1. Key Concept 7.1 Testing the Hypothesis \\(\\beta_j = \\beta_{j,0}\\) Against the Alternative \\(\\beta_j \\neq \\beta_{j,0}\\) Compute the standard error of \\(\\hat{\\beta_j}\\) Compute the \\(t\\)-statistic, \\[t = \\frac{\\hat{\\beta}_j - \\beta_{j,0}} {SE(\\hat{\\beta_j})}\\] Compute the \\(p\\)-value, \\[p\\text{-value} = 2 \\Phi(-|t^{act}|)\\] where \\(t^{act}\\) is the value of the \\(t\\)-statistic actually computed. Reject the hypothesis at the 5% significance level if the \\(p\\)-value is less than \\(0.05\\) or, equivalently, if \\(|t^{act}| &gt; 1.96\\). The standard error and (typically) the \\(t\\)-statistic and the corresponding \\(p\\)-value for testing \\(\\beta_j = 0\\) are computed automatically by suitable R functions, e.g. by summary(). It is straightforward to verify that principles of testing a single hypothesis about the significance of a coefficient in the multiple regression model is just as in in the simple regression model. You can easily see this by inspecting the coefficient summary of the regression model \\[ TestScore = \\beta_0 - \\beta_1 \\times size - \\beta_2 \\times english + u \\] already discussed in Chapter 6. Let us review this: model &lt;- lm(score ~ size + english, data = CASchools) summary(model) ## ## Call: ## lm(formula = score ~ size + english, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.845 -10.240 -0.308 9.815 43.461 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 686.03224 7.41131 92.566 &lt; 2e-16 *** ## size -1.10130 0.38028 -2.896 0.00398 ** ## english -0.64978 0.03934 -16.516 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.46 on 417 degrees of freedom ## Multiple R-squared: 0.4264, Adjusted R-squared: 0.4237 ## F-statistic: 155 on 2 and 417 DF, p-value: &lt; 2.2e-16 You may check that these quantities are computed as in the simple regression model by computing the \\(t\\)-statistics or \\(p\\)-values by hand using the output above and R as a calculator. For example, using the definition of the \\(p\\)-value for a two-sided test as given in Key Concept 7.1, we can confirm the \\(p\\)-value for a test of the hypothesis that the coeffiecient \\(\\beta_0\\), that is the coefficient on (intercept), is zero to be approximatley zero. 2*(1-pnorm(abs(92.566))) ## [1] 0 Remember that, given a vector of quantiles, pnorm() calculates the associated probabilities for the standard normal distribution by default which is suitable here since we approximate with the standard normal distribution. Key Concept 7.2 Confidence Intervals for a Single Coefficient in Multiple Regression A \\(95\\%\\) two-sided confidence interval for the coefficient \\(\\beta_j\\) is an interval that contains the true value of \\(\\beta_j\\) with a \\(95 \\%\\) probability; that is, it contains the true value of \\(\\beta_j\\) in \\(95 \\%\\) of all randomly drawn samples. Equivalently, it is the set of values of \\(\\beta_j\\) that cannot be rejected by a \\(5 \\%\\) two-sided hypothesis test. When the sample size is large, the \\(95 \\%\\) confidence interval for \\(\\beta_j\\) is \\[\\left[\\hat{\\beta_j}- 1.96 \\times SE(\\hat{\\beta}_j), \\hat{\\beta_j} + 1.96 \\times SE(\\hat{\\beta_j})\\right].\\] "],
["an-application-to-test-scores-and-the-student-teacher-ratio.html", "7.2 An Application to Test Scores and the Student-Teacher Ratio", " 7.2 An Application to Test Scores and the Student-Teacher Ratio Let us take a look at the regression from section 6.3 again. Computing confidence intervals for the individual coefficients in the multiple regression model can be done analogously as in the simple regression model using the function confint(). model &lt;- lm(score ~ size + english, data = CASchools) confint(model) ## 2.5 % 97.5 % ## (Intercept) 671.4640580 700.6004311 ## size -1.8487969 -0.3537944 ## english -0.7271113 -0.5724424 We note that \\(95\\%\\) confidence intervals for all three coefficients are computed. If we want to compute confidence intervals at another level, say \\(\\alpha=0.1\\), we just have to set the argument level in our call of confint() accordingly. confint(model, level = 0.9) ## 5 % 95 % ## (Intercept) 673.8145793 698.2499098 ## size -1.7281904 -0.4744009 ## english -0.7146336 -0.5849200 The output now reports the desired \\(90\\%\\) confidence intervals for all model coefficients. Knowing how to use R to make inference about the coefficients in multiple regression models, you can now answer the following question: Can the null hypothesis that a change in the student-teacher ratio, size, has no significant influence on test scores, scores, — if we control for the percentage of students learning English in the district, english, — be rejected at the \\(10\\%\\) and the \\(5\\%\\) level of significance? The outputs above tell us that zero is not an element of the computed confidence intervals for the coefficient of size such that we can reject the null hypothesis at significance levels of \\(5\\%\\) and \\(10\\%\\). Note that rejection at the \\(5\\%\\)-level implies rejection at the \\(10\\%\\) level (why?). The same conclusion can be made when beholding the \\(p\\)-value for size: \\(0.00398 &lt; 0.05 = \\alpha\\). The \\(95\\%\\) confidence interval tells us that we can be \\(95\\%\\) confident that a one-unit decrease in the student-teacher ratio has an effect on test scores that lies in the interval with a lower bound of \\(-1.8488\\) and an upper bound of \\(-0.3538\\). Another Augmentation of the Model What is the average effect on test scores of reducing the student-teacher ratio when the expenditures per pupil and the percentage of english learning pupils are held constant? We can pursue this question by augmenting our model equation by an additional regressor that is a measure for spendings per pupil. Using ?CASchools we find that CASchools contains the variable expenditure which provides expenditures per student. The model we want the estimate now is \\[ TestScore = \\beta_0 + \\beta_1 \\times size + \\beta_2 \\times english + \\beta_3 \\times expenditure + u \\] with \\(expenditure\\) the total amount of expenditures per pupil in the district (thousands of dollars). Let us now estimate the model: # Scale expenditure to thousands of dollars CASchools$expenditure &lt;- CASchools$expenditure/1000 # estimate the model model &lt;- lm(score ~ size + english + expenditure, data = CASchools) summary(model) ## ## Call: ## lm(formula = score ~ size + english + expenditure, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.340 -10.111 0.293 10.318 43.181 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 649.57795 15.20572 42.719 &lt; 2e-16 *** ## size -0.28640 0.48052 -0.596 0.55149 ## english -0.65602 0.03911 -16.776 &lt; 2e-16 *** ## expenditure 3.86790 1.41212 2.739 0.00643 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.35 on 416 degrees of freedom ## Multiple R-squared: 0.4366, Adjusted R-squared: 0.4325 ## F-statistic: 107.5 on 3 and 416 DF, p-value: &lt; 2.2e-16 We see that the estimated effect of a one unit change in the student-teacher ratio on test scores with expenditures per pupil and the share of english learning pupils held constant is \\(-0.29\\) which is rather small. What is more, the coefficient on \\(size\\) is not significantly different from zero anymore even at the level of \\(10\\%\\) since \\(p\\text{-value}=0.55\\). Can you come up with an interpretation for these findings (see Chapter 7.1 of the book)? The insignificance of \\(\\beta_1\\) could be due to a larger standard error of \\(\\hat{\\beta}_1\\) resulting from adding \\(expenditure\\) to the model so that we are estimating the true coefficent on \\(size\\) less precisely. This illustrates the issue of strongly correlated regressors (imperfect multicollinearity). The correlation between \\(size\\) and \\(expenditure\\) can be computed using cor(). # compute the sample correlation between &#39;size&#39; and &#39;expenditure&#39; cor(CASchools$size, CASchools$expenditure) ## [1] -0.6199822 Altogether we conclude that the new model provides no evidence that changing the student-teacher ratio, e.g. by hiring new teachers, has any effect on the test scores while keeping expenditures per student and the share of English learners constant at the same time. "],
["joint-hypothesis-testing-using-the-f-statistic.html", "7.3 Joint Hypothesis Testing Using the \\(F\\)-Statistic", " 7.3 Joint Hypothesis Testing Using the \\(F\\)-Statistic The estimated model is \\[ \\widehat{TestScore} = \\underset{(15.21)}{649.58} -\\underset{(0.48)}{0.29} \\times size - \\underset{(0.04)}{0.66} \\times english + \\underset{(1.41)}{3.87} \\times expenditure. \\] Now, can we reject the hypothesis that the coefficient on \\(size\\) and the coefficient on \\(expenditure\\) are zero? To answer this question, we have to resort to methods that allow joint hypothesis testing. A joint hypothesis imposes restrictions on multiple regression coefficients. This is different from conducting individual \\(t\\)-tests where a resitriction is imposed on a single coefficient. Chapter 7.2 of the book explains why testing hypotheses about the model coefficients one at a time is different from testing them jointly. The homoskedasticity-only \\(F\\)-Statistic is given by \\[ F = \\frac{(SSR_{restricted} - SSR_{unrestricted})/q}{SSR_{unrestricted} / (n-k_{unrestricted}-1)} \\] with \\(SSR_{restricted}\\) being the sum of squared residuals from the restricted regression, i.e. the regression where we impose the restriction. \\(SSR_{unrestricted}\\) is the sum of squared residuals from the full model, \\(q\\) is the number of restriction under the null and \\(k\\) is the number of regressors in the unrestricted regression. Luckily, it is fairly easy to conduct \\(F\\)-tests in R. We can use the function linearHypothesis() which is contained in the package car. # estimate the multiple regression model model &lt;- lm(score ~ size + english + expenditure, data = CASchools) # execute the function on the model object and provide both linear restrictions # to be tested as strings linearHypothesis(model, c(&quot;size=0&quot;, &quot;expenditure=0&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## size = 0 ## expenditure = 0 ## ## Model 1: restricted model ## Model 2: score ~ size + english + expenditure ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 418 89000 ## 2 416 85700 2 3300.3 8.0101 0.000386 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the output we can infer that the \\(F\\)-statistic for this joint hypothesis test is about \\(8.01\\) and the corresponding \\(p\\)-value is \\(0.0004\\). Thus, we can reject the null hypothesis that both coefficients are zero at any level of significance commonly used in practice. Note The standard output of a model summary also reports an \\(F\\)-statistic and the corresponding \\(p\\)-value. The null hypothesis belonging to this \\(F\\)-test is that all of the population coefficients in the model except for the intercept are zero, so the hypotheses pair is \\[H_0: \\beta_1=0, \\ \\beta_2 =0, \\ \\beta_3 =0 \\quad \\text{vs.} \\quad H_1: \\beta_j \\neq 0 \\ \\text{for at least one} \\ j=1,2,3.\\] This is also called the overall regression F-statistic and the null hypothesis is obviously different from testing if only \\(\\beta_1\\) and \\(\\beta_3\\) being zero. We will now check whether the \\(F\\)-statistic belonging to the \\(p\\)-value listed in the model’s summary coincides with the result reported by linearHypothesis(). # execute the function on the model object and provide the linear restriction # to be tested as strings linearHypothesis(model, c(&quot;size=0&quot;, &quot;english=0&quot;, &quot;expenditure=0&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## size = 0 ## english = 0 ## expenditure = 0 ## ## Model 1: restricted model ## Model 2: score ~ size + english + expenditure ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 419 152110 ## 2 416 85700 3 66410 107.45 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Access the F-statistic from the model&#39;s summary summary(model)$fstatistic ## value numdf dendf ## 107.4547 3.0000 416.0000 The test rejects the null hypothesis that the model has no power in explaining test scores. "],
["confidence-sets-for-multiple-coefficients.html", "7.4 Confidence Sets for Multiple Coefficients", " 7.4 Confidence Sets for Multiple Coefficients Based on the \\(F\\)-statistic that we have previously encountered, we can specify confidence sets. Confidence sets are analogous to confidence intervals for single coefficients. As such, confidence sets consist of combinations of coefficients that contain the true combination of coefficients in, \\(95\\%\\) say, of all cases if we could draw random samples infinitely, just as in the univariate case. Put differently, a confidence set is the set of all coefficient combinations for which we cannot reject a joint null hypothesis tested using an \\(F\\)-test. If we consider two coefficients, their confidence set is an ellipse which is centered around the point defined by both coefficient estimates. Again, there is a very convenient way to plot the confidence set for two coefficients of model objects, namely the function confidenceEllipse() which is also coming with the car package. In the following, we plot the \\(95\\%\\) confidence ellipse for the coefficients on size and expenditure from the regression conducted above. By specifying the additional argument fill, the confidence set is colored. # Draw the 95% confidence set for coefficients on size and expenditure confidenceEllipse(model, fill = T, which.coef = c(&quot;size&quot;, &quot;expenditure&quot;), main = &quot;95% Confidence Set&quot; ) We see that the ellipse is centered around \\((-0.29, 3.87)\\), the pair of coefficients estimates on \\(size\\) and \\(expenditure\\). What is more, \\((0,0)\\) is not element of the \\(95\\%\\) confidence set so that we can reject \\(H_0: \\beta_1 = 0, \\ \\beta_3 = 0\\). "],
["model-specification-for-multiple-regression.html", "7.5 Model Specification for Multiple Regression", " 7.5 Model Specification for Multiple Regression Choosing a regression specifaction, i.e. selecting the variables to be included in a regression model, can be quite cumbersome. However, there are some guidelines on how to proceed. The goal is clear: obtaining an unbiased and precise estimate of the causal effect of interest. As a starting point, one should think about omitted variables, that is to avoid a possible estimation bias by using suitable control variables (omitted variables bias in the context of multiple regression is explained in Key Concept 7.3). A second step could be to compare different regression model specifications by measures of fit. However, as we shall see one should not rely solely on \\(\\overline{R^2}\\). Key Concept 7.3 Omitted Variable Bias in Multiple Regression Omitted variable bias is the bias in the OLS estimator that arises when one or more included regressors are correlated with an omitted variable. For omitted variable bias to arise, two things must be true: At least one of the included regressors must be correlated with the omitted variable. The omitted variable must be a determinant of the dependent variable, \\(Y\\). We will now discuss an example were we face a potential omitted variable bias in a multiple regression model: Consider again the estimated regression equation \\[ \\widehat{TestScore} = \\underset{(8.7)}{686.0} - \\underset{(0.43)}{1.10} \\times size - \\underset{(0.031)}{0.650} \\times english. \\] We are interested in estimating the causal effect of class size on test score. In this estimation, there might be a bias due to omitting “outside learning opportunities” from our regression sice a measure like this could be a determinant of the students test scores and could also be correlated with both regressors already included in the model (so that both conditions of Key Concept 7.3 are fulfilled). “outside learning opportunities” are a complicated concept that is difficult to quantify. A surrogate we can consider instead is the students’ economic backgroud which should be strongly related to the outside learning opportunities: think of wealthy parents that are able to provide time and/or money for private tuition of their children. We thus augment the model with the variable lunch, the percentage of students that qualify for a free or subsidized lunch in school due to family incomes below a certain threshold, and estimate the model again. # estimate the model and print the summary to console model &lt;- lm(score ~ size + english + lunch, data = CASchools) summary(model) ## ## Call: ## lm(formula = score ~ size + english + lunch, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.849 -5.151 -0.308 5.243 31.501 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 700.14996 4.68569 149.423 &lt; 2e-16 *** ## size -0.99831 0.23875 -4.181 3.54e-05 *** ## english -0.12157 0.03232 -3.762 0.000193 *** ## lunch -0.54735 0.02160 -25.341 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.08 on 416 degrees of freedom ## Multiple R-squared: 0.7745, Adjusted R-squared: 0.7729 ## F-statistic: 476.3 on 3 and 416 DF, p-value: &lt; 2.2e-16 Thus, the estimated regression line is \\[ \\widehat{TestScore} = \\underset{(4.7)}{700.15} - \\underset{(0.24)}{1.00} \\times size - \\underset{(0.032)}{0.12} \\times english + \\underset{(0.022)}{0.55} \\times lunch. \\] We observe no substantial changes in the conclusion about the effect of \\(size\\) on \\(TestScore\\): the coefficient on \\(size\\) changes by only \\(0.1\\) and keeps its significance. Although the difference in estimated coefficients is not big in this case, it might be a good idea to keep lunch in the model to make the assumption of conditional mean independence more credible (see Chapter 7.5 of the book). Model Specification in Theory and in Practice Key Concept 7.4 names some common pitfalls when using \\(R^2\\) and \\(\\overline{R^2}\\) to evaluate the predictive ability of regression models. Key Concept 7.4 \\(R^2\\) and \\(\\overline{R^2}\\): What They Tell You — and What They Don’t The \\(R^2\\) and \\(\\overline{R^2}\\) tell you whether the regressors are good at predicting, or “explaining” the values of the independent variable in the sample of data at hand. if the \\(R^2\\) (or \\(\\overline{R^2}\\)) is nearly \\(1\\), then the regressors produce good prediction of the dependent variable in that sample, in the sense that the variance of OLS residuals is small compared to the variance of the dependent variable. If the \\(R^2\\) (or \\(\\overline{R^2}\\)) is nearly \\(0\\), the opposite is true. The \\(R^2\\) and \\(\\overline{R^2}\\) do not tell you whether: An included variable is statistically significant. The regressors are the true cause of the movements in the dependent variable There is omitted variable bias. You have chosen the most appropriate set of regressors. For example, think of regressing \\(TestScore\\) on \\(PLS\\) which measures the available parking lot space in thousand square feet. You are likely to observe a significant coefficient of reasonable magnitude and moderate to high values for \\(R^2\\) and \\(\\overline{R^2}\\). The reason for this is that parking lot space is correlated with many determinants of the test score like location, class size, financial endowment and so on. Although we do not have observations on \\(PLS\\), we can use R to generate some relatively realistic data. # Generate observations for parking lot space CASchools$PLS &lt;- 5 + 0.6 * CASchools$expenditure + 0.6*CASchools$income ## [1] 13.614000 5.894400 5.386800 5.386800 5.448200 6.249000 3.946200 ## [8] 4.904400 4.431000 6.968000 5.358600 4.431000 3.201000 4.967400 ## [15] 5.778000 4.472400 3.729600 4.658400 4.213200 3.419400 4.764600 ## [22] 5.778000 4.443000 5.778000 4.811400 5.113800 4.789909 4.383000 ## [29] 5.360400 5.132400 3.967800 7.445400 4.875969 6.858600 7.033335 ## [36] 6.799500 5.758800 8.734800 13.236000 5.825400 6.889767 4.906800 ## [43] 4.904400 4.500000 6.030300 4.399200 7.548946 9.106200 7.265477 ## [50] 7.071000 8.437200 6.283200 8.043070 7.381080 9.242443 8.257200 ## [57] 8.510400 5.319000 7.798200 6.955200 4.967400 6.543386 8.040375 ## [64] 6.648600 5.449200 8.034000 8.760975 6.669600 5.053800 6.999000 ## [71] 4.383000 8.238360 6.669600 6.218657 4.189800 8.680680 8.791800 ## [78] 6.377400 5.868600 6.393600 7.162800 4.431000 6.405600 5.983200 ## [85] 4.954800 5.337600 7.237200 5.657337 5.352200 6.058800 7.098000 ## [92] 6.931800 11.824200 4.263000 8.235000 6.058800 8.257200 5.799000 ## [99] 6.158400 6.941520 4.443000 7.246200 6.769800 6.021000 5.030400 ## [106] 8.178000 8.476600 6.759675 8.034000 7.162800 4.954800 8.536036 ## [113] 6.283200 8.545741 8.439600 9.580800 6.158400 6.361482 6.313800 ## [120] 6.742800 6.033600 6.709800 4.431000 8.147400 5.778000 9.164400 ## [127] 8.420550 6.143800 5.691000 9.017925 5.868600 7.915200 7.329600 ## [134] 8.445600 15.292400 8.920200 5.983200 8.746800 4.431000 7.405800 ## [141] 7.458800 8.943600 7.601940 6.199800 7.183200 7.696200 6.024000 ## [148] 8.001000 5.912400 6.855600 9.078600 8.746800 6.160800 9.355714 ## [155] 6.742800 9.030825 9.247906 6.648600 6.058800 8.578980 7.081800 ## [162] 5.983200 9.245400 9.176280 9.319757 9.775800 7.725000 9.973800 ## [169] 7.529930 8.555143 10.995600 7.649486 6.855600 5.298000 6.705600 ## [176] 8.536800 9.016200 6.312600 11.175600 13.517400 10.979034 7.131450 ## [183] 5.955600 8.518200 5.778000 6.383400 8.685000 8.080200 5.799000 ## [190] 7.624800 10.693800 10.293600 11.753782 8.518200 7.147200 8.031600 ## [197] 6.932400 11.415599 9.844200 8.434800 11.440800 12.462000 6.158400 ## [204] 8.746800 6.121200 7.501200 8.343600 5.991600 8.227200 6.058800 ## [211] 6.774600 7.935000 8.178000 8.773800 5.265600 6.202800 9.793800 ## [218] 6.393800 8.746800 11.178360 6.669600 8.758600 9.295800 9.763200 ## [225] 8.178000 7.436400 9.198600 7.599600 8.227200 10.668201 6.334000 ## [232] 6.313800 7.945800 13.483800 9.410793 14.090250 6.989400 9.813601 ## [239] 8.444400 8.062200 11.002800 6.855600 9.449950 9.467400 8.437200 ## [246] 7.304400 7.740000 6.582600 10.461120 8.856000 8.761800 8.113400 ## [253] 10.001400 8.113350 7.510800 10.995600 6.879600 6.058800 9.219000 ## [260] 6.186800 10.191975 5.767200 12.666300 9.883987 11.021400 9.992467 ## [267] 11.126533 8.508000 10.817733 8.151600 7.584000 6.661800 4.500000 ## [274] 9.303600 13.704840 11.893876 9.808200 9.775800 7.948800 8.406000 ## [281] 8.497800 9.462600 8.690000 8.525400 9.100200 7.550550 6.330600 ## [288] 10.173600 10.625400 9.763200 10.740480 7.100400 11.172120 7.458600 ## [295] 9.100200 12.525450 6.383400 15.017772 10.054681 9.178200 10.421667 ## [302] 13.257600 13.705199 10.991925 15.018000 13.174200 11.278100 6.383400 ## [309] 10.504800 11.155800 6.669600 8.458600 7.584000 12.561857 8.508000 ## [316] 7.095600 11.202600 10.399200 10.504800 9.343200 9.340350 9.451200 ## [323] 12.282000 5.825400 6.669600 9.792599 13.180199 5.360400 11.998200 ## [330] 11.296338 10.871199 14.200425 13.283400 12.327600 5.691000 7.100400 ## [337] 11.279400 6.330600 9.311400 12.979036 12.657450 9.100200 11.470200 ## [344] 10.572001 9.100200 6.385800 12.443433 12.939600 10.593600 10.625400 ## [351] 16.689600 11.155800 8.557800 13.654800 7.584000 6.742800 10.173600 ## [358] 10.625400 15.373200 11.235600 8.065800 9.108000 11.470200 8.227200 ## [365] 10.504800 7.100400 7.963800 9.813601 11.175600 14.239800 7.760400 ## [372] 8.343600 9.256800 9.228600 13.517400 14.283659 14.761800 21.704400 ## [379] 20.073001 12.288134 12.080400 11.607900 21.704400 13.718001 10.625400 ## [386] 8.506200 24.655799 13.174200 9.638400 15.442200 12.284401 21.486001 ## [393] 8.140200 12.053400 10.969801 15.143829 15.037800 18.377100 16.485130 ## [400] 18.504000 20.495700 25.938000 16.768650 29.963400 33.196799 21.288599 ## [407] 20.580599 23.177142 21.205199 18.631200 24.158400 21.486001 25.938000 ## [414] 30.406199 24.241200 17.230199 25.040465 14.239800 5.971200 7.501200 + CASchools$size/100 ## [1] 0.1788991 0.2152466 0.1869723 0.1735714 0.1867133 0.2140625 0.1950000 ## [8] 0.2089412 0.1994737 0.2080556 0.2123810 0.2100000 0.2060000 0.2000822 ## [15] 0.1802778 0.2025196 0.1697787 0.1650980 0.2270402 0.1991111 0.1833333 ## [22] 0.2261905 0.1944828 0.2505263 0.2067544 0.1868235 0.2284553 0.1926667 ## [29] 0.1925000 0.2054545 0.2060697 0.2107268 0.2153581 0.1990400 0.2119407 ## [36] 0.2186535 0.1832965 0.1622857 0.1917857 0.2027737 0.2298614 0.2044444 ## [43] 0.1982085 0.2320522 0.1926697 0.2330189 0.2118829 0.2087179 0.1901749 ## [50] 0.2191938 0.2010124 0.2147651 0.2006579 0.2037509 0.2244648 0.2289524 ## [57] 0.2049797 0.2000000 0.2225658 0.2156436 0.1947737 0.1767002 0.2194756 ## [64] 0.2178339 0.1914000 0.1811050 0.2068243 0.2262361 0.2178650 0.1858293 ## [71] 0.2154545 0.2115289 0.1663333 0.2114438 0.1978182 0.1898373 0.1766767 ## [78] 0.1775499 0.1527273 0.1400000 0.2059613 0.1631169 0.2112796 0.1748801 ## [85] 0.1788679 0.1930676 0.2089231 0.2128684 0.2019560 0.2495000 0.1813043 ## [92] 0.2000000 0.1872951 0.1825000 0.1899257 0.1988764 0.1937895 0.2046259 ## [99] 0.2229157 0.2070474 0.1906005 0.2023247 0.1969012 0.2036254 0.1975422 ## [106] 0.1937977 0.2292351 0.1937340 0.1915516 0.2130000 0.1830357 0.2107926 ## [113] 0.1879121 0.1962662 0.1959016 0.2087187 0.2111500 0.2008452 0.1991049 ## [120] 0.1781285 0.1813333 0.1922221 0.1866071 0.1960000 0.1928384 0.2281818 ## [127] 0.1880922 0.2137363 0.2002041 0.2149861 0.1542857 0.2240000 0.2012709 ## [134] 0.1903797 0.1734216 0.1701863 0.2080000 0.2115385 0.1845833 0.1914082 ## [141] 0.1940766 0.1956896 0.2150120 0.1752941 0.1643017 0.1979654 0.1718613 ## [148] 0.1761589 0.2012537 0.2216667 0.1996154 0.1903945 0.1522436 0.2114475 ## [155] 0.1964390 0.2104869 0.2017544 0.2139130 0.2000832 0.2029137 0.1766667 ## [162] 0.1822055 0.2027100 0.2019895 0.2138424 0.2097368 0.2000000 0.1715328 ## [169] 0.2234977 0.2217007 0.1818182 0.1895714 0.1974533 0.1642623 0.1662540 ## [176] 0.1638177 0.2007416 0.1799544 0.1939130 0.1642857 0.1672949 0.2441345 ## [183] 0.1826415 0.1895504 0.2103896 0.2074074 0.1810000 0.1984615 0.2160000 ## [190] 0.2244242 0.2301438 0.1774892 0.1828664 0.1926544 0.2266667 0.1929412 ## [197] 0.1736364 0.1982143 0.2043378 0.2103721 0.1992462 0.1900986 0.2382222 ## [204] 0.1936908 0.1982857 0.1525886 0.1716129 0.2181333 0.1907471 0.2578512 ## [211] 0.1821261 0.1816606 0.1697297 0.2150087 0.2060000 0.1699029 0.2077955 ## [218] 0.1551247 0.1988506 0.2139882 0.2049751 0.1936376 0.1765958 0.2101796 ## [225] 0.1905565 0.2253846 0.2110787 0.2005135 0.1420176 0.1847687 0.1863542 ## [232] 0.2094595 0.2108548 0.1869288 0.2086808 0.1982558 0.1975000 0.1950000 ## [239] 0.1839081 0.1878676 0.1977018 0.1933333 0.2146392 0.2308492 0.2106299 ## [246] 0.1868687 0.2077024 0.1930556 0.2013280 0.2066963 0.2228155 0.2060027 ## [253] 0.2082734 0.1922492 0.1765477 0.1700000 0.1649773 0.1978261 0.2230216 ## [260] 0.1773077 0.2044836 0.2037169 0.2016479 0.2161538 0.2056143 0.1995551 ## [267] 0.2118387 0.1881042 0.2057838 0.1832461 0.1882063 0.2081633 0.2000000 ## [274] 0.1968182 0.1939018 0.2092732 0.1994437 0.2079109 0.1920354 0.1902439 ## [281] 0.1762058 0.2023715 0.1929374 0.1882998 0.2033949 0.1922900 0.1789130 ## [288] 0.1951881 0.1908451 0.1993548 0.1887326 0.2014179 0.2355637 0.2146479 ## [295] 0.1919101 0.2013080 0.2580000 0.1877774 0.1910981 0.1970109 0.1861594 ## [302] 0.2099721 0.2000000 0.2098325 0.2164262 0.2002967 0.1981140 0.1800000 ## [309] 0.1935811 0.2017912 0.2111986 0.2338974 0.2218182 0.1994283 0.1778826 ## [316] 0.1470588 0.1904077 0.2089195 0.1983851 0.1952191 0.2068622 0.1818182 ## [323] 0.1889224 0.2488889 0.1858065 0.1804000 0.1773399 0.2145455 0.1992342 ## [330] 0.2033942 0.2254608 0.2110344 0.1819743 0.2010768 0.1915984 0.1954545 ## [337] 0.2088889 0.1839150 0.1917990 0.1939771 0.2167827 0.1928889 0.2034927 ## [344] 0.2096416 0.1946039 0.1928572 0.2091979 0.2090021 0.2059574 0.1937500 ## [351] 0.1995122 0.1884973 0.1811787 0.1918341 0.2200000 0.2158416 0.2038889 ## [358] 0.1629310 0.1827778 0.1937472 0.1890909 0.1640693 0.1559140 0.1870694 ## [365] 0.1832985 0.1790235 0.1891156 0.2032497 0.2002457 0.2400000 0.1760784 ## [372] 0.1934853 0.1967847 0.1872861 0.1588235 0.2005491 0.1798825 0.1696629 ## [379] 0.1923937 0.1919586 0.1959906 0.2054348 0.1858848 0.1560419 0.1529304 ## [386] 0.1765537 0.1757976 0.2233333 0.1875000 0.1810241 0.2025641 0.1880207 ## [393] 0.1877231 0.2040521 0.1865079 0.2070707 0.2200000 0.1769978 0.2148329 ## [400] 0.1670103 0.1957567 0.1725806 0.1737526 0.1734931 0.1626228 0.1770045 ## [407] 0.2012882 0.1826539 0.1454214 0.1915261 0.1736574 0.1513899 0.1784266 ## [414] 0.1540704 0.1886534 0.1647413 0.1786263 0.2188586 0.2020000 0.1903640 + rnorm(nrow(CASchools), sd = 1) ## [1] -0.684764603 -1.095682586 0.130790021 -1.263658479 0.738842699 ## [6] -2.073234410 -2.150330108 -0.163760454 0.642429171 0.479397261 ## [11] -0.997398952 -0.872645921 -0.246647550 -0.723506755 -0.343724270 ## [16] -0.260033473 0.918108210 -0.807420026 1.005226366 0.225582239 ## [21] 1.747164453 0.057598553 -0.167597605 0.390169131 -0.165966204 ## [26] -1.177242637 1.016585229 -1.662247262 -0.304434712 -0.212043832 ## [31] 0.206359902 0.080891352 0.636668967 0.797239827 0.332228635 ## [36] 0.164362769 -1.932129397 0.126537912 -0.256603031 -0.893559409 ## [41] 0.138744000 -0.292853215 -0.041938815 -0.767915856 0.383838827 ## [46] -0.028586471 -0.369403581 1.204250730 0.409838076 -2.038198133 ## [51] 0.970181755 1.632086761 -0.982257418 0.695618397 -1.571772209 ## [56] 1.346091870 0.256550349 -0.679910100 0.603187944 0.180379912 ## [61] -0.687661359 -0.950803855 -2.017528032 -0.898849721 0.902816733 ## [66] -0.574267062 1.351562515 -1.130210215 1.824744453 1.126350431 ## [71] 0.784806583 -0.625571116 2.387686134 0.114264682 0.208256733 ## [76] -0.603261289 0.397785180 -0.726041269 -0.960500552 -0.928405359 ## [81] -1.151164841 -0.492600546 0.100445899 0.234423114 -0.443345132 ## [86] 0.439341345 -0.110004711 -1.563857809 0.334052397 -2.267353954 ## [91] -0.545637221 -1.071885505 -0.513441640 -1.558857024 0.022456360 ## [96] 0.238060882 -1.060288092 0.259231918 -0.247726846 -0.379930420 ## [101] -0.950145903 0.550510752 -0.458005289 -0.227880346 -1.215149028 ## [106] -0.387434271 -0.574095184 0.939863806 -0.493817633 -0.876306635 ## [111] -0.637124920 -0.568766150 -0.595434607 0.592580511 0.915041069 ## [116] 0.676002569 -0.889888474 0.685847253 1.203503615 1.252422771 ## [121] 0.281835003 0.076208021 0.432140460 -0.448859257 0.532562525 ## [126] 1.288715403 -0.430703787 0.466905378 0.609655954 -0.787101808 ## [131] -0.408317346 0.534651504 0.548865306 0.086660663 1.659597821 ## [136] 0.220145997 -0.652861408 0.995493876 -0.968285570 -1.851447981 ## [141] -0.587840313 -1.463826062 0.719751903 0.537845801 -0.393129444 ## [146] 0.578143469 -0.393324233 -0.153497366 -1.210033322 -1.482073498 ## [151] -1.334563409 0.151845380 -1.198426761 -0.839091233 -0.613089007 ## [156] 0.837303781 -0.669462911 -0.126441078 1.928462350 0.362346634 ## [161] 0.150318152 -0.698985084 0.556352841 0.717730583 -1.241618582 ## [166] 0.754506597 -0.374439358 -0.734911827 -1.287230373 -0.799314339 ## [171] -0.247742036 1.070215475 -1.039956347 -0.058830756 -0.387642913 ## [176] -0.828891023 -1.581503044 1.013567161 -0.708857531 2.886130921 ## [181] -0.323334956 -0.405533210 -0.805958681 1.213872482 0.826962029 ## [186] -1.562443114 0.470362043 -0.006793334 -0.382742842 -1.009907310 ## [191] -0.174454341 0.196747368 -0.704159030 -0.505996285 -0.201564521 ## [196] -1.927791968 0.493790400 -1.227803777 0.666626985 -0.920708216 ## [201] -0.002430735 0.038584453 1.535772091 0.878332340 0.143182783 ## [206] -0.494419086 0.054020927 1.184651114 1.198002188 0.053399717 ## [211] 1.237604751 -0.614753862 -0.112047180 0.403310225 -2.262559845 ## [216] -0.408515990 -0.384897120 0.382004775 -1.593279414 0.260031320 ## [221] 1.914436417 1.196620868 -1.097963936 -0.069639168 -0.578875290 ## [226] -0.018285534 1.183987125 0.062246428 -0.730426455 1.685789394 ## [231] -0.247679504 0.981583341 0.050939771 0.181821247 -0.676278260 ## [236] -0.798106023 -1.170235157 0.458655601 0.914224077 0.772225163 ## [241] -0.745974980 -0.807966349 1.002125486 -0.361488962 1.673929270 ## [246] 0.823185163 -1.111345557 -0.545439700 1.387326176 1.219220564 ## [251] 0.184129552 0.068723082 -0.644767020 -0.809929939 -0.286699432 ## [256] -1.044376929 0.090147704 0.440444270 0.249283151 -1.782711428 ## [261] -0.789435970 1.591718357 0.464174042 -1.366519690 0.515090506 ## [266] 3.095396728 -2.106679911 -0.248219396 -0.288419281 0.709503134 ## [271] -0.681190000 -0.825862683 0.744971390 -0.219847882 1.822217112 ## [276] -1.810040036 -0.124774640 -0.509767260 -0.538255787 -0.380855740 ## [281] -0.245724812 0.407816558 1.224321451 1.084595439 0.393657516 ## [286] -2.699374804 0.429426136 0.497358143 -1.500956564 -0.471692736 ## [291] -0.175812786 1.291725610 1.263911860 0.168591577 -0.847819420 ## [296] 0.427618132 -0.369618781 0.833045783 1.598104966 -0.785652867 ## [301] -0.633317171 -0.209854438 0.240288704 -0.058006392 2.128450728 ## [306] 0.348153438 0.258984333 -0.364636225 -0.459751015 0.796726392 ## [311] -0.282487925 0.567385213 -2.696532038 1.242240340 -1.008171886 ## [316] 1.340742045 -0.868440569 -0.025070786 0.583684417 1.143124743 ## [321] 0.992914119 1.365407823 0.377733917 -1.052470383 0.193177796 ## [326] 1.825205025 -0.927376638 0.596338630 1.131564170 1.389139470 ## [331] -0.750065082 -0.987804164 -0.776962122 0.092821237 1.504327554 ## [336] 0.863043125 -0.323392171 0.310847244 -0.888834720 0.429640804 ## [341] -0.150276374 0.562218299 -0.937019297 -0.866312382 1.062180853 ## [346] 0.765771409 -0.160478857 -0.059013263 0.033891791 0.424264572 ## [351] 1.000542518 0.437958903 -0.466812224 -0.761185372 -0.946862432 ## [356] 0.658340842 -1.547751216 -0.229632993 -0.476667591 -0.116662217 ## [361] -0.516013656 0.212152447 1.665453724 -0.056798559 -0.703417953 ## [366] -0.657110264 1.152065938 0.229979597 -1.083297633 0.872222961 ## [371] -0.022586329 -0.346843746 -0.723963631 1.108702633 1.087194331 ## [376] 0.960741103 -0.750869282 0.172004922 1.154973577 1.616708795 ## [381] 1.268444457 0.779823336 -2.122732558 -1.108635975 0.346629900 ## [386] 0.554232802 -0.456031847 0.168124530 0.292356009 1.074569377 ## [391] 0.139702034 1.996046560 1.780748907 1.145873315 1.218322957 ## [396] -0.352127744 -0.456497420 -1.657438251 0.409559810 -1.097360627 ## [401] -0.066461771 -1.417005807 -0.816107169 -0.635333564 -1.541997276 ## [406] -0.820133712 1.325088842 0.014290827 -1.336913694 1.110701885 ## [411] -0.306482529 -2.008279426 0.015754810 -0.045253965 0.473229629 ## [416] -1.243092175 -1.545241676 -0.123645326 0.948471625 -1.378329309 # plot parking lot space against test score plot(CASchools$PLS, CASchools$score, xlab = &quot;Parking Lot Space&quot;, ylab = &quot;Test Score&quot;, pch = 20, col = &quot;steelblue&quot; ) # regress test score on PLS summary(lm(score ~ PLS, data = CASchools)) ## ## Call: ## lm(formula = score ~ PLS, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -50.146 -14.206 0.689 13.513 50.127 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.236e+02 7.732e+00 80.652 &lt; 2e-16 *** ## PLS 9.581e-03 2.405e-03 3.984 7.99e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.72 on 418 degrees of freedom ## Multiple R-squared: 0.03659, Adjusted R-squared: 0.03428 ## F-statistic: 15.87 on 1 and 418 DF, p-value: 7.989e-05 \\(PLS\\) is generated as a linear function of \\(expenditure\\), \\(income\\), \\(size\\) and a random disturbance. Therefore the data suggest that there is some postive relationship between parking lot space and test score. In fact, when estimating the model \\[\\begin{align} \\widehat{TestScore} = \\beta_0 + \\beta_1 \\times PLS + u \\tag{7.1} \\end{align}\\] using lm() we find that the coefficient on \\(PLS\\) is positive and significantly different from zero. Also \\(R^2\\) and \\(\\overline{R^2}\\) are about \\(0.47\\) which is a lot more than the roughly \\(0.05\\) observed when regressing the test scores on the class sizes only. This suggests that increasing the parking lot space boosts a school’s test scores and that model (7.1) does even better in explaining heterogeneity in the dependent variable than a model with \\(size\\) as the only regressor. Keeping in mind how \\(PLS\\) is constructed this comes of no surprise. It is evident that the high \\(R^2\\) cannot be used to the conclude that the estimated relation between parking lot space and test scores is causal: the (relatively) high \\(R^2\\) is due to correlation between \\(PLS\\) and other determinantes and/or control variables. Increasing parking lot space is not an appropriate measure to generate more learning success! "],
["analysis-of-the-test-score-data-set.html", "7.6 Analysis of the Test Score Data Set", " 7.6 Analysis of the Test Score Data Set Chapter 6 and some of the previous sections have stressed that it is important to include control variables in regression models if it is plausible that there are omitted factors that cannot be measured directly. Recall that in our example of test scores we are interested in estimating the causal effect of a change in the student-teacher ratio on test scores. In what follows, we will provide an example how to use multiple regression models in order to alleviate omitted variable bias and we will demonstrate how to report results using R. So far we have considered two variables that control for unobservable student characteristics which correlate with the student-teacher ratio and are assumed to have an impact on test scores: \\(english\\), the percentage of english learning students \\(lunch\\), the share of students that qualify for a subsidized or even a free lunch at school Another new variable provided with CASchools is \\(calworks\\), the percentage of students that qualify for the CalWorks income assistance programm. Students eligible for CalWorks live in families with a total income below the threshold for the subsidized lunch programm so both variables are indicators for the share of economically disadvantaged children. Using R we can confirm that both indicators are highly correlated: # estimate the correlation between &#39;calworks&#39; and &#39;lunch&#39; cor(CASchools$calworks, CASchools$lunch) ## [1] 0.7394218 There is no unabigious way to proceed when deciding which variable to use. In any case it is not a good idea to use both variables as regressors having in mind consequences of colinearity. Therefore, we will also consider alternative model specifications. For a start, we plot student characteristics against test scores. par(mfrow = c(1,3)) m &lt;- rbind(c(1, 2), c(3, 0)) layout(m) plot(score ~ english, data = CASchools, col = &quot;steelblue&quot;, pch = 20, xlim = c(0, 100), cex.main = 0.8, main = &quot;Percentage of English language learners&quot;) plot(score ~ lunch, data = CASchools, col = &quot;steelblue&quot;, pch = 20, cex.main = 0.8, main = &quot;Percentage qualifying for reduced price lunch&quot;) plot(score ~ calworks, data = CASchools, col = &quot;steelblue&quot;, pch = 20, xlim = c(0, 100), cex.main = 0.8, main = &quot;Percentage qualifying for income assistance&quot;) We see that all relationships are negative. We can use R to estimate the correlation coefficients. # estimate correlation between student characteristics and test scores cor(CASchools$score, CASchools$english) ## [1] -0.6441238 cor(CASchools$score, CASchools$lunch) ## [1] -0.868772 cor(CASchools$score, CASchools$calworks) ## [1] -0.6268533 We will consider \\(5\\) different model equations: \\[\\begin{align*} (I) \\quad \\widehat{TestScore} =&amp; \\, \\beta_0 + \\beta_1 \\times size + u \\\\ (II) \\quad \\widehat{TestScore} =&amp; \\, \\beta_0 + \\beta_1 \\times size + \\beta_2 \\times english + u \\\\ (III) \\quad \\widehat{TestScore} =&amp; \\, \\beta_0 + \\beta_1 \\times size + \\beta_2 \\times english + \\beta_3 \\times lunch + u \\\\ (IV) \\quad \\widehat{TestScore} =&amp; \\, \\beta_0 + \\beta_1 \\times size + \\beta_2 \\times english + \\beta_4 \\times calworks + u \\\\ (V) \\quad \\widehat{TestScore} =&amp; \\, \\beta_0 + \\beta_1 \\times size + \\beta_2 \\times english + \\beta_3 \\times lunch + \\beta_4 \\times calworks + u \\end{align*}\\] The best way to communicate regression results is in a table. The stargazer package is very convenient for this purpose. It provides a function that generates professionally looking HTML and LATEX tables that satisfy scienctific standards. One simply has to provide one or multiple object(s) of class lm and the rest is done by the function stargazer(). # load the stargazer library library(stargazer) # estimate different model specifications spec1 &lt;- lm(score ~ size , data = CASchools) spec2 &lt;- lm(score ~ size + english, data = CASchools) spec3 &lt;- lm(score ~ size + english + lunch, data = CASchools) spec4 &lt;- lm(score ~ size + english + calworks, data = CASchools) spec5 &lt;- lm(score ~ size + english + lunch + calworks, data = CASchools) # generate a Latex table using stargazer stargazer(spec1, spec2, spec3, spec4, spec5, column.labels = c(&quot;(I)&quot;, &quot;(II)&quot;, &quot;(III)&quot;, &quot;(IV)&quot;, &quot;(V)&quot;) ) Dependent variable: score (I)(II)(III)(IV)(V) spec1spec2spec3spec4spec5 size-2.280***-1.101***-0.998***-1.308***-1.014*** (0.480)(0.380)(0.239)(0.307)(0.240) english-0.650***-0.122***-0.488***-0.130*** (0.039)(0.032)(0.033)(0.034) lunch-0.547***-0.529*** (0.022)(0.032) calworks-0.790***-0.048 (0.053)(0.061) Constant698.933***686.032***700.150***697.999***700.392*** (9.467)(7.411)(4.686)(6.024)(4.698) Observations420420420420420 R20.0510.4260.7750.6290.775 Adjusted R20.0490.4240.7730.6260.773 Residual Std. Error18.581 (df = 418)14.464 (df = 417)9.080 (df = 416)11.654 (df = 416)9.084 (df = 415) F Statistic22.575*** (df = 1; 418)155.014*** (df = 2; 417)476.306*** (df = 3; 416)234.638*** (df = 3; 416)357.054*** (df = 4; 415) Note:*p**p***p The table states that \\(scores\\) is the dependent variable and that we consider \\(5\\) models. We see that the coloumns of Table 7.1 contain all the information provided by summary() for the regression models spec1 to spec5: the coefficient section of the table presents coefficients estimates equipped with signifiance codes (asterisks) and standard errors in parantheses below. Although there a no \\(t\\)-statistics, it is straightforward for the reader to compute them simply by dividing a coefficient estimate by the correspondung standard error. In the botton of the table we find summary statistics for each model and a legend. For an in-depth discussion of the tabular presentation of regression results, see chapter 7.6 of the book. What can we conclude from the model comparison? We see that adding control variables roughly halves the coefficient on size. Also the coefficient is not very sensitive to the set of control variables used. The conclusion is that decreasing the student-teacher ratio ceteris paribus by one unit leads to an estimated average increase in test scores of about \\(1\\) point. Adding student characteristics as controls boosts \\(R^2\\) and \\(\\overline{R^2}\\) from \\(0.049\\) (spec1) up to \\(0.773\\) (spec3 and spec5), so we can consider these variables as suitable predictors for test scores. Morever, the estimated coefficients on all control variables are consistent with the impressions gained from figure 7.2. We see that the control variables are not always individually statistically significant: for example in spec5 we see that the coefficient on \\(calworks\\) is not significantly different from zero at the level of \\(5\\%\\) since \\(\\lvert-0.048/0.061\\rvert=0.79 &lt; 1.64\\). We also observe that the effect on the estimate (and its standard error) of the coefficient on \\(size\\) of adding \\(calworks\\) to the base specification spec3 is negligible. We can therefore consider calworks as a redundant control variable in this setting. "],
["exercises-5.html", "7.7 Exercises", " 7.7 Exercises 1. Hypothesis Testing in a Multiple Regression Model - t statistics and p values Reconsider the Boston data set and the following estimated model (with homoscedasticity-only standard errors in parentheses) from the previous chapter: \\[\\widehat{medv}_i = \\underset{(0.75)}{32.828} \\underset{(0.05)}{-0.994} \\times lstat_i \\underset{(0.04)}{-0.083} \\times crim_i + \\underset{(0.01)}{0.038} \\times age_i.\\] Just as in the simple linear regression framework we can conduct hypothesis tests for each coefficent in the multiple regression framework as well. To do so assume that we want to test the null \\(H_0:\\beta_j=0\\) against the alternative \\(H_1:\\beta_j\\ne 0\\) for all \\(j\\). The packages AER and MASS have been loaded. The coefficients of the model as well as the corresponding standard errors are already available in coef and se, respectively. Instructions: Compute t statistics for each coefficient by using the predefined objects coef and se. Assign them to tstat. Compute p values for each coefficient and assign them to pval. Check with the help of logical operators whether the hypotheses are rejected at a 1% significance level. library(AER) library(MASS) coef # Compute t statistics for each coefficient. Assign them to tstat. # Compute p values for each coefficient. Assign them to pval. # Check whether the hypotheses are rejected at a 1% significance level. # Compute t statistics for each coefficient. Assign them to tstat. tstat test_object(\"tstat\") test_object(\"pval\") test_or(test_output_contains(\"pval 0.01\")) success_msg(\"All but the coefficent for the crime rate (crim) can be rejected at a 1% significance level.\") Hints: The t statistic for each coefficient is defined as \\(t=\\frac{\\widehat{\\beta}_j-\\beta_{j,0}}{SE(\\widehat{\\beta}_j)}\\). The p value can be computed as \\(2\\Phi(-|t^{act}|)\\) where \\(t^{act}\\) denotes the computed t statistic. 2. Hypothesis Testing in a Multiple Regression Model - Confidence Intervals Again consider the model \\[\\widehat{medv}_i = \\underset{(0.75)}{32.828} \\underset{(0.05)}{-0.994} \\times lstat_i \\underset{(0.04)}{-0.083} \\times crim_i + \\underset{(0.01)}{0.038} \\times age_i.\\] which is available as mod in your working environment. The packages AER and MASS have been loaded. Instruction: Construct a 99% confidence interval for all model coefficients. Decide whether we can reject the null \\(H_0:\\beta_j=0\\) for all \\(j\\). library(AER) library(MASS) mod # Construct a 99% confidence interval for all coefficients # Construct a 99% confidence interval for all coefficients confint(mod, level = 0.99) test_function('confint', args = c('object', 'level')) success_msg('Correct! Analogously to the previous exercise we can see that 0 is not an element for all confidence intervals but for the one of the crime rate. Hence, as expected, the test decision remains the same.') Hint: You can use confint() to construct confidence intervals. The confidence level can be set via level. 3. Hypothesis Testing in a Multiple Regression Model - Can we do more robust? mod, the lm object with the estimated model \\[\\widehat{medv}_i = 32.828 - 0.994 \\times lstat_i - 0.083 \\times crim_i + 0.038 \\times age_i.\\] is available in your working environment. The packages AER and MASS have been loaded. Instructions: Print a coefficient summary using heteroscedasticity-robust standard errors. Access entries of the matrix created by coeftest() to check with the help of logical operators whether the hypotheses are rejected at a 1% significance level. library(AER) library(MASS) mod # Print a coefficient summary using robust standard errors. # Check whether the hypotheses are rejected at a 1% significance level. # Print a coefficient summary using robust standard errors. coeftest(mod, vcov. = vcovHC) # Check whether the hypotheses are rejected at a 1% significance level. coeftest(mod, vcov. = vcovHC)[, 4] test_function_result('coeftest') test_or(test_output_contains(\"coeftest(mod, vcov. = vcovHC)[, 4] 0.01\", incorrect_msg = 'Not correct! Please make sure you select the correct entries of the matrix.')) success_msg('Correct! We see that by using robust standard errors the coefficient for the crime rate (crim) becomes significant, whereas the coefficient for the average age of the buildings (age) becomes insignificant at a 1% significance level.') Hints: Inside of coeftest() one can set the argument vcov. to force the function to use robust standard errors. The p values are contained in the fourth column of the matrix. Use square brackets to access them. 4. Joint Hypothesis Testing - F Test I Besides testing single hypotheses we can also test joint hypotheses which impose restrictions on multiple regression coefficients. For example in the model \\[medv_i = \\beta_0 + \\beta_1\\times lstat_i + \\beta_2\\times crim_i + \\beta_3\\times age_i + u_i\\] we could test the null \\(H_0: \\beta_2=\\beta_3\\) vs. the alternative \\(H_1: \\beta_2\\ne\\beta_3\\) (which in fact is a joint hypothesis as we impose a restriction on two regression coefficients). The basic idea behind testing such a hypothesis is now to conduct two regressions. For one of the regressions we incorporate the restriction (we call this the restricted model), whereas for the other regression the restriction is left out (we call this the unrestricted model). From this starting point we can then construct a test statistic which, under the null, follows a well known distribution, namely a \\(F\\) distribution (see next exercise). However in this exercise we start with the initial steps or computations necessary to construct the test statistic. So please do the following: The packages AER and MASS have been loaded. Instructions: Estimate the restricted model, that is, the model where the restriction is assumed to be true. Save it in model_res. Compute the SSR of the restricted model and assign it to RSSR. Estimate the unrestricted model, that is, the model where the restriction is assumed to be false. Save it in model_unres. Compute the SSR of the unrestricted model and assign it to USSR. library(AER) library(MASS) # Estimate the restricted model and save it in model_res. # Compute the SSR of the restricted model and assign it to RSSR. # Estimate the unrestricted model and save it in model_unres. # Compute the SSR of the unrestricted model and assign it to USSR. # Estimate the restricted model and save it in model_res. model_res test_or({ test_function('lm', args = c('formula', 'data'), not_called_msg = \"Funktion nicht oder falsch aufgerufen.\", args_not_specified_msg = \"Funktion nicht oder falsch aufgerufen.\" ) },{ sol % override_solution(\"model_res % check_function('lm') %>% check_arg('formula') %>% check_equal() },{ sol % override_solution(\"model_res % check_function('lm') %>% check_arg('formula') %>% check_equal() }) test_object(\"model_res\", eval = F) test_object(\"RSSR\") test_or({ test_function('lm', args = c('formula', 'data'), not_called_msg = \"Funktion nicht oder falsch aufgerufen.\", args_not_specified_msg = \"Funktion nicht oder falsch aufgerufen.\" ) },{ sol % override_solution(\"model_unres % check_function('lm') %>% check_arg('formula') %>% check_equal() },{ sol % override_solution(\"model_unres % check_function('lm') %>% check_arg('formula') %>% check_equal() }) test_object(\"model_unres\", eval = F) test_object(\"USSR\") success_msg(\"Correct! Note that the SSR of the restricted model is always greater or equal than the SSR of the unrestricted model.\") Hints: The restricted model can be written as \\(medv_i = \\beta_0 + \\beta_1\\times lstat_i + \\beta_2\\times crim_i + \\beta_2\\times age_i + u_i\\) which, after rearranging, can be expressed as \\(medv_i = \\beta_0 + \\beta_1\\times lstat_i + \\beta_2\\times(crim_i+age_i) + u_i\\). The SSR is defined as the sum of the squared residuals. Note that the residuals of a regression model are available as residuals in the corresponding lm object. So you can access them as usual via the $-operator. 5. Joint Hypothesis Testing - F Test II After estimating the necessary models and computing the SSR we can now compute the test statistic and conduct the hypothesis test. As mentioned in the last exercise the test statistic follows a \\(F\\) distribution or more precisely a \\(F_{q,n-k-1}\\) distribution where \\(q\\) and \\(k\\) denote the number of restrictions under the null and regressors in the unrestricted model, respectively. The packages AER and MASS have been loaded. Both models (model_res and model_unres) as well as their SSR (RSSR and USSR) are available in your working environment. Instructions: Compute the F statistic and assign it to Fstat. Compute the p value and assign it to pval. Check with the help of logical operators whether the null is rejected at a 1% significance level. Verify your result by using linearHypothesis() and printing out the results. library(AER) library(MASS) model_res # Compute the F statistic and assign it to Fstat. # Compute the p value and assign it to pval. # Check whether the null is rejected at a 1% significance level. # Verify your result with linearHypothesis(). # Compute the F statistic and assign it to Fstat. Fstat test_object(\"Fstat\") test_object(\"pval\") test_or(test_output_contains(\"pval 0.01\")) test_function_result(\"linearHypothesis\") success_msg(\"Correct! The null hypothesis is rejected at a 1% significance level.\") Hints: The F statistic is defined as \\(\\frac{RSSR-USSR/q}{USSR/(n-k-1)}\\). The p value can be computed as \\(1-F_{q,n-k-1}(F^{act})\\) where \\(F_{q,n-k-1}\\) denotes the cdf of the F distribution (pf()) with degrees of freedom \\(q\\) and \\(n-k-1\\) and \\(F^{act}\\) the computed F statistic. linearHypothesis() expects the unrestricted model as well as the null hypothesis. 6. Joint Hypothesis Testing - Confidence Set As you know from previous chapters constructing a confidence set for a single regression coefficient results in a simple confidence interval on the real line. However if we consider \\(n\\) regression coefficients jointly (as we do in a joint hypothesis testing setting) we move from \\(\\mathbb{R}\\) to \\(\\mathbb{R}^n\\) resulting in a n-dimensional confidence set. For the sake of illustration we then often choose \\(n=2\\), so that we end up with a representable two-dimensional plane. Now recall the estimated model \\[\\widehat{medv}_i = \\underset{(0.75)}{32.828} \\underset{(0.05)}{-0.994} \\times lstat_i \\underset{(0.04)}{-0.083} \\times crim_i + \\underset{(0.01)}{0.038} \\times age_i.\\] which is available as mod in your working environment and assume we want to test the null \\(H_0: \\beta_2=\\beta_3=0\\) vs. \\(H_1: \\beta_2\\ne 0\\) or \\(\\beta_3\\ne 0\\). The packages AER and MASS have been loaded. Instructions: Construct a 99% confidence set for the coefficients of crim and lstat, that is a two-dimensional confidence set. Can you reject the null stated above? Verify your visual inspection by conducting a corresponding F test. library(AER) library(MASS) mod # Construct a 99% confidence set for the coefficents of crim and lstat. # Conduct a corresponding F test. # Construct a 99% confidence set for crim and lstat. confidenceEllipse(mod, which.coef = c(\"crim\", \"lstat\"), levels = 0.99) # Conduct a corresponding F test. linearHypothesis(mod, c(\"crim = 0\", \"lstat = 0\")) test_or({ fun % check_function('confidenceEllipse') fun %>% check_arg('model') %>% check_equal() fun %>% check_arg('which.coef') %>% check_equal() fun %>% check_arg('levels') %>% check_equal() }, { fun % override_solution('confidenceEllipse(mod, which.coef = c(\"lstat\", \"crim\"), levels = 0.99)') %>% check_function('confidenceEllipse') fun %>% check_arg('model') %>% check_equal() fun %>% check_arg('which.coef') %>% check_equal() fun %>% check_arg('levels') %>% check_equal() }) test_function_result('linearHypothesis') success_msg(\"Correct! Since (0,0) is not an element of the 99% confidence set, the null hypothesis is rejected at a 1% significance level.\") Hints: You can use confidenceEllipse() to construct a two dimensional confidence set. Besides the coefficients for which the confidence set shall be constructed (which.coef), you have to specify the confidence level (levels). As usual you can use linearHypothesis() to conduct a F test. However note that we test two restrictions now, hence you have to pass a vector consisting of both linear restrictions. "]
]
