[
["index.html", "Using R for Introduction to Econometrics 1 Preface", " Using R for Introduction to Econometrics Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer 2017-10-16 1 Preface What this book is (and what it is not) This book is an interactive script aiming to provide students of economic sciences with a platform-independent E-learning arrangement that seamlessly intertwines both, teaching of theoretical core knowledge and empirical skills in undergraduate econometrics. Thereby, the focus clearly lays on empirical applications with the freely available statistical software package R and omitts tedious derivations and formal proof. The script is aligned on Introduction to Econometrics (Stock and Watson, 2014), a standard textbook in introductory econometrics. While the book does well in motivating theory by real-world applications, we want to take it a step further and intend to enable students not only to learn how to reproduce results of case studies with R but also to strengthen their ability to use the newly acquired skills in other empirical applications. Therefore, this book is neither an introduction to R, nor is it indended to be a replacement for the textbook let alone a full course in econometrics. "],
["probability-theory.html", "2 Probability Theory Random Variables and Probability Distributions Probability Distributions of Continuous Random Variables Random Sampling and the Distribution of Sample Averages", " 2 Probability Theory This chapter reviews some basic concepts of probability theory and demonstrates how they can be applied in R. Most of the statistical functionalities in R’s standard version are collected in the stats package. It provides simple functions which compute descriptive measures and faciliate calculus involving a variety of probability distributions but also holds more sophisticated routines that e.g. enable the user to estimate a large number of models based on the same data or help to conduct extensive simulation studies. Execute library(help = &quot;stats&quot;) in the console to view the documentation and a complete list of all functions gathered in stats. In what follows, we lay our focus on (some of) the probability distributions that are handled by R and show how to use the relevant functions to solve simple problems. Thereby we will repeat some core concepts of probability theory. Among other things, You will learn how to draw random numbers, how to compute densities, probabilities, quantiles and alike. As we shall see, it is very convenient to rely on these routines, especially when writing Your own functions. Random Variables and Probability Distributions For a start, let us briefly review some basic concepts in probability. The mutually exclusive results of a random process are called the outcomes. ‘Mutually exclusive’ means that only one of the possible outcomes is observed. We refer to the probability of an outcome as the proportion of the time that the outcome occurs in the long run, that is if the experiment is repeated very often. The set of all possible outcomes of a random variable is called the sample space. An event is a subset of the sample space and consists of one or more outcomes. These indeas are unified in the concept of a random variable which is a numerical summary of random outcomes. Random variables can be discrete or continuous. Discrete random variables have discrete outcomes, e.g. \\(0\\) and \\(1\\). A continuous random variable takes on a continuum of possible values. Probability Distributions of Discrete Random Variables A typical example for a discrete random variable \\(D\\) is the result of a die roll: in terms of a random experiment this is nothing but randomly selecting a sample of size \\(1\\) from a set of numbers which are mutually exclusive outcomes. Here, the sample space is \\(\\{1,2,3,4,5,6\\}\\) and we can think of many different events, e.g. ‘the observed outcome lies between \\(2\\) and \\(5\\)’. A basic function to draw random samples from a specified set of elements is the the function sample(), see ?sample. We can use it to simulate the random outcome of a die roll. Let’s role the dice! sample(1:6, 1) ## [1] 5 The probability distribution of a discrete random variable is the list of all possible values of the variable and thier probabilities which sum to \\(1\\). The cumulative probability distribution function states the probability that the random variable is less than or equal to a particular value. For the die roll, this is straightforward to set up Outcome 1 2 3 4 5 6 Probability distribution 1/6 1/6 1/6 1/6 1/6 1/6 Cumulative probability distribution 1/6 2/6 3/6 4/6 5/6 1 We can easily plot both functions using R. Since the probability equals \\(1/6\\) for each outcome, we set up the vector probability by using the rep() function which replicates a given value a specified number of times. # generate the vector of probabilities probability &lt;- rep(1/6,6) # plot the probabilites plot(probability, xlab = &quot;outcomes&quot;, main = &quot;Probability Distribution&quot; ) For the cumulative probability distribution we need the cumulative probabilities i.e. we need the cumulative sums of the vector probability. These sums can be computed using cumsum(). #generate the vector of cumulative probabilities cum_probability &lt;- cumsum(probability) # plot the probabilites plot(cum_probability, xlab = &quot;outcomes&quot;, main = &quot;Cumulative Probability Distribution&quot; ) Bernoulli Trials The set of elements sample() draws from does not have to consist of numbers only. We might as well simulate coin tossing with outcomes \\(H\\) (head) and \\(T\\) (tail). sample(c(&quot;H&quot;,&quot;T&quot;),1) ## [1] &quot;H&quot; The result of a coin toss is a Bernoulli distributed random variable i.e. a variable with to possible distinct outcomes. Imagine you are about to toss a coin \\(10\\) times in a row and wonder how likely it is to end up with a sequence of outcomes like \\[ H \\, H \\, T \\, T \\,T \\,H \\,T \\,T \\, H \\, H .\\] This is a typical example of a Bernoulli experiment as it consists of \\(n=10\\) Bernoulli trials that are independent of each other and we are interested in the likelihood of observing \\(k=5\\) successes \\(H\\) that occur with probability \\(p=0.5\\) (assuming a fair coin) in each trial. It is a well known result that the number of successes \\(k\\) follows a binomial distribution \\[ k \\sim B(n,p). \\] The probability of observing \\(k\\) successes in the experiment \\(B(n,p)\\) is hence given by \\[f(k)=P(k)=\\begin{pmatrix}n\\\\ k \\end{pmatrix} \\cdot p^k \\cdot q^{n-k}=\\frac{n!}{k!(n-k)!} \\cdot p^k \\cdot q^{n-k}\\] where \\(\\begin{pmatrix}n\\\\ k \\end{pmatrix}\\) is a binomial coefficient. In R, we can solve the problem stated above by means of the function dbinom() which calculates the probability of the binomial distribution for parameters x, size, and prob, see ?binom. dbinom(x = 5, size = 10, prob = 0.5 ) ## [1] 0.2460938 We conclude that the probability of observing Head \\(k=5\\) times when tossing the coin \\(n=10\\) times is about \\(24.6\\%\\). Now assume we are interested in \\(P(4 \\leq k \\leq 7)\\) i.e. the probability of observing \\(4\\), \\(5\\), \\(6\\) or \\(7\\) successes for \\(B(10,0.5)\\). This is easily computed by providing a vector as the x argument in our call of dbinom() and summing up using sum(). sum( dbinom(x = 4:7, size = 10, prob = 0.5 ) ) ## [1] 0.7734375 The Probability distribution of a discrete random variable is nothing but a list of all possible outcomes that can occur and their respective probabilities. In our coin tossing example, we face \\(11\\) possible outcomes for \\(k\\) # set up vector of possible outcomes k &lt;- 0:10 To visualize the probability distribution function of \\(k\\) we may therefore simply call # assign probabilities probability &lt;- dbinom(x = k, size = 10, prob = 0.5 ) # plot outcomes against probabilities plot(x = k, y = probability, main = &quot;Probability Distribution Function&quot;) In a similar fashion we may plot the cumulative distribution function of \\(k\\) by executing the following code chunk: prob &lt;- cumsum( dbinom(x =0:10, size = 10, prob = 0.5 ) ) k &lt;- 0:10 plot(x = k, y = prob, main = &quot;Cumulative Distribution Function&quot;) Expected Values, Mean and Variance The expected value of a random variable is the long-run average value of the random variable over many repeated trials. For a discrete random variable, the expected value is computed as a weighted average of its possible outcomes whereby the weights are the related probabilities. This is formalized in Key Concept 2.1. Key Concept 2.1 Expected Value and the Mean Suppose the random variable \\(Y\\) takes on \\(k\\) possible values, \\(y_1, \\dots, y_k\\), where \\(y_1\\) denotes the first value, \\(y_2\\) denotes the second value, and so forth, and that the probability that \\(Y\\) takes on \\(y_1\\) is \\(p_1\\), the probability that \\(Y\\) takes on \\(y_2\\) is \\(p_2\\) and so forth. The expected value of \\(Y\\), \\(E(Y)\\) is defined as \\[ E(Y) = y_1 p_1 + y_2 p_2 + \\cdots + y_k p_k = \\sum_{i=1}^k y_i p_i \\] where the notation \\(\\sum_{i=1}^k y_i p_i\\) means “the sum of \\(y_i\\) \\(p_i\\) for \\(i\\) running from \\(1\\) to \\(k\\)”. The expected value of \\(Y\\) is also called the mean of \\(Y\\) or the expectation of \\(Y\\) and is denoted by \\(\\mu_y\\). In the dice example, the random variable, \\(D\\) say, takes on \\(6\\) possible values \\(d_1 = 1, d_2 = 2, \\dots, d_6 = 6\\). Assuming a fair dice, each of the \\(6\\) outcomes occurs with a probability of \\(1/6\\). It is therefore easy to calculate the exact value of \\(E(D)\\) by hand: \\[ E(D) = 1/6 \\sum_{i=1}^6 d_i = 3.5 \\] Here, this is simply the average of the natural numbers from \\(1\\) to \\(6\\) since all wights \\(p_i\\) are \\(1/6\\). Convince Yourself that this can be easily calculated using the function mean() which computes the arithmetic mean of a numeric vector. mean(1:6) ## [1] 3.5 An example of sampling with replacement is rolling a dice three times in a row. # set random seed for reproducibility set.seed(1) # rolling a dice three times in a row sample(1:6, 3, replace = T) ## [1] 2 3 4 Of course we could also consider a much bigger number of trials, \\(10000\\) say. Doing so, it would be pointless to simply print the results to the console: by default R displays up to \\(1000\\) entries of large vectors and omitts the remainder (give it a go). Eyeballing the numbers does not reveal too much. Instead let us calculate the sample average of the outcomes using mean() and see if the result comes close to the expected value \\(E(D)=3.5\\). # set random seed for reproducibility set.seed(1) # compute the sample mean of 10000 die rolls mean( sample(1:6, 10000, replace = T ) ) ## [1] 3.5039 We find the sample mean to be fairly close to the expected value. (ref to WLLN) Other frequently encountered measures are the variance and the standard deviation. Both are measures of the dispersion of a random variable. Key Concept 2.2 Variance and Standard Deviation The Variance of the discrete random variable \\(Y\\), denoted \\(\\sigma^2_Y\\), is \\[ \\sigma^2_Y = \\text{Var}(Y) = E\\left[(Y-\\mu_y)^2\\right] = \\sum_{i=1}^k (y_i - \\mu_y)^2 p_i \\] The standard deviation of \\(Y\\) is \\(\\sigma_Y\\), the square root of the variance. The units of the standard deviation are the same as the units of \\(Y\\). The variance as defined in Key Concept 2.2 is not implemented as a function in R. Instead we have the function var() which computes the sample variance \\[ s^2_Y = \\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\overline{y})^2. \\] Remember that \\(s^2_Y\\) is different from the so called population variance of \\(Y\\), \\[ \\text{Var}(Y) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\mu_Y)^2, \\] since it measures how the data is dispersed around the sample average \\(\\overline{y}\\) instead of the population mean \\(\\mu_Y\\). This becomes clear when we look at our dice rolling example. For \\(D\\) we have \\[ \\text{Var}(D) = 1/6 \\sum_{i=1}^6 (d_i - 3.5)^2 = 2.92 \\] which is obviously different from the result of \\(s^2\\) as computed by var(). var(1:6) ## [1] 3.5 Probability Distributions of Continuous Random Variables Since a continuous random variable takes on a continuum of possible values, we cannot use the concept of a probability distribution as used for discrete random variables. Instead, the probability distribution of a continuous random variable is summarized by its probability density function (PDF). The cumulative probability distribution function (CDF) for a continuous random variable is defined just as in the discrete case. Hence, the cumulative probability distribution of a continuous random variables states the probability that the random variable is less than or equal to a particular value. For completeness, we present revisions of Key Concepts 2.1 and 2.2 for the continuous case. Key Concept 2.3 Probabilities, Expected Value and Variance of a Continuous Random Variable Let \\(f_Y(y)\\) denote the probability density function of \\(Y\\). Because probabilities cannot be negative, we have \\(f_Y\\geq 0\\) for all \\(y\\). The Probability that \\(Y\\) falls between \\(a\\) and \\(b\\), \\(a &lt; b\\) is \\[ P(a \\leq Y \\leq b) = \\int_a^b f_Y(y) \\mathrm{d}y. \\] We further have that \\(P(-\\infty \\leq Y \\leq \\infty) = 1\\) and therefore \\(\\int_{-\\infty}^{\\infty} f_Y(y) \\mathrm{d}y = 1\\). As for the discrete case, the expected value of \\(Y\\) is the probability weighted average of its values. Due to continuity, we use intergrals instead of sums. The expected value of \\(Y\\) is \\[ E(Y) = \\mu_Y = \\int y f_Y(y) \\mathrm{d}y. \\] The variance is the expected value of \\((Y - \\mu_Y)^2\\). We thus have \\[ \\text{Var}(Y) = \\sigma_Y^2 = \\int (y - \\mu_Y)^2 f_Y(y) \\mathrm{d}y. \\] Let us discuss an example: Consider the continuous random variable \\(X\\) with propability density function \\[ f_X(x) = \\frac{3}{x^4}, x&gt;1. \\] We can show analytically that the integral of \\(f_X(x)\\) over the real line equals \\(1\\). \\[\\begin{align} \\int f_X(x) \\mathrm{d}x =&amp; \\int_{1}^{\\infty} \\frac{3}{x^4} \\mathrm{d}x \\\\ =&amp; \\lim_{t \\rightarrow \\infty} \\int_{1}^{t} \\frac{3}{x^4} \\mathrm{d}x \\\\ =&amp; \\lim_{t \\rightarrow \\infty} -x^{-3} \\rvert_{x=1}^t \\\\ =&amp; -\\left(\\lim_{t \\rightarrow \\infty}\\frac{1}{t^3} - 1\\right) \\\\ =&amp; 1 \\end{align}\\] The expectation of \\(X\\) can be computed as follows: \\[\\begin{align} E(X) = \\int x \\cdot f_X(x) \\mathrm{d}x =&amp; \\int_{1}^{\\infty} x \\cdot \\frac{3}{x^4} \\mathrm{d}x \\\\ =&amp; - \\frac{3}{2} x^{-2} \\rvert_{x=1}^{\\infty} \\\\ =&amp; -\\frac{3}{2} \\left( \\lim_{t \\rightarrow \\infty} \\frac{1}{t^2} - 1 \\right) \\\\ =&amp; \\frac{3}{2} \\end{align}\\] Note that the variance of \\(X\\) can be expressed as \\(\\text{Var}(X) = E(X^2) - E(X)^2\\). Since \\(E(X)\\) has been computed in the previous step, we seek \\(E(X^2)\\): \\[\\begin{align} E(X^2)= \\int x^2 \\cdot f_X(x) \\mathrm{d}x =&amp; \\int_{1}^{\\infty} x^2 \\cdot \\frac{3}{x^4} \\mathrm{d}x \\\\ =&amp; -3 x^{-1} \\rvert_{x=1}^{\\infty} \\\\ =&amp; -3 \\left( \\lim_{t \\rightarrow \\infty} \\frac{1}{t} - 1 \\right) \\\\ =&amp; 3 \\end{align}\\] So we have shown that the area under the curve equals one, that the expectation is \\(E(X)=\\frac{3}{2} \\ \\) and we found the variance to be \\(\\text{Var}(X) = \\frac{3}{4}\\). However, this was quite tedious and, as we shall see soon, an analytic approach is not applicable for some probability density functions e.g. if integrals have no closed form solutions. Luckily, R enables us to find the results derived above in an instant. The tool we use for this is the function integrate(). First, we have to define the functions we want to calculate integrals for as R functions, i.e. the PDF \\(f_X(x)\\) as well as the expressions \\(x\\cdot f_X(x)\\) and \\(x^2\\cdot f_X(x)\\). # define functions f &lt;- function(x) 3/x^4 g &lt;- function(x) x*f(x) h &lt;- function(x) x^2*f(x) Next, we use integrate() and set lower and upper limits of integration to \\(1\\) and \\(\\infty\\) using arguments lower and upper. By default, integrate() prints the result along with an estimate of the calculation error to the console. However, the outcome is not a numeric value one can do further calculation with readily. In order to get only a numeric value of the integral, we need to use the $ operator in conjunction with value. # calculate area under curve AUC &lt;- integrate(f, lower = 1, upper = Inf ) AUC ## 1 with absolute error &lt; 1.1e-14 # calculate E(X) EX &lt;- integrate(g, lower = 1, upper = Inf) EX ## 1.5 with absolute error &lt; 1.7e-14 # calculate Var(X) VarX &lt;- integrate(h, lower = 1, upper = Inf )$value - EX$value^2 VarX ## [1] 0.75 Although there is a wide variety of distributions, the ones most often encountered in econometrics are the normal, chi-squared, Student \\(t\\) and \\(F\\) distributions. Therefore we will discuss some core R functions that allow to do calculations involving densities, probabilities and quantiles of these distributions. Every probability distribution that R handles has four basic functions whose names consist of a prefix followed by a root name. As an example, take the normal distribution. The root name of all four functions associated with the normal distribution is norm. The four prefixes are d for “density” - probability function / probability density function p for “probability” - cumulative distribution function q for “quantile” - quantile function (inverse cumulative distribution function) r for “random” - random number generator Thus, for the normal distribution we have the R functions dnorm(), pnorm(), qnorm() and rnorm(). The Normal Distribution The probably most important probability distribution considered here is the normal distribution. This is not least due to the special role of the standard normal distribution and the Central Limit Theorem which is treated shortly during the course of this section. Distributions of the normal family have a familiar symmetric, bell-shaped probability density. A normal distribution is characterized by its mean \\(\\mu\\) and its standard deviation \\(\\sigma\\) what is concisely expressed by \\(N(\\mu,\\sigma^2)\\). The normal distribution has the PDF \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-(x - μ)^2/(2 σ^2)}. \\] For the standard normal distribution we have \\(\\mu=0\\) and \\(\\sigma=1\\). Standard normal variates are often denoted \\(Z\\). Usually, the standard normal PDF is denoted by \\(\\phi\\) and the standard normal CDF is denoted by \\(\\Phi\\). Hence, \\[ \\phi(c) = \\Phi&#39;(c) \\ \\ , \\ \\ \\Phi(c) = P(Z \\leq c) \\ \\ , \\ \\ Z \\sim N(0,1). \\] In R, we can conveniently obtain density values of normal distributions using the function dnorm(). Let us draw a plot of the standard normal density function using curve() and dnorm(). # draw a plot of the N(0,1) pdf curve(dnorm(x), xlim=c(-3.5, 3.5), ylab = &quot;Density&quot;, main = &quot;Standard Normal Density Function&quot; ) We can obtain the density at different positions by passing a vector of quantiles to dnorm(). # compute denstiy at x=-1.96, x=0 and x=1.96 dnorm(x = c(-1.96, 0, 1.96)) ## [1] 0.05844094 0.39894228 0.05844094 Similary as for the PDF, we can plot the standard normal CDF using curve() and pnorm(). # plot the standard normal CDF curve(pnorm(x), xlim=c(-3.5, 3.5), ylab = &quot;Density&quot;, main = &quot;Standard Normal Cumulative Distribution Function&quot; ) We can also use R to calculate the probability of events associated with a standard normal random variate. Let us say we are interested in \\(P(Z \\leq 1.337)\\). For some general continuous random variable \\(Z\\) on \\([-\\infty,\\infty]\\) with density function \\(g(x)\\) we would have to determine \\(G(x)\\), the antiderivative of \\(g(x)\\) since \\[ P(Z \\leq 1,337 ) = G(1,337) = \\int_{-\\infty}^{1,337} g(x) \\mathrm{d}x. \\] However, if \\(Z \\sim N(0,1)\\), we have \\(g(x)=\\phi(x)\\) so there is no analytic solution to the integral above and it is cumbersome to come up with an approximation. However, we may circumvent this using R in different ways. The first approach makes use of the function integrate() which allows to solve one-dimensional integration problems using a numerical method. For this, we first define the function we want to compute the integral of as a R function f. In our example, f needs to be the standard normal density function and hence takes a single argument x. Following the definition of \\(\\phi(x)\\) we define f as # define the standard normal PDF as a R function f &lt;- function(x) { 1/(sqrt(2 * pi)) * exp(-0.5 * x^2) } Let us check if this function enables us to compute standard normal density values by passing it a vector of quantiles. # define vector of quantiles quants &lt;- c(-1.96,0,1.96) # compute density values f(quants) ## [1] 0.05844094 0.39894228 0.05844094 # compare to results produced by dnorm() f(quants) == dnorm(quants) ## [1] TRUE TRUE TRUE Notice that the results produces by f() are indeed equivalent to those given by dnorm(). Next, we call integrate() on f() and further specify the arguments lower and upper, the lower and upper limits of integration. # integrate f() integrate(f, lower = -Inf, upper = 1.337 ) ## 0.9093887 with absolute error &lt; 1.7e-07 We find that the probability of observing \\(Z \\leq 1,337\\) is about \\(0.9094\\%\\). A second and much more convenient way is to use the function pnorm() which also allows calculus involving the standard normal cumulative distribution function. # compute a probability using pnorm() pnorm(1.337) ## [1] 0.9093887 The result matches the outcome of the approach using ìntegrate(). Let us discuss some further examples: A commonly known result is that \\(95\\%\\) probability mass of a standard normal lies in the intervall \\([-1.96, 1.96]\\), that is in a distance of about \\(2\\) standard deviations to the mean. We can easily confirm this by calculating \\[ P(-1.96 \\leq Z \\leq 1.96) = 1-2\\times P(Z \\leq -1.96) \\] due to symmetry of the standard normal PDF. Thanks to R, we can abondon the table of the standard normal CDF again and instead solve this by means of the function pnorm(). # compute the probability 1 - 2 * (pnorm(-1.96)) ## [1] 0.9500042 Now consider a random variable \\(Y\\) with \\(Y \\sim N(5,25)\\). As You should already know from Your statistics courses it is not possible to make any statement of probability without prior standardizing as shown in Key Concept 2.4. Key Concept 2.4 Computing Probabilities Involving Normal Random Variables Suppose \\(Y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\): \\[Y \\sim N(\\mu, \\sigma^2)\\] Then \\(Y\\) is standardized by substracting its mean and dividing by its standard deviation: \\[ Z = \\frac{Y -\\mu}{\\sigma} \\] Let \\(c_1\\) and \\(c_2\\) denote two numbers whereby \\(c_1 &lt; c_2\\) and further \\(d_1 = (c_1 - \\mu) / \\sigma\\) and \\(d_2 = (c_2 - \\mu)/\\sigma\\). Then \\[\\begin{align} P(Y \\leq c_2) =&amp; \\, P(Z \\leq d_2) = \\Phi(d_2) \\\\ P(Y \\geq c_1) =&amp; \\, P(Z \\geq d_1) = 1 - \\Phi(d_1) \\\\ P(c_1 \\leq Y \\leq c_2) =&amp; \\, P(d_1 \\leq Z \\leq d_2) = \\Phi(d_2) - \\Phi(d_1) \\end{align}\\] R functions that handle the normal distribution can perform this standardization. If we are interested in \\(P(3 \\leq Y \\leq 4)\\) we can use pnorm() and adjust for a mean and/or a standard deviation that deviate from \\(\\mu=0\\) and \\(\\sigma = 1\\) by specifying the arguments mean and sd accordingly. Attention: pnorm() requires the argument sd which is the standard deviation, not the variance! pnorm(4, mean = 5, sd = 5) - pnorm(3, mean = 5, sd = 5) ## [1] 0.07616203 An extension of the normal distribution in a univariate setting is the multivariate normal distribution. The PDF of two random normal variables \\(X\\) and \\(Y\\) is given by \\[\\begin{align} g_{X,Y}(x,y) =&amp; \\, \\frac{1}{2\\pi\\sigma_X\\sigma_Y\\sqrt{1-\\rho_{XY}^2}} \\\\ \\cdot &amp; \\, \\exp \\left\\{ \\frac{1}{-2(1-\\rho_{XY}^2)} \\left[ \\left( \\frac{x-\\mu_x}{\\sigma_x} \\right)^2 - 2\\rho_{XY}\\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\\left( \\frac{y-\\mu_Y}{\\sigma_Y} \\right) + \\left( \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2 \\right] \\right\\}. \\tag{2.1} \\end{align}\\] Equation (2.1) contains the bivariate normal PDF. Admittedly, it is hard to gain insights from this complicated expression. Instead, let us consider the special case where \\(X\\) and \\(Y\\) are uncorrelated standard normal random variables with density functions \\(f_X(x)\\) and \\(f_Y(y)\\) and we assume that they have a joint normal distribution. We then have the parameters \\(\\sigma_X = \\sigma_Y = 1\\), \\(\\mu_X=\\mu_Y=0\\) (due to marginal standard normality) and \\(\\rho_{XY}=0\\) (due to uncorrelatedness). The joint probability density function of \\(X\\) and \\(Y\\) then becomes \\[ g_{X,Y}(x,y) = f_X(x) f_Y(y) = \\frac{1}{2\\pi} \\cdot \\exp \\left\\{ -\\frac{1}{2} \\left[x^2 + y^2 \\right] \\right\\}, \\tag{2.2} \\] the PDF of the bivariate standard normal distribution. The next plot provides an interactive three dimensional plot of (2.2). By moving the mouse curser over the plot You can see that the density is rotationally invariant. The Chi-Squared Distribution Another distribution relevant in econometric day-to-day work is the chi-squared distribution. It is often needed when testing special types of hypotheses frequently ecountered when dealing with regression models. The sum of \\(M\\) squared independent standard normal distributed random variables follows a chi-squared distribution with \\(M\\) degrees of freedom. \\[ Z_1^2 + \\dots + Z_M^2 = \\sum_{m=1}^M Z_m^2 \\sim \\chi^2_M \\ \\ \\text{with} \\ \\ Z_m \\overset{i.i.d.}{\\sim} N(0,1) \\label{eq:chisq}\\] A \\(\\chi^2\\) distributed random variable with \\(M\\) degrees of freedom has expectation \\(M\\), mode at \\(M-2\\) for \\(n \\geq 2\\) and variance \\(2 \\cdot M\\). For example, if we have \\[ Z_1,Z_2,Z_3 \\overset{i.i.d.}{\\sim} N(0,1) \\] it holds that \\[ Z_1^2+Z_2^2+Z_3^3 \\sim \\chi^2_3. \\tag{2.3} \\] By means of the code below, we can display the PDF and the CDF of a \\(\\chi^2_3\\) random variable in a single plot. This is achieved by setting the argument add = TRUE in the second call of curve(). Further we adjust limits of both axes using xlim and ylim and choose different colors to make both functions better distinguishable. The plot is completed by adding a legend with help of the function legend(). # plot the PDF curve(dchisq(x, df=3), xlim=c(0,10), ylim = c(0,1), col=&quot;blue&quot;, main=&quot;p.d.f. and c.d.f of Chi-Squared Distribution, m = 3&quot; ) # add the CDF to the plot curve(pchisq(x, df=3), xlim=c(0,10), add = TRUE, col=&quot;red&quot; ) # add a legend to the plot legend(&quot;topleft&quot;, c(&quot;PDF&quot;,&quot;CDF&quot;), col = c(&quot;blue&quot;,&quot;red&quot;), lty = c(1,1) ) Notice that, since the outcomes of a \\(\\chi^2_M\\) distributed random variable are always positive, the domain of the related PDF and CDF is \\(\\mathbb{R}_{\\geq0}\\). As expectation and variance depend (solely) on the degrees of freedom, the distribution’s shape changes drastically if we vary the number of squared standard normals that are summed up. This relation is often depicted by overlaying densities for different \\(M\\), see e.g. the Wikipedia Article. Of course, one can easily reproduce such a plot using R. Again we start by plotting the density of the \\(\\chi_1^2\\) distribution on the intervall \\([0,15]\\) with curve(). In the next step, we loop over degrees of freedom \\(m=2,...,7\\) and add a density curve for each \\(m\\) to the plot. We also adjust the line color for each iteration of the loop by setting col = m. At last, we add a legend that displays degrees of freedom and the associated colors. # plot the density for m=1 curve(dchisq(x, df=1), xlim=c(0,15), xlab = &quot;x&quot;, ylab = &quot;Density&quot;, main=&quot;Chi-Square Distributed Random Variables&quot; ) # add densities for m=2,...,7 to the plot using a for loop for (m in 2:7) { curve(dchisq(x, df = m), xlim = c(0,15), add = T, col = m ) } # add a legend legend(&quot;topright&quot;, as.character(1:7), col = 1:7 , lty = 1, title = &quot;D.f.&quot; ) It is evident that increasing the degrees of freedom shifts the distribution to the right (the modus becomes larger) and increases its dispersion (the distribution’s variance grows). The Student \\(t\\) Distribution Let \\(Z\\) be a standard normal variate, \\(W\\) a random variable that follows a \\(\\chi^2_M\\) distribution with \\(M\\) degrees of freedom and further assume that \\(Z\\) and \\(W\\) are independently distributed. Then it holds that \\[ \\frac{Z}{\\sqrt{W/M}} =:X \\sim t_M \\] and we say that \\(X\\) follows a student \\(t\\) distribution (or simple \\(t\\) distribution) with \\(M\\) degrees of freedom. As for the \\(\\chi^2_M\\) distribution, a \\(t\\) distribution depends on the degrees of freedom \\(M\\). \\(t\\) distributions are symmetric, bell-shaped and look very similar to a normal distribution, especially when \\(M\\) is large. This is not a coincidence: for a sufficient large \\(M\\), a \\(t_M\\) distribution can be approximated by the standard normal distribution. This approximation works reasonably well for \\(M\\geq 30\\). As we will show later by means of a small simulation study, the \\(t_{\\infty}\\) distribution is the standard normal distribution. A \\(t_M\\) distributed random variable has an expectation value if \\(M&gt;1\\) and a variance if \\(n&gt;2\\). \\[\\begin{align} E(X) =&amp; 0 \\ , \\ M&gt;1 \\\\ \\text{Var}(X) =&amp; \\frac{M}{M-2} \\ , \\ M&gt;2 \\end{align}\\] Let us graph some \\(t\\) distributions with different \\(M\\) and compare them with the standard normal distribution. # plot the standard normal density curve(dnorm(x), xlim=c(-4,4), xlab = &quot;x&quot;, lty=2, ylab = &quot;Density&quot;, main=&quot;Theoretical Densities of t-Distributions&quot; ) # plot the t density for m=2 curve(dt(x, df=2), xlim=c(-4,4), col=2, add = T ) # plot the t density for m=4 curve(dt(x, df=4), xlim=c(-4,4), col=3, add=T ) # plot the t density for m=25 curve(dt(x, df=25), xlim=c(-4,4), col=4, add=T ) # add a legend legend(&quot;topright&quot;, c(&quot;N(0,1)&quot;,&quot;M=2&quot;,&quot;M=4&quot;,&quot;M=25&quot;), col = 1:4, lty = c(2,1,1,1) ) The plot indicates what has been claimed in the previous paragraph: as the degrees of freedom increase, the shape of the \\(t\\) distribution comes closer to that of a standard normal bell. Already for \\(M=25\\) we find little difference to the dashed line which belongs to the standard normal density curve. If \\(M\\) is small, we find the distribution to have slightly havier tails than a standard normal, i.e. it has a “fatter” bell shape. The \\(F\\) Distribution Another ratio of random variables important to econometricians is the ratio of two indpendently \\(\\chi^2\\) distributed random variables that are divided by their degrees of freedom. Such a quantity follows a \\(F\\) distribution with numerator degrees of freedom \\(M\\) and denominator degrees of freedom \\(n\\), denoted \\(F_{M,n}\\). The distribution was first derived by George Snedecor but was named in honor of Sir Ronald Fisher. \\[ \\frac{W/M}{V/n} \\sim F_{M,n} \\ \\ \\text{with} \\ \\ W \\sim \\chi^2_M \\ \\ , \\ \\ V \\sim \\chi^2_n \\] By definition, the domain of both PDF and CDF of a \\(F_{M,n}\\) distributed random variable is \\(\\mathbb{R}_{\\geq0}\\). Say we have a \\(F\\) distributed random variable \\(Y\\) with numerator degrees of freedom \\(3\\) and denominator degrees of freedom \\(14\\) and are interested in \\(P(Y \\geq 2)\\). This can be computed with help of the function pf(). By setting the argument lower.tail to TRUE we ensure that R computes \\(1- P(Y \\leq 2)\\), i.e. the probability mass in the tail right of \\(2\\). pf(2, 3, 13, lower.tail = F) ## [1] 0.1638271 We can visualize this probability by drawing a line plot of the related density function and adding a color shading with polygon(). # define coordinate vectors for vertices of the polygon x &lt;- c(2, seq(2, 10, 0.01), 10) y &lt;- c(0, df(seq(2, 10, 0.01), 3, 14), 0) # draw density of F_{3, 14} curve(df(x ,3 ,14), ylim = c(0, 0.8), xlim = c(0, 10), ylab = &quot;Density&quot;, main = &quot;Density Function&quot; ) # draw the polygon polygon(x, y, col=&quot;orange&quot;) The \\(F\\) distribution is related to many other distributions. An important special case encountered in econometrics arises if the denominator degrees of freedom are large such that the \\(F_{M,n}\\) distribution can be approximated by the \\(F_{M,\\infty}\\) distribution which turns out to be simply the distribution of a \\(\\chi^2_M\\) random variable divided by its degrees of freedom \\(M\\). \\[ W/M \\sim F_{M,\\infty} \\ \\ , \\ \\ W \\sim \\chi^2_M \\] Random Sampling and the Distribution of Sample Averages To clarify the basic idea of random sampling, let us jump back to the die rolling example: Suppose we are rolling the dice \\(n\\) times. This means we are interested in the outcomes of \\(n\\) random processes \\(Y_i, \\ i=1,...,n\\) which are characterized by the same distribution. Since these outcomes are selected randomly, they are random variables themselves and their realisations will differ each time we draw a sample, i.e. each time we roll the dice \\(n\\) times. Furthermore, each observation is randomly drawn from the same population, that is the numbers from \\(1\\) to \\(6\\), and their individual distribution is the same. Hence we say that \\(Y_1,\\dots,Y_n\\) are identically distributed. Moreover, we know that the value of any of the \\(Y_i\\) does not provide any information on the remainder of the sample. In our example, rolling a six as the first observation in our sample does not alter the distributions of \\(Y_2,\\dots,Y_n\\): all numbers are equally likely to occur. This means that all \\(Y_i\\) are also independently distributed. Thus, we say that \\(Y_1,\\dots,Y_n\\) are independently and identically distributed (i.i.d). The dice example is the simplest sampling scheme used in statistics. That is why it is called simple random sampling. This concept is condensed in Key Concept 2.5. Key Concept 2.5 Simple Random Sampling and i.i.d. Random Variables In simple random sampling, \\(n\\) objects are drawn at random from a population. Each object is equally likely to end up in the sample. We denote the value of the random variable \\(Y\\) for the \\(i^{th}\\) randomly drawn object as \\(Y_i\\). Since all objects are equally likely to be drawn and the distribution of \\(Y_i\\) is the same for all \\(i\\), the \\(Y_i, \\dots, Y_n\\) are independently and identically distributed (i.i.d.). This means the distribution of \\(Y_i\\) is the same for all \\(i=1,\\dots,n\\) and \\(Y_1\\) is distributed independently of \\(Y_2, \\dots, Y_n\\) \\(Y_2\\) is distributed independently of \\(Y_1, Y_3, \\dots, Y_n\\) and so forth. What happens if we consider functions of the sample data? Consider the example of rolling a dice two times in a row once again. A sample now consists of two independent random draws from the set \\(\\{1,2,3,4,5,6\\}\\). In view of the aforementioned, it is apparent that any function of these two random variables is also random, e.g. their sum. Convince Yourself by executing the code below several times. sum(sample(1:6, 2, replace = T)) ## [1] 6 Clearly this sum, let us call it \\(S\\), is a random variable as it depends on randomly drawn summands. For this example, we can completely enumerate all outcomes and hence write down the theoretical probability distribution of our function of the sample data, \\(S\\): We face \\(6^2=36\\) possible pairs. Those pairs are \\[\\begin{align} &amp;(1,1) (1,2) (1,3) (1,4) (1,5) (1,6) \\\\ &amp;(2,1) (2,2) (2,3) (2,4) (2,5) (2,6) \\\\ &amp;(3,1) (3,2) (3,3) (3,4) (3,5) (3,6) \\\\ &amp;(4,1) (4,2) (4,3) (4,4) (4,5) (4,6) \\\\ &amp;(5,1) (5,2) (5,3) (5,4) (5,5) (5,6) \\\\ &amp;(6,1) (6,2) (6,3) (6,4) (6,5) (6,6) \\end{align}\\] Thus, possible outcomes for \\(S\\) are \\[ \\left\\{ 2,3,4,5,6,7,8,9,10,11,12 \\right\\} . \\] Enumeration of outcomes yields \\[\\begin{align} P(S) = \\begin{cases} 1/36, \\ &amp; S = 2 \\\\ 2/36, \\ &amp; S = 3 \\\\ 3/36, \\ &amp; S = 4 \\\\ 4/36, \\ &amp; S = 5 \\\\ 5/36, \\ &amp; S = 6 \\\\ 6/36, \\ &amp; S = 7 \\\\ 5/36, \\ &amp; S = 8 \\\\ 4/36, \\ &amp; S = 9 \\\\ 3/36, \\ &amp; S = 10 \\\\ 2/36, \\ &amp; S = 11 \\\\ 1/36, \\ &amp; S = 12 \\end{cases} \\end{align}\\] We can also compute \\(E(S)\\) and \\(\\text{Var}(S)\\) as stated in Key Concept 2.1 and Key Concept 2.2. # Vector of outcomes S &lt;- 2:12 # Vector of probabilities PS &lt;- c(1:6,5:1)/36 # Expectation of S ES &lt;- S %*% PS; ES ## [,1] ## [1,] 7 # Variance of S VarS &lt;- (S - c(ES))^2 %*% PS; VarS ## [,1] ## [1,] 5.833333 (The %*% operator is used to compute the scalar product of two vectors.) So the distribution of \\(S\\) is known. It is also evident that its distribution differs considerably from the marginal distribution, i.e. the distribution of a single die roll’s outcome, \\(D\\) . Let us visualize this using barplots. # divide the plotting area in one row with two columns par(mfrow = c(1, 2)) # plot the distribution of S names(PS) &lt;- 2:12 barplot(PS, ylim=c(0,0.2), xlab = &quot;S&quot;, ylab =&quot;Probability&quot;, col=&quot;steelblue&quot;, space=0, main=&quot;Sum of Two Die Rolls&quot; ) # plot the distribution of D probability &lt;- rep(1/6,6) names(probability) &lt;- 1:6 barplot(probability, ylim=c(0,0.2), xlab = &quot;D&quot;, col=&quot;steelblue&quot;, space = 0, main = &quot;Outcome of a single Die Roll&quot; ) Many econometric procedures deal with averages of sampled data. It is almost always assumed that observations are drawn randomly from a larger, unkown population. As demonstrated for the sample function \\(S\\), computing an average of a random sample also has the effect to make the average a random variable itself. This random variable in turn has a probability distribution which is called the sampling distribution. Knowledge about the sampling distribution of an average is therefore crucial for understanding the performance of econometric procedures. The sample average of a sample of \\(n\\) observations \\(Y_1, \\dots, Y_n\\) is \\[ \\overline{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\frac{1}{n} (Y_1 + Y_2 + \\cdots + Y_n). \\] \\(\\overline{Y}\\) is also called the sample mean. Mean and Variance of the Sample Mean Denote \\(\\mu_Y\\) and \\(\\sigma_Y^2\\) the mean and the variance of the \\(Y_i\\) and suppose that all observations \\(Y_1,\\dots,Y_n\\) are i.i.d. such that in particular mean and variance are the same for all \\(i=1,\\dots,n\\). Then we have that \\[ E(\\overline{Y}) = E\\left(\\frac{1}{n} \\sum_{i=1}^n Y_i \\right) = \\frac{1}{n} E\\left(\\sum_{i=1}^n Y_i\\right) = \\frac{1}{n} \\sum_{i=1}^n E\\left(Y_i\\right) = \\frac{1}{n} \\cdot n \\cdot \\mu_Y = \\mu_Y \\] and \\[\\begin{align} \\text{Var}(\\overline{Y}) =&amp; \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^n Y_i \\right) \\\\ =&amp; \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(Y_i) + \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1, j\\neq i}^n \\text{cov}(Y_i,Y_j) \\\\ =&amp; \\frac{\\sigma^2_Y}{n} \\\\ =&amp; \\sigma_{\\overline{Y}}^2. \\end{align}\\] Note that the second summand vanishes since \\(\\text{cov}(Y_i,Y_j)=0\\) for \\(i\\neq j\\) due to independence of the observations. Consequently, the standard deviation of the sample mean is given by \\[ \\sigma_{\\overline{Y}} = \\frac{\\sigma_Y}{\\sqrt{n}}. \\] It is worthwhile to mention that these results hold irrespective of the underlying distribution of the \\(Y_i\\). The Sampling Distribution of \\(\\overline{Y}\\) when \\(Y\\) Is Normally Distributed If the \\(Y_1,\\dots,Y_n\\) are i.i.d. draws from a normal distribution with mean \\(\\mu_Y\\) and variance \\(\\sigma_Y^2\\), the following holds for their sample average \\(\\overline{Y}\\): \\[ \\overline{Y} \\sim N(\\mu_y, \\sigma_Y^2/n) \\tag{2.4} \\] For example, if a sample \\(Y_i\\) with \\(i=1,\\dots,10\\) is drawn from a standard normal distribution with mean \\(\\mu_Y = 0\\) and variance \\(\\sigma_Y^2=1\\) we have \\[ \\overline{Y} \\sim N(0,0.1).\\] We can use R’s random number generation facilities to verifiy this result. The basic idea is to simulate outcomes of the true distribution of \\(\\overline{Y}\\) by repeatedly drawing random samples of 10 observation from the \\(N(0,1)\\) distribution and computing their respective averages. If we do this for a large number of repititions, the simulated dataset of averages should quite accurately reflect the theoretical distribution of \\(\\overline{Y}\\) if the theoretical result holds. The approach sketched above is an example of what is commonly known as Monte Carlo Simulation or Monte Carlo Experiment. To perform this simulation in R, we proceed as follows: Choose a sample size n and the number of samples to be drawn reps. Use the function replicate() in conjunction with rnorm() to draw n observations from the standard normal distribution rep times. Note: the outcome of replicate() is a matrix with dimensions n \\(\\times\\) rep. It contains the drawn samples as columns. Compute sample means using colMeans(). This function computes the mean of each column i.e. of each sample and returns a vector. # Set sample size and number of samples n &lt;- 10 reps &lt;- 10000 # Perform random sampling samples &lt;- replicate(reps, rnorm(n)) # 10 x 10000 sample matrix # Compute sample means sample.avgs &lt;- colMeans(samples) After performing these steps we end up with a vector of sample averages. You can check the vector property of sample.avgs: # Check that sample.avgs is a vector is.vector(sample.avgs) ## [1] TRUE # print the first few entries to the console head(sample.avgs) ## [1] -0.12406767 -0.10649421 -0.01033423 -0.39905236 -0.41897968 -0.90883537 A straightforward approach to examine the distribution of univariate numerical data is to plot it as a histogram and compare it to some known or assumed distribution. This comparison can be done with help of a suitable statistical test or by simply eyeballing some graphical representations of these distributions. For our simulated sample averages, we will do the latter by means of the functions hist() and curve(). By default, hist() will give us a frequency histogram i.e. a bar chart where observations are grouped into ranges, also called bins. The ordinate reports the number of observations falling into each of the bins. Instead, we want it to report density estimates for comparison purposes. This is achieved by setting the argument freq = FALSE. The number of bins is adjusted by the argument breaks. Using curve(), we overlay the histogram with a red line which represents the theoretical density of a \\(N(0, 0.1)\\) distributed random variable. Remember to use the argument add = TRUE to add the curve to the current plot. Otherwise R will open a new graphic device and discard the histogram plot! # Plot the density histogram hist(sample.avgs, ylim=c(0,1.4), col=&quot;steelblue&quot; , freq = F, breaks = 20 ) # overlay the theoretical distribution of sample averages on top of the histogram curve(dnorm(x, sd = 1/sqrt(n)), col=&quot;red&quot;, lwd=&quot;2&quot;, add=T ) From inspection of the plot we can tell that the distribution of \\(\\overline{Y}\\) is indeed very close to that of a \\(N(0, 0.1)\\) distributed random variable so that evidence obained from the Monte Carlo Simulation supports the theoretical claim. Let us discuss another example where using simple random sampling in a simulation setup helps to verify a well known result. As discussed before, the Chi-squared distribution with \\(m\\) degrees of freedom arises as the distribution of the sum of \\(m\\) independent squared standard normal distributed random variables. To visualize the claim stated in equation (2.3), we proceed similarly as in the example before: Choose the degrees of freedom DF and the number of samples to be drawn reps. Draw reps random samples of size DF from the standard normal distribution using replicate(). For each sample, by squaring the outcomes and summing them up columnwise. Store the results Again, we produce a density estimate for the distribution underlying our simulated data using a density histogram and overlay it with a line graph of the theoretical density function of the \\(\\chi^2_3\\) distribution. # Number of repititions reps &lt;- 10000 # Set degrees of freedom of a chi-Square Distribution DF &lt;- 3 # Sample 10000 column vectors à 3 N(0,1) R.V.S Z &lt;- replicate(reps, rnorm(DF)) # Column sums of squares X &lt;- colSums(Z^2) # Histogram of column sums of squares hist(X, freq = F, col=&quot;steelblue&quot;, breaks = 40, ylab=&quot;Density&quot;, main=&quot;&quot; ) # Add theoretical density curve(dchisq(x, df = DF), type = &#39;l&#39;, lwd = 2, col=&quot;red&quot;, add = T ) Large Sample Approximations to Sampling Distributions Sampling distributions as considered in the last section play an important role in the development of econometric methods. In general, there are two different approaches in characterizing sampling distributions: an “exact” approach and an “approximate” approach. The exact approach aims to find a general formula for the sampling distribution that holds for any sample size \\(n\\). We call this the exact distribution or finite sample distribution. In the previous examples of die rolling and normal variates, we have dealt with functionals of random variables whose sample distributions are excactly known in the sense that we can write them down as analytical expressions and do calculations. However, this is not always possible. For \\(\\overline{Y}\\), result (2.4) tells us that normality of the \\(Y_i\\) implies normality of \\(\\overline{Y}\\) (we demonstrated this for the special case of \\(Y_i \\overset{i.i.d.}{\\sim} N(0,1)\\) with \\(n=10\\) using a simulation study that involves random sampling). Unfortunately, the exact distribution of \\(\\overline{Y}\\) is unknown, very hard to derive or even untractable if we drop the assumption of \\(Y\\) having a normal distribution. Therefore, as can be guessed from its name, the “approximate” approach aims to find an approximation to the sampling distribution wherby it is required that the sample size \\(n\\) is large. A distribution that is used as a large-sample approximation to the sampling distribution is also called the asymptotic distribution. This is due to the fact that the asymptotic distribution is the sampling distribution for \\(n \\rightarrow \\infty\\) i.e. the approximation becomes exact if the sample size goes to infinity. However, there are cases where the difference between the sampling distribution and the asymptotic distribution is negligible for moderate or even small samples sizes so that approximations work very good. In this section we will discuss two well known results that are used to approximate sampling distributions and thus consitute key tools in econoemtric theory: the law of large numbers and the central limit theorem. The law of large numbers states that in large samples, \\(\\overline{Y}\\) is close to \\(\\mu_Y\\) with high probability. The central limit theorem says that the sampling distribution of the standardized sample average, that is \\((\\overline{Y} - \\mu_Y)/\\sigma_{\\overline{Y}}\\) is asymptotically normal distributed. It is particualarly interesting that both results do not depend on the distribution of \\(Y\\). In other words, beeing unable to describe the complicated sampling distribution of \\(\\overline{Y}\\) if \\(Y\\) is not normal, approximations of the latter using the central limit theorem simplify the development and applicability of econometric procedures enormously. This is a key component underlying the theory of statistical inference for regression models. Both results are summarized in Key Concept 2.6 and Key Concept 2.7. Key Concept 2.6 Convergence in Probability, Consistency and the Law of Large Numbers The sample average \\(\\overline{Y}\\) converges in probability to \\(\\mu_Y\\) — we say that \\(\\overline{Y}\\) is consistent for \\(\\mu_Y\\) — if the probability that \\(\\overline{Y}\\) is in the range \\((\\mu_Y - \\epsilon)\\) to \\((\\mu_Y + \\epsilon)\\) becomes arbitrarly close to \\(1\\) as \\(n\\) increases for any constant \\(\\epsilon &gt; 0\\). We write this short as \\[ \\overline{Y} \\xrightarrow[]{p} \\mu_Y. \\] Consider the independently and identically distributed random variables \\(Y_i, i=1,\\dots,n\\) with expectation \\(E(Y_i)=\\mu_Y\\) and variance \\(\\text{Var}(Y_i)=\\sigma^2_Y\\). Under the condition that \\(\\sigma^2_Y&lt; \\infty\\), that is large outliers are unlikely, the law of large numbers states \\[ \\overline{Y} \\xrightarrow[]{p} \\mu_Y. \\] The core statement of the law of large numbers is that under quite general conditions, the probability of obtaining a sample average \\(\\overline{Y}\\) that is close to \\(\\mu_Y\\) is high if we have a large sample size. Consider the example of repeatedly tossing a coin where \\(Y_i\\) is the result of the \\(i^{th}\\) coin toss. \\(Y_i\\) is a Bernoulli distributed random variable with \\[ P(Y_i) = \\begin{cases} p, &amp; Y_i = 1 \\\\ 1-p, &amp; Y_i = 0 \\end{cases} \\] where \\(p = 0.5\\) as we assume a fair coin. It is straightforward to show that \\[ \\mu_Y = p = 0.5. \\] Say \\(p\\) is the probabiliy of observing head and denote \\(R_n\\) the proportion of heads in the first \\(n\\) tosses, \\[ R_n = \\frac{1}{n} \\sum_{i=1}^n Y_i. \\tag{2.5}\\] According to the law of large numbers, the observed proportion of heads converges in probability to \\(\\mu_Y = 0.5\\), the probability of tossing head in a single coin toss, \\[ R_n \\xrightarrow[]{p} \\mu_Y=0.5 \\ \\ \\text{as} \\ \\ n \\rightarrow \\infty. \\] The following application simulates \\(1000\\) coin tosses with a fair coin and computes the fraction of heads observed for each additional toss interactively. The result is a random path that, as stated by the law of large numbers, shows a tendency to approach the vaule of \\(0.5\\) as \\(n\\) grows. We can use R to compute and illustrate such paths by simulation. The procedure is as follows: Sample N observations from the Bernoulli distribution e.g. using sample(). Calculate the proportion of heads \\(R_n\\) as in (2.5). A way to achieve this is to call cumsum() on the vector of observations Y to obtain its cumulative sum and then divide by the respective number of observations. We continue by plotting the path and also add a dashed line for the benchmark \\(R_n = p = 0.5\\). # set random seed set.seed(1) # Set number of coin tosses and simulate N &lt;- 30000 Y &lt;- sample(0:1, N, replace =T) # Calculate R_n for 1:N S &lt;- cumsum(Y) R &lt;- S/(1:N) # Plot the path. plot(R, ylim=c(0.3, 0.7), type = &quot;l&quot;, col = &quot;steelblue&quot;, lwd = 2, xlab = &quot;n&quot;, ylab = &quot;R_n&quot;, main = &quot;Converging Share of Heads in Repeated Coin Tossing&quot; ) # Add a dashed line for R_n = 0.5 lines(c(0,N), c(0.5, 0.5), col = &quot;darkred&quot;, lty = 2, lwd = 1 ) There are several things to be said about this plot. The blue graph shows the observed proportion of heads when tossing a coin \\(n\\) times. Since the \\(Y_i\\) are radnom variables, \\(R_n\\) is a random variate, too. The path depicted is only one of many possible realisations of \\(R_n\\) as it is determined by the \\(30000\\) observations sampled from the Bernoulli distribution. Thus, the code chunk above produces a differnt path every time You execute it (try this!). If the number of coin tosses \\(n\\) is small, we observe the proportion of heads to be anything but close to its theoretical value, \\(\\mu_Y = 0.5\\). However, as more and more observation are included in the sample we find that the path stabilizes in neighbourhood of \\(0.5\\). This is the message to take away: the average of multiple trials shows a clear tendency to converge to its expected value as the sample size increases, just as claimed by the law of large numbers. Key Concept 2.6 The Central Limit Theorem Suppose that \\(Y_1,\\dots,Y_n\\) are independently and identically distributed random variables with expectation \\(E(Y_i)=\\mu_Y\\) and variance \\(\\text{Var}(Y_i)=\\sigma^2_Y\\), \\(0&lt;\\sigma^2_Y&lt;\\infty\\). The central limit theorem states that, if the sample size \\(n\\) goes to infinity, the distribution of the scaled (by \\(\\sqrt{n}\\)) standardized sample average \\[ \\frac{\\overline{Y} - \\mu_Y}{\\sigma_{\\overline{Y}}} = \\frac{\\overline{Y} - \\mu_Y}{\\sigma_Y/\\sqrt{n}} \\ \\] becomes arbitrarily well approximated by the standard normal distribution. According to the central limit theorem, the distribution of the sample mean \\(\\overline{Y}\\) of the bernoulli distributed random variables \\(Y_i\\), \\(i=1,...,n\\) is well approximated by the normal distribution with parameters \\(\\mu_Y=p=0.5\\) and \\(\\sigma^2_{\\overline{Y}} = p(1-p) = 0.25\\) for large \\(n\\). Consequently, for the standardized sample mean we conclude that the ratio \\[ \\frac{\\overline{Y} - 0.5}{0.5/\\sqrt{n}} \\tag{2.6}\\] should be well approximated by the standard normal distribution \\(N(0,1)\\). We employ another simulation study to demonstrate this graphically. The idea is as follows. Draw a large number of random samples, \\(10000\\) say, of size \\(n\\) from the Bernoulli distribution and compute the sample averages. Standardize the averages as shown in (2.6). Next, visualize the distribution of the generated standardized sample averages by means of a density histogram and compare to the standard normal distribution. Repeat this for different sample sizes \\(n\\) to see how increasing the sample size \\(n\\) impacts the simulated distribution of the averages. In R, we realized this as follows: We start by defining that the next four subsequently generated figures shall be drawn in a \\(2\\times2\\) array such that they can be easily compared. This is done by calling par(mfrow = c(2, 2)) before the figures are generated. We define the number of repetitions reps as \\(10000\\) and create a vector of sample sizes named sample.sizes. We consider samples of sizes \\(2\\), \\(10\\), \\(50\\) and \\(100\\). Next, we combine two for() loops to simulate the data and plot the distributions. The inner loop generates \\(10000\\) random samples, each consisting of n observations that are drawn from the bernoulli distribution, and computes the standardized averages. The outer loop executes the inner loop for the different sample sizes n and produces a plot for each iteration. # Subdivide the plot panel into a 2-by-2 array par(mfrow = c(2, 2)) # Set number of repetitions and the sample sizes reps &lt;- 10000 sample.sizes &lt;- c(2, 10, 50, 100) # outer loop (loop over the sample sizes) for (n in sample.sizes) { samplemean &lt;- rep(0,reps) #initialize the vector of sample menas stdsamplemean &lt;- rep(0,reps) #initialize the vector of standardized sample menas # inner loop (loop over repetitions) for (i in 1:reps) { x &lt;- rbinom(n,1,0.5) samplemean[i] &lt;- mean(x) stdsamplemean[i] &lt;- sqrt(n)*(mean(x)-0.5)/0.5 } # plot the histogram and overlay it with the N(0,1) density for every iteration hist(stdsamplemean, col = &quot;steelblue&quot;, breaks = 40, freq = FALSE, xlim=c(-3, 3), ylim = c(0, 0.4), xlab = paste(&quot;n =&quot;, n), main = &quot;&quot; ) curve(dnorm(x), lwd = 2, col=&quot;darkred&quot;, add = TRUE ) } We see that the simulated sampling distribution of the standardized average tends to deviate strongly from the standard normal distribution if the sample size is small, e.g. for \\(n=5\\) and \\(n=10\\). However as \\(n\\) grows, the histograms are approaching the bell shape of a standard normal and we can be confident that the approximation works quite well as seen for \\(n=100\\). "],
["a-review-of-statistics-using-r.html", "3 A Review of Statistics using R Estimation of the Population Mean Properties of the Population Mean Hypothesis Tests Concerning the Population Mean Confidence intervals for the Population Mean Comparing Means from Different Populations An Application to the Gender Gap of Earnings Scatterplots, Sample Covariance and Sample Correlation", " 3 A Review of Statistics using R This section reviews important statistical concepts: Estimation Hypothesis testing Confidence intervals Since these types of statistical methods are heavily used in econometrics, we will discuss them in the context of inference about an unknown population mean and discuss several applications in R. Estimation of the Population Mean Key Concept 3.1 Estimators and Estimates Estimators are functions of sample data that are drawn randomly from an unknown population. Estimates are numerical values computed by estimators based on the sample data. Estimators are random variables because they are functions of random data. Estimates are nonrandom numbers. Think of some economic variable, for example hourly earnings of college graduates, denoted by \\(Y\\). Suppose we are interested in \\(\\mu_Y\\) the mean of \\(Y\\). In order to exactly calculate \\(\\mu_Y\\) we would have to interview every graduated member of the working population in the economy. We simply cannot do this for time and cost reasons. However, we could draw a random sample from \\(n\\) i.i.d. observations \\(Y_1, \\dots, Y_n\\) and estimate \\(\\mu_Y\\) using one of the simplest estimators in the sense of Key Concept 3.1 one can think of: \\[ \\overline{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i, \\] the sample mean of \\(Y\\). Then again, we could use an even simpler estimator for \\(\\mu_Y\\): the very first observation in the sample, \\(Y_1\\). Is \\(Y_1\\) a good estimator? For now, assume that \\[ Y \\sim \\chi_{12}^2 \\] which is not too unreasonable as the measure is nonnegative and we expect many hourly earnings to be in a range of \\(5€\\) to \\(15€\\). Moreover, it is common for income distributions to be skewed to the right. # plot the chi_12^2 distribution curve(dchisq(x, df=12), from = 0, to = 40, ylab = &quot;density&quot;, xlab = &quot;hourly earnings in Euro&quot; ) We draw a sample of \\(n=100\\) observations and take the first observation \\(Y_1\\) as an estimate for \\(\\mu_Y\\) # set seed for reproducibility set.seed(1) # sample from the chi_12^2 distribution, keep only the first observation rchisq(n = 100, df = 12)[1] ## [1] 8.257893 The estimate \\(8.26\\) is not too far away from \\(\\mu_Y = 12\\) but it is somewhat intuitive that we could do better: the estimator \\(Y_1\\) discards a lot of information and its variance is the population variance: \\[ \\text{Var}(Y_1) = \\text{Var}(Y) = 2 \\cdot 12 = 24 \\] This brings us to the following question: What is a ‘good’ estimator in the first place? This question is tackled in Key Concepts 3.2 and 3.3 Key Concept 3.2 Bias, Consistency and Efficiency Disirable characteristics of an estimator are unbiasedness, consitency and Efficiency. Unbiasedness: If the mean of the sampling distribution of some estimator \\(\\hat\\mu_Y\\) for the population mean \\(\\mu_Y\\) equals \\(\\mu_Y\\) \\[ E(\\hat\\mu_Y) = \\mu_Y \\] we say that the estimator is unbiased for \\(\\mu_Y\\). The bias of \\(\\hat\\mu_Y\\) is \\(0\\): \\[ E(\\hat\\mu_Y) - \\mu_Y = 0\\] Consistency: We want the uncertainty of the estimator \\(\\mu_Y\\) to decrease as the number of observations in the sample grows. More precisely, we want the proabability that the estimate \\(\\hat\\mu_Y\\) falls within a small interval of the true value \\(\\mu_Y\\) to get increasingly closer to \\(1\\) as \\(n\\) grows. We write this as \\[ \\hat\\mu_Y \\xrightarrow{p} \\mu_Y. \\] Variance and efficiency: We want the estimator to be efficient. Suppose we have two estimators, \\(\\hat\\mu_Y\\) and \\(\\overset{\\sim}{\\mu}_Y\\) and for some given sample size \\(n\\) it holds that \\[ E(\\hat\\mu_Y) = E(\\overset{\\sim}{\\mu}_Y) = \\mu_Y \\] but \\[\\text{Var}(\\hat\\mu_Y) &lt; \\text{Var}(\\overset{\\sim}{\\mu}_Y).\\] We then would prefer to use \\(\\hat\\mu_Y\\) as it has a lower variance than \\(\\overset{\\sim}{\\mu}_Y\\), meaning that \\(\\hat\\mu_Y\\) is more efficient in using the information provided by the observations in the sample. Key Concept 3.3 Efficiency of \\(\\overline{Y}\\): The BLUE property Let \\(\\hat\\mu_Y\\) be a linear and unbiased estimator of \\(\\mu_Y\\) in the fashion of \\[ \\hat\\mu_Y = \\frac{1}{n} \\sum_{i=1}^n a_i Y_i\\] with nonrandom constants \\(a_i\\). We see that \\(\\hat\\mu_Y\\) is a weighted average of the \\(Y_i\\) and the \\(a_i\\) are weights. For these type of estimators, \\(\\overline{Y}\\) with \\(a_i = 1\\) for all \\(i = 1, \\dots, n\\) is the most efficient estimator. We say that \\(\\overline{Y}\\) is the BestLinear Unbiased Estimator (BLUE). Properties of the Population Mean To examine properties of the sample mean as an estimator for the corresponding population mean, consider the following R example. We generate a population pop which consists observations \\(Y_i \\ , \\ i=1,\\dots,10000\\) that stem from a normal distribution with mean \\(\\mu = 10\\) and variance \\(\\sigma^2 = 1\\). To investigate how the estimator \\(\\hat{\\mu} = \\bar{Y}\\) behaves we can draw random samples from this population and calculate \\(\\bar{Y}\\) for each of them. This is easily done by making use of the function replicate(). Its argument expr is evaluated n times. In this case we draw samples of sizes \\(n=5\\) and \\(n=25\\), compute the sample means and repeat this exactly \\(n=25000\\) times. For comparison purposes we store results for the estimator \\(Y_1\\), the first observation in a sample for a sample of size \\(5\\) separately. # generate a fictive population pop &lt;- rnorm(10000, 10, 1) # sample form pop and estimate the mean est1 &lt;- replicate(expr = mean(sample(x = pop, size = 5)), n = 25000) est2 &lt;- replicate(expr = mean(sample(x = pop, size = 25)), n = 25000) fo &lt;- replicate(expr = sample(x = pop, size = 5)[1], n = 25000) Check that est1 and est2 are vectors of length \\(25000\\): # check if object type is vector is.vector(est1) ## [1] TRUE is.vector(est2) ## [1] TRUE # check lengths length(est1) ## [1] 25000 length(est2) ## [1] 25000 The code chunk below produces a plot of the sampling distributions of the estimators \\(\\bar{Y}\\) and \\(Y_1\\) on the basis of the \\(25000\\) samples in each case. We also plot a curve depicting the density function of the \\(N(10,1)\\) distribution. # plot density estimate Y_1 plot(density(fo), col = &#39;green&#39;, lwd = 2, ylim = c(0,2), xlab = &#39;estimates&#39;, main = &#39;Sampling Distributions of Unbiased Estimators&#39; ) # add density estimate for the distribution of the sample mean with n=5 to the plot lines(density(est1), col = &#39;steelblue&#39;, lwd = 2, bty = &#39;l&#39; ) # add density estimate for the distribution of the sample mean with n=25 to the plot lines(density(est2), col = &#39;red2&#39;, lwd = 2 ) # add a vertical line marking the true parameter abline(v = 10, lty = 2) # add N(10,1) density to the plot curve(dnorm(x, mean=10), lwd = 2, lty = 2, add = T ) # add a legend legend(&quot;topleft&quot;, legend = c(&quot;N(10,1)&quot;, expression(Y[1]), expression(bar(Y) ~ n==5), expression(bar(Y) ~ n==25) ), lty = c(2, 1, 1, 1), col = c(&#39;black&#39;,&#39;green&#39;, &#39;steelblue&#39;, &#39;red2&#39;), lwd = 2 ) At first, notice how all sampling distributions (represented by the solid lines) are centered around \\(\\mu = 10\\). This is evidence for the unbiasedness of \\(Y_1\\) and \\(\\overline{Y}\\). Of course, the theoretical density the \\(N(10,1)\\) distribution is centered at \\(10\\), too. Next, have a look add the spread of the sampling distributions. Several things are remarkable: First, the sampling distribution of \\(Y_1\\) (green curve) tracks the density of the \\(N(10,1)\\) distribution (black dashed line) pretty closely In fact, the sampling distribution of \\(Y_1\\) is the \\(N(10,1)\\) distribution. This is less surprising if You keep in mind that \\(Y_1\\) estimator does nothing but reporting an observation that is randomly selected from a population with \\(N(10,1)\\) distribution. Hence, \\(Y_1 \\sim N(10,1)\\). Note that this result is invariant to the sample size \\(n\\): the sampling distribution of \\(Y_1\\) is always the population distribution, no how large the sample is. Second, both sampling distributions of \\(\\overline{Y}\\) show less dispersion than the sampling distribution of \\(Y_1\\). This means that \\(\\overline{Y}\\) has a lower variance than \\(Y_1\\). In view of Key Concepts 3.2 and 3.3, we find that \\(\\overline{Y}\\) is a more efficient estimator than \\(Y_1\\). In fact, one can show that this holds for all \\(n&gt;1\\). Third, \\(\\overline{Y}\\) shows a behaviour that is termed consistency (see Key Concept 3.2). Notice that the blue and the red density curves are much more concentrated around \\(\\mu=10\\) then the green one. As the number of observations is increased from \\(1\\) to \\(5\\), the sampling distribution tightens around the true parameter. This effect is more dominant as the sample size is increased to \\(25\\). This implies that the probability of obtaining estimates that are close to the true value increases with \\(n\\). A more precise way to express consitency of an estimator \\(\\hat\\mu\\) for a parameter \\(\\mu\\) is \\[ P(|\\hat{\\mu} - \\mu|&lt;\\epsilon) \\xrightarrow[n \\rightarrow \\infty]{p} 1 \\quad \\text{for any}\\quad\\epsilon&gt;0.\\] This expression says that the probability of observing a deviation from the true value \\(\\mu\\) that is smaller than some arbitrary \\(\\epsilon &gt; 0\\) converges to \\(1\\) as \\(n\\) grows. Note that consistency does not require unbiasedness: We encourage You to go ahead and modify the code. Try out different values for the sample size and see how the sampling distribution of \\(\\overline{Y}\\) changes! \\(\\overline{Y}\\) is the least squares estimator of \\(\\mu_Y\\) Assume You have some observations \\(Y_1,\\dots,Y_n\\) on \\(Y \\sim N(10,1)\\) (which is unknown) and would like to find an estimator \\(m\\) that predicts the observations as good as possible where good means to choose \\(m\\) such that the total deviation between the predicted value and the observed values is small. Mathematically this means we want to find an \\(m\\) that minimizes \\[\\begin{equation} \\sum_{i=1}^n (Y_i - m)^2. \\tag{3.1} \\end{equation}\\] Think of \\(Y_i - m\\) as the comitted mistake when predicting \\(Y_i\\) by \\(m\\). We could just as well minimize the sum of absolute deviations from \\(m\\) but minimizing the sum of squared deviations is mathematically more convenient and leads, roughly speaking, to the same result. That is why the estimator we are looking for is called the least squares estimator. As It turns out \\(m = \\overline{Y}\\), the estimator of \\(\\mu_Y=10\\) is this wanted estimator. We can show this by generating a random sample of fair size and plotting (3.1) as a function of \\(m\\). # define the function and vectorize it sqm &lt;- function(m) { sum((y-m)^2) } sqm &lt;- Vectorize(sqm) # draw random sample and compute the mean y &lt;- rnorm(100, 10, 1) mean(y) ## [1] 10.00543 # plot the objective function curve(sqm(x), from = -50, to = 70, xlab = &quot;m&quot;, ylab =&quot;sqm(m)&quot; ) # add vertical line at mean(y) abline(v = mean(y), lty = 2, col = &quot;darkred&quot; ) Notice that (3.1) is a quadratic function so there is only one minimum. The plot shows that this minimum lies exactly at the sample mean of the sample data. Some R functions can only interact with functions that take a vector as input evaluate the function body on every values of the vector, for example curve(). We call such functions vectorized functions and it is often a good idea to write vectorized functions although this is cumbersome in some cases. Having a vectorized function in R is never a drawback since these functions work on both single values and vectors. Let us look at the function sqm() which is nonvectorized sqm &lt;- function(m) { sum((y-m)^2) #body of the function } Providing e.g. c(1,2,3) as the argument m would cause an error since then the operation y-m is invalid: the vecors y and m are of incompatible dimensions. This is why we cannot use sqm() in conjunction with curve(). Here comes Vectorize() into play. It generates a vectorized version of a non-vectorized function. Why Random Sampling is important So far, we assumed (somtimes implicitly) that observed data \\(Y_1, \\dots, Y_n\\) are the result of a sampling process that satisfies the assumption of i.i.d. random sampling. It is very important that this assumption is fulfilled when estimating a population mean using \\(\\overline{Y}\\). If this is not the case, estimates are biased. Let us fall back to pop, the fictive population of \\(10000\\) observations and compute the population mean \\(\\mu_{\\texttt{pop}}\\): # compute the population mean of pop mean(pop) ## [1] 9.992604 Next we sample \\(10\\) observations from pop with sample() and estimate \\(\\mu_{\\texttt{pop}}\\) using \\(\\overline{Y}\\) repeatedly. However this time we use a sampling scheme that deviates from simple random sampling: instead of ensuring that each member of the population has the same chance to end up in a sample, we assign a higher probability of beeing sampled to the \\(2500\\) smallest observations of the population by setting the argument prop to a suitable vector of probability weights: # simulate outcome for the sample mean when the i.i.d. assumption fails est3 &lt;- replicate(n = 25000, expr = mean(sample(x = sort(pop), size = 10, prob = c(rep(4,2500),rep(1,7500)) ) ) ) # compute the sample mean of the outcomes mean(est3) ## [1] 9.443454 Next we plot the sampling distribution of \\(\\overline{Y}\\) for this non-i.i.d. case an compare it to the sampling distribution when the i.i.d. assumption holds. # sampling distribution of sample mean, i.i.d. holds, n=25 plot(density(est2), col = &#39;red2&#39;, lwd = 2, xlim = c(8,11), xlab = &#39;estimates&#39;, main = &#39;When the i.i.d. Assumption Fails&#39; ) # sampling distribution of sample mean, i.i.d. fails, n=25 lines(density(est3), col = &#39;steelblue&#39;, lwd = 2 ) # add a legend legend(&quot;topleft&quot;, legend = c(expression(bar(Y) ~ &quot;,&quot; ~ n==25 ~ &quot;, i.i.d. fails&quot;), expression(bar(Y) ~ &quot;,&quot; ~ n==25 ~ &quot;, i.i.d. holds&quot;) ), lty = c(1, 1), col = c(&#39;red2&#39;, &#39;steelblue&#39;), lwd = 2 ) We find that in this case failure of the i.i.d. assumption implies that, on average, we underestimate \\(\\mu_Y\\) using \\(\\overline{Y}\\): the corresponding distribution of \\(\\overline{Y}\\) is shifted to the left. In other words, \\(\\overline{Y}\\) is a biased estimator for \\(\\mu_Y\\) if the i.i.d. assumption does not hold. Hypothesis Tests Concerning the Population Mean In this section we briefly review concepts in hypothesis testing and discuss how to conduct hypothesis tests in R. We focus on drawing inference about an unkown population mean. About Hypotheses and Hypothesis Testing In a significance test we want to exploit the information contained in a random sample as evidence in favour or against a hypothesis. Essentially, hypotheses are simple question that can be answered by ‘yes’ or ‘no’. When conducting a hypothesis test we always deal with two different hypotheses: The null hypothesis, denoted \\(H_0\\) is the hypothesis we are interested in testing The alternative hypothesis, denoted \\(H_1\\), is the hypothesis that holds if the null hypothesis is false The null hypothesis that the population mean of \\(Y\\) equals the value \\(\\mu_{Y,0}\\) is written down as \\[ H_0: E(Y) = \\mu_{Y,0}. \\] The alternative hypothesis states what holds if the null hypothesis is false. Often the alternative hypothesis chosen is the most general one, \\[ H_1: E(Y) \\neq \\mu_{Y,0}, \\] meaning that \\(E(Y)\\) may be anything else but the value as the null hypothesis. This is called a two-sided alternative. For brevity, we will only consider the case of a two-sided alternative in the subsequent sections of this chapter. \\(p\\)-Value Assume that the null hypothesis is true. The \\(p\\)-value is the probability of drawing data and observing a corresponding test statistics that is at least as adverse to what is stated under the null hypothesis as the test statistic actually computed using the sample data. In context of population mean and sample mean, this definition can be stated mathematically in the following way: \\[\\begin{equation} p \\text{-value} = P_{H_0}\\left[ \\lvert \\overline{Y} - \\mu_{Y,0} \\rvert &gt; \\lvert \\overline{Y}^{act} - \\mu_{Y,0} \\rvert \\right] \\tag{3.2} \\end{equation}\\] In (3.2), \\(\\overline{Y}^{act}\\) is the acutally computed mean of the random sample. Visualized, the \\(p\\)-value is the area in the part of tails of the distribution of \\(\\overline{Y}\\) that lies beyond \\[ \\mu_{Y,0} \\pm \\lvert \\overline{Y}^{act} - \\mu_{Y,0} \\rvert. \\] Consequently, in order to compute the \\(p\\)-value as in (3.2), knowledge about the sampling distribution of \\(\\overline{Y}\\) when the null hypothesis is true is required. However in most cases the sampling distribution of \\(\\overline{Y}\\) is unkown. Furtunately, due to the large-sample normal approximation (see chapter 3) we know that under the null hypothesis \\[ \\overline{Y} \\sim N(\\mu_{Y,0}, \\, \\sigma^2_{\\overline{Y}}) \\ \\ , \\ \\ \\sigma^2_{\\overline{Y}} = \\frac{\\sigma_Y^2}{n} \\] and thus \\[ \\frac{\\overline{Y} - \\mu_{Y,0}}{\\sigma_Y/\\sqrt{n}} \\sim N(0,1). \\] So in large samples, the \\(p\\)-value can be computed without knowledge about the sampling distribution of \\(\\overline{Y}\\). Calculating the \\(p\\)-Value When \\(\\sigma_Y\\) Is Known For now, let us assume that \\(\\sigma_\\overline{Y}\\) is known. Then we can rewrite (3.2) as \\[\\begin{align} p \\text{-value} =&amp; \\, P_{H_0}\\left[ \\left\\lvert \\frac{\\overline{Y} - \\mu_{Y,0}}{\\sigma_\\overline{Y}} \\right\\rvert &gt; \\left\\lvert \\frac{\\overline{Y}^{act} - \\mu_{Y,0}}{\\sigma_\\overline{Y}} \\right\\rvert \\right] \\\\ =&amp; \\, 2 \\cdot \\Phi \\left[ - \\left\\lvert \\frac{\\overline{Y}^{act} - \\mu_{Y,0}}{\\sigma_\\overline{Y}} \\right\\rvert\\right]. \\tag{3.3} \\end{align}\\] so the \\(p\\)-value can be seen as the area in the tails of the \\(N(0,1)\\) distribution that lies beyond \\[\\begin{equation} \\pm \\left\\lvert \\frac{\\overline{Y}^{act} - \\mu_{Y,0}}{\\sigma_\\overline{Y}} \\right\\rvert \\tag{3.4} \\end{equation}\\] Whew, that was a lot of theory. Now we use R to visualize what is stated in (3.3) and (3.4). The next code chunck replicates figure 3.1 of the book. # plot the standard normal density on the domain [-4,4] curve(dnorm(x), xlim = c(-4,4), main = &#39;Calculating a p-value&#39;, yaxs = &#39;i&#39;, xlab = &#39;z&#39;, ylab = &#39;&#39;, lwd = 2, axes = &#39;F&#39; ) # add x-axis axis(1, at = c(-1.5,0,1.5), padj = 0.75, labels = c(expression(-frac(bar(Y)^&quot;act&quot;~-~bar(mu)[Y,0],sigma[bar(Y)])), 0, expression(frac(bar(Y)^&quot;act&quot;~-~bar(mu)[Y,0],sigma[bar(Y)]))) ) # shade p-value/2 region in left tail polygon(x = c(-6, seq(-6,-1.5,0.01),-1.5), y = c(0, dnorm(seq(-6,-1.5,0.01)),0), col = &#39;steelblue&#39; ) ## shade p-value/2 region in right tail polygon(x = c(1.5, seq(1.5, 6, 0.01), 6), y = c(0, dnorm(seq(1.5, 6, 0.01)), 0), col = &#39;steelblue&#39; ) Sample Variance, Sample Standard Deviation and Standard Error If \\(\\sigma^2_Y\\) is unknown, it must be estimated. This can be done efficiently using the sample variance \\[\\begin{equation} s_y^2 = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\overline{Y})^2. \\end{equation}\\] Furthermore \\[\\begin{equation} s_y = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\overline{Y})^2}. \\end{equation}\\] is a suitable estimator for the standard deviation of \\(Y\\). In R, \\(s_y\\) is implemented in the function sd(), see ?sd. Using R we can get a notion that \\(s_y\\) is a consistent estimator for \\(\\sigma_Y\\), that is \\[ s_Y \\overset{p}{\\longrightarrow} \\sigma_Y. \\] The idea here is to generate a large number of samples \\(Y_1,\\dots,Y_n\\) where \\(Y\\sim N(10,10)\\), estimate \\(\\sigma_Y\\) using \\(s_y\\) and investigate how the distribution of \\(s_Y\\) changes as \\(n\\) grows. # vector of sample sizes n &lt;- c(10000, 5000, 2000, 1000, 500) # sample observations, estimate using sd() and plot estimated distributions s2_y &lt;- replicate(n = 10000, expr = sd(rnorm(n[1], 10, 10))) plot(density(s2_y), main = expression(&#39;Sampling Distributions of&#39; ~ s[y]), xlab = expression(s[y]), lwd = 2 ) for (i in 2:length(n)) { s2_y &lt;- replicate(n = 10000, expr = sd(rnorm(n[i],10,10))) lines(density(s2_y), col=i, lwd=2) } # add a legend legend(&quot;topleft&quot;, legend = c(expression(n==10000), expression(n==5000), expression(n==2000), expression(n==1000), expression(n==500) ), col = 1:5, lwd = 2 ) The plot shows that the distribution of \\(s_Y\\) tightens around the true value \\(\\sigma_Y = 10\\) as \\(n\\) increases. The function that estimates the standard deviation of an estimator is called the standard error of the estimator. Key Concept 3.4 summarizes the terminology in the context of the sample mean. Key Concept 3.4 The Standard Error of \\(\\overline{Y}\\) Take an i.i.d. sample \\(Y_1, \\dots, Y_n\\). The mean of \\(Y\\) can be consistently estimated using \\(\\overline{Y}\\), the sample mean of the \\(Y_i\\). Since \\(\\overline{Y}\\) is a random variable, it has a sampling distribution with variance \\(\\frac{\\sigma_Y^2}{n}\\). The standard error of \\(\\overline{Y}\\), denoted \\(SE(\\overline{Y})\\) is an estimator of the standard deviation \\(\\overline{Y}\\): \\[ SE(\\overline{Y}) = \\hat\\sigma_\\overline{Y} = \\frac{s_Y}{\\sqrt{n}} \\] The caret (^) over \\(\\sigma\\) indicates that \\(\\hat\\sigma_\\overline{Y}\\) is an estimator for \\(\\sigma_\\overline{Y}\\). As an example to underpin Key Concept 3.4, consider a sample of \\(n=100\\) i.i.d. observations of the bernoulli distributed variable \\(Y\\) with success probability \\(p=0.1\\) and thus \\(E(Y)=p=0.1\\) and \\(\\text{Var}(Y)=p(1-p)\\). \\(E(Y)\\) can be estimated by \\(\\overline{Y}\\) which then has variance \\[ \\sigma^2_\\overline{Y} = p(1-p)/n = 0.0009 \\] and standard deviation \\[ \\sigma_\\overline{Y} = \\sqrt{p(1-p)/n} = 0.03. \\] In this case the standard error of \\(\\overline{Y}\\) is given as \\[ SE(\\overline{Y}) = \\sqrt{\\overline{Y}(1-\\overline{Y})/n} \\] Let verify whether \\(\\overline{Y}\\) and \\(SE(\\overline{Y})\\) estimate the respective true values on average. # draw 10000 samples of size 100 and estimate the mean of Y and # estimate the standard error of the sample mean mean_estimates &lt;- numeric(10000) se_estimates &lt;- numeric(10000) for (i in 1:10000) { s &lt;- sample(0:1, size = 100, prob = c(0.9, 0.1), replace = T ) mean_estimates[i] &lt;- mean(s) se_estimates[i] &lt;- sqrt(mean(s)*(1-mean(s))/100) } mean(mean_estimates) ## [1] 0.099693 mean(se_estimates) ## [1] 0.02953467 Both estimators seem to be unbiased for the true parameters. Calculating the \\(p\\)-value When \\(\\sigma_Y\\) is Unknown When \\(\\sigma_Y\\) is unkown, the \\(p\\)-value for a hypothesis test about \\(\\mu_Y\\) using \\(\\overline{Y}\\) can be computed by replacing \\(\\sigma_\\overline{Y}\\) in (3.3) by the standard error \\(SE(\\overline{Y}) = \\hat\\sigma_Y\\). Then, \\[ p\\text{-value} = 2\\cdot\\Phi\\left(-\\left\\lvert \\frac{\\overline{Y}^{act}-\\mu_{Y,0}}{SE(\\overline{Y})} \\right\\rvert \\right). \\] This is easily done in R: # sample and estimate, compute standard error and make a hypothesis samplemean_act &lt;- mean( sample(0:1, prob = c(0.9,0.1), replace = T, size = 100 ) ) SE_samplemean &lt;- sqrt(samplemean_act * (1-samplemean_act)/100) mean_h0 &lt;- 0.1 #true null hypothesis # compute the pvalue pvalue &lt;- 2 * pnorm(-abs(samplemean_act-mean_h0)/SE_samplemean) pvalue ## [1] 0.5382527 The \\(t\\)-statistic In hypothesis testing, the standardized sample average \\[\\begin{equation} t = \\frac{\\overline{Y} - \\mu_{Y,0}}{SE(\\overline{Y})} \\tag{3.5} \\end{equation}\\] is called \\(t\\)-statistic. This \\(t\\)-statistic has an important role when testing hypothesis about \\(\\mu_Y\\). It is a prominent example of a test statistic. Implicitly, we already have computed a \\(t\\)-statistic for \\(\\overline{Y}\\) in the previous code chunk. # compute a t-statistic for the sample mean tstatistic &lt;- (samplemean_act - mean_h0) / SE_samplemean tstatistic ## [1] 0.6154575 Using R we can show that if \\(\\mu_{Y,0}\\) equals the true value, that is the null hypothesis is true, (3.5) is approximately distributed \\(N(0,1)\\) when \\(n\\) is large. # initialize empty vector for t-statistics tstatistics &lt;- numeric(10000) # set sample size n &lt;- 300 # simulate 10000 t-statistics for (i in 1:10000) { s &lt;- sample(0:1, size = n, prob = c(0.9, 0.1), replace = T ) tstatistics[i] &lt;- (mean(s)-0.1)/(sqrt(mean(s)*(1-mean(s))/n)) } # plot density and compare to N(0,1) density plot(density(tstatistics), xlab = &#39;t-statistic&#39;, main = &#39;Distribution of the t-statistic when n=300&#39;, lwd = 2, xlim = c(-4,4), col = &#39;steelblue&#39; ) # N(0,1) density (dashed) curve(dnorm(x), add = T, lty = 2, lwd= 2 ) Judging from the plot, the normal approximation works reasonably well for the chosen sample size. This normal approximation has already been used in the definition of the \\(p\\)-value, see (3.5). Hypothesis Testing with a Prespecified Significance Level Key Concept 3.5 The Terminology of Hypothesis Testing In hypothesis testing, two types of mistakes are possible: The null hypothesis is rejected although it is true (\\(\\alpha\\)-error / type-I-error) The null hypothesis is not rejected although it is false (\\(\\beta\\)-error / type-II-error) The significance level of the test is the probability to commit a type-I-error we are willing to accept in advance. E.g. using a prespecified significance level of \\(0.05\\), we reject the null hypothesis if and only if the \\(p\\)-value is less than \\(0.05\\). The significance level is chosen before the test is conducted. An equivalent procedure is to reject the null hypothesis if the test statistic observed is, in absolute value terms, larger than the critical value of the test statistic. The critical value is determined by the significance level chosen and defines two disjoint sets of values which are called acceptance region and rejection region. The acceptance region contains all values of the test statistic for which the test does not reject while the rejection region contains all the values for which the test does reject. The \\(p\\)-value is the probability that, in repeated sampling under the same conditions, meaning i.i.d. sampling, the same null hypothesis and the same sample size, a test statistic is observed that provides just as much evidence against the null hypothesis as the test statistic actually observed. The actual probability that the test rejects the true null hypothesis is called the size of the test. In an ideal setting, the size does not exceed the significance level. The probability that the test correctly rejects a false null hypothesis is called power. Reconsider pvalue computed further above: # check whether p-value &lt; 0.05 pvalue &lt; 0.05 ## [1] FALSE The condition is not fulfilled so we do not reject the null hypotheis (remember that the null hypothesis is true in this example). When working with a \\(t\\)-statistic instead, it is equivalent to apply the following rule: \\[ \\text{Reject } H_0 \\text{ if } \\lvert t^{act} \\rvert &gt; 1.96 \\] We reject the null hypothesis at the significance level of \\(5\\%\\) if the computed \\(t\\)-statistic lies beyond the critical value of 1.96 in absolute value terms. \\(1.96\\) is the \\(0.05\\)-quantile of the standard normal distribution. # check the critical value qnorm(p = 0.05) ## [1] -1.644854 # check whether the null is rejected using the t-statistic computed further above abs(tstatistic) &gt; 1.96 ## [1] FALSE As when using the \\(p\\)-value, we cannot reject the null hypothesis using the corresponding \\(t\\)-statistic. Key Concept 3.6 summarizes the procedure of performing a two-sided hypothesis about the population mean \\(E(Y)\\). Key Concept 3.6 Testing the Hypothesis \\(E(Y) = \\mu_{Y,0}\\) Against the Alternative \\(E(Y) \\neq \\mu_{Y,0}\\) Estimate \\(\\mu_{Y}\\) using \\(\\overline{Y}\\) and compute the standard error of \\(\\overline{Y}\\), \\(SE(\\overline{Y})\\). Compute the \\(t\\)-statistic. Compute the \\(p\\)-value and reject the null hypothesis at the \\(5\\%\\) level of significance if the \\(p\\)-value is smaller than \\(0.05\\) or equivalently, if \\[ \\left\\lvert t^{act} \\right\\rvert &gt; 1.96. \\] One-sided Alternatives Sometimes we are interested in finding evidence that the mean is bigger or smaller than the some value hypothesized under the null. One can come up with many examples here but, to stick to the book, take the presumed wage differential between good and less educated working individuals. Since we hope that this differential exists, a relevant alternative (to the null hypothesis that there is no wage differential) is that good educated individuals earn more, i.e. that the average hourly wage for this group, \\(\\mu_Y\\) is bigger than \\(\\mu_{Y,0}\\) the know average wage of less educated workers. This is an example of a right-sided test and the hypotheses pair is chosen as \\[ H_0: \\mu_Y = \\mu_{Y,0} \\ \\ \\text{vs} \\ \\ H_1: \\mu_Y &gt; \\mu_{Y,0}. \\] We reject the null hypothesis if the computed test-statistic is larger than the critical value \\(1.64\\), the \\(0.95\\)-quantile of the \\(N(0,1)\\) distribution. This ensures that \\(1-0.95=5\\%\\) probability mass remains in the area to the right of the critical value. Similar as before we can visualize this in R using the function polygon(). # plot the standard normal density on the domain [-4,4] curve(dnorm(x), xlim = c(-4,4), main = &#39;Rejection Region of a Right-Sided Test&#39;, yaxs = &#39;i&#39;, xlab = &#39;t-statistic&#39;, ylab = &#39;&#39;, lwd = 2, axes = &#39;F&#39; ) # add x-axis axis(1, at = c(-4,0,1.64,4), padj = 0.5, labels = c(&#39;&#39;,0,expression(Phi^-1~(.95)==1.64),&#39;&#39;) ) # shade rejection region in right tail polygon(x = c(1.64, seq(1.64, 4, 0.01), 4), y = c(0, dnorm(seq(1.64, 4, 0.01)), 0), col = &#39;darkred&#39; ) In an analogously manner for the left-sided test we have \\[ H_0: \\mu_Y = \\mu_{Y,0} \\ \\ \\text{vs.} \\ \\ H_1: \\mu_Y &lt; \\mu_{Y,0}. \\] The null is rejected if the observed test statistic falls short of the critical value which, for a test at the \\(0.05\\) level of significance, is given by \\(-1.64\\), the \\(0.05\\)-quantile of the \\(N(0,1)\\) distribution. \\(5\\%\\) probability mass lies to the left of the critical value. It is straight forward to adapt the code chunk above to the case of a left-sided test. We only have to fiddle with the color shading and the tick marks. # plot the standard normal density on the domain [-4,4] curve(dnorm(x), xlim = c(-4,4), main = &#39;Rejection Region of a Left-Sided Test&#39;, yaxs = &#39;i&#39;, xlab = &#39;t-statistic&#39;, ylab = &#39;&#39;, lwd = 2, axes = &#39;F&#39; ) # add x-axis axis(1, at = c(-4,0,-1.64,4), padj = 0.5, labels = c(&#39;&#39;,0,expression(Phi^-1~(.05)==-1.64),&#39;&#39;) ) # shade rejection region in right tail polygon(x = c(-4, seq(-4, -1.64, 0.01), -1.64), y = c(0, dnorm(seq(-4, -1.64, 0.01)), 0), col = &#39;darkred&#39; ) Confidence intervals for the Population Mean As stressed before, we will never estimate the exact value of a the population mean of \\(Y\\) using a random sample. However, we can compute confidence intervals for the population mean. In general, a confidence interval for a unkown parameter is a set of values that contains the true parameter with a prespecified probability, the confidence level. Confidence intervals are computed using the information available in the sample. Since this information is the result of a random process, confidence intervals are random variables themselves. Key Concept 3.7 shows how to compute confidence intervals for the unknown population mean \\(E(Y)\\). Key Concept 3.6 Confidence Intervals for the Population Mean A \\(95\\%\\) confidence interval for \\(\\mu_Y\\) is a random variable that contains the true \\(\\mu_Y\\) in \\(95\\%\\) of all possible random samples. When \\(n\\) is large we can use the normal approximation. Then, \\(99\\%\\), \\(95\\%\\), \\(90\\%\\) confidence intervals are \\[\\begin{align} &amp;99\\%\\text{ confidence interval for } \\mu_Y = \\left\\{ \\overline{Y} \\pm 2.58 \\times SE(\\overline{Y}) \\right\\}. \\\\ &amp;95\\%\\text{ confidence interval for } \\mu_Y = \\left\\{ \\overline{Y} \\pm 1.96 \\times SE(\\overline{Y}) \\right\\}. \\\\ &amp;90\\%\\text{ confidence interval for } \\mu_Y = \\left\\{ \\overline{Y} \\pm 1.64 \\times SE(\\overline{Y}) \\right\\}. \\end{align}\\] These confidence intervals are sets of null hypotheses we cannot reject in a two-sided hypothesis test at the given level of confidence. Now consider the following statements. The interval \\[ \\left\\{ \\overline{Y} \\pm 1.96 \\times SE(\\overline{Y}) \\right\\} \\] covers the true value of \\(\\mu_Y\\) with a probability of \\(95\\%\\). We have computed \\(\\overline{Y} = 5.1\\) and \\(SE(\\overline{Y}=2.5\\) so the interval \\[ \\left\\{ 5.1 \\pm 1.96 \\times 2.5 \\right\\} = \\left[0.2,10\\right] \\] covers the true value of \\(\\mu_Y\\) with a probability of \\(95\\%\\). While 1. is right (this is exactly in line with the definition above), 2. is completely wrong and none of Your lecturers wants to read such a sentence in a term paper, written exam or similar, believe us. The difference is that, while 1. is the definition of a random variable, 2. is one possible outcome of this random variable so there is no meaning in making any probabilistic statement about it. Either the computed interval does cover \\(\\mu_Y\\) or it does not! In R, testing hypothesis about the mean of a population on the basis of a random sample is very easy due to functions like t.test() from the stats package. It procudes an object of type list. Luckily, one of the most simple ways to use t.test() is when You want to obtain a \\(95\\%\\) confidence interval for some population mean. We start by generating some random data and calling t.test() in conjunction with ls() to obtain a breakdown of the output components. # set random seed set.seed(1) # generate some sample data sampledata &lt;- rnorm(100,10,10) # checke type typeof(t.test(sampledata)) ## [1] &quot;list&quot; # display list elements produced by t.test ls( t.test(sampledata) ) ## [1] &quot;alternative&quot; &quot;conf.int&quot; &quot;data.name&quot; &quot;estimate&quot; &quot;method&quot; ## [6] &quot;null.value&quot; &quot;p.value&quot; &quot;parameter&quot; &quot;statistic&quot; Though we find that many items are reported, at the moment we are interested in computing a \\(95\\%\\) confidence set for the mean. t.test(sampledata)$&quot;conf.int&quot; ## [1] 9.306651 12.871096 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 This tells us that the \\(95\\%\\) confidence interval is \\[ \\left[9.31, 12.87\\right]. \\] In this example, the computed interval does cover the true \\(\\mu_Y\\) which we know to be \\(10\\). Let us have a look at the whole standard output produced by t.test(). t.test(sampledata) ## ## One Sample t-test ## ## data: sampledata ## t = 12.346, df = 99, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 9.306651 12.871096 ## sample estimates: ## mean of x ## 11.08887 We see that t.test() not only computes a \\(95\\%\\) confidence interval but automatically conducts a two-sided significance test of the hypothesis \\(H_0: \\mu_Y = 0\\) at the level of \\(5\\%\\) and reports relevant parameters thereof: the alternative hypothesis, the estimated mean, the resulting \\(t\\)-statistic, the degrees of freedom of the underlying \\(t\\) distribution (t.test() does not perform the normal approximation) and the corresponding \\(p\\)-value. Very convenient! In this example, we come to the conclusion that the population mean is not significantly different from \\(0\\) at the level of \\(5\\%\\) (which is correct), since \\(\\mu_Y = 0\\) is element of the \\(95\\%\\) confidence interval \\[ 0 \\in \\left[-0.27,0.12\\right]. \\] We come to an equivalent result when using the \\(p\\)-value rejection rule: \\[ p = 0.456 &gt; 0.05 \\] Comparing Means from Different Populations Suppose You are interested in the means of two different populations, denote them \\(\\mu_1\\) and \\(\\mu_2\\). More specifically You are interested whether these population means are different from each other and plan an using a hypothesis test to verifiy this on the basis of independent sample data from both populations. A suitable pair of hypotheses then is \\[\\begin{equation} H_0: \\mu_1 - \\mu_2 = d_0 \\ \\ \\text{vs.} \\ \\ H_1: \\mu_1 - \\mu_2 \\neq d_0 \\tag{3.6} \\end{equation}\\] where \\(d_0\\) denotes the hypothesized difference in means. The book teaches us that \\(H_0\\) can be tested with the \\(t\\)-statistic \\[\\begin{equation} t=\\frac{(\\overline{Y}_1 - \\overline{Y}_2) - d_0}{SE(\\overline{Y}_1 - \\overline{Y}_2)} \\tag{3.7} \\end{equation}\\] where \\[\\begin{equation} SE(\\overline{Y}_1 - \\overline{Y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}. \\end{equation}\\] This is called a two sample \\(t\\)-test. For large \\(n_1\\) and \\(n_2\\), (3.7) is standard normal distributed under the null hypothesis. Anlog to the simple \\(t\\)-test we can compute confidence intervals for the true difference in population means: \\[ (\\overline{Y}_1 - \\overline{Y}_2) \\pm 1.96 \\times SE(\\overline{Y}_1 - \\overline{Y}_2) \\] is a \\(95\\%\\) confidence interval for \\(d\\). In R, Hypotheses as in (3.6) can be tested with t.test(), too. Note that t.test() chooses \\(d_0 = 0\\) by default. This can be changed by setting the argument mu accordingly. # set random seed set.seed(1) # draw data from two different populations with equal mean sample_pop1 &lt;- rnorm(100, 10, 10) sample_pop2 &lt;- rnorm(100, 10, 20) # perform a two sample t-test t.test(sample_pop1, sample_pop2) ## ## Welch Two Sample t-test ## ## data: sample_pop1 and sample_pop2 ## t = 0.872, df = 140.52, p-value = 0.3847 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.338012 6.028083 ## sample estimates: ## mean of x mean of y ## 11.088874 9.243838 We find that the two sample \\(t\\)-test does not reject the (true) null hypothesis that \\(d_0 = 0\\). An Application to the Gender Gap of Earnings In this section discusses how to reproduce the results presented in the box ‘The Gender Gap of Earnings of College Graduates in the United States’ in the book. In order to reproduce table 3.1 You need to download the replication data which is hosted by Pearson and can be found and downloaded here. Download the data for chapter three as an excel spreadsheet (cps_ch3.xlsx). This data set contains data that ranges from \\(1992\\) to \\(2008\\) and earnings are reported in prices of \\(2008\\). There are several ways to import the .xlsx-files into R. Our suggestion is the function read_excel() from the readxl package. The package is not part of R’s standard distribution and has to be installed manually. # install and load the readxl package ## install.packages(&#39;readxl&#39;) library(readxl) You are now ready to import the data set. Make sure You use the correct path to the downloaded file! In our example, the file is saved in a subfolder (data) of the working directory. If You are not sure what Your current working directory is, use getwd(), see also ?getwd(). This will give You the path that points to the place R is currently looking for files. # import the data into R cps &lt;- read_excel(path = &#39;data/cps_ch3.xlsx&#39;) Next, install and load the package dyplr. This package provides some handy functions that simplify data wrangling a lot. It makes use of the %&gt;% operator. In general, the aim of pipe operators is to increase readability of written code. The pipe operator %&gt;%, also known as magrittr, is relatively new to R. It was originally introduced with the package magrittr but is available for several R packages. The most prominent ones are plotly and dplyr. See the following link for more on the magrittr package. The basic idea is to simplify a sequece of function calls by chaining them. 1:10 %>% mean # [1] 5.5 # is equivalent to mean(1:10) # [1] 5.5 # install and load the dplyr package ## install.packages(&#39;dplyr&#39;) library(&#39;dplyr&#39;) First, get an overview over the data set. Next, use %&gt;% and some functions from the dplyr package to group the observations by gender and year and compute descriptive statistics for both groups. # Get an overview of the data structure head(cps) ## # A tibble: 6 x 3 ## a_sex year ahe08 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1992 17.16203 ## 2 1 1992 15.33856 ## 3 1 1992 22.94229 ## 4 2 1992 13.28334 ## 5 1 1992 22.12292 ## 6 2 1992 12.16761 # group data by gender and year and compute the mean, standard deviation # and number of observations for each group avgs &lt;- cps %&gt;% group_by(a_sex, year) %&gt;% summarise(mean(ahe08), sd(ahe08), n() ) # print results to the console print(avgs) ## # A tibble: 10 x 5 ## # Groups: a_sex [?] ## a_sex year `mean(ahe08)` `sd(ahe08)` `n()` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 1992 23.27382 10.172081 1594 ## 2 1 1996 22.47544 10.103141 1379 ## 3 1 2000 24.88314 11.599727 1303 ## 4 1 2004 25.12169 12.008435 1894 ## 5 1 2008 24.97840 11.778632 1838 ## 6 2 1992 20.04629 7.868418 1368 ## 7 2 1996 18.98048 7.951608 1230 ## 8 2 2000 20.73938 9.359327 1181 ## 9 2 2004 21.02373 9.363071 1735 ## 10 2 2008 20.87478 9.657140 1871 With the pipe operator %&gt;% we simply chain different R functions that produce compatible input and ouput. In the code above, we take the dataset cps and use it as an input for the function group_by(). The output of group_by is subsequently used as an input for summarise() and so forth. Now that we have computed the statistics of interest for both genders, we can investigate how the gap in earnings between both groups evolves over time. # split the data set by gender male &lt;- avgs %&gt;% filter(a_sex == 1) female &lt;- avgs %&gt;% filter(a_sex == 2) # Rename columns of both splits colnames(male) &lt;- c(&quot;Sex&quot;, &quot;Year&quot;, &quot;Y_bar_m&quot;, &quot;s_m&quot;, &quot;n_m&quot;) colnames(female) &lt;- c(&quot;Sex&quot;, &quot;Year&quot;, &quot;Y_bar_f&quot;, &quot;s_f&quot;, &quot;n_f&quot;) # Estimate Gender gaps, compute standard errors and confidence intervals for all dates gap &lt;- male$Y_bar_m - female$Y_bar_f gap_se &lt;- sqrt(male$s_m^2 / male$n_m + female$s_f^2 / female$n_f) gap_ci_l &lt;- gap - 1.96 * gap_se gap_ci_u &lt;- gap + 1.96 * gap_se result &lt;- cbind(male[,-1], female[,-(1:2)], gap, gap_se, gap_ci_l, gap_ci_u) # print results to the console print(result, digits = 3) ## Year Y_bar_m s_m n_m Y_bar_f s_f n_f gap gap_se gap_ci_l gap_ci_u ## 1 1992 23.3 10.2 1594 20.0 7.87 1368 3.23 0.332 2.58 3.88 ## 2 1996 22.5 10.1 1379 19.0 7.95 1230 3.49 0.354 2.80 4.19 ## 3 2000 24.9 11.6 1303 20.7 9.36 1181 4.14 0.421 3.32 4.97 ## 4 2004 25.1 12.0 1894 21.0 9.36 1735 4.10 0.356 3.40 4.80 ## 5 2008 25.0 11.8 1838 20.9 9.66 1871 4.10 0.354 3.41 4.80 We observe virtually the same results as the ones presented in the book. the computed statistics suggest that there is a gender gap in earnings. Note that we can reject the null hypothesis that the gap is zero for all periodes. Further, estimates of the gap and bounds of the 95% confidence intervals indicate that the gap has been quite stable over the recent past. Scatterplots, Sample Covariance and Sample Correlation A scatterplot represents two dimensional data, for example \\(n\\) observation on \\(X_i\\) and \\(Y_i\\), by points in a cartesian coordinate system. It is very easy to generate scatterplots using the plot() function in R. Let’s generate some fictional data on age and earnings of workers and plot it. # set random seed set.seed(123) # generate data set X &lt;- runif(n = 100, min = 18, max = 70 ) Y &lt;- X + rnorm(n=100, 50, 15) # plot observations plot(X, Y, type = &quot;p&quot;, main = &quot;A Scatterplot of X and Y&quot;, xlab = &quot;Age&quot;, ylab = &quot;Earnings&quot;, col = &quot;steelblue&quot;, pch = 19 ) The plot shows positive correlation between age and earnings. This in line with the assumption that older workers earn more than those that the joined the working population recently. Sample Covariance and Correlation By now You should be familiar with the concepts of variance and covariance. If not, we recommend You to work Your way through chapter 2 of the book (again). As for the variance, covariance and correlation of two variables are properties that relate to the (unknown) joint probability distribution of these variable. Just as the individual population variances of both variables, we can estimate covariance and correlation by means of suitable estimators using a random sample \\((X_i,Y_i)\\), \\(i=1,\\dots,n\\). The sample covariance \\[ s_{XY} = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y}) \\] is an estimator for the population variance of \\(X\\) and \\(Y\\) whereas the sample correlation \\[ r_{XY} = \\frac{s_{XY}}{s_Xs_Y} \\] can be used to estimate the population correlation, a standardized measure for the strength of the linear relationship between \\(X\\) and \\(Y\\). See chapter 3.7 in the book for a more detailed treatment of these estimators. As for variance and standard deviation, these estimators are implemented as R functions in the stats package. We can use them to estimate population covariance and population correlation the fictional data on age and earnings. # compute sample covariance of X and Y cov(X,Y) ## [1] 213.934 # compute sample correlation between X and Y cor(X,Y) ## [1] 0.706372 # equivalent way to compute the sample correlation cov(X,Y)/(sd(X) * sd(Y)) ## [1] 0.706372 The estimates indicate that \\(X\\) and \\(Y\\) are moderately correlated. The next code chunk uses the function mvnorm() from package MASS to generate bivariate example data with different degree of correlation. library(MASS) # set random seed set.seed(1) # positive correlation (0.81) example1 &lt;- mvrnorm(100, mu = c(0,0), Sigma = matrix(c(2,2,2,3), ncol = 2), empirical = TRUE ) # negative correlation (-0.81) example2 &lt;- mvrnorm(100, mu = c(0,0), Sigma = matrix(c(2,-2,-2,3), ncol = 2), empirical = TRUE ) # no correlation example3 &lt;- mvrnorm(100, mu = c(0,0), Sigma = matrix(c(1,0,0,1), ncol = 2), empirical = TRUE ) # no correlation (quadratic relationship) X &lt;- runif(n=100,-3,3) Y &lt;- -X^2 + rnorm(n=100,sd=0.5) example4 &lt;- cbind(X,Y) # estimate ## Plots # divide plot area as 2-by-2 array par(mfrow=c(2,2)) plot(example1, col=&#39;steelblue&#39;, pch=20, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, main=&quot;Correlation = 0.81&quot;) plot(example2, col=&#39;steelblue&#39;, pch=20, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, main=&quot;Correlation = -0.81&quot;) plot(example3, col=&#39;steelblue&#39;, pch=20, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, main=&quot;Correlation = 0&quot;) plot(example4, col=&#39;steelblue&#39;, pch=20, xlab = &#39;X&#39;, ylab = &#39;Y&#39;, main=&quot;Correlation = 0&quot;) "],
["lrwor.html", "4 Linear Regression with One Regressor Estimating the Coefficients of the Linear Regression Model Measures of fit The Least Squares Assumptions The Sampling Distribution of the OLS Estimator", " 4 Linear Regression with One Regressor This chapter introduces the basics in linear regression and shows how to perform regression analysis in R. In linear regression, the aim is to model the relationship between a dependent variable \\(Y\\) and one or more explanatory variables denoted as \\(X_1, X_2, \\dots, X_k\\). Following the book we will focus on the concept of simple linear regression throughout the whole chapter. In simple linear regression, there is just one explanatory variable \\(X_1\\). If for example a school cuts the class sizes by hiring new teachers, that is the school lowers the student-teacher ratios of their classes, \\(X_1\\), how would this affect the performance of the students involved in a standardized test,\\(Y\\)? With linear regression we can not only examine whether the student-teacher ratio does have an impact on the test results but we can also learn about the direction and the strength of this effect. To start with an easy example, consider the following combinations of average test score and the average student-teacher ratio in some fictional schools. 1 2 3 4 5 6 7 TestScore 680 640 670 660 630 660.0 635 STR 15 17 19 20 22 23.5 25 To work with these data in R we begin by creating two vectors: one for the student-teacher ratios (STR) and one for test scores (TestScore), both containing the data from the table above. # Create sample data STR &lt;- c(15, 17, 19, 20, 22, 23.5, 25) TestScore &lt;- c(680, 640, 670, 660, 630, 660, 635) # Print out sample data STR ## [1] 15.0 17.0 19.0 20.0 22.0 23.5 25.0 TestScore ## [1] 680 640 670 660 630 660 635 If we use a simple linear regression model, we assume that the true relationship between both variables can be represented by a straight line, formally \\[ Y = b \\cdot X + a. \\] For now, let us suppose that the true function which relates test score and student-teacher ratio to each other is \\[TestScore = 713 - 3 \\times STR.\\] If possible, it is always a good idea to visualize the data You work with in an appropriate way. For our purpose it is suitable to use the function plot() to produce a scatterplot with STR on the \\(X\\)-axis and TestScore on the \\(Y\\) axis. An easy way to do so is to call plot(y_variable ~ x_variable) whereby y_variable and x_variable are placeholders for the vectors of observations we want to plot. Furthermore, we might want to add the true relationship to the plot. To draw a straight line, R provides the function abline(). We just have to call this function with arguments a (representing the intercept) and b (representing the slope) after executing plot() in order to add the line to our scatterplot. The following code reproduces figure 4.1 from the textbook. # create a scatter plot of the data plot(TestScore ~ STR) # add the true relationship to the plot abline(a = 713, b = -3) We find that our line does not touch any of the points although we claimed that it represents the true relationship. The reason for this is the core problem of statistics, randomness. Most of the time there are influences which cannot be explained in a purely deterministic fashion and thus exacerbate finding the true relationship. In order to account for these differences between observed data and the true relationship, we extend our model from above by an error term \\(u\\) which covers these random effects. Put differently, \\(u\\) accounts for all the differences between the true regression line and the actual observed data. Beside pure randomness, these deviations could also arise from measerment errors or, as will be discussed later, could be the consequence of leaving out other factors that are relevant in explaining the dependent variable. Which other factor are plausible in our example? For one thing, the test scores might be driven by the teachers quality and the background of the students. It is also imaginable that in some classes, the students were lucky on the test days and thus achieved higher scores. For now, we will summarize such influences by an additive component: \\[ TestScore = \\beta_0 + \\beta_1 \\times STR + \\text{other factors} \\] Of course this idea is very general as it can be easily extented to other situations that can be described with a linear model. The basic linear regression function we will work with hence is \\[ Y_i = \\beta_0 + \\beta_1 X_i + u_i. \\] Key Concept 4.1 summarizes the terminology of the simple linear regression model. Key Concept 4.1 Terminology for the Linear Regression Model with a Single Regressor The linear regression model is \\[Y_i = \\beta_0 + \\beta_1 X_1 + u_i \\] where the subscript \\(i\\) runs over the observations, \\(i = 1\\), …, \\(n\\) \\(Y_i\\) is the dependent variable, the regressand, or simply the left-hand variable \\(X_i\\) is the independent variable, the regressor, or simply the right-hand variable \\(Y = \\beta_0 + \\beta_1 X\\) is the population regression line also called the population regression function \\(\\beta_0\\) is the intercept of the population regression line \\(\\beta_1\\) is the slope of the population regression line \\(u_i\\) is the error term Estimating the Coefficients of the Linear Regression Model In practice, the intercept \\(\\beta_0\\) and slope \\(\\beta_1\\)of the population regression line are unknown. Therefore, we must employ data to estimate both unknown parameters. In the following a real world example will be used to demonstrate how this is achieved. We want to relate test scores to student-teacher ratios measured in californian schools. The test score is the district-wide average of reading and math scores for fifth graders. Again, the class size is measured as the number of students divided by the number of teachers (the student-teacher ratio). As for the data, the California School dataset (CASchools) comes with a R package called AER, an acronym for Applied Econometrics with R. After installing the package with install.packages(&quot;AER&quot;) and attaching it with library(&quot;AER&quot;) the dataset can be loaded using the data function. # install the AER package (once) install.packages(&quot;AER&quot;) # load the AER package library(AER) # load the the data set in the workspace data(CASchools) Note that once a package has been installed it is available for use at further occasions when invoked with library() — there is no need to run install.packages(&quot;...&quot;) again! For several reasons it is interesting to know what kind of object we are dealing with. class(object_name) returns the type (class) of an object. Depending on the class of an object some functions (such as plot() and summary()) behave differently. Let us check the class of the object CASchools. class(CASchools) ## [1] &quot;data.frame&quot; It turns out that CASchools is of class data.frame which is a convienient format to work with. With help of the function head() we get a first overview of our data. This function shows only the first 6 rows of the data set which prevents an overcrowded console output. Press ctrl + L to clear the console. This command deletes any code that has been typed in and executed by You or printed to the console by R functions. Good news is: anything else is left untouched. You neither loose defined variables and alike nor the code history. It is still possible to recall previously executed R commands using the up and down keys. If You are working in RStudio, press ctrl + Up on Your keyboard (CMD + Up on a mac) to review a list of previously entered commands. head(CASchools) ## district school county grades students ## 1 75119 Sunol Glen Unified Alameda KK-08 195 ## 2 61499 Manzanita Elementary Butte KK-08 240 ## 3 61549 Thermalito Union Elementary Butte KK-08 1550 ## 4 61457 Golden Feather Union Elementary Butte KK-08 243 ## 5 61523 Palermo Union Elementary Butte KK-08 1335 ## 6 62042 Burrel Union Elementary Fresno KK-08 137 ## teachers calworks lunch computer expenditure income english read ## 1 10.90 0.5102 2.0408 67 6384.911 22.690001 0.000000 691.6 ## 2 11.15 15.4167 47.9167 101 5099.381 9.824000 4.583333 660.5 ## 3 82.90 55.0323 76.3226 169 5501.955 8.978000 30.000002 636.3 ## 4 14.00 36.4754 77.0492 85 7101.831 8.978000 0.000000 651.9 ## 5 71.50 33.1086 78.4270 171 5235.988 9.080333 13.857677 641.8 ## 6 6.40 12.3188 86.9565 25 5580.147 10.415000 12.408759 605.7 ## math ## 1 690.0 ## 2 661.9 ## 3 650.9 ## 4 643.5 ## 5 639.9 ## 6 605.4 We find that the dataset consists of plenty of variables and most of them are numeric. By the way: an alternative to class() and head() is str() which is deduced from ‘structure’ and gives a comprehensive overview of the object. Try this! Turning back to CASchools, the two variables we are intersted in (i.e. average test score and the student-teacher ratio) are not included. However, it is possible to calculate both from the provided data. To obtain the student-teacher ratios, we simply divide the number of students by the number of teachers. The avarage test score is the arithmetic mean of the test score for reading and the score of the math test. The next code chunk shows how the two variables can be constructed and how they are appended to CASchools which is a data.frame. # compute STR and append it to CASchools CASchools$STR &lt;- CASchools$students/CASchools$teachers # compute TestScore and append it to CASchools CASchools$score &lt;- (CASchools$read + CASchools$math)/2 If we ran head(CASchools) again we would find the two variables of interest as additional columns named STR and score (check this!). Table 4.1 from the text book summarizes the distribution of test scores and student-teacher ratios. There are several functions which can be used to produce similar results within R: mean() (computes the arithmetic mean of the provided numbers) sd() (computes the sample standard deviation) quantile() (returns a vector of the specified quantiles for the data) The next code chunk shows how to achieve this. First, we compute summary statistics on the coloumns STR and score of CASchools. In order to have a nice display format we gather the computed measures in a data.frame object named DistributionSummary. # compute sample averages of STR and score avg_STR &lt;- mean(CASchools$STR) avg_score &lt;- mean(CASchools$score) # compute sample standard deviations of STR and score sd_STR &lt;- sd(CASchools$STR) sd_score &lt;- sd(CASchools$score) # set up a vector of percentiles and compute the quantiles quantiles &lt;- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9) quant_STR &lt;- quantile(CASchools$STR, quantiles) quant_score &lt;- quantile(CASchools$score, quantiles) # gather everything in a data.frame DistributionSummary &lt;- data.frame( Average = c(avg_STR, avg_score), StandardDeviation = c(sd_STR, sd_score), quantile = rbind(quant_STR, quant_score) ) # print the summary to the console DistributionSummary ## Average StandardDeviation quantile.10. quantile.25. ## quant_STR 19.64043 1.891812 17.3486 18.58236 ## quant_score 654.15655 19.053347 630.3950 640.05000 ## quantile.40. quantile.50. quantile.60. quantile.75. ## quant_STR 19.26618 19.72321 20.0783 20.87181 ## quant_score 649.06999 654.45000 659.4000 666.66249 ## quantile.90. ## quant_STR 21.86741 ## quant_score 678.85999 The standard distribution (the Base R package) of R already contains a summary function which can be applied to objects of class data.frame. Type and execute summary(STR)! As done for the sample data, we use plot() for a visual survey. This allows us to detect specific characteristics of our data, such as outliers which are hard to discover by looking at mere numbers. This time we add some additional arguments to the plot() function. The first argument in our call of plot(), score ~ STR, is again a formula that states the dependent variable and the regressor. However, this time the two variables are not saved in seperate vectors but are columns of CASchools. Therefore, R would not find the variables without the argument data beeing correctly specified. data must be in accordance with the name of the data.frame to which the variables belong, in this case CASchools. Further arguments are used to change the appearance of the plot: while main adds a title, xlab and ylab are adding custom labels to both axes. plot(score ~ STR, data = CASchools, main = &quot;Scatterplot of TestScore and STR&quot;, xlab = &quot;STR (X)&quot;, ylab = &quot;Test Score (Y)&quot; ) The plot (figure 4.2 in the book) shows the scatterplot of all observations on student-teacher ratio and Test score. We see that the points are strongly scatterd and an apparent relationship cannot be detected by only looking at them. Yet it can be assumed that both variables are negatively correlated, that is we expect to observe lower test scores in bigger classes. The function cor() (type and execute ?cor for further info), can be used to compute the correlation between 2 numerical vectors. cor(CASchools$STR, CASchools$score) ## [1] -0.2263627 As the scatterplot already suggests, the correlation is negative but rather weak. The task we are facing now is to find a line which fits best to the data. Of course we could simply stick with graphical inspection and correlation analysis and then select the best fitting line by eyeballing. However, this is pretty unscientific and prone to subjective perception: different students would draw different regression lines. On this account, we are interested in techniques that are more sophisticated. Such a technique is ordinary least squares (OLS) estimation. The Ordinary Least Squares Estimator The OLS estimator chooses the regression coefficients such that the estimated regression line is as close as possible to the observed data points. Thereby closeness is measured by the sum of the squared mistakes made in predicting \\(Y\\) given \\(X\\). Let \\(b_0\\) and \\(b_1\\) be some estimators of \\(\\beta_0\\) and \\(\\beta_1\\). Then the sum of squared estimation mistakes can be expressed as \\[ \\sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2. \\] The OLS estimator in the simple regression model is the pair of estimators for intercept and slope which minimizes the expression above. The derivation of the OLS estimators for both parameters are presented in Appendix 4.1 of the book. The results are summarized in Key Concept 4.2. Key Concept 4.2 The OLS Estimator, Predicted Values, and Residuals The OLS estimators of the slope \\(\\beta_1\\) and the intercept \\(\\beta_0\\) in the simple linear regression model are \\[\\begin{align} \\hat\\beta_1 &amp; = \\frac{ \\sum_{i = 1}^n (X_i - \\overline{X})(Y_i - \\overline{Y}) } { \\sum_{i=1}^n (X_i - \\overline{X})^2} \\\\ \\\\ \\hat\\beta_0 &amp; = \\overline{Y} - \\hat\\beta_1 \\overline{X} \\end{align}\\] The OLS predicted values \\(\\widehat{Y}_i\\) and residuals \\(\\hat{u}_i\\) are \\[\\begin{align} \\widehat{Y}_i &amp; = \\hat\\beta_0 + \\hat\\beta_1 X_i,\\\\ \\\\ \\hat{u}_i &amp; = Y_i - \\widehat{Y}_i. \\end{align}\\] The estimated intercept \\(\\hat{\\beta}_0\\), the slope parameter \\(\\hat{\\beta}_1\\), and the residuals \\(\\left(\\hat{u}_i\\right)\\) are computed from a sample of \\(n\\) observations of \\(X_i\\) and \\(Y_i\\), \\(i\\), \\(...\\), \\(n\\). These are estimates of the unkown true population intercept \\(\\left(\\beta_0 \\right)\\), slope \\(\\left(\\beta_1\\right)\\), and error term \\((u_i)\\). We are aware that the results presented in Key Concept 4.2 are not very intuitive at first glance. The following interactice application aims to help You understand the mechanics of OLS. You can add observations by clicking into the coordinate system where the data are represented by points. If two or more observations are available, the application computes a regression line using OLS and some statistics which are displayed in the right panel. The results are updated as You add further observations to the left panel. A double-click resets the application i.e. all data are removed. There are many possible ways to compute \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) in R. For example, we could implement the formulas presented in Key Concept 4.2 with two of R’s most basic functions: mean() and sum(). attach(CASchools) #allows to use the variables contained in CASchools directly # compute beta_1 beta_1 &lt;- sum((STR - mean(STR))*(score - mean(score))) / sum((STR - mean(STR))^2) # compute beta_0 beta_0 &lt;- mean(score) - beta_1 * mean(STR) # print the results to the console beta_1 ## [1] -2.279808 beta_0 ## [1] 698.9329 Of course there are also other and even more manual ways to do the same tasks. Luckily, OLS is one of the most widely-used estimation techniques. Being a statistical programming language, R already contains a built-in function named lm() (linear model) which can be used to carry out regression analysis. The first argument of the function to be specified is, similar as in plot(), the regression formula with the basic syntax y ~ x where y is the dependent variable and x the explanatory variable. The argument data sets the data set to be used in the regression. We now revisit the example from the book where the relationship between the test scores and the class sizes is analysed. The following code uses lm() to replicate the results presented in figure 4.3 in the book. # estimate the model and assign the result to linear_model linear_model &lt;- lm(score ~ STR, data = CASchools) # Print the standard output of the estimated lm object to the console linear_model ## ## Call: ## lm(formula = score ~ STR, data = CASchools) ## ## Coefficients: ## (Intercept) STR ## 698.93 -2.28 Let us add the estimated regression line to the plot. This time we also enlarge ranges of both axes by setting the arguments xlim and ylim. # plot the data plot(score ~ STR, data = CASchools, main = &quot;Scatterplot of TestScore and STR&quot;, xlab = &quot;STR (X)&quot;, ylab = &quot;Test Score (Y)&quot;, xlim = c(10, 30), ylim = c(600, 720) ) # add the regression line abline(linear_model) Did you notice that this time, we did not pass the intercept and slope parameters to abline? If you call abline on an object of class lm that only contains a single regressor variable, R draws the regression line automatically! Measures of fit After estimating a linear regression, the question occurs how well that regression line describes the data. Are the observations tightly clustered arround the regression line, or are they spread out? Both, the \\(R^2\\) and the standard error of the regression (\\(SER\\)) measure how well the OLS Regression line fits the data. The \\(R^2\\) The \\(R^2\\) is the fraction of sample variance of \\(Y_i\\) that is explained by \\(X_i\\). Mathemethically, the \\(R^2\\) can be written as the ratio of the explained sum of squares to the total sum of squares. The explained sum of squares (\\(ESS\\)) is the sum of squared deviations of the predicted values, \\(\\hat{Y_i}\\), from the average of the \\(Y_i\\). The total sum of squares (\\(TSS\\)) is the sum of squared deviations of the \\(Y_i\\) from their average. \\[\\begin{align} ESS &amp; = \\sum_{i = 1}^n \\left( \\hat{Y_i} - \\overline{Y} \\right)^2 \\\\ \\\\ TSS &amp; = \\sum_{i = 1}^n \\left( Y_i - \\overline{Y} \\right)^2 \\\\ \\\\ R^2 &amp; = \\frac{ESS}{TSS} \\end{align}\\] Since \\(TSS = ESS + SSR\\) we can also write \\[ R^2 = 1- \\frac{SSR}{TSS} \\] where \\(SSR\\) is the sum of squared residuals, a measure for the errors made when predicting the \\(Y\\) by \\(X\\). The \\(SSR\\) is defined as \\[ SSR = \\sum_{i=1}^n \\hat{u}_i^2. \\] \\(R^2\\) lies between \\(0\\) and \\(1\\). It is easy to see that a perfect fit, i.e. no errors made when fitting the regression line, implies \\(R^2 = 1\\) since then we have \\(SSR=0\\). On the contrary, if our estimated regression line does not explain any variation in the \\(Y_i\\), we have \\(ESS=0\\) and consequently \\(R^2=0\\). Standard Error of the Regression The Standard Error of the Regression (\\(SER\\)) is an estimator of the standard deviation of the regression error \\(\\hat{u}_i\\). As such it measure the magnitude of a typical deviation from the regression, i.e. the magnitude of a typical regression error. \\[ SER = s_{\\hat{u}} = \\sqrt{s_{\\hat{u}}^2} \\ \\ \\ \\text{where} \\ \\ \\ s_{\\hat{u} }^2 = \\frac{1}{n-2} \\sum_{i = 1}^n \\hat{u}^2_i = \\frac{SSR}{n - 2} \\] Remember that the \\(u_i\\) are unobserved. That is why we use their estimated counterparts, the residuals \\(\\hat{u}_i\\) instead. See chapter 4.3 of the book for a more detailed comment on the \\(SER\\). Application to the Test Score Data Both measures of fit can be obtained by using the function summary() with the lm object provided as the only argument. Whereas the function lm() only prints out the estimated coefficients to the console, summary provides additional predefined information such as the regression’s \\(R^2\\) and the \\(SER\\). mod_summary &lt;- summary(linear_model) mod_summary ## ## Call: ## lm(formula = score ~ STR, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.727 -14.251 0.483 12.822 48.540 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 698.9329 9.4675 73.825 &lt; 2e-16 *** ## STR -2.2798 0.4798 -4.751 2.78e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.58 on 418 degrees of freedom ## Multiple R-squared: 0.05124, Adjusted R-squared: 0.04897 ## F-statistic: 22.58 on 1 and 418 DF, p-value: 2.783e-06 The \\(R^2\\) in the output is called ‘Multiple R-squared’ and has the value \\(0.051\\). Hence, \\(5.1 \\%\\) of the variance of the dependent variable \\(score\\) is explained by the explanatory variable \\(STR\\). That is the regression explains some of the variance but much of the variation in test scores remains unexplained (cf. figure 4.3 in the book). The \\(SER\\) is called ‘Residual standard error’ and takes the value \\(18.58\\). The unit of the \\(SER\\) is the same as the unit of the dependent variable. In our context we can interpret the value as follows: on average the deviation of the actual achieved test score and the regression line is \\(18.58\\) points. Now, let us check whether the summary() function uses the same definitions for \\(R^2\\) and \\(SER\\) as we do by computing them manually. # compute R^2 manually SSR &lt;- sum(mod_summary$residuals^2) TSS &lt;- sum((score - mean(score))^2) R2 &lt;- 1 - SSR/TSS # print the value to the console R2 ## [1] 0.05124009 # compute SER manually n &lt;- nrow(CASchools) SER &lt;- sqrt(SSR / (n-2)) # print the value to the console SER ## [1] 18.58097 We find that the results coincide. Note that the values provided by summary() are rounded to two decimal places. Can You Do this using R? The Least Squares Assumptions OLS performs well under a quite broad variety of different circumstances. However, there are some assumptions which are posed on the data which need to be satisfied in order to achieve reliable results. Key Concept 4.3 The Least Squares Assumptions \\[Y_i = \\beta_0 + \\beta_1 X_i + u_i \\text{, } i = 1, ...,n\\] where The error term \\(u_i\\) has conditional mean zero given \\(X_i\\): \\(E(u_i|X_i) = 0\\) \\((X_i,Y_i), i = 1,...,n\\) are independent and identically distributed (i.i.d.) draws from their joint distribution Large outliers are unlikely: \\(X_i\\) and \\(Y_i\\) have nonzero finite fourth moments Assumption #1: The Error Term has Conditional Mean of Zero This means that no matter which value we choose for \\(X\\), the error term \\(u\\) must not show any systematic pattern and must have a mean of \\(0\\). Consider the case that \\(E(u) = 0\\) but for low and high values of \\(X\\), the error term tends to be positive and for midrange values of \\(X\\) the error tends to be negative. We can use R to construct such an example. To do so we generate our own data using R’s build in random number generators. We will use the following functions You should be familiar with: runif() (generates uniformly distributed random numbers) rnorm() (generates nomally distributed random numbers) predict() (does predictions based on the results of model fitting functions like lm()) lines() (adds line segments to an existing plot) We start by creating a vector containing values that are randomly scattered on the domain \\([-5,5]\\). For our example we decide to generate uniformly distributed random numbers. This can be done with the function runif(). We also need to simulate the error term. For this we generate normally distributed random numbers with a mean equal to \\(0\\) and a variance of \\(1\\) using rnorm(). The \\(Y\\) values are obtained as a quadratic function of the \\(X\\) values and the error. After generating the data we estimate both a simple regression model and a quadratic model that also includes the regressor \\(X^2\\). Finally, we plot the simulated data and add a the estimated regression line of a simple regression model as well as the predictions made with a quadratic model to compare the fit graphically. # set a random seed to make the results reproducible set.seed(321) # simulate the data X &lt;- runif(50, min = -5, max = 5) u &lt;- rnorm(50, sd = 5) ## the true relation Y &lt;- X^2 + 2*X + u # estimate a simple regression model mod_simple &lt;- lm(Y ~ X) # predict using a quadratic model prediction &lt;- predict(lm(Y ~ X + I(X^2)), data.frame(X = sort(X))) # plot the results plot(Y ~ X) abline(mod_simple, col = &quot;red&quot;) lines(sort(X), prediction) This shows what is meant by \\(E(u_i|X_i) = 0\\): Using the quadratic model (represented by the black curve) we see that there are no systematic deviations of the observation from the predicted relation. It is credible that the assumption is not violated when such a model is employed. However, using a simple linear regression model we see that the assumption is probably violated as \\(E(u_i|X_i)\\) varies with the \\(X_i\\). Assumption #2: All \\((X_i, Y_i)\\) are Independently and Identically Distributed Most common sampling schemes used when collecting data from populations produce i.i.d. samples. For example, we could use R’s random number generator to randomly select student IDs from a university’s enrollment list and record age \\(X\\) and earnings \\(Y\\) of the corresponding students. This is a typical example of simple random sampling and ensures that all the \\((X_i,Y_i)\\) are drawn randomly from the same population. A prominent example where the i.i.d. assumption is not fulfilled is time series data where we have observations on the same unit over time. For example, take \\(X\\) as the number of workers employed by a production company over the course of time. Due to technological change, the company makes job cuts periodically but there are also some non-deterministic influences that relate to economics, politics and alike. Using R we can simulate such a process and plot it. We start the series with a total of 5000 workers and simulate the reduction of employment with a simple autoregressive process that exhibits a downward trend and has normal distributed errors:1 \\[ employment_t = 0.98 \\cdot employment_{t-1} + u_t \\] # set random seed set.seed(7) # initialize the employment vector X &lt;- c(5000,rep(NA,99)) # generate a date vector Date &lt;- seq(as.Date(&quot;1951/1/1&quot;), as.Date(&quot;2050/1/1&quot;), &quot;years&quot;) # generate time series observations with random influences for (i in 2:100) X[i] &lt;- 0.98*X[i-1] + rnorm(1, sd=200) #plot the results plot(Date, X, type = &quot;l&quot;, col=&quot;steelblue&quot;, ylab = &quot;Workers&quot;, xlab=&quot;Time (t)&quot;) It is evident that the observations on \\(X\\) cannot be independnet in this example: the level of today’s employment is correlated with tomorrows employment level. Thus, the i.i.d. assumption is violated for \\(X\\). Assumption #3: Large outliers are unlikely It is easy to come up with situations where extreme observations, i.e. observations that deviate considerably from the usual range of the data, may occur. Such observations are called outliers. Technically speaking, assumption #3 requires that \\(X\\) and \\(Y\\) have a finite kurtosis.2 Common cases where we want to exclude or (if possible) correct such outliers is when they are apperently typos, conversion errors or measurement errors. Even if it seems that extreme observations have been recorded correctly, it is advisable to exclude them before estimating a model since OLS suffers from sensitivity to outliers. What does this mean? One can show that extreme observation receive heavy weighting in the computation done with OLS. Therefore, outliers can lead to strongly distorted estimates of regression coefficient. To get a better impression of this, consider the following application where we have placed some sample data on \\(X\\) and \\(Y\\) which are highly correlated. The relation between \\(X\\) and \\(Y\\) seems to be explained pretty good by the plotted regression line: all of the blue dots lie close to the red line and we have \\(R^2=0.92\\). Now go ahead and add a further observation at, say, \\((18,2)\\). This clearly is an outlier. The result is quite striking: the estimated regression line differs greatly from the one we adjudged to fit the data well. The slope is heavily downward biased and \\(R^2\\) decreased to a mere \\(29\\%\\)! Double-click inside the coordinate system to reset the app. Feel free to experiment. Choose different coordinates for the outlier or add additional ones. The following code roughly reproduces what is shown in figure 4.5 in the book. As done above we use sample data generated using R’s random number functions rnorm() and runif(). We estimate simple regression models based on the original data set and a modified set where one observation is change to be an outlier and plot the results. In order to understand the complete code You should be familiar with the function sort() which sorts the entries of a numeric vector in ascending order. # set random seed set.seed(123) # generate the data X &lt;- sort(runif(10, min = 30, max = 70 )) Y &lt;- rnorm(10 , mean = 200, sd = 50) Y[9] &lt;- 2000 # fit model with outlier fit &lt;- lm(Y ~ X) # fit model without outlier fitWithoutOutlier &lt;- lm(Y[-9] ~ X[-9]) # plot the results plot(Y ~ X) abline(fit) abline(fitWithoutOutlier, col = &quot;red&quot;) The Sampling Distribution of the OLS Estimator Because the OLS estimators \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) are computed from a randomly drawn sample, the estimators themselves are random variables with a probability distribution — the so-called sampling distribution of the estimators — which describes the values they could take over different random samples. Although the sampling distribution of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) can be complicated when the sample size is small and generally differs with the number of observation, \\(n\\), it is possible to make certain statements about it that hold for all \\(n\\). In particular \\[ E(\\hat{\\beta_0}) = \\beta_0 \\ \\ \\text{and} \\ \\ E(\\hat{\\beta_1}) = \\beta_1,\\] that is, \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are unbiased estimators of \\(\\beta_0\\) and \\(\\beta_1\\), the true parameters. If the sample is sufficiently large, by the central limit theorem the joint sampling distribution of the estimators is well approximated by the bivariate normal distribution (2.1). This implies that the marginal distributions are also normal in large samples. Core facts on the large-sample distribution of \\(\\beta_0\\) and \\(\\beta_1\\) are presented in Key Concept 4.4. Key Concept 4.4 Large Sample Distribution of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) If the least squares assumptions in Key Concept 4.3 hold, then in large samples \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) have a jointly normal sampling distribution. The large sample normal distribution of \\(\\hat\\beta_1\\) is \\(N(\\beta_1, \\sigma^2_{\\hat\\beta_1})\\), where the variance of the distribution, \\(\\sigma^2_{\\hat\\beta_1}\\), is \\[ \\sigma^2_{\\hat\\beta_1} = \\frac{1}{n} \\frac{Var \\left[ \\left(X_i - \\mu_X \\right) u_i \\right]} {\\left[ Var \\left(X_i \\right) \\right]^2} \\tag{4.1}. \\] The large sample normal distribution of \\(\\hat\\beta_0\\) is \\(N(\\beta_0, \\sigma^2_{\\hat\\beta_0})\\), where \\[ \\sigma^2_{\\hat\\beta_0} = \\frac{1}{n} \\frac{Var \\left( H_i u_i \\right)}{ \\left[ E \\left(H_i^2 \\right) \\right]^2 } \\ , \\ \\text{where} \\ \\ H_i = 1 - \\left[ \\frac{\\mu_X} {E \\left( X_i^2\\right)} \\right] X_i. \\tag{4.2} \\] R Simulation Study 1 Whether Key Koncept 4.4 really holds can be verified using R. First we build our own population of \\(100000\\) observations in total. To do this we need values for our independent variable \\(X\\), for the error term \\(u\\), and the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\). With all this combined in a simple regression model, we can compute our dependent variable \\(Y\\). In our example we generate the numbers \\(X_i\\), \\(i = 1\\), … ,\\(100000\\) by drawing a random sample from a uniform distribution on the interval \\([0,20]\\). The realisations of the error terms \\(u_i\\) are drawn from a standard normal distribution with parameters \\(\\mu = 0\\) and \\(\\sigma^2 = 100\\) (note that rnorm() requires \\(\\sigma\\) as input for the argument sd, see ?rnorm). Furthermore we chose \\(\\beta_0 = -2\\) and \\(\\beta_1 = 3.5\\) so the true model is \\[ Y_i = -2 + 3.5 \\cdot X_i. \\] Finally, we store the results in a data.frame. # simulate data N &lt;- 100000 X &lt;- runif(N, min = 0, max = 20) u &lt;- rnorm(N, sd = 10) # population regression Y &lt;- -2 + 3.5 * X + u population &lt;- data.frame(X, Y) From now on we will consider the previously generated data as the true population (which of course would be unknown in a real world application, otherwise there would not be a reason to do draw a random sample in the first place). The knowledge about the true population and the true relationship between \\(Y\\) and \\(X\\) can be used to verify the statements made in Key Concept 4.4. First, let us calculate the true variances \\(\\sigma^2_\\hat{\\beta_0}\\) and \\(\\sigma^2_\\hat{\\beta_1}\\) for a randomly drawn sample of size \\(n = 100\\). # set sample size n &lt;- 100 # compute the variance of hat_beta_0 H_i &lt;- 1 - mean(X) / mean(X^2) * X var_b0 &lt;- var(H_i * u) / (n * mean(H_i^2)^2 ) # compute the variance of hat_beta_1 var_b1 &lt;- var( ( X - mean(X) ) * u ) / (100 * var(X)^2) # print variances to the console var_b0 ## [1] 4.045066 var_b1 ## [1] 0.03018694 Now let us assume that we do not know the true values of \\(\\beta_0\\) and \\(\\beta_1\\) and that it is not possible to observe the whole population. However, we can observe a random sample of \\(n\\) observations. Then, it would not be possible to compute the true parameters but we could obtain estimates of \\(\\beta_0\\) and \\(\\beta_1\\) from the sample data using OLS. However, we know that these estimates are outcomes of random variables themselves since the observations are randomly sampled from the population. Key Concept 4.4. describes their distributions for large \\(n\\). When drawing a single sample of size \\(n\\) it is not possible to make any statement about these distributions. Things change if we repeat the sampling scheme many times and compute the estimates for each sample: using such a procedure we simulate outcomes of the respective distributions. To achieve this in R, we employ the following approach: We assign the number of repetitions, say \\(10000\\), to reps. Then we initialize a matrix fit were the estimates obtained in each sampling iteration shall be stored row-wise. Thus fit has to be an array of dimensions reps\\(\\times2\\). In the next step we draw reps random sample of size n from the population and obtain the OLS estimates for each sample. The results are stored as row entries in the outcome matrix fit. This is done using a for() loop. At last, we estimate variances of both coefficient estimators using the sampled outcomes and plot histograms of the latter. We also add plot of the density functions belonging to the distributions that follow from Key Concept 4.4. The function bquote() is used to obtain math expressions in the titels and labels of both plots. See ?bquote. # set repetitions and sample size n &lt;- 100 reps &lt;- 10000 # initialize the matrix of outcomes fit &lt;- matrix(ncol = 2, nrow = reps) # loop sampling and estimating of the coefficients for (i in 1:reps){ sample &lt;- population[sample(1:N, n),] fit[i, ] &lt;- lm(Y ~ X, data = sample)$coefficients } # compute variance estimates using outcomes var(fit[ ,1]) ## [1] 4.057089 var(fit[ ,2]) ## [1] 0.03021784 # plot histograms of beta_0 estimates hist(fit[ ,1], main = bquote(The ~ Distribution ~ of ~ 10000 ~ beta[0] ~ Estimates), xlab = bquote(hat(beta)[0]), freq = F) # add true distribution to plot curve(dnorm(x,-2,sqrt(var_b0)), add = T, col=&quot;darkred&quot;) # plot histograms of beta_1 estimates hist(fit[ ,2], main = bquote(The ~ Distribution ~ of ~ 10000 ~ beta[1] ~ Estimates), xlab = bquote(hat(beta)[1]), freq = F) # add true distribution to plot curve(dnorm(x,3.5,sqrt(var_b1)), add = T, col=&quot;darkred&quot;) We are now able to say the following: first, our variance estimates are in favour of the claims made in Key Concept 4.4 since they come close to the computed theoretical values. Second, the histograms suggest that the estimators distributions indeed follow normal distributions which can be fairly approximated by the respective normal distributions stated in Key Concept 4.4. R Simulation Study 2 A further result implied by Key Concept 4.4 is that both estimators are consistent i.e. they converge in probability to their true value. This is since their variances converge to \\(0\\) as \\(n\\) increases. We can check this by repeating the simulation above for an increasing sequence of sample sizes. This means we no langer assign the sample size but a vector of sample sizes: n &lt;- c(...). Let us look at the distributions of \\(\\beta_1\\). The idea here is to add an additional call of for() to the code. This is done in order to loop over the vector of sample sizes n. For each of the sample sizes we carry out the same simulation as before but plot a density estimate for the outcomes of each iteration over n. Notice that we have to change n to n[j] in the inner loop to ensure that the j\\(^{th}\\) element of n is used. In the simulation, we use sample sizes \\(100, 250, 1000\\) and \\(3000\\). Consequently we have a total of four distinct simulations using different sample sizes. # set random seed for reproducibility set.seed(1) # set repetitions and the vector of sample sizes reps &lt;- 1000 n &lt;- c(100, 250, 1000, 3000) # initialize the matrix of outcomes fit &lt;- matrix(ncol = 2, nrow = reps) # devide the plot panel in a 2-by-2 array par(mfrow = c(2,2)) #### Loop sampling and plotting #### # outer loop over n for (j in 1:length(n)) { # inner loop: sampling and estimating of the coefficients for (i in 1:reps){ sample &lt;- population[sample(1:N, n[j]), ] fit[i, ] &lt;- lm(Y ~ X, data = sample)$coefficients } # draw density estimates plot(density(fit[,2]), xlim=c(2.5,4.5), col=j, main = paste(&quot;n=&quot;, n[j]), xlab = bquote(hat(beta)[1])) } We find that, as \\(n\\) increases, the distribution of \\(\\hat\\beta_1\\) concentrates around its mean, i.e. its variance decreases. Put differently, the likelihood of observering estimates close to the true value of \\(\\beta_1 = 3.5\\) grows as we increase the sample size. The same behaviour could be observed if we would analyze the distribution of \\(\\hat\\beta_0\\) instead. R Simulation Study 3 Furthermore, (4.1) reveals that the variance of the OLS estimator for \\(\\beta_1\\) decreases as the variance of the \\(X_i\\) increases. In other words, as we increase the amount of information provided by the regressor, that is increasing \\(Var(X)\\), which is used to estimate \\(\\beta_1\\), we are more confident that the estimate is close to the true value (i.e. \\(Var(\\hat\\beta_1)\\) decreases). We can visualize this by reproducing figure 4.6 from the book. To do this, we sample \\(100\\) observations \\((X,Y)\\) from a bivariate normal distribution with \\[E(X)=E(Y)=5,\\] \\[Var(X)=Var(Y)=5\\] and \\[Cov(X,Y)=4.\\] Formally, this is written down as \\[\\begin{align} \\begin{pmatrix} X \\\\ Y \\\\ \\end{pmatrix} \\overset{i.i.d.}{\\sim} &amp; \\ \\mathcal{N} \\left[ \\begin{pmatrix} 5 \\\\ 5 \\\\ \\end{pmatrix}, \\ \\begin{pmatrix} 5 &amp; 4 \\\\ 4 &amp; 5 \\\\ \\end{pmatrix} \\right]. \\tag{4.3} \\end{align}\\] To carry out the random sampling, we make use of the function mvtnorm() from the package MASS which allows to draw random samples from multivariate normal distributions, see ?mvtnorm. Next, we use the subset() function to split the sample into two subsets such that the first set, set1, consists of observations that fulfill the condition \\(\\lvert X - \\overline{X} \\rvert &gt; 1\\) and the second set, set2, includes the remainder of the sample. We then plot both sets and use different colors to make them distinguishable. # load the MASS package library(MASS) # set random seed for reproducibility set.seed(4) # simulate bivarite normal data bvndata &lt;- mvrnorm(100, mu = c(5,5), Sigma = cbind(c(5,4),c(4,5)) ) # assign column names / convert to data.frame colnames(bvndata) &lt;- c(&quot;X&quot;,&quot;Y&quot;) bvndata &lt;- as.data.frame(bvndata) # subset the data set1 &lt;- subset(bvndata, abs(mean(X) - X) &gt; 1) set2 &lt;- subset(bvndata, abs(mean(X) - X) &lt;= 1) # plot both data sets plot(set1, xlab = &quot;X&quot;, ylab = &quot;Y&quot;, pch = 19) points(set2, col = &quot;steelblue&quot;, pch = 19) It is clear that observations that are close to the sample average of the \\(X_i\\) have less variance than those that are farther away. Now, if we were to draw a line as accurately as possible through either of the two sets it is obvious that choosing the observations indicated by the black dots, i.e. using the set of observations which has larger variance than the blue ones, would result in a more precise line. Now, let us use OLS to estimate and draw the regression lines for both sets of observations. # estimate both regression lines lm.set1 &lt;- lm(Y ~ X, data = set1) lm.set2 &lt;- lm(Y ~ X, data = set2) # add both lines to the plot abline(lm.set1, col=&quot;green&quot;) abline(lm.set2, col=&quot;red&quot;) Evidently, the green regression line does far better in describing data sampled from the bivariate normal distribution stated in (4.3) than the red line. This is a nice example why we are interested in a high variance of the regressor \\(X\\): more variance in the \\(X_i\\) means more information from which the precision of the estimation benefits. See chapter 14 in the book for more on autoregressive processes and time series analysis in general.↩ See chapter 4.4 in the book.↩ "],
["hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html", "5 Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model Testing Two-Sided Hypotheses Concerning \\(\\beta_1\\) Confidence Intervals for Regression Coefficients Regression when \\(X\\) is a Binary Variable Heteroskedasticity and Homoskedasticity The Gauss-Markov Theorem Using the \\(t\\)-Statistic in Regression When the Sample Size Is Small", " 5 Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model In this chapter, we continue with the treatment of the simple linear regression model. The following subsction discuss how we may use our knowledge about the sampling distribution of the OLS estimator in order to make statements regarding its uncertainty. These subsections cover the following topics: Testing Hypotheses about regression coefficients Confidence intervals for regression coefficients Regression when \\(X\\) is a dummy variable Heteroskedasticity and Homoskedasticity Testing Two-Sided Hypotheses Concerning \\(\\beta_1\\) Using the fact that \\(\\hat{\\beta}_1\\) is approximately normal distributed in large samples (see Key Concept 4.4), testing hypothesis about the true value \\(\\beta_1\\) can be done with the same approach as discussed in chapter 3.2. Key Concept 5.1 General Form of the \\(t\\)-Statistic Remember from chapter 3 that a general \\(t\\)-statistic has the form \\[ t = \\frac{\\text{estimated value} - \\text{hypothesized value}}{\\text{standard error of the estimator}}. \\] Key Concept 5.2 Testing Hyothesis about \\(\\beta_1\\) For testing the hypothesis \\(H_0: \\beta_1 = \\beta_{1,0}\\), we need to perform the following steps: Compute the standard error of \\(\\hat{\\beta}_1\\), \\(SE(\\hat{\\beta}_1)\\) \\[ SE(\\hat{\\beta}_1) = \\sqrt{ \\hat{\\sigma}^2_{\\hat{\\beta}_1} } \\ \\ , \\ \\ \\hat{\\sigma}^2_{\\hat{\\beta}_1} = \\frac{1}{n} \\times \\frac{\\frac{1}{n-2} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\hat{u_i}^2 }{ \\left[ \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\right]^2}. \\] Compute the \\(t\\)-statistic \\[ t = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{ SE(\\hat{\\beta}_1) }. \\] Now, given a two sided alternative (\\(H_1:\\beta_1 \\neq \\beta_{1,0}\\)) we reject at the \\(5\\%\\) level if \\(|t^{act}| &gt; 1.96\\) or, equivalently, if the \\(p\\)-value is less than \\(0.05\\). Recall the definition of the \\(p\\)-value: \\[\\begin{align} p \\text{-value} =&amp; \\, \\text{Pr}_{H_0} \\left[ \\left| \\frac{ \\hat{\\beta}_1 - \\beta_{1,0} }{ SE(\\hat{\\beta}_1) } \\right| &gt; \\left| \\frac{ \\hat{\\beta}_1^{act} - \\beta_{1,0} }{ SE(\\hat{\\beta}_1) } \\right| \\right] \\\\ =&amp; \\, \\text{Pr}_{H_0} (|t| &gt; |t^{act}|) \\\\ =&amp; \\, 2 \\cdot \\Phi(-|t^{act}|) \\end{align}\\] The last equality holds due to the normal approximation for large samples. Consider again the OLS regression stored in linear_model from Chapter 4 that gave us the regression line \\[ \\widehat{TestScore} \\ = \\underset{(9.47)}{698.9} - \\underset{(0.49)}{2.28} \\times STR \\ , \\ R^2=0.051 \\ , \\ SER=18.6. \\] For testing a hypothesis about the slope parameter (the coefficient on \\(STR\\)), we need \\(SE(\\hat{\\beta}_1)\\), the standard error of the respective point estimator. As common in the literature, standard errors are presented in parantheses below the point estimates. As can be witnessed in Key Concept 5.1 it is rather cumbersome to compute the standard error and thus the \\(t\\)-statistic by hand. The question You should be asking Yourself right now is obvious: can we obtain these values with minimum effort using R? Yes, we can. Let us first use summary() to get a summary on the estimated coefficients in linear_model. # print the summary of coefficients to the console summary(linear_model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 698.932949 9.4674911 73.824516 6.569846e-242 ## STR -2.279808 0.4798255 -4.751327 2.783308e-06 When looking at the second column of the coefficients’ summary, we discover values for \\(SE(\\hat\\beta_0)\\) and \\(SE(\\hat\\beta_1)\\). Also, in the third column, named t value, we find \\(t\\)-statistics \\(t^{act}\\) suitable for tests of the individual hypotheses \\(H_0: \\beta_0=0\\) and \\(H_0: \\beta_1=0\\). Furthermore, the output provides us with \\(p\\)-values corresponding to both tests against the two-sided alternatives \\(H_1:\\beta_0\\neq0\\) respectively \\(H_1:\\beta_1\\neq0\\) in the fourth column of the table. Let us have a closer look at the test of \\[H_0: \\beta_1=0 \\ \\ \\ vs. \\ \\ \\ H_1: \\beta_1 \\neq 0.\\] Using our revisited knowledge about \\(t\\)-statistics we find that \\[ t^{act} = \\frac{-2.279808 - 0}{0.4798255} \\approx - 4.75. \\] What does this tell us about the significance of the estimated coefficient? We reject the null hypothesis at the \\(5\\%\\) level of significance since \\(|t^{act}| &gt; 1.96\\) that is the observed test statistic falls into the region of rejection. Or, alternatively and leading to the same result, we have \\(p\\text{-value} = 2.78*10^{-6} &lt; 0.05\\). We conclude that the coefficient is significantly different from zero. With other words, our analysis provides evidence that the clase size has an influence on the students test scores. We say that \\(\\beta_1\\) is significantly different from \\(0\\) at the level of \\(5\\%\\). Note that, although the difference is negligible in the present case as we will see later, summary() does not perform the normal approximation but calculates \\(p\\)-values using the appropriate \\(t\\)-distribution instead. Generally, the degrees of freedom are determined in the following manner: \\[ \\text{DF} = n - k - 1 \\] where \\(n\\) is the number of observations used to estimate the model and \\(k\\) is the number of regressors, excluding the intercept. In our case, we have \\(n=420\\) observations and the only regressor is \\(STR\\) so \\(k=1\\). A sleek way to determine the model degress of freedom using R is # determine degrees of freedom linear_model$df.residual ## [1] 418 Hence, for the sampling distribution of \\(\\hat\\beta_1\\) we have \\[ \\hat\\beta_1 \\sim t_{418}\\] such that the \\(p\\)-value for a two-sided significance test can be obtained by executing the following code: 2 * pt(-4.751327, df = 418) ## [1] 2.78331e-06 The result is very close to the value provided by summary. However since \\(n\\) is sufficiently large one could just as well use the standard normal density to compute the \\(p\\)-value: 2 * pnorm(-4.751327) ## [1] 2.02086e-06 The difference is indeed negligible. These findings tell us that, if \\(H_0: \\beta_1 = 0\\) is true and we were to repeat the whole process of gathering observations and estimating the model, chances of observing a \\(\\hat\\beta_1 \\geq |-4.75|\\) are roughly \\(1:359285\\) — so higher chances than winning the lottory next saturday but still very unlikely! Using R we may visualise how such a statement is made when using the normal approximation. This reflects the principles depicted in figure 5.1 in the book. Do not let the following code chunk deter You: the code is somewhat longer than the usual examples and looks unappealing but there is a lot of repetition since color shadings and annotations are added on both tails of the normal distribution. We recommend You to execute the code step by step in order to see how the graph is augmented with the annotations. # Plot the standard normal on the domain [-6,6] t &lt;- seq(-6,6,0.01) plot(x = t, y = dnorm(t, 0, 1), type = &quot;l&quot;, col = &quot;steelblue&quot;, lwd = 2, yaxs = &quot;i&quot;, axes = F, ylab = &quot;&quot;, main = &quot;Calculating the p-Value of a Two-Sided Test When t^act = -4.75&quot;, cex.lab = 0.7 ) tact &lt;- -4.75 axis(1, at = c(0,-1.96,1.96,-tact,tact), cex.axis=0.7) # Shade the critical regions using polygon() ## critical region in left tail polygon(x = c(-6, seq(-6,-1.96,0.01),-1.96), y = c(0, dnorm(seq(-6,-1.96,0.01)),0), col = &#39;orange&#39; ) ## critical region in right tail polygon(x = c(1.96, seq(1.96, 6, 0.01), 6), y = c(0, dnorm(seq(1.96, 6, 0.01)), 0), col = &#39;orange&#39; ) # Add arrows and texts indicating critical regions and the p-value arrows(-3.5, 0.2, -2.5, 0.02, length = 0.1) arrows(3.5, 0.2, 2.5, 0.02, length = 0.1) arrows(-5, 0.16, -4.75, 0, length = 0.1) arrows(5, 0.16, 4.75, 0, length = 0.1) text(-3.5,0.22, labels = paste(&quot;0.025=&quot;,expression(alpha),&quot;/2&quot;,sep = &quot;&quot;), cex = 0.7) text(3.5,0.22, labels = paste(&quot;0.025=&quot;,expression(alpha),&quot;/2&quot;,sep = &quot;&quot;), cex = 0.7) text(-5,0.18, labels = expression(paste(&quot;-|&quot;,t[act],&quot;|&quot;)), cex = 0.7) text(5,0.18, labels = expression(paste(&quot;|&quot;,t[act],&quot;|&quot;)), cex = 0.7) # Add ticks indicating critical values at the 0.05-level, t^act and -t^act rug(c(-1.96,1.96), ticksize = 0.145, lwd = 2, col = &quot;darkred&quot;) rug(c(-tact,tact), ticksize = -0.0451, lwd = 2, col = &quot;darkgreen&quot;) The \\(p\\)-Value is the area under the curve to left of \\(-4.75\\) plus the area under the curve to the right of \\(4.75\\). As we already know from the calculations above, this value is very small. Confidence Intervals for Regression Coefficients As we already know, estimates of the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are afflicted with sampling uncertainty, see chapter 4. Therefore, we will never estimate the exact true value of these parameters from sample data in an empirical application. However, we may construct confidence intervals for the intercept and the slope parameter. A \\(95\\%\\) confidence interval for \\(\\beta_i\\) has two equivalent definitions: The interval is the set of values for which a hypothesis test to the level of \\(5\\%\\) cannot be rejected. The interval has a probability of \\(95\\%\\) to contain the true value of \\(\\beta_i\\). So in \\(95\\%\\) of all samples that could be drawn, the confidence interval will cover the true value of \\(\\beta_i\\). We also say that the interval has a confidence level of \\(95\\%\\). The idea is summarized in Key Concept 5.3. Key Concept 5.3 A Confidence Interval for \\(\\beta_i\\) Imagine You could draw all possible random samples of given size. The interval that contains the true value \\(\\beta_i\\) in \\(95\\%\\) of all samples is given by the expression \\[ \\text{KI}_{0.95}^{\\beta_i} = \\left[ \\hat{\\beta}_i - 1.96 \\times SE(\\hat{\\beta}_i) \\, , \\, \\hat{\\beta}_i + 1.96 \\times SE(\\hat{\\beta}_i) \\right]. \\] Equivalently, this interval can be seen as the set of null hypotheses for which a \\(5\\%\\) two-sided hypothesis test does not reject. R Simulation Study 5.1 To get a better understanding of confidence intervalls we will conduct another simulation study. For now, assume that we are confronted with the following sample of \\(n=100\\) observations on a single variable \\(Y\\) where \\[ Y_i \\overset{i.i.d}{\\sim} N(5,25) \\ \\ \\forall \\ i = 1, \\dots, 100.\\] # set random seed for reproducibility set.seed(4) # generate and plot the sample data Y &lt;- rnorm(n = 100, mean = 5, sd =5 ) plot(Y, pch=19, col = &quot;steelblue&quot; ) We assume that the data is generated by the model \\[ Y_i = \\mu + \\epsilon_i \\] where \\(\\mu\\) is the unknown constant and we know that \\(\\epsilon_i \\overset{i.i.d.}{\\sim} N(0,25)\\). In this model, the OLS estimator for \\(\\mu\\) is given by \\[ \\hat\\mu = \\overline{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i \\] (try to verify this!) i.e. the sample average of the \\(Y_i\\). It further holds that \\[ SE(\\hat\\mu) = \\frac{\\sigma_{\\epsilon}}{\\sqrt{n}} = \\frac{5}{\\sqrt{100}}. \\] A large sample \\(95\\%\\) confidence intervall for \\(\\mu\\) is then given by \\[\\begin{equation} KI^{\\mu}_{0.95} = \\left[\\hat\\mu - 1.96 \\times \\frac{5}{\\sqrt{100}} \\ , \\ \\hat\\mu + 1.96 \\times \\frac{5}{\\sqrt{100}} \\right]. \\tag{5.1} \\end{equation}\\] It is fairly easy to compute this interval in R by hand. The following code chunck generates a named vector containing the interval bounds: cbind( CIlower = mean(Y) - 1.96 * 5/10, CIupper = mean(Y) + 1.96 * 5/10 ) ## CIlower CIupper ## [1,] 4.502625 6.462625 Nowing that \\(\\mu = 5\\) we see that our example covers the true value for the present sample. As opposed to real world examples, we can use R to get a better understanding of confidence intervals by repeatedly sampling data, estimating \\(\\mu\\) and computing the confidence interval for \\(\\mu\\) as in (5.1). The procedure is as follows: We initialize the vectors lower and upper in which the simulated interval boundaries are to be saved. We want to simulate \\(10000\\) intervals so both vectors are set to have this length. We use a for() loop to sample \\(100\\) observations from the \\(N(5,25)\\) distribution and compute \\(\\hat\\mu\\) as well as the boundaries of the confidence interval in every iteration of the loop. At last we join lower and upper in an array. # set random seed set.seed(1) # initialize vectors of lower and upper interval boundaries lower &lt;- numeric(10000) upper &lt;- numeric(10000) # loop sampling / estimation / CI for(i in 1:10000) { Y &lt;- rnorm(100, mean = 5, sd =5) lower[i] &lt;- mean(Y) - 1.96 * 5/10 upper[i] &lt;- mean(Y) + 1.96 * 5/10 } # join vectors of interval boundaries CIs &lt;- cbind(lower, upper) According to Key Concept 5.3 we expect that the fraction of the \\(10000\\) simulated intervals saved in the array CIs that contain the true value \\(\\mu=5\\) should be roughly \\(95\\%\\). We can check this using logical operators. sum(CIs[,1] &lt;= 5 &amp; 5 &lt;= CIs[,2])/10000 ## [1] 0.9487 The simulation shows that the fraction of intervals covering \\(\\mu=5\\), i.e. those intervals for which \\(H_0: \\mu = 5\\) cannot be rejected is close to the theoretical value of \\(95%\\). Let us draw a plot of the first \\(100\\) simulated confidence intervals and indicate those which do not cover the true value of \\(\\mu\\). We do this by adding horizonal lines representing the confidence intervals on top of each other. # identify intervals not covering mu # (4 intervals out of 100) ID &lt;- which(!(CIs[1:100,1] &lt;= 5 &amp; 5 &lt;= CIs[1:100,2])) # initialize the plot plot(0, xlim = c(3,7), ylim = c(1,100), ylab = &quot;Sample&quot;, xlab = expression(mu), main = &quot;Confidence Intervals: Correct H0&quot;) # setup color vector colors &lt;- rep(gray(0.6), 100) colors[ID] &lt;- &quot;red&quot; # draw reference line at mu=5 abline(v=5, lty=2) # add horizontal bars representing the CIs for(j in 1:100) { lines(c(CIs[j,1], CIs[j,2]), c(j,j), col = colors[j], lwd=2) } We find that for the first 100 samples, the true null hypthesis is rejected in four cases so these intervals do not cover \\(\\mu=5\\). We have indicated the intervals which lead to a rejection of the true null hypothesis by red color. Let us now turn back to the example of test scores and class sizes.The regression model from chapter 4 is stored in linear_model. An easy way to get \\(95\\%\\) confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\), the coefficients on (intercept) and STR, is to use the function confint(). We only have to provide a fitted model object as the argument object to this function. The confidence level is set to \\(95\\%\\) by default but can be modified by setting the argument level, see ?confint. confint(object = linear_model) ## 2.5 % 97.5 % ## (Intercept) 680.32312 717.542775 ## STR -3.22298 -1.336636 Let us check if the calculation is done as we expect it to be. For \\(\\beta_1\\), that is the coefficient on STR, according to the formula presented above the interval borders are computed as \\[ -2.279808 \\pm 1.96 \\times 0.4798255 \\, \\Rightarrow \\, \\text{KI}_{0.95}^{\\beta_1} = \\left[ -3.22, -1.34 \\right] \\] so this actually leads to the same interval. Obviously, this interval does not contain the value zero what, as we have already seen in the previous section, leads to rejection of the null hypothesis \\(\\beta_{1,0} = 0\\). Regression when \\(X\\) is a Binary Variable Instead of using a continuous regressor \\(X\\), we might be interested in running the regression \\[ Y_i = \\beta_0 + \\beta_1 D_i + u_i \\tag{5.2} \\] where \\(D_i\\) is binary variable, a so-called dummy variable. For example, we may define \\(D_i\\) in the following way: \\[ D_i = \\begin{cases} 1 \\ \\ \\text{if $STR$ in $i^{th}$ school district &lt; 20} \\\\ 0 \\ \\ \\text{if $STR$ in $i^{th}$ school district $\\geq$ 20} \\\\ \\end{cases} \\tag{5.3} \\] The regression model now is \\[ TestScore_i = \\beta_0 + \\beta_1 D_i + u_i. \\tag{5.4} \\] Let us see how these data look like in a scatter plot: # Create the dummy variable as defined above using a for loop for (i in 1:nrow(CASchools)) { if (CASchools$STR[i] &lt; 20) { CASchools$D[i] &lt;- 1 } else { CASchools$D[i] &lt;- 0 } } # Plot the data plot(CASchools$D, CASchools$score, # provide the data to be ploted pch=20, # use filled circles as plot symbols cex=0.5, # set size of plot symbols to 0.5 col=&quot;Steelblue&quot;, # set the symbols&#39; color to &quot;Steelblue&quot; xlab=expression(D[i]), # Set title and axis names ylab=&quot;Test Score&quot;, main = &quot;Dummy Regression&quot; ) We see that with \\(D\\) as the regressor, it is not useful to think of \\(\\beta_1\\) as a slope parameter since \\(D_i \\in \\{0,1\\}\\), i.e. we only observe two discrete values instead of a continuoum of regressor values lying (in some range) on the real line. Simply put, there is no continuous line depicting the conditional expectation function \\(E(TestScore_i | D_i)\\) since this function is solely defined for \\(X\\)-positions \\(0\\) and \\(1\\). Therefore, the interpretation of the coefficients in our regression model is as follows: \\(E(Y_i | D_i = 0) = \\beta_0\\) so \\(\\beta_0\\) is the expected test score in districts where \\(D_i=0\\) i.e. where \\(STR\\) is below \\(20\\). \\(E(Y_i | D_i = 1) = \\beta_0 + \\beta_1\\) or, using the result above, \\(\\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)\\). Thus, \\(\\beta_1\\) is the difference in group specific expectations, i.e. the difference in expected test score between districts with \\(STR &lt; 20\\) and those with \\(STR \\geq 20\\). We will now use R to estimate the dummy regression model as defined by equations (5.2) - (5.3) . # estimate the dummy regression model dummy_model &lt;- lm(score ~ D, data = CASchools) summary(dummy_model) ## ## Call: ## lm(formula = score ~ D, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -50.496 -14.029 -0.346 12.884 49.504 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 650.077 1.393 466.666 &lt; 2e-16 *** ## D 7.169 1.847 3.882 0.00012 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.74 on 418 degrees of freedom ## Multiple R-squared: 0.0348, Adjusted R-squared: 0.0325 ## F-statistic: 15.07 on 1 and 418 DF, p-value: 0.0001202 One can see that the expected test score in districts with \\(STR &lt; 20\\) (\\(D_i = 1\\)) is predicted to be \\(650.1 + 7.17 = 657.27\\) while districs with \\(STR \\geq 20\\) (\\(D_i = 0\\)) are expected to have an average test score of only \\(650.1\\). Group specific predictions can be added to the plot by execution of the following code chunk: # add group specific predictions to the plot points(x = CASchools$D, y = predict(dummy_model), col = &quot;red&quot;, pch = 20 ) Here we use the function predict() to obtain estimates of the group specific means. The red dots represent these sample group averages. Accordingly, \\(\\hat{\\beta}_1 = 7.17\\) can be seen as the difference in group averages. By inspection of the output generated with summary(dummy_model) we may also find an answer to the question whether there is a statistically significant difference in group means. This in turn would support the hypothesis that students perform differently when they are taught in small classes rather than in large groups. We can assess this by a two-tailed test of the hypothesis \\(H_0: \\beta_1 = 0\\). Conviniently the \\(t\\)-statistic and the corresponding \\(p\\)-value for this test are computed defaultly by summary()! Since t value \\(= 3.88 &gt; 1.96\\) we reject the null hypothesis at the \\(5\\%\\) level of significance. The same conclusion can be made when using the \\(p\\)-value which reports significance to the \\(0.00012\\%\\) level. As done with linear_model, we may alternatively use the confint() function to compute a \\(95\\%\\) confidence interval for the true difference in means and see if the hypothesised value is an element of this confidence set. # confidence intervals for coefficients in the dummy regression confint(dummy_model) ## 2.5 % 97.5 % ## (Intercept) 647.338594 652.81500 ## D 3.539562 10.79931 We reject the hypothesis that there is no difference between group means at the \\(5\\%\\) significance level since \\(\\beta_{1,0} = 0\\) lies outside of \\([3.54, 10.8]\\), the \\(95\\%\\) confidence interval for the coefficient on \\(D\\). Heteroskedasticity and Homoskedasticity All inference made in the previous chapters relies on the assumption that the error variance does not vary as regressor values change. But this will not necessarily be the case in most empirical applications. Key Concept 5.4 Heteroskedasticity and Homoskedasticity We say that the error term of our regression model is homoskedastic if the variance of the conditional distribution of \\(u_i\\) given \\(X_i\\), \\(Var(u_i|X_i=x)\\), is constant for all observations in our sample \\[ \\text{Var}(u_i|X_i=x) = \\sigma^2 \\ \\forall \\ i=1,\\dots,n. \\] If instead there is dependence of the conditional variance of \\(u_i\\) on \\(X_i\\), the error term is said to be heteroskedastic. We then write \\[ \\text{Var}(u_i|X_i=x) = \\sigma_i^2 \\ \\forall \\ i=1,\\dots,n. \\] Homoskedasticity is a special case of heteroskedasticity. For a better understanding of heteroskedasticity, we generate some bivariate heteroskedastic data, estimate a linear regression model and then use boxplots to depict the conditional distributions of the residuals. # load scales package for custom color opacities library(scales) # Genrate some heteroskedastic data set.seed(123) x &lt;- rep(c(10,15,20,25),each=25) e &lt;- rnorm(100, sd=12) i &lt;- order(runif(100, max=dnorm(e, sd=12))) y &lt;- 720 - 3.3 * x + e[rev(i)] # Estimate the model mod &lt;- lm(y ~ x) # Plot the data plot(x=x, y=y, main=&quot;An Example of Heteroskedasticity&quot;, xlab = &quot;Student-Teacher Ratio&quot;, ylab = &quot;Test Score&quot;, cex = 0.5, pch = 19, xlim = c(8,27), ylim = c(600,710) ) # Add the regression line to the plot abline(mod, col=&quot;darkred&quot;) # Add boxplots to the plot boxplot(y ~ x, add = TRUE, at = c(10,15,20,25), col = alpha(&quot;gray&quot;, 0.4), border = &quot;black&quot; ) For this artificial data it is straightforward to see that we face unequal conditional error variances. Specifically, we observe that the variance in test scores (and therefore the variance of the errors committed) increases with the student teacher ratio. A Real-World Example for Heteroskedasticity Think about the economic value of education: if there would not be an expected economic value-added to receiving education at university, You probably would not be reading this script right now. A starting point to empirical verification of such a relation exists is to have data on individuals that are in an employment relationship. More precisely, we need data on wages and education in order to work with a model like \\[ wage_i = \\beta_0 + \\beta_1 \\cdot education_i + u_i. \\] What can be presumed about this relation? It is likely that, on average, higher educated workers earn more money than workers with less education so we expect to estimate an upward sloping regression line. Also it seems plausible that workers with better education are more likely to meet the requirements for the well-paid jobs. However, workers with low education will have no shot at those well-paid jobs. Therefore it seems plausible that the distribution of earnings spreads out as education increases. In other words: we expect that there is heteroskedasticity! To verify this empirically we may use real data on hourly earnings and the number of years of education of employees. Such data can be found in CPSSWEducation. This data set is part of the package AER and stems from the Current Population Survey (CPS) which is conducted periodically by the Bureau of Labor Statistics in the US. The subsequent code chunks demonstrate how to load the data into R and how to produce a plot in the fashion of figure 5.3 in the book. # load package and attach data library(AER) data(&quot;CPSSWEducation&quot;) attach(CPSSWEducation) # get an overview summary(CPSSWEducation) ## age gender earnings education ## Min. :29.0 female:1202 Min. : 2.137 Min. : 6.00 ## 1st Qu.:29.0 male :1748 1st Qu.:10.577 1st Qu.:12.00 ## Median :29.0 Median :14.615 Median :13.00 ## Mean :29.5 Mean :16.743 Mean :13.55 ## 3rd Qu.:30.0 3rd Qu.:20.192 3rd Qu.:16.00 ## Max. :30.0 Max. :97.500 Max. :18.00 # estimate a simple regression model labor_model &lt;- lm(earnings ~ education) # plot observations and add the regression line plot(education, earnings, ylim = c(0,150) ) abline(labor_model, col=&quot;steelblue&quot;, lwd=2) From inspecting the plot we can tell that the mean of the distribution of earnings increases with the level of education. This is also suggested by formal analysis: the estimated regression model stored in labor_mod asserts that there is a positive relation between years of education and earnings. labor_model ## ## Call: ## lm(formula = earnings ~ education) ## ## Coefficients: ## (Intercept) education ## -3.134 1.467 The estimated regression equation states that, on average, an additional year of education increases a workers hourly earnings by about \\(\\$ 1.47\\). Once more we use confint() to obtain a \\(95\\%\\) confidence interval for both regression coefficients. confint(labor_model) ## 2.5 % 97.5 % ## (Intercept) -5.015248 -1.253495 ## education 1.330098 1.603753 Since the intervall is \\([1.33, 1.60]\\) we can reject the hypothesis that the coefficient on education is zero at the \\(5\\%\\) level. What is more, the plot indicates that there is heteroskedasticity: if we assume the regression line to be a reasonably good representation of the conditional mean function \\(E(earnings_i\\vert education_i)\\), the dispersion of hourly earnings around that function cleary increases with the level of education, i.e. the variance of the distribution of earnings increases. In other words: the variance of the residuals increases with the years of education so that the regression errors are heteroskedastic. This example makes a case that it is doubtful to assume homoskedasticity in many economic applications. Should We Care About Heteroskedasticity? To answer this question, let us see how the variance of \\(\\hat\\beta_1\\) is computed under the assumption of homoskedasticity. In this case we have \\[ \\sigma^2_{\\hat\\beta_1} = \\frac{\\sigma^2_u}{n \\cdot \\sigma^2_X} \\tag{5.5} \\] which is a simplified version of the general equation (4.1) presented in Key Concept 4.4. See Appendix 5.1 of the book for details on the derivation. The summary() function in R estimates (5.5) by \\[ \\overset{\\sim}{\\sigma}^2_{\\hat\\beta_1} = \\frac{SER^2}{\\sum_{i=1}^n (X_i - \\overline{X})^2} \\ \\ \\text{where} \\ \\ SER=\\frac{1}{n-2} \\sum_{i=1}^n \\hat u_i^2. \\] Thus summary() estimates the homoskedasticity-only standard error \\[ \\sqrt{ \\overset{\\sim}{\\sigma}^2_{\\hat\\beta_1} } = \\sqrt{ \\frac{SER^2}{\\sum(X_i - \\overline{X})^2} }. \\] This in fact is an estimator for the standard deviation of the estimator \\(\\hat{\\beta}_1\\) that is inconsistent for the true value \\(\\sigma^2_{\\hat\\beta_1}\\) when there is heteroskedasticity. The implication is that \\(t\\)-statistics computed in the manner of Key Concept 5.1 do not have a standard normal distribution, even in large samples. This issue may invalidate inference drawn when using the previously treated tools for hypothesis testing: we should be cautious when making statements about the significance of regression coefficients on the basis of \\(t\\)-statistics as computed by summary() or confidence intervals produced by confint() if it is doubtful for the assumption of homoskedasticity to hold! We will now use R to compute the homoskedasticity-only standard error estimate for \\(\\hat{\\beta}_1\\) in the test score regression model linear_model by hand and see if it matches the value produced by summary(). # Store model summary in &#39;mod&#39; model &lt;- summary(linear_model) # Extract the standard error of the regression from model summary SER &lt;- model$sigma # Compute the variation in &#39;size&#39; V &lt;- (nrow(CASchools)-1) * var(CASchools$STR) # Compute the standard error of the slope parameter&#39;s estimator and print it SE.beta_1.hat &lt;- sqrt(SER^2/V) SE.beta_1.hat ## [1] 0.4798255 # Use logical operators to see if the value computed by hand matches the one provided # in mod$coefficients. Round estimates to four decimal places round(model$coefficients[2,2], 4) == round(SE.beta_1.hat, 4) ## [1] TRUE Indeed, the estimated values are equal. Computation of Heteroskedasticity-Robust Standard Errors Cosistent estimation of \\(\\sigma_{\\hat{\\beta}_1}\\) under heteroskedasticity is granted when the following robust estimator is used. \\[ SE(\\hat{\\beta}_1) = \\sqrt{ \\frac{ \\frac{1}{n-2} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\hat{u}_i^2 }{ \\left[ \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\right]^2} } \\tag{5.6} \\] Standard error estimates computed this way are also referred to as Eicker-Huber-White standard errors. It can be quite cumbersome to do this calculation by hand. Luckily, there are R function for that purpose. A convenient one, named vcovHC() is part of the sandwich package. This function can compute a variety of standard error estimators. The one brought forward in (5.6) is computed when the argument type is set to &quot;HC0&quot;. Let us now compute robust standard error estimates for the coefficients in linear_model. # load the sandwich package library(sandwich) # compute robust standard error estimates vcov &lt;- vcovHC(linear_model, type = &quot;HC0&quot;) vcov ## (Intercept) STR ## (Intercept) 106.908469 -5.3383689 ## STR -5.338369 0.2685841 The output of vcovHC() is the variance-covariance matrix of coefficient estimates. We are interested in the square root of the diagonal elements of this matrix since these values are the standard error estimates we seek. When we have k &gt; 1 regressors, writing down the equations for a regression model becomes very messy. A more convinient way to denote and estimate so-called multiple regression models is matrix algebra. This is why functions like vcovHC() produce matrices. In the simple linear regression model, the variances and covariances of the coefficient estimators can be gathered in the variance-covariance matrix \\[\\begin{equation} \\text{Var} \\begin{pmatrix} \\hat\\beta_0 \\\\ \\hat\\beta_1 \\end{pmatrix} = \\begin{pmatrix} \\text{Var}(\\hat\\beta_0) &amp; \\text{Cov}(\\hat\\beta_0,\\hat\\beta_1) \\\\ \\text{Cov}(\\hat\\beta_0,\\hat\\beta_1) &amp; \\text{Var}(\\hat\\beta_1) \\end{pmatrix} \\end{equation}\\] which is a symmetric matrix. So vcovHC() gives us \\(\\widehat{\\text{Var}}(\\hat\\beta_0)\\), \\(\\widehat{\\text{Var}}(\\hat\\beta_1)\\) and \\(\\widehat{\\text{Cov}}(\\hat\\beta_0,\\hat\\beta_1)\\) but most of the time we are interested in the diagonal elements of the estimated matrix. # compute the square root of the diagonal elements in vcov robust_se &lt;- sqrt(diag(vcov)) robust_se ## (Intercept) STR ## 10.339655 0.518251 Now assume we want to generate a coefficient summary as provided by summary() but with robust standard error estimates for the coefficient estimators, robust \\(t\\)-statistics and corresponding \\(p\\)-values for the regression model linear_model. This can be done using coeftest() from the package lmtest, see ?coeftest. Further we specify in the argument vcov. that vcov, the Eicker-Huber-White estimate of the variance matrix we have computed before should be used. # We invoke the function `coeftest()` on our model coeftest(linear_model, vcov. = vcov) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 698.93295 10.33966 67.597 &lt; 2.2e-16 *** ## STR -2.27981 0.51825 -4.399 1.382e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that values reported in the column Std. Error equal the ones received using sqrt(diag(vcov)). How severe are the implications of using homoskedasticity-only standard errors in the presence of heteroskedasticity? The answer is: it depends. As mentioned above we may face the risk of drawing wrong conclusions when conducting significance tests. Let us illustrate this by generating another example of a heteroskedastic data set and use it to estimate a simple regression model. We take \\[ Y_i = \\beta_1 \\cdot X_i + u_i \\ \\ , \\ \\ u_i \\overset{i.i.d.}{\\sim} N(0,0.36 \\cdot X_i^2) \\] with \\(\\beta_1=1\\) as the data generating process. The assumption of homoskedasticity is violated since the variance of the errors is a non-linear increasing function of \\(X_i\\) but the errors have zero mean and are i.i.d. such that the assumptions made in Key Concept 4.3 are not violated. As before, the true conditional mean function we are interested in estimating is \\[ E(Y_i\\vert X_i) = X_i. \\] # set random seed set.seed(21) # generate heteroskedastic data X &lt;- 1:1000 Y &lt;- rnorm(n = 1000, mean = X, sd = 0.6*X) # estimate a simple regression model reg &lt;- lm(Y ~ X) We plot the data and add the regression line. # plot the data plot(X, Y, pch = 19, col=&quot;steelblue&quot;, cex = 0.8) # add the regression line to the plot abline(reg, col = &quot;darkred&quot;, lwd = 1.5) The plot clearly shows that the data are heteroskedastic as the variance of \\(Y\\) grows with \\(X\\). We continue by conducting a significance test of the (true) null hypothesis \\(H_0: \\beta_1 = 1\\) twice, once using the homoskedasticity-only standard error formula and once with the robust version (5.6). An idiomatic way to do this in R is the function linearHypothesis() from the package car, see ?linearHypothesis. It allows to test linear hypotheses about parameters in linear models in a similar way as done with a \\(t\\)-statistic and offers various robust covariance matrix estimators. We test by comparing the tests’ \\(p\\)-values to the significance level of \\(5\\%\\). linearHypothesis() computes a test statistic that follows an \\(F\\) distribution under the null hypothesis. We will not loose too much words on the theory behind it at this time. In general, the core idea of the \\(F\\) test is to compare the fit of different models. When testing a hypothesis about a single coefficient using a \\(F\\) test, one can show that the test statistic is simply the square of the corresponding \\(t\\)-statistic: \\[ F = t^2 = \\frac{\\hat\\beta_i - \\beta_{i,0}}{SE(\\hat\\beta_i)} \\sim F_{1,n-k-1} \\] In linearHypothesis(), the hypothesis must be provided as a string. The function returns an object of class anova which contains further information on the test that can be accessed using the $ operator. For example, we can obtain the test’s \\(p\\)-value by adding $‘Pr(&gt;F)’ right behind the function call. # test using default standard error linearHypothesis(reg, hypothesis.matrix = &quot;X = 1&quot;)$&#39;Pr(&gt;F)&#39;[2] &lt; 0.05 ## [1] TRUE # test using robust standard error linearHypothesis(reg, hypothesis.matrix = &quot;X = 1&quot;, white.adjust = &quot;hc0&quot;)$&#39;Pr(&gt;F)&#39;[2] &lt; 0.05 ## [1] FALSE This is a good example of what can go wrong if we do not care for heteroskedasticity: for the data set at hand the default method rejects the null hypothesis \\(\\beta_1 = 1\\) although it is true. Using the robust standard error though the test does not reject the null. Of course we could argue that this is just a coincidence and both tests are equally well in maintaining the type I error rate of \\(5\\%\\). This can be further investigated by computing Monte Carlo estimates of the rejection frequencies of both tests on the basis of a large number of random samples. We proceed as follows: initialize vectors t and t.rob as type numeric with length \\(10000\\). Using a for() loop, we generate \\(10000\\) heteroskedastic random samples of size \\(1000\\), estimate the regression model and check whether the tests wrongly reject the null at the level of \\(5\\%\\) using comparison operators. The results are stored in the respective vectors t and t.rob. After the simulation, we compute the fraction of rejections for both tests. # initialize vectors t and t.rob t &lt;- numeric(10000) t.rob &lt;- numeric(10000) # loop sampling and estimation for (i in 1:10000) { # sample data X &lt;- 1:1000 Y &lt;- rnorm(n = 1000, mean = X, sd = 0.6*X) # estimate regression model reg &lt;- lm(Y ~ X) # homoskedasdicity-only significance test t[i] &lt;- linearHypothesis(reg, &quot;X = 1&quot;)$&#39;Pr(&gt;F)&#39;[2] &lt; 0.05 # robust significance test t.rob[i] &lt;- linearHypothesis(reg, &quot;X = 1&quot;, white.adjust = &quot;hc0&quot;)$&#39;Pr(&gt;F)&#39;[2] &lt; 0.05 } # compute fraction of rejections cbind(t = sum(t), t.rob = sum(t.rob)) / 10000 ## t t.rob ## [1,] 0.0762 0.0524 The results show that we face an increased risk of falsely rejecting the null using the homoskedasticity-only standard error for the testing problem at hand: with the common standard error estimator, \\(7.62\\%\\) of all tests reject the null hypothesis falsely. In contrast, with the robust test statistic we are close to the nominal level of \\(5\\%\\). The Gauss-Markov Theorem When estimating regression models, we know that the results of the estimation procedure are outcomes of a random process. However, when using unbiased estimators, at least on average, we estimate the true parameter. When comparing different unbiased estimators, it is therefore interesting to know which one has the highest precision: being aware that the likelihood of estimating the exact value of the parameter of interest is \\(0\\) in an empirical application, we want to make sure that the likelihood of obtaining an estimate very close to the true value is as high as possible. We want to use the estimator with the lowest variance of all unbiased estimators. The Gauss-Markov theorem states that the OLS estimator has this property under certain conditions. Key Concept 5.5 The Gauss-Markov Theorem for \\(\\hat{\\beta}_1\\) Suppose that the assumptions made in Key Concept 4.3 hold and that the errors are homoskedastic. The OLS estimator is the best (in the sense of smallest variance) linear conditionally unbiased estimator (BLUE) in this setting. Let us have a closer look at what this means: Estimators of \\(\\beta_1\\) that are linear functions of the \\(Y_1, \\dots, Y_n\\) and that are unbiased conditionally on the regressor \\(X_1, \\dots, X_n\\) can be written as \\[ \\overset{\\sim}{\\beta}_1 = \\sum_{i=1}^n a_i Y_i \\] where the \\(a_i\\) are weights that are allowed to depend on the \\(X_i\\) but not on the \\(Y_i\\). We already know that \\(\\overset{\\sim}{\\beta}_1\\) has a sampling distribution: \\(\\overset{\\sim}{\\beta}_1\\) is a linear function of the \\(Y_i\\) which are random variables. If now \\[ E(\\overset{\\sim}{\\beta}_1 | X_1, \\dots, X_n) = \\beta_1 \\] we say that \\(\\overset{\\sim}{\\beta}_1\\) is a linear unbiased estimator of \\(\\beta_1\\), conditionally on the \\(X_1, \\dots, X_n\\). We may ask if \\(\\overset{\\sim}{\\beta}_1\\) is also the best estimator in this class, i.e. the most efficient one of all linear unbiased estimators where “most efficient” means smallest variance. The weights \\(a_i\\) play an important role here and it turns out that OLS uses just the right weights to have the BLUE property. R Simulation Study: BLUE Estimator Consider the case of a regression of \\(Y_i,\\dots,Y_n\\) only on a constant. Here, the \\(Y_i\\) are assumed to be a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\). We know that the OLS estimator in this model is simply the sample mean: \\[\\begin{equation} \\hat{\\beta}_1 = \\overline{\\beta}_1 = \\sum_{i=1}^n \\underbrace{\\frac{1}{n}}_{=a_i} Y_i \\tag{5.2} \\end{equation}\\] Clearly, each observation is weighted by \\[a_i = \\frac{1}{n}.\\] and We also know that \\(\\text{Var}(\\hat{\\beta}_1)=\\text{Var}(\\hat\\beta_1)=\\frac{\\sigma^2}{n}\\). We will now use R for a simulation study that illustrates what happens to the variance of (5.2) if different weights \\[ w_i = \\frac{1 \\pm \\epsilon}{n} \\] are assigned to either half of the sample \\(Y_1, \\dots, Y_n\\) instead of using \\(\\frac{1}{n}\\), the weights implied by OLS. # Set sample size and number of repititions n &lt;- 100 reps &lt;- 1e5 # Choose epsilon and create a vector of weights as defined above epsilon &lt;- 0.8 w &lt;- c(rep((1+epsilon)/n,n/2), rep((1-epsilon)/n,n/2) ) # Draw a random sample y_1,...,y_n from the standard normal distribution, # use both estimators 1e5 times and store the result in thr vectors ols and # weightedestimator ols &lt;- rep(NA, reps) weightedestimator &lt;- rep(NA, reps) for (i in 1:reps) { y &lt;- rnorm(n) ols[i] &lt;- mean(y) weightedestimator[i] &lt;- crossprod(w,y) } # Plot kernel density estimates of the estimators&#39; distributions ## OLS plot(density(ols), col = &quot;purple&quot;, lwd = 3, main = &quot;Density of OLS and Weighted Estimator&quot;, xlab = &quot;Estimates&quot; ) ## Weighted lines(density(weightedestimator), col = &quot;steelblue&quot;, lwd = 3 ) ## Add a dashed line at 0 and a legend to the plot abline(v = 0, lty = 2) legend(&#39;topright&#39;, c(&quot;OLS&quot;,&quot;Weighted&quot;), col = c(&quot;purple&quot;,&quot;steelblue&quot;), lwd = 3 ) What conclusion can we draw from the result? Both estimators seem to be unbiased: the means of their estimated distributions are zero. The weightedestimator is less efficient than the ols estimator: there is higher dispersion when weights are \\(w_i = \\frac{1 \\pm 0.8}{100}\\) instead of \\(w_i=\\frac{1}{100}\\) as required by the OLS solution. Hence, our simulation results confirm what is stated by the Gauss-Markov Theorem. Using the \\(t\\)-Statistic in Regression When the Sample Size Is Small The three OLS assumptions discussed in chapter 4 (see Key Concept 4.3) are the foundation results on the large sample distribution of the OLS estimators in the simple regression model. What can be said about the distribution of the estimators and their \\(t\\)-statistics when the sample size is small and the population distribution of the data is unkown? Provided that the three least squares assumptions hold and the errors are normally distributed and homoskedastic (we refer to these conditions as the homoskedastic normal regression assumptions), we have normally distributed estimators and \\(t\\)-distributed test tatistics. Recall the definition of a \\(t\\)-distributed variable \\[ \\frac{Z}{\\sqrt{W/m}} \\sim t_m\\] where \\(Z\\) is a standard normal random variable, \\(W\\) is \\(\\chi^2\\) distributed with \\(m\\) degrees of freedom and \\(Z\\) and \\(W\\) are independent. See section 5.6 in the book for a more detailed discussion of the small sample distribution of \\(t\\)-statistics in regression. Let us simulate the distribution of regression \\(t\\)-statistics based on a large number of small random samples (\\(n=20\\)) and compare their simulated distributions to their theoretical distribution which sould be \\(t_{18}\\), the \\(t\\)-distribution with \\(18\\) degrees of freedom (recall that \\(\\text{DF}=n-k-1\\)). # initialize vectors beta_0 &lt;- numeric(10000) beta_1 &lt;- numeric(10000) # loop sampling / estimation / t statistics for (i in 1:10000) { X &lt;- runif(20,0,20) Y &lt;- rnorm(n = 20, mean = X) reg &lt;- summary(lm(Y ~ X)) beta_0[i] &lt;- (reg$coefficients[1,1] - 0)/(reg$coefficients[1,2]) beta_1[i] &lt;- (reg$coefficients[2,1] - 1)/(reg$coefficients[2,2]) } # plot distributions and compare with t_18 density function par(mfrow = c(1,2)) # plot simulated density of beta_0 plot(density(beta_0), lwd= 2 , main = expression(widehat(beta)[0]), xlim = c(-4, 4) ) # add t_18 density to the plot curve(dt(x, df = 18), add = T, col = &quot;red&quot;, lwd = 2, lty = 2 ) # plot simulated density of beta_1 plot(density(beta_1), lwd = 2, main = expression(widehat(beta)[1]), xlim=c(-4,4) ) # add t_18 density to the plot curve(dt(x, df = 18), add = T, col = &quot;red&quot;, lwd = 2, lty = 2 ) The outcomes are consistent with our expectations: the empirical distributions of both estimators seem to track the \\(t_{18}\\) distribution quite closely. "],
["references.html", "References", " References "]
]
