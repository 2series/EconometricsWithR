\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Using R for Introduction to Econometrics},
            pdfauthor={Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Using R for Introduction to Econometrics}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2018-04-26}

\usepackage{booktabs}
%\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{rotating, graphicx}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\makeatletter % undo the wrong changes made by mathspec
\let\RequirePackage\original@RequirePackage
\let\usepackage\RequirePackage
\makeatother

\newenvironment{rmdknit}
    {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    }
    { 
    \\\\\hline
    \end{tabular} 
    \end{center}
    }

\newenvironment{rmdnote}
    {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    }
    { 
    \\\\\hline
    \end{tabular} 
    \end{center}
    }
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Preface}\label{preface}

\begin{center}\includegraphics[width=0.45\linewidth]{images/URFITE_logo} \end{center}

The interest in the freely available statistical programming language
and software environment R is soaring. By the time we wrote first drafts
for this project, more than 11000 add-ons (many of them cutting-edge)
were made available on the Comprehensive R Archieve Network
(\href{https://cran.r-project.org/}{CRAN}), an extensive network of FTP
servers around the world that store identical and up-to-date versions of
R code and its documentation. R dominates other (commercial) software
for statistical computing in most fields of research in applied
statistics but the benefits of it being freely available, open source
and having a large and constantly growing community of users that
contribute to CRAN render R more and more appealing for empirical
economists and econometricians at the same time.

A striking advantage of using R in econometrics courses is that it
allows students to explicitly document their analysis step-by-step such
that it is easy to update and to expand. This allows to re-use code for
similar applications with different data. More important for the purpose
of education, R programs are fully reproducible which makes it
straightforward for others to comprehend and validate the results.

Presenting bare R code chunks on lecture slides clearly has a deterring
effect which is why we offer tutorials where we introduce students to
both econometric theory and its applications using R for some time now.
There are some excelent books that introduce R and its applications to
econometrics like Kleiber \& Zeilis (2008) and Hetekar (2010). However,
we think that these works are somewhat difficult to access for
undergraduate students in economics having little understanding of
econometric methods and predominantly no experience in programming at
all. Consequently, we have started to compile a collection of
reproducible reports for use in class. These reports provide guidance on
how to implement selected applications from the textbook
\emph{Introduction to Econometrics} Stock \& Watson (2014) on which the
lecture is based, using R. This process has been faciliated considerably
with the release of knitr, an R package for dynamic report generation
which allows to seamlessly combine pure text, Latex and R code in a
variety of output formats, including PDF and HTML.

This book is an interactive script in the style of a reproducible
research report. \textbf{U}sing \textbf{R} \textbf{f}or
\textbf{I}ntroduction \textbf{t}o \textbf{E}conometrics (\emph{URFITE})
aims to provide students of economic sciences with a
platform-independent e-learning arrangement that seamlessly intertwines
theoretical core knowledge and empirical skills in undergraduate
econometrics. Thereby, the focus is set on empirical applications with R
and we leave out tedious derivations and formal proofs wherever we can.
We were inspired by \emph{Using R for Introductory Econometrics} (Heiss,
2016) which builds on the popular \emph{Introductory Econometrics} by
Wooldridge (2015) and domonstrates how to replicate the applications
discussed therin using R.

Similarly, \emph{URFITE} is closely aligned on Stock \& Watson (2014).
While the book does very well in motivating theory by real-world
applications, we want to take it a step further and would like to enable
students not only to learn how results of case studies can be replicated
with R but we also intend to strengthen their ability in using the newly
acquired skills in other empirical applications.

Put differently, this book is neither an introduction to R, nor is it
intended to be a replacement for the textbook let alone a full course in
undergraduate econometrics. Instead, we would like to encourage you to
work through chapters of Stock and Watson (2014) and use \emph{URFITE}
as an empirically orientated companion to the book.

Throughout this script, interactive R programming exercises are used as
supplements to code chunks that display how previously discussed
techniques can be implemented within R. These programming exercises are
realized using the
\href{https://github.com/datacamp/datacamp-light}{DataCamp light widget}
and are backed by an R-session which is maintained on DataCamp's
servers.

The widget consists of two tabs. script.R mimics a .R-file, a file
format that is commonly used for storing R code. Lines starting with a
\# are commented out, that is they are not recognized as code.
Furthermore, script.R works like an exercise sheet where you may write
down the solution you come up with. If you hit the button run,
submission correctness tests are run and you will be notified whether
your approach is correct. If not, you will receive feedback suggesting
improvements or hints. The other tab, R Console, is a fully functional R
console that can be used for trying out solutions to exercises before
submitting them. Of course you may submit (almost any) arbitrary R code
and use the console to play around and explore. Simply type a command
and hit the enter key on your keyboard.

As an example, the line of code presented in the code chunk below tells
R to compute the number of packages available on CRAN.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(}\KeywordTok{available.packages}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

You may click on R Console, enter and execute this command (the R
session is initialized by clicking anywhere into the widget. This might
take a few seconds.)

\begin{verbatim}
<code data-type="sample-code">
    # Create a variable a, equal to 5


    # Print out a
\end{verbatim}

\chapter{check the number of R packages available on
CRAN}\label{check-the-number-of-r-packages-available-on-cran}

\begin{verbatim}
</code>

<code data-type="solution">
    # Create a variable a, equal to 5
    a <- 5

    # Print out a
    print(a)
</code>

<code data-type="sct">
    test_object("a")
    test_function("print")
    test_function("nrow")
    test_function("available.packages")
    success_msg("Great job!")
</code>

<div data-type="hint">
Use the assignment operator (<code> <- </code>) to create the variable <code>a</code>.
</div>
\end{verbatim}

\chapter{Introduction}\label{introduction}

\chapter{Probability Theory}\label{probability-theory}

This chapter reviews some basic concepts of probability theory and
demonstrates how they can be applied in R.

Most of the statistical functionalities in R's standard version are
collected in the stats package. It provides simple functions which
compute descriptive measures and faciliate calculus involving a variety
of probability distributions but also holds more sophisticated routines
that e.g.~enable the user to estimate a large number of models based on
the same data or help to conduct extensive simulation studies. Execute
\texttt{library(help\ =\ "stats")} in the console to view the
documentation and a complete list of all functions gathered in
\texttt{stats}.

In what follows, we lay our focus on (some of) the probability
distributions that are handled by R and show how to use the relevant
functions to solve simple problems. Thereby we will repeat some core
concepts of probability theory. Among other things, You will learn how
to draw random numbers, how to compute densities, probabilities,
quantiles and alike. As we shall see, it is very convenient to rely on
these routines, especially when writing Your own functions.

\section{Random Variables and Probability
Distributions}\label{random-variables-and-probability-distributions}

For a start, let us briefly review some basic concepts in probability.

\begin{itemize}
\tightlist
\item
  The mutually exclusive results of a random process are called the
  \emph{outcomes}. `Mutually exclusive' means that only one of the
  possible outcomes is observed.
\item
  We refer to the \emph{probability} of an outcome as the proportion of
  the time that the outcome occurs in the long run, that is if the
  experiment is repeated very often.
\item
  The set of all possible outcomes of a random variable is called the
  \emph{sample space}.
\item
  An \emph{event} is a subset of the sample space and consists of one or
  more outcomes.
\end{itemize}

These indeas are unified in the concept of a \emph{random variable}
which is a numerical summary of random outcomes. Random variables can be
\emph{discrete} or \emph{continuous}.

\begin{itemize}
\tightlist
\item
  Discrete random variables have discrete outcomes, e.g. \(0\) and
  \(1\).
\item
  A continuous random variable takes on a continuum of possible values.
\end{itemize}

\subsection*{Probability Distributions of Discrete Random
Variables}\label{probability-distributions-of-discrete-random-variables}
\addcontentsline{toc}{subsection}{Probability Distributions of Discrete
Random Variables}

A typical example for a discrete random variable \(D\) is the result of
a die roll: in terms of a random experiment this is nothing but randomly
selecting a sample of size \(1\) from a set of numbers which are
mutually exclusive outcomes. Here, the sample space is
\(\{1,2,3,4,5,6\}\) and we can think of many different events, e.g. `the
observed outcome lies between \(2\) and \(5\)'.

A basic function to draw random samples from a specified set of elements
is the the function \texttt{sample()}, see \texttt{?sample}. We can use
it to simulate the random outcome of a die roll. Let's role the die!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

The probability distribution of a discrete random variable is the list
of all possible values of the variable and thier probabilities which sum
to \(1\). The cumulative probability distribution function states the
probability that the random variable is less than or equal to a
particular value.

For the die roll, this is straightforward to set up

\begin{longtable}[]{@{}lllllll@{}}
\toprule
Outcome & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\midrule
\endhead
Probability distribution & 1/6 & 1/6 & 1/6 & 1/6 & 1/6 &
1/6\tabularnewline
Cumulative probability distribution & 1/6 & 2/6 & 3/6 & 4/6 & 5/6 &
1\tabularnewline
\bottomrule
\end{longtable}

We can easily plot both functions using R. Since the probability equals
\(1/6\) for each outcome, we set up the vector \texttt{probability} by
using the \texttt{rep()} function which replicates a given value a
specified number of times.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate the vector of probabilities }
\NormalTok{probability <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{) }

\CommentTok{# plot the probabilites }
\KeywordTok{plot}\NormalTok{(probability, }\DataTypeTok{xlab =} \StringTok{"outcomes"}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Probability Distribution"}
\NormalTok{     ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-4-1} \end{center}

For the cumulative probability distribution we need the cumulative
probabilities i.e.~we need the cumulative sums of the vector
\texttt{probability}. These sums can be computed using
\texttt{cumsum()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#generate the vector of cumulative probabilities }
\NormalTok{cum_probability <-}\StringTok{ }\KeywordTok{cumsum}\NormalTok{(probability) }

\CommentTok{# plot the probabilites }
\KeywordTok{plot}\NormalTok{(cum_probability, }
     \DataTypeTok{xlab =} \StringTok{"outcomes"}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Cumulative Probability Distribution"}
\NormalTok{     ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-5-1} \end{center}

\subsection*{Bernoulli Trials}\label{bernoulli-trials}
\addcontentsline{toc}{subsection}{Bernoulli Trials}

The set of elements \texttt{sample()} draws from does not have to
consist of numbers only. We might as well simulate coin tossing with
outcomes \(H\) (head) and \(T\) (tail).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"H"}\NormalTok{,}\StringTok{"T"}\NormalTok{),}\DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "T"
\end{verbatim}

The result of a coin toss is a Bernoulli distributed random variable
i.e.~a variable with to possible distinct outcomes.

Imagine you are about to toss a coin \(10\) times in a row and wonder
how likely it is to end up with a sequence of outcomes like

\[ H \, H \, T \, T \,T \,H \,T \,T \, H \, H .\]

This is a typical example of a Bernoulli experiment as it consists of
\(n=10\) Bernoulli trials that are independent of each other and we are
interested in the likelihood of observing \(k=5\) successes \(H\) that
occur with probability \(p=0.5\) (assuming a fair coin) in each trial.

It is a well known result that the number of successes \(k\) follows a
binomial distribution

\[ k \sim B(n,p). \]

The probability of observing \(k\) successes in the experiment
\(B(n,p)\) is hence given by

\[f(k)=P(k)=\begin{pmatrix}n\\ k \end{pmatrix} \cdot p^k \cdot
q^{n-k}=\frac{n!}{k!(n-k)!} \cdot p^k \cdot q^{n-k}\]

where \(\begin{pmatrix}n\\ k \end{pmatrix}\) is a binomial coefficient.

In R, we can solve the problem stated above by means of the function
\texttt{dbinom()} which calculates the probability of the binomial
distribution for parameters \texttt{x}, \texttt{size}, and
\texttt{prob}, see \texttt{?binom}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =} \DecValTok{5}\NormalTok{,}
       \DataTypeTok{size =} \DecValTok{10}\NormalTok{,}
       \DataTypeTok{prob =} \FloatTok{0.5}
\NormalTok{       ) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2460938
\end{verbatim}

We conclude that the probability of observing Head \(k=5\) times when
tossing the coin \(n=10\) times is about \(24.6\%\).

Now assume we are interested in \(P(4 \leq k \leq 7)\) i.e.~the
probability of observing \(4\), \(5\), \(6\) or \(7\) successes for
\(B(10,0.5)\). This is easily computed by providing a vector as the
\texttt{x} argument in our call of \texttt{dbinom()} and summing up
using \texttt{sum()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}
  \KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =} \DecValTok{4}\OperatorTok{:}\DecValTok{7}\NormalTok{, }
         \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }
         \DataTypeTok{prob =} \FloatTok{0.5}
\NormalTok{         )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7734375
\end{verbatim}

The Probability distribution of a discrete random variable is nothing
but a list of all possible outcomes that can occur and their respective
probabilities. In our coin tossing example, we face \(11\) possible
outcomes for \(k\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up vector of possible outcomes}
\NormalTok{k <-}\StringTok{ }\DecValTok{0}\OperatorTok{:}\DecValTok{10}
\end{Highlighting}
\end{Shaded}

To visualize the probability distribution function of \(k\) we may
therefore simply call

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# assign probabilities}
\NormalTok{probability <-}\StringTok{ }\KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ k,}
                      \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }
                      \DataTypeTok{prob =} \FloatTok{0.5}
\NormalTok{                      )}

\CommentTok{# plot outcomes against probabilities}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ k, }
     \DataTypeTok{y =}\NormalTok{ probability,}
     \DataTypeTok{main =} \StringTok{"Probability Distribution Function"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-10-1} \end{center}

In a similar fashion we may plot the cumulative distribution function of
\(k\) by executing the following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob <-}\StringTok{ }\KeywordTok{cumsum}\NormalTok{(}
              \KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =}\DecValTok{0}\OperatorTok{:}\DecValTok{10}\NormalTok{, }
                     \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{prob =} \FloatTok{0.5}
\NormalTok{                     )}
\NormalTok{              )}

\NormalTok{k <-}\StringTok{ }\DecValTok{0}\OperatorTok{:}\DecValTok{10} 
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ k, }
     \DataTypeTok{y =}\NormalTok{ prob,}
     \DataTypeTok{main =} \StringTok{"Cumulative Distribution Function"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-11-1} \end{center}

\subsection*{Expected Values, Mean and
Variance}\label{expected-values-mean-and-variance}
\addcontentsline{toc}{subsection}{Expected Values, Mean and Variance}

The expected value of a random variable is the long-run average value of
the random variable over many repeated trials. For a discrete random
variable, the expected value is computed as a weighted average of its
possible outcomes whereby the weights are the related probabilities.
This is formalized in Key Concept 2.1.

Key Concept 2.1

Expected Value and the Mean

Suppose the random variable \(Y\) takes on \(k\) possible values,
\(y_1, \dots, y_k\), where \(y_1\) denotes the first value, \(y_2\)
denotes the second value, and so forth, and that the probability that
\(Y\) takes on \(y_1\) is \(p_1\), the probability that \(Y\) takes on
\(y_2\) is \(p_2\) and so forth. The expected value of \(Y\), \(E(Y)\)
is defined as

\[ E(Y) = y_1 p_1 + y_2 p_2 + \cdots + y_k p_k = \sum_{i=1}^k y_i p_i \]

where the notation \(\sum_{i=1}^k y_i p_i\) means ``the sum of \(y_i\)
\(p_i\) for \(i\) running from \(1\) to \(k\)''. The expected value of
\(Y\) is also called the mean of \(Y\) or the expectation of \(Y\) and
is denoted by \(\mu_y\).

In the die example, the random variable, \(D\) say, takes on \(6\)
possible values \(d_1 = 1, d_2 = 2, \dots, d_6 = 6\). Assuming a fair
die, each of the \(6\) outcomes occurs with a probability of \(1/6\). It
is therefore easy to calculate the exact value of \(E(D)\) by hand:

\[ E(D) = 1/6 \sum_{i=1}^6 d_i = 3.5 \]

Here, this is simply the average of the natural numbers from \(1\) to
\(6\) since all wights \(p_i\) are \(1/6\). Convince Yourself that this
can be easily calculated using the function \texttt{mean()} which
computes the arithmetic mean of a numeric vector.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5
\end{verbatim}

An example of sampling with replacement is rolling a die three times in
a row.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# rolling a die three times in a row}
\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 4
\end{verbatim}

Of course we could also consider a much bigger number of trials,
\(10000\) say. Doing so, it would be pointless to simply print the
results to the console: by default R displays up to \(1000\) entries of
large vectors and omitts the remainder (give it a go). Eyeballing the
numbers does not reveal too much. Instead let us calculate the sample
average of the outcomes using \texttt{mean()} and see if the result
comes close to the expected value \(E(D)=3.5\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# compute the sample mean of 10000 die rolls}
\KeywordTok{mean}\NormalTok{(}
    \KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }
           \DecValTok{10000}\NormalTok{, }
           \DataTypeTok{replace =}\NormalTok{ T}
\NormalTok{           )}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5039
\end{verbatim}

We find the sample mean to be fairly close to the expected value. (ref
to WLLN)

Other frequently encountered measures are the variance and the standard
deviation. Both are measures of the \emph{dispersion} of a random
variable.

Key Concept 2.2

Variance and Standard Deviation

The Variance of the discrete \emph{random variable} \(Y\), denoted
\(\sigma^2_Y\), is
\[ \sigma^2_Y = \text{Var}(Y) = E\left[(Y-\mu_y)^2\right] = \sum_{i=1}^k (y_i - \mu_y)^2 p_i \]
The standard deviation of \(Y\) is \(\sigma_Y\), the square root of the
variance. The units of the standard deviation are the same as the units
of \(Y\).

The variance as defined in Key Concept 2.2 \emph{is not} implemented as
a function in R. Instead we have the function \texttt{var()} which
computes the \emph{sample variance}

\[ s^2_Y = \frac{1}{n-1} \sum_{i=1}^n (y_i - \overline{y})^2. \]

Remember that \(s^2_Y\) is different from the so called \emph{population
variance} of \(Y\),

\[ \text{Var}(Y) = \frac{1}{N} \sum_{i=1}^N (y_i - \mu_Y)^2, \]

since it measures how the data is dispersed around the sample average
\(\overline{y}\) instead of the population mean \(\mu_Y\). This becomes
clear when we look at our die rolling example. For \(D\) we have

\[ \text{Var}(D) = 1/6 \sum_{i=1}^6 (d_i - 3.5)^2 = 2.92  \] which is
obviously different from the result of \(s^2\) as computed by
\texttt{var()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5
\end{verbatim}

\section{Probability Distributions of Continuous Random
Variables}\label{probability-distributions-of-continuous-random-variables}

Since a continuous random variable takes on a continuum of possible
values, we cannot use the concept of a probability distribution as used
for discrete random variables. Instead, the probability distribution of
a continuous random variable is summarized by its \emph{probability
density function} (PDF).

The cumulative probability distribution function (CDF) for a continuous
random variable is defined just as in the discrete case. Hence, the
cumulative probability distribution of a continuous random variables
states the probability that the random variable is less than or equal to
a particular value.

For completeness, we present revisions of Key Concepts 2.1 and 2.2 for
the continuous case.

Key Concept 2.3

Probabilities, Expected Value and Variance of a Continuous Random
Variable

Let \(f_Y(y)\) denote the probability density function of \(Y\). Because
probabilities cannot be negative, we have \(f_Y\geq 0\) for all \(y\).
The Probability that \(Y\) falls between \(a\) and \(b\), \(a < b\) is
\[ P(a \leq Y \leq b) = \int_a^b f_Y(y) \mathrm{d}y. \] We further have
that \(P(-\infty \leq Y \leq \infty) = 1\) and therefore
\(\int_{-\infty}^{\infty} f_Y(y) \mathrm{d}y = 1\).

As for the discrete case, the expected value of \(Y\) is the probability
weighted average of its values. Due to continuity, we use intergrals
instead of sums.

The expected value of \(Y\) is

\[ E(Y) =  \mu_Y = \int y f_Y(y) \mathrm{d}y. \]

The variance is the expected value of \((Y - \mu_Y)^2\). We thus have

\[ \text{Var}(Y) =  \sigma_Y^2 = \int (y - \mu_Y)^2 f_Y(y) \mathrm{d}y. \]

Let us discuss an example:

Consider the continuous random variable \(X\) with propability density
function

\[ f_X(x) = \frac{3}{x^4}, x>1. \]

\begin{itemize}
\tightlist
\item
  We can show analytically that the integral of \(f_X(x)\) over the real
  line equals \(1\).
\end{itemize}

\begin{align}
 \int f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} \frac{3}{x^4} \mathrm{d}x \\
  =& \lim_{t \rightarrow \infty} \int_{1}^{t} \frac{3}{x^4} \mathrm{d}x \\
  =& \lim_{t \rightarrow \infty}  -x^{-3} \rvert_{x=1}^t \\
  =& -\left(\lim_{t \rightarrow \infty}\frac{1}{t^3} - 1\right) \\
  =& 1
\end{align}

\begin{itemize}
\tightlist
\item
  The expectation of \(X\) can be computed as follows:
\end{itemize}

\begin{align}
 E(X) = \int x \cdot f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} x \cdot \frac{3}{x^4} \mathrm{d}x \\
  =& - \frac{3}{2} x^{-2} \rvert_{x=1}^{\infty} \\
  =& -\frac{3}{2} \left( \lim_{t \rightarrow \infty} \frac{1}{t^2} - 1 \right) \\
  =& \frac{3}{2}
\end{align}

\begin{itemize}
\tightlist
\item
  Note that the variance of \(X\) can be expressed as
  \(\text{Var}(X) = E(X^2) - E(X)^2\). Since \(E(X)\) has been computed
  in the previous step, we seek \(E(X^2)\):
\end{itemize}

\begin{align}
 E(X^2)= \int x^2 \cdot f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} x^2 \cdot \frac{3}{x^4} \mathrm{d}x \\
  =& -3 x^{-1} \rvert_{x=1}^{\infty} \\
  =& -3 \left( \lim_{t \rightarrow \infty} \frac{1}{t} - 1 \right) \\
  =& 3
\end{align}

So we have shown that the area under the curve equals one, that the
expectation is \(E(X)=\frac{3}{2} \ \) and we found the variance to be
\(\text{Var}(X) = \frac{3}{4}\). However, this was quite tedious and, as
we shall see soon, an analytic approach is not applicable for some
probability density functions e.g.~if integrals have no closed form
solutions.

Luckily, R enables us to find the results derived above in an instant.
The tool we use for this is the function \texttt{integrate()}. First, we
have to define the functions we want to calculate integrals for as R
functions, i.e.~the PDF \(f_X(x)\) as well as the expressions
\(x\cdot f_X(x)\) and \(x^2\cdot f_X(x)\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define functions}
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\DecValTok{3}\OperatorTok{/}\NormalTok{x}\OperatorTok{^}\DecValTok{4}
\NormalTok{g <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{*}\KeywordTok{f}\NormalTok{(x)}
\NormalTok{h <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{^}\DecValTok{2}\OperatorTok{*}\KeywordTok{f}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

Next, we use \texttt{integrate()} and set lower and upper limits of
integration to \(1\) and \(\infty\) using arguments \texttt{lower} and
\texttt{upper}. By default, \texttt{integrate()} prints the result along
with an estimate of the calculation error to the console. However, the
outcome is not a numeric value one can do further calculation with
readily. In order to get only a numeric value of the integral, we need
to use the \texttt{\$} operator in conjunction with \texttt{value}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate area under curve}
\NormalTok{AUC <-}\StringTok{ }\KeywordTok{integrate}\NormalTok{(f, }
                 \DataTypeTok{lower =} \DecValTok{1}\NormalTok{, }
                 \DataTypeTok{upper =} \OtherTok{Inf}
\NormalTok{                 )}
\NormalTok{AUC }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1 with absolute error < 1.1e-14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate E(X)}
\NormalTok{EX <-}\StringTok{ }\KeywordTok{integrate}\NormalTok{(g,}
                \DataTypeTok{lower =} \DecValTok{1}\NormalTok{,}
                \DataTypeTok{upper =} \OtherTok{Inf}\NormalTok{)}
\NormalTok{EX}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1.5 with absolute error < 1.7e-14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate Var(X)}
\NormalTok{VarX <-}\StringTok{ }\KeywordTok{integrate}\NormalTok{(h,}
                  \DataTypeTok{lower =} \DecValTok{1}\NormalTok{,}
                  \DataTypeTok{upper =} \OtherTok{Inf}
\NormalTok{                  )}\OperatorTok{$}\NormalTok{value }\OperatorTok{-}\StringTok{ }\NormalTok{EX}\OperatorTok{$}\NormalTok{value}\OperatorTok{^}\DecValTok{2} 
\NormalTok{VarX}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.75
\end{verbatim}

Although there is a wide variety of distributions, the ones most often
encountered in econometrics are the normal, chi-squared, Student \(t\)
and \(F\) distributions. Therefore we will discuss some core R functions
that allow to do calculations involving densities, probabilities and
quantiles of these distributions.

Every probability distribution that R handles has four basic functions
whose names consist of a prefix followed by a root name. As an example,
take the normal distribution. The root name of all four functions
associated with the normal distribution is norm. The four prefixes are

\begin{itemize}
\tightlist
\item
  d for ``density'' - probability function / probability density
  function
\item
  p for ``probability'' - cumulative distribution function
\item
  q for ``quantile'' - quantile function (inverse cumulative
  distribution function)
\item
  r for ``random'' - random number generator
\end{itemize}

Thus, for the normal distribution we have the R functions
\texttt{dnorm()}, \texttt{pnorm()}, \texttt{qnorm()} and
\texttt{rnorm()}.

\subsection*{The Normal Distribution}\label{the-normal-distribution}
\addcontentsline{toc}{subsection}{The Normal Distribution}

The probably most important probability distribution considered here is
the normal distribution. This is not least due to the special role of
the standard normal distribution and the Central Limit Theorem which is
treated shortly during the course of this section. Distributions of the
normal family have a familiar symmetric, bell-shaped probability
density. A normal distribution is characterized by its mean \(\mu\) and
its standard deviation \(\sigma\) what is concisely expressed by
\(N(\mu,\sigma^2)\). The normal distribution has the PDF

\[ f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x - μ)^2/(2 σ^2)}. \]

For the standard normal distribution we have \(\mu=0\) and \(\sigma=1\).
Standard normal variates are often denoted \(Z\). Usually, the standard
normal PDF is denoted by \(\phi\) and the standard normal CDF is denoted
by \(\Phi\). Hence,

\[ \phi(c) = \Phi'(c) \ \ , \ \ \Phi(c) = P(Z \leq c) \ \ , \ \ Z \sim N(0,1).
\] In R, we can conveniently obtain density values of normal
distributions using the function \texttt{dnorm()}. Let us draw a plot of
the standard normal density function using \texttt{curve()} and
\texttt{dnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# draw a plot of the N(0,1) pdf}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x),}
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{, }\FloatTok{3.5}\NormalTok{),}
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
      \DataTypeTok{main =} \StringTok{"Standard Normal Density Function"}
\NormalTok{      ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-18-1} \end{center}

We can obtain the density at different positions by passing a vector of
quantiles to \texttt{dnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute denstiy at x=-1.96, x=0 and x=1.96}
\KeywordTok{dnorm}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.96}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{1.96}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05844094 0.39894228 0.05844094
\end{verbatim}

Similary as for the PDF, we can plot the standard normal CDF using
\texttt{curve()} and \texttt{pnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal CDF}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{pnorm}\NormalTok{(x), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{, }\FloatTok{3.5}\NormalTok{), }
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
      \DataTypeTok{main =} \StringTok{"Standard Normal Cumulative Distribution Function"}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-20-1} \end{center}

We can also use R to calculate the probability of events associated with
a standard normal random variate.

Let us say we are interested in \(P(Z \leq 1.337)\). For some general
continuous random variable \(Z\) on \([-\infty,\infty]\) with density
function \(g(x)\) we would have to determine \(G(x)\), the
antiderivative of \(g(x)\) since

\[ P(Z \leq 1,337 ) = G(1,337) = \int_{-\infty}^{1,337} g(x) \mathrm{d}x.  \]

However, if \(Z \sim N(0,1)\), we have \(g(x)=\phi(x)\) so there is no
analytic solution to the integral above and it is cumbersome to come up
with an approximation. However, we may circumvent this using R in
different ways. The first approach makes use of the function
\texttt{integrate()} which allows to solve one-dimensional integration
problems using a numerical method. For this, we first define the
function we want to compute the integral of as a R function \texttt{f}.
In our example, \texttt{f} needs to be the standard normal density
function and hence takes a single argument \texttt{x}. Following the
definition of \(\phi(x)\) we define \texttt{f} as

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the standard normal PDF as a R function}
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \DecValTok{1}\OperatorTok{/}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{pi)) }\OperatorTok{*}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5} \OperatorTok{*}\StringTok{ }\NormalTok{x}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let us check if this function enables us to compute standard normal
density values by passing it a vector of quantiles.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define vector of quantiles}
\NormalTok{quants <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.96}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{1.96}\NormalTok{)}

\CommentTok{# compute density values}
\KeywordTok{f}\NormalTok{(quants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05844094 0.39894228 0.05844094
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compare to results produced by dnorm()}
\KeywordTok{f}\NormalTok{(quants) }\OperatorTok{==}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(quants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE TRUE TRUE
\end{verbatim}

Notice that the results produces by \texttt{f()} are indeed equivalent
to those given by \texttt{dnorm()}.

Next, we call \texttt{integrate()} on \texttt{f()} and further specify
the arguments \texttt{lower} and \texttt{upper}, the lower and upper
limits of integration.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# integrate f()}
\KeywordTok{integrate}\NormalTok{(f, }
          \DataTypeTok{lower =} \OperatorTok{-}\OtherTok{Inf}\NormalTok{, }
          \DataTypeTok{upper =} \FloatTok{1.337}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.9093887 with absolute error < 1.7e-07
\end{verbatim}

We find that the probability of observing \(Z \leq 1,337\) is about
\(0.9094\%\).

A second and much more convenient way is to use the function
\texttt{pnorm()} which also allows calculus involving the standard
normal cumulative distribution function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute a probability using pnorm()}
\KeywordTok{pnorm}\NormalTok{(}\FloatTok{1.337}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9093887
\end{verbatim}

The result matches the outcome of the approach using
\texttt{ìntegrate()}.

Let us discuss some further examples:

A commonly known result is that \(95\%\) probability mass of a standard
normal lies in the intervall \([-1.96, 1.96]\), that is in a distance of
about \(2\) standard deviations to the mean. We can easily confirm this
by calculating

\[ P(-1.96 \leq Z \leq 1.96) = 1-2\times P(Z \leq -1.96) \] due to
symmetry of the standard normal PDF. Thanks to R, we can abondon the
table of the standard normal CDF again and instead solve this by means
of the function \texttt{pnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the probability}
\DecValTok{1} \OperatorTok{-}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\FloatTok{1.96}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9500042
\end{verbatim}

Now consider a random variable \(Y\) with \(Y \sim N(5,25)\). As You
should already know from Your statistics courses it is not possible to
make any statement of probability without prior standardizing as shown
in Key Concept 2.4.

Key Concept 2.4

Computing Probabilities Involving Normal Random Variables

Suppose \(Y\) is normally distributed with mean \(\mu\) and variance
\(\sigma^2\): \[Y
\sim N(\mu, \sigma^2)\] Then \(Y\) is standardized by substracting its
mean and dividing by its standard deviation:
\[ Z = \frac{Y -\mu}{\sigma} \] Let \(c_1\) and \(c_2\) denote two
numbers whereby \(c_1 < c_2\) and further \(d_1 = (c_1 - \mu) / \sigma\)
and \(d_2 = (c_2 - \mu)/\sigma\). Then

\begin{align} 
P(Y \leq c_2) =& \, P(Z \leq d_2) = \Phi(d_2) \\ 
P(Y \geq c_1) =& \, P(Z \geq d_1) = 1 - \Phi(d_1) \\ 
P(c_1 \leq Y \leq c_2) =& \, P(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1) 
\end{align}

R functions that handle the normal distribution can perform this
standardization. If we are interested in \(P(3 \leq Y \leq 4)\) we can
use \texttt{pnorm()} and adjust for a mean and/or a standard deviation
that deviate from \(\mu=0\) and \(\sigma = 1\) by specifying the
arguments \texttt{mean} and \texttt{sd} accordingly. \textbf{Attention}:
\texttt{pnorm()} requires the argument \texttt{sd} which is the standard
deviation, not the variance!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{5}\NormalTok{) }\OperatorTok{-}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{5}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.07616203
\end{verbatim}

An extension of the normal distribution in a univariate setting is the
multivariate normal distribution. The PDF of two random normal variables
\(X\) and \(Y\) is given by

\begin{align}
g_{X,Y}(x,y) =& \, \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho_{XY}^2}} \\ 
\cdot & \, \exp \left\{ \frac{1}{-2(1-\rho_{XY}^2)} \left[ \left( \frac{x-\mu_x}{\sigma_x} \right)^2 - 2\rho_{XY}\left( \frac{x-\mu_X}{\sigma_X} \right)\left( \frac{y-\mu_Y}{\sigma_Y} \right) + \left( \frac{y-\mu_Y}{\sigma_Y} \right)^2 \right]  \right\}. \label{eq:bivnorm}
\end{align}

Equation \eqref{eq:bivnorm} contains the bivariate normal PDF. Admittedly,
it is hard to gain insights from this complicated expression. Instead,
let us consider the special case where \(X\) and \(Y\) are uncorrelated
standard normal random variables with density functions \(f_X(x)\) and
\(f_Y(y)\) and we assume that they have a joint normal distribution. We
then have the parameters \(\sigma_X = \sigma_Y = 1\), \(\mu_X=\mu_Y=0\)
(due to marginal standard normality) and \(\rho_{XY}=0\) (due to
uncorrelatedness). The joint probability density function of \(X\) and
\(Y\) then becomes

\[ g_{X,Y}(x,y) = f_X(x) f_Y(y) = \frac{1}{2\pi} \cdot \exp \left\{ -\frac{1}{2} \left[x^2 + y^2 \right]  \right\}, \tag{2.2}  \]

the PDF of the bivariate standard normal distribution. The next plot
provides an interactive three dimensional plot of (2.2). By moving the
mouse curser over the plot You can see that the density is rotationally
invariant.

\subsection*{The Chi-Squared
Distribution}\label{the-chi-squared-distribution}
\addcontentsline{toc}{subsection}{The Chi-Squared Distribution}

Another distribution relevant in econometric day-to-day work is the
chi-squared distribution. It is often needed when testing special types
of hypotheses frequently ecountered when dealing with regression models.

The sum of \(M\) squared independent standard normal distributed random
variables follows a chi-squared distribution with \(M\) degrees of
freedom.

\[ Z_1^2 + \dots + Z_M^2 = \sum_{m=1}^M Z_m^2 \sim \chi^2_M \ \ \text{with} \ \ Z_m \overset{i.i.d.}{\sim} N(0,1) \label{eq:chisq}\]

A \(\chi^2\) distributed random variable with \(M\) degrees of freedom
has expectation \(M\), mode at \(M-2\) for \(n \geq 2\) and variance
\(2 \cdot M\).

For example, if we have

\[ Z_1,Z_2,Z_3 \overset{i.i.d.}{\sim} N(0,1) \]

it holds that

\[ Z_1^2+Z_2^2+Z_3^3 \sim \chi^2_3. \tag{2.3} \] By means of the code
below, we can display the PDF and the CDF of a \(\chi^2_3\) random
variable in a single plot. This is achieved by setting the argument
\texttt{add\ =\ TRUE} in the second call of \texttt{curve()}. Further we
adjust limits of both axes using \texttt{xlim} and \texttt{ylim} and
choose different colors to make both functions better distinguishable.
The plot is completed by adding a legend with help of the function
\texttt{legend()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the PDF}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{3}\NormalTok{), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{), }
      \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }
      \DataTypeTok{col=}\StringTok{"blue"}\NormalTok{, }
      \DataTypeTok{main=}\StringTok{"p.d.f. and c.d.f of Chi-Squared Distribution, m = 3"}
\NormalTok{      )}

\CommentTok{# add the CDF to the plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{pchisq}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{3}\NormalTok{), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{), }
      \DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }
      \DataTypeTok{col=}\StringTok{"red"}
\NormalTok{      )}

\CommentTok{# add a legend to the plot}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{"PDF"}\NormalTok{,}\StringTok{"CDF"}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-28-1} \end{center}

Notice that, since the outcomes of a \(\chi^2_M\) distributed random
variable are always positive, the domain of the related PDF and CDF is
\(\mathbb{R}_{\geq0}\).

As expectation and variance depend (solely) on the degrees of freedom,
the distribution's shape changes drastically if we vary the number of
squared standard normals that are summed up. This relation is often
depicted by overlaying densities for different \(M\), see e.g.~the
Wikipedia Article.

Of course, one can easily reproduce such a plot using R. Again we start
by plotting the density of the \(\chi_1^2\) distribution on the
intervall \([0,15]\) with \texttt{curve()}. In the next step, we loop
over degrees of freedom \(m=2,...,7\) and add a density curve for each
\(m\) to the plot. We also adjust the line color for each iteration of
the loop by setting \texttt{col\ =\ m}. At last, we add a legend that
displays degrees of freedom and the associated colors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the density for m=1}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{1}\NormalTok{), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{15}\NormalTok{), }
      \DataTypeTok{xlab =} \StringTok{"x"}\NormalTok{, }
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
      \DataTypeTok{main=}\StringTok{"Chi-Square Distributed Random Variables"}
\NormalTok{      )}

\CommentTok{# add densities for m=2,...,7 to the plot using a for loop }
\ControlFlowTok{for}\NormalTok{ (m }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
  \KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df =}\NormalTok{ m),}
        \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{15}\NormalTok{), }
        \DataTypeTok{add =}\NormalTok{ T, }
        \DataTypeTok{col =}\NormalTok{ m}
\NormalTok{        )}
\NormalTok{\}}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \KeywordTok{as.character}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{), }
       \DataTypeTok{col =} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{ , }
       \DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }
       \DataTypeTok{title =} \StringTok{"D.f."}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-29-1} \end{center}

It is evident that increasing the degrees of freedom shifts the
distribution to the right (the modus becomes larger) and increases its
dispersion (the distribution's variance grows).

\hypertarget{thetdist}{\subsection*{\texorpdfstring{The Student \(t\)
Distribution}{The Student t Distribution}}\label{thetdist}}
\addcontentsline{toc}{subsection}{The Student \(t\) Distribution}

Let \(Z\) be a standard normal variate, \(W\) a random variable that
follows a \(\chi^2_M\) distribution with \(M\) degrees of freedom and
further assume that \(Z\) and \(W\) are independently distributed. Then
it holds that

\[ \frac{Z}{\sqrt{W/M}} =:X \sim t_M \] and we say that \(X\) follows a
student \(t\) distribution (or simple \(t\) distribution) with \(M\)
degrees of freedom.

As for the \(\chi^2_M\) distribution, a \(t\) distribution depends on
the degrees of freedom \(M\). \(t\) distributions are symmetric,
bell-shaped and look very similar to a normal distribution, especially
when \(M\) is large. This is not a coincidence: for a sufficient large
\(M\), a \(t_M\) distribution can be approximated by the standard normal
distribution. This approximation works reasonably well for \(M\geq 30\).
As we will show later by means of a small simulation study, the
\(t_{\infty}\) distribution \emph{is} the standard normal distribution.

A \(t_M\) distributed random variable has an expectation value if
\(M>1\) and a variance if \(n>2\).

\begin{align}
  E(X) =& 0 \ , \ M>1 \\
  \text{Var}(X) =& \frac{M}{M-2} \ , \ M>2
\end{align}

Let us graph some \(t\) distributions with different \(M\) and compare
them with the standard normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal density}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{), }
      \DataTypeTok{xlab =} \StringTok{"x"}\NormalTok{, }
      \DataTypeTok{lty=}\DecValTok{2}\NormalTok{, }
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
      \DataTypeTok{main=}\StringTok{"Theoretical Densities of t-Distributions"}
\NormalTok{      )}

\CommentTok{# plot the t density for m=2}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{2}\NormalTok{), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{), }
      \DataTypeTok{col=}\DecValTok{2}\NormalTok{, }
      \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{      )}

\CommentTok{# plot the t density for m=4}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{4}\NormalTok{), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{), }
      \DataTypeTok{col=}\DecValTok{3}\NormalTok{, }
      \DataTypeTok{add=}\NormalTok{T}
\NormalTok{      )}

\CommentTok{# plot the t density for m=25}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{25}\NormalTok{), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{), }
      \DataTypeTok{col=}\DecValTok{4}\NormalTok{, }
      \DataTypeTok{add=}\NormalTok{T}
\NormalTok{      )}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{"N(0,1)"}\NormalTok{,}\StringTok{"M=2"}\NormalTok{,}\StringTok{"M=4"}\NormalTok{,}\StringTok{"M=25"}\NormalTok{), }
       \DataTypeTok{col =} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-30-1} \end{center}

The plot indicates what has been claimed in the previous paragraph: as
the degrees of freedom increase, the shape of the \(t\) distribution
comes closer to that of a standard normal bell. Already for \(M=25\) we
find little difference to the dashed line which belongs to the standard
normal density curve. If \(M\) is small, we find the distribution to
have slightly havier tails than a standard normal, i.e.~it has a
``fatter'' bell shape.

\subsection*{\texorpdfstring{The \(F\)
Distribution}{The F Distribution}}\label{the-f-distribution}
\addcontentsline{toc}{subsection}{The \(F\) Distribution}

Another ratio of random variables important to econometricians is the
ratio of two indpendently \(\chi^2\) distributed random variables that
are divided by their degrees of freedom. Such a quantity follows a \(F\)
distribution with numerator degrees of freedom \(M\) and denominator
degrees of freedom \(n\), denoted \(F_{M,n}\). The distribution was
first derived by George Snedecor but was named in honor of
\href{https://en.wikipedia.org/wiki/Ronald_Fisher}{Sir Ronald Fisher}.

\[ \frac{W/M}{V/n} \sim F_{M,n} \ \ \text{with} \ \ W \sim \chi^2_M \ \ , \ \ V \sim \chi^2_n \]

By definition, the domain of both PDF and CDF of a \(F_{M,n}\)
distributed random variable is \(\mathbb{R}_{\geq0}\).

Say we have a \(F\) distributed random variable \(Y\) with numerator
degrees of freedom \(3\) and denominator degrees of freedom \(14\) and
are interested in \(P(Y \geq 2)\). This can be computed with help of the
function \texttt{pf()}. By setting the argument \texttt{lower.tail} to
\texttt{TRUE} we ensure that R computes \(1- P(Y \leq 2)\), i.e.~the
probability mass in the tail right of \(2\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pf}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DataTypeTok{lower.tail =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1638271
\end{verbatim}

We can visualize this probability by drawing a line plot of the related
density function and adding a color shading with \texttt{polygon()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define coordinate vectors for vertices of the polygon}
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{10}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{df}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{3}\NormalTok{, }\DecValTok{14}\NormalTok{), }\DecValTok{0}\NormalTok{)}

\CommentTok{# draw density of F_\{3, 14\}}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{df}\NormalTok{(x ,}\DecValTok{3}\NormalTok{ ,}\DecValTok{14}\NormalTok{), }
      \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.8}\NormalTok{), }
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{,}
      \DataTypeTok{main =} \StringTok{"Density Function"}
\NormalTok{      )}

\CommentTok{# draw the polygon}
\KeywordTok{polygon}\NormalTok{(x, y, }\DataTypeTok{col=}\StringTok{"orange"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-32-1} \end{center}

The \(F\) distribution is related to many other distributions. An
important special case encountered in econometrics arises if the
denominator degrees of freedom are large such that the \(F_{M,n}\)
distribution can be approximated by the \(F_{M,\infty}\) distribution
which turns out to be simply the distribution of a \(\chi^2_M\) random
variable divided by its degrees of freedom \(M\).

\[ W/M \sim F_{M,\infty} \ \ , \ \ W \sim \chi^2_M \]

\section{Random Sampling and the Distribution of Sample
Averages}\label{random-sampling-and-the-distribution-of-sample-averages}

To clarify the basic idea of random sampling, let us jump back to the
die rolling example:

Suppose we are rolling the die \(n\) times. This means we are interested
in the outcomes of \(n\) random processes \(Y_i, \ i=1,...,n\) which are
characterized by the same distribution. Since these outcomes are
selected randomly, they are \emph{random variables} themselves and their
realisations will differ each time we draw a sample, i.e.~each time we
roll the die \(n\) times. Furthermore, each observation is randomly
drawn from the same population, that is the numbers from \(1\) to \(6\),
and their individual distribution is the same. Hence we say that
\(Y_1,\dots,Y_n\) are identically distributed. Moreover, we know that
the value of any of the \(Y_i\) does not provide any information on the
remainder of the sample. In our example, rolling a six as the first
observation in our sample does not alter the distributions of
\(Y_2,\dots,Y_n\): all numbers are equally likely to occur. This means
that all \(Y_i\) are also independently distributed. Thus, we say that
\(Y_1,\dots,Y_n\) are independently and identically distributed
(\emph{i.i.d}). The die example uses the most simple sampling scheme.
That is why it is called \emph{simple random sampling}. This concept is
condensed in Key Concept 2.5.

Key Concept 2.5

Simple Random Sampling and i.i.d. Random Variables

In simple random sampling, \(n\) objects are drawn at random from a
population. Each object is equally likely to end up in the sample. We
denote the value of the random variable \(Y\) for the \(i^{th}\)
randomly drawn object as \(Y_i\). Since all objects are equally likely
to be drawn and the distribution of \(Y_i\) is the same for all \(i\),
the \(Y_i, \dots, Y_n\) are independently and identically distributed
(i.i.d.). This means the distribution of \(Y_i\) is the same for all
\(i=1,\dots,n\) and \(Y_1\) is distributed independently of
\(Y_2, \dots, Y_n\) \(Y_2\) is distributed independently of
\(Y_1, Y_3, \dots, Y_n\) and so forth.

What happens if we consider functions of the sample data? Consider the
example of rolling a die two times in a row once again. A sample now
consists of two independent random draws from the set
\(\{1,2,3,4,5,6\}\). In view of the aforementioned, it is apparent that
any function of these two random variables is also random, e.g.~their
sum. Convince Yourself by executing the code below several times.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

Clearly this sum, let us call it \(S\), is a random variable as it
depends on randomly drawn summands. For this example, we can completely
enumerate all outcomes and hence write down the theoretical probability
distribution of our function of the sample data, \(S\):

We face \(6^2=36\) possible pairs. Those pairs are

\begin{align}
  &(1,1)    (1,2)   (1,3)   (1,4)   (1,5)   (1,6) \\ 
  &(2,1)    (2,2)   (2,3)   (2,4)   (2,5)   (2,6) \\ 
  &(3,1)    (3,2)   (3,3)   (3,4)   (3,5)   (3,6) \\ 
  &(4,1)    (4,2)   (4,3)   (4,4)   (4,5)   (4,6) \\ 
  &(5,1)    (5,2)   (5,3)   (5,4)   (5,5)   (5,6) \\ 
  &(6,1)    (6,2)   (6,3)   (6,4)   (6,5)   (6,6)
\end{align}

Thus, possible outcomes for \(S\) are

\[ \left\{ 2,3,4,5,6,7,8,9,10,11,12 \right\} . \] Enumeration of
outcomes yields

\begin{align}
  P(S) = 
  \begin{cases} 
    1/36, \ & S = 2 \\ 
    2/36, \ & S = 3 \\
    3/36, \ & S = 4 \\
    4/36, \ & S = 5 \\
    5/36, \ & S = 6 \\
    6/36, \ & S = 7 \\
    5/36, \ & S = 8 \\
    4/36, \ & S = 9 \\
    3/36, \ & S = 10 \\
    2/36, \ & S = 11 \\
    1/36, \ & S = 12
  \end{cases}
\end{align}

We can also compute \(E(S)\) and \(\text{Var}(S)\) as stated in Key
Concept 2.1 and Key Concept 2.2.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Vector of outcomes}
\NormalTok{S <-}\StringTok{ }\DecValTok{2}\OperatorTok{:}\DecValTok{12}

\CommentTok{# Vector of probabilities}
\NormalTok{PS <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{,}\DecValTok{5}\OperatorTok{:}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{36}

\CommentTok{# Expectation of S}
\NormalTok{ES <-}\StringTok{ }\NormalTok{S }\OperatorTok{%*%}\StringTok{ }\NormalTok{PS; ES}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]    7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Variance of S}
\NormalTok{VarS <-}\StringTok{ }\NormalTok{(S }\OperatorTok{-}\StringTok{ }\KeywordTok{c}\NormalTok{(ES))}\OperatorTok{^}\DecValTok{2} \OperatorTok{%*%}\StringTok{ }\NormalTok{PS; VarS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]
## [1,] 5.833333
\end{verbatim}

(The \texttt{\%*\%} operator is used to compute the scalar product of
two vectors.)

So the distribution of \(S\) is known. It is also evident that its
distribution differs considerably from the marginal distribution,
i.e.~the distribution of a single die roll's outcome, \(D\) . Let us
visualize this using barplots.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# divide the plotting area in one row with two columns}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{# plot the distribution of S}
\KeywordTok{names}\NormalTok{(PS) <-}\StringTok{ }\DecValTok{2}\OperatorTok{:}\DecValTok{12}

\KeywordTok{barplot}\NormalTok{(PS, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.2}\NormalTok{), }
        \DataTypeTok{xlab =} \StringTok{"S"}\NormalTok{, }
        \DataTypeTok{ylab =}\StringTok{"Probability"}\NormalTok{, }
        \DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }
        \DataTypeTok{space=}\DecValTok{0}\NormalTok{, }
        \DataTypeTok{main=}\StringTok{"Sum of Two Die Rolls"}
\NormalTok{        )}

\CommentTok{# plot the distribution of D }
\NormalTok{probability <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{)}
\KeywordTok{names}\NormalTok{(probability) <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{6}

\KeywordTok{barplot}\NormalTok{(probability, }
        \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.2}\NormalTok{), }
        \DataTypeTok{xlab =} \StringTok{"D"}\NormalTok{, }
        \DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }
        \DataTypeTok{space =} \DecValTok{0}\NormalTok{, }
        \DataTypeTok{main =} \StringTok{"Outcome of a single Die Roll"}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-35-1} \end{center}

Many econometric procedures deal with averages of sampled data. It is
almost always assumed that observations are drawn randomly from a
larger, unkown population. As demonstrated for the sample function
\(S\), computing an average of a random sample also has the effect to
make the average a random variable itself. This random variable in turn
has a probability distribution which is called the sampling
distribution. Knowledge about the sampling distribution of an average is
therefore crucial for understanding the performance of econometric
procedures.

The \emph{sample average} of a sample of \(n\) observations
\(Y_1, \dots, Y_n\) is

\[ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i = \frac{1}{n} (Y_1 + Y_2 + \cdots + Y_n). \]
\(\overline{Y}\) is also called the sample mean.

\subsection*{Mean and Variance of the Sample
Mean}\label{mean-and-variance-of-the-sample-mean}
\addcontentsline{toc}{subsection}{Mean and Variance of the Sample Mean}

Denote \(\mu_Y\) and \(\sigma_Y^2\) the mean and the variance of the
\(Y_i\) and suppose that all observations \(Y_1,\dots,Y_n\) are i.i.d.
such that in particular mean and variance are the same for all
\(i=1,\dots,n\). Then we have that

\[ E(\overline{Y}) = E\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) = \frac{1}{n} E\left(\sum_{i=1}^n Y_i\right) = \frac{1}{n} \sum_{i=1}^n E\left(Y_i\right) = \frac{1}{n} \cdot n \cdot \mu_Y = \mu_Y    \]
and

\begin{align}
  \text{Var}(\overline{Y}) =& \text{Var}\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) \\
  =& \frac{1}{n^2} \sum_{i=1}^n \text{Var}(Y_i) + \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1, j\neq i}^n \text{cov}(Y_i,Y_j) \\
  =& \frac{\sigma^2_Y}{n} \\
  =& \sigma_{\overline{Y}}^2.
\end{align}

Note that the second summand vanishes since \(\text{cov}(Y_i,Y_j)=0\)
for \(i\neq j\) due to independence of the observations.

Consequently, the standard deviation of the sample mean is given by

\[ \sigma_{\overline{Y}} = \frac{\sigma_Y}{\sqrt{n}}. \]

It is worthwhile to mention that these results hold irrespective of the
underlying distribution of the \(Y_i\).

\subsubsection*{\texorpdfstring{The Sampling Distribution of
\(\overline{Y}\) when \(Y\) Is Normally
Distributed}{The Sampling Distribution of \textbackslash{}overline\{Y\} when Y Is Normally Distributed}}\label{the-sampling-distribution-of-overliney-when-y-is-normally-distributed}
\addcontentsline{toc}{subsubsection}{The Sampling Distribution of
\(\overline{Y}\) when \(Y\) Is Normally Distributed}

If the \(Y_1,\dots,Y_n\) are i.i.d. draws from a normal distribution
with mean \(\mu_Y\) and variance \(\sigma_Y^2\), the following holds for
their sample average \(\overline{Y}\):

\[ \overline{Y} \sim N(\mu_y, \sigma_Y^2/n) \tag{2.4} \]

For example, if a sample \(Y_i\) with \(i=1,\dots,10\) is drawn from a
standard normal distribution with mean \(\mu_Y = 0\) and variance
\(\sigma_Y^2=1\) we have

\[ \overline{Y} \sim N(0,0.1).\]

We can use R's random number generation facilities to verifiy this
result. The basic idea is to simulate outcomes of the true distribution
of \(\overline{Y}\) by repeatedly drawing random samples of 10
observation from the \(N(0,1)\) distribution and computing their
respective averages. If we do this for a large number of repititions,
the simulated dataset of averages should quite accurately reflect the
theoretical distribution of \(\overline{Y}\) if the theoretical result
holds.

The approach sketched above is an example of what is commonly known as
\emph{Monte Carlo Simulation} or \emph{Monte Carlo Experiment}. To
perform this simulation in R, we proceed as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a sample size \texttt{n} and the number of samples to be drawn
  \texttt{reps}.
\item
  Use the function \texttt{replicate()} in conjunction with
  \texttt{rnorm()} to draw \texttt{n} observations from the standard
  normal distribution \texttt{rep} times. \textbf{Note}: the outcome of
  \texttt{replicate()} is a matrix with dimensions \texttt{n} \(\times\)
  \texttt{rep}. It contains the drawn samples as \emph{columns}.
\item
  Compute sample means using \texttt{colMeans()}. This function computes
  the mean of each column i.e.~of each sample and returns a vector.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set sample size and number of samples}
\NormalTok{n <-}\StringTok{ }\DecValTok{10}
\NormalTok{reps <-}\StringTok{ }\DecValTok{10000}

\CommentTok{# Perform random sampling}
\NormalTok{samples <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(reps, }\KeywordTok{rnorm}\NormalTok{(n)) }\CommentTok{# 10 x 10000 sample matrix}

\CommentTok{# Compute sample means}
\NormalTok{sample.avgs <-}\StringTok{ }\KeywordTok{colMeans}\NormalTok{(samples)}
\end{Highlighting}
\end{Shaded}

After performing these steps we end up with a vector of sample averages.
You can check the vector property of \texttt{sample.avgs}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Check that sample.avgs is a vector}
\KeywordTok{is.vector}\NormalTok{(sample.avgs) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print the first few entries to the console}
\KeywordTok{head}\NormalTok{(sample.avgs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.12406767 -0.10649421 -0.01033423 -0.39905236 -0.41897968 -0.90883537
\end{verbatim}

A straightforward approach to examine the distribution of univariate
numerical data is to plot it as a histogram and compare it to some known
or assumed distribution. This comparison can be done with help of a
suitable statistical test or by simply eyeballing some graphical
representations of these distributions. For our simulated sample
averages, we will do the latter by means of the functions
\texttt{hist()} and \texttt{curve()}.

By default, \texttt{hist()} will give us a frequency histogram i.e.~a
bar chart where observations are grouped into ranges, also called bins.
The ordinate reports the number of observations falling into each of the
bins. Instead, we want it to report density estimates for comparison
purposes. This is achieved by setting the argument
\texttt{freq\ =\ FALSE}. The number of bins is adjusted by the argument
\texttt{breaks}.

Using \texttt{curve()}, we overlay the histogram with a red line which
represents the theoretical density of a \(N(0, 0.1)\) distributed random
variable. Remember to use the argument \texttt{add\ =\ TRUE} to add the
curve to the current plot. Otherwise R will open a new graphic device
and discard the histogram plot!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot the density histogram}
\KeywordTok{hist}\NormalTok{(sample.avgs, }
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1.4}\NormalTok{), }
     \DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{ , }
     \DataTypeTok{freq =}\NormalTok{ F, }
     \DataTypeTok{breaks =} \DecValTok{20}
\NormalTok{     )}

\CommentTok{# overlay the theoretical distribution of sample averages on top of the histogram}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{sd =} \DecValTok{1}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)), }
      \DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd=}\StringTok{"2"}\NormalTok{, }
      \DataTypeTok{add=}\NormalTok{T}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-38-1} \end{center}

From inspection of the plot we can tell that the distribution of
\(\overline{Y}\) is indeed very close to that of a \(N(0, 0.1)\)
distributed random variable so that evidence obained from the Monte
Carlo Simulation supports the theoretical claim.

Let us discuss another example where using simple random sampling in a
simulation setup helps to verify a well known result. As discussed
before, the \protect\hyperlink{chisquare}{Chi-squared} distribution with
\(m\) degrees of freedom arises as the distribution of the sum of \(m\)
independent squared standard normal distributed random variables.

To visualize the claim stated in equation (2.3), we proceed similarly as
in the example before:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose the degrees of freedom \texttt{DF} and the number of samples to
  be drawn \texttt{reps}.
\item
  Draw \texttt{reps} random samples of size \texttt{DF} from the
  standard normal distribution using \texttt{replicate()}.
\item
  For each sample, by squaring the outcomes and summing them up
  columnwise. Store the results
\end{enumerate}

Again, we produce a density estimate for the distribution underlying our
simulated data using a density histogram and overlay it with a line
graph of the theoretical density function of the \(\chi^2_3\)
distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Number of repititions}
\NormalTok{reps <-}\StringTok{ }\DecValTok{10000}

\CommentTok{# Set degrees of freedom of a chi-Square Distribution}
\NormalTok{DF <-}\StringTok{ }\DecValTok{3} 

\CommentTok{# Sample 10000 column vectors à 3 N(0,1) R.V.S}
\NormalTok{Z <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(reps, }\KeywordTok{rnorm}\NormalTok{(DF)) }

\CommentTok{# Column sums of squares}
\NormalTok{X <-}\StringTok{ }\KeywordTok{colSums}\NormalTok{(Z}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\CommentTok{# Histogram of column sums of squares}
\KeywordTok{hist}\NormalTok{(X, }
     \DataTypeTok{freq =}\NormalTok{ F, }
     \DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{breaks =} \DecValTok{40}\NormalTok{, }
     \DataTypeTok{ylab=}\StringTok{"Density"}\NormalTok{, }
     \DataTypeTok{main=}\StringTok{""}
\NormalTok{     )}

\CommentTok{# Add theoretical density}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df =}\NormalTok{ DF), }
      \DataTypeTok{type =} \StringTok{'l'}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }
      \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-39-1} \end{center}

\subsection*{Large Sample Approximations to Sampling
Distributions}\label{large-sample-approximations-to-sampling-distributions}
\addcontentsline{toc}{subsection}{Large Sample Approximations to
Sampling Distributions}

Sampling distributions as considered in the last section play an
important role in the development of econometric methods. In general,
there are two different approaches in characterizing sampling
distributions: an ``exact'' approach and an ``approximate'' approach.

The exact approach aims to find a general formula for the sampling
distribution that holds for any sample size \(n\). We call this the
\emph{exact distribution} or \emph{finite sample distribution}. In the
previous examples of die rolling and normal variates, we have dealt with
functionals of random variables whose sample distributions are
\emph{excactly known} in the sense that we can write them down as
analytical expressions and do calculations. However, this is not always
possible. For \(\overline{Y}\), result (2.4) tells us that normality of
the \(Y_i\) implies normality of \(\overline{Y}\) (we demonstrated this
for the special case of \(Y_i \overset{i.i.d.}{\sim} N(0,1)\) with
\(n=10\) using a simulation study that involves random sampling).
Unfortunately, the \emph{exact} distribution of \(\overline{Y}\) is
unknown, very hard to derive or even untractable if we drop the
assumption of \(Y\) having a normal distribution.

Therefore, as can be guessed from its name, the ``approximate'' approach
aims to find an approximation to the sampling distribution wherby it is
required that the sample size \(n\) is large. A distribution that is
used as a large-sample approximation to the sampling distribution is
also called the \emph{asymptotic distribution}. This is due to the fact
that the asymptotic distribution \emph{is} the sampling distribution for
\(n \rightarrow \infty\) i.e.~the approximation becomes exact if the
sample size goes to infinity. However, there are cases where the
difference between the sampling distribution and the asymptotic
distribution is negligible for moderate or even small samples sizes so
that approximations work very good.

In this section we will discuss two well known results that are used to
approximate sampling distributions and thus consitute key tools in
econoemtric theory: the \emph{law of large numbers} and the
\emph{central limit theorem}. The law of large numbers states that in
large samples, \(\overline{Y}\) is close to \(\mu_Y\) with high
probability. The central limit theorem says that the sampling
distribution of the standardized sample average, that is
\((\overline{Y} - \mu_Y)/\sigma_{\overline{Y}}\) is asymptotically
normal distributed. It is particualarly interesting that both results do
not depend on the distribution of \(Y\). In other words, beeing unable
to describe the complicated sampling distribution of \(\overline{Y}\) if
\(Y\) is not normal, approximations of the latter using the central
limit theorem simplify the development and applicability of econometric
procedures enormously. This is a key component underlying the theory of
statistical inference for regression models. Both results are summarized
in Key Concept 2.6 and Key Concept 2.7.

Key Concept 2.6

Convergence in Probability, Consistency and the Law of Large Numbers

The sample average \(\overline{Y}\) converges in probability to
\(\mu_Y\) --- we say that \(\overline{Y}\) is \emph{consistent} for
\(\mu_Y\) --- if the probability that \(\overline{Y}\) is in the range
\((\mu_Y - \epsilon)\) to \((\mu_Y + \epsilon)\) becomes arbitrarly
close to \(1\) as \(n\) increases for any constant \(\epsilon > 0\). We
write this short as

\[ \overline{Y} \xrightarrow[]{p} \mu_Y. \]

Consider the independently and identically distributed random variables
\(Y_i, i=1,\dots,n\) with expectation \(E(Y_i)=\mu_Y\) and variance
\(\text{Var}(Y_i)=\sigma^2_Y\). Under the condition that
\(\sigma^2_Y< \infty\), that is large outliers are unlikely, the law of
large numbers states

\[ \overline{Y} \xrightarrow[]{p} \mu_Y. \]

The core statement of the law of large numbers is that under quite
general conditions, the probability of obtaining a sample average
\(\overline{Y}\) that is close to \(\mu_Y\) is high if we have a large
sample size.

Consider the example of repeatedly tossing a coin where \(Y_i\) is the
result of the \(i^{th}\) coin toss. \(Y_i\) is a Bernoulli distributed
random variable with

\[ P(Y_i) = \begin{cases} p, & Y_i = 1 \\ 1-p, & Y_i = 0 \end{cases} \]
where \(p = 0.5\) as we assume a fair coin. It is straightforward to
show that

\[ \mu_Y = p = 0.5. \] Say \(p\) is the probabiliy of observing head and
denote \(R_n\) the proportion of heads in the first \(n\) tosses,

\[ R_n = \frac{1}{n} \sum_{i=1}^n Y_i. \tag{2.5}\]

According to the law of large numbers, the observed proportion of heads
converges in probability to \(\mu_Y = 0.5\), the probability of tossing
head in a single coin toss,

\[ R_n \xrightarrow[]{p} \mu_Y=0.5 \ \ \text{as} \ \ n \rightarrow \infty.  \]
The following application simulates \(1000\) coin tosses with a fair
coin and computes the fraction of heads observed for each additional
toss interactively. The result is a random path that, as stated by the
law of large numbers, shows a tendency to approach the vaule of \(0.5\)
as \(n\) grows.

We can use R to compute and illustrate such paths by simulation. The
procedure is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample \texttt{N} observations from the Bernoulli distribution
  e.g.~using \texttt{sample()}.
\item
  Calculate the proportion of heads \(R_n\) as in (2.5). A way to
  achieve this is to call \texttt{cumsum()} on the vector of
  observations \texttt{Y} to obtain its cumulative sum and then divide
  by the respective number of observations.
\end{enumerate}

We continue by plotting the path and also add a dashed line for the
benchmark \(R_n = p = 0.5\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# Set number of coin tosses and simulate}
\NormalTok{N <-}\StringTok{ }\DecValTok{30000}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, N, }\DataTypeTok{replace =}\NormalTok{T)}

\CommentTok{# Calculate R_n for 1:N}
\NormalTok{S <-}\StringTok{ }\KeywordTok{cumsum}\NormalTok{(Y)}
\NormalTok{R <-}\StringTok{ }\NormalTok{S}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N)}

\CommentTok{# Plot the path.}
\KeywordTok{plot}\NormalTok{(R, }
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{), }
     \DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"n"}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"R_n"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Converging Share of Heads in Repeated Coin Tossing"}
\NormalTok{     )}

\CommentTok{# Add a dashed line for R_n = 0.5}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,N), }
      \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }
      \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }
      \DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{1}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-40-1} \end{center}

There are several things to be said about this plot.

\begin{itemize}
\item
  The blue graph shows the observed proportion of heads when tossing a
  coin \(n\) times.
\item
  Since the \(Y_i\) are radnom variables, \(R_n\) is a random variate,
  too. The path depicted is only one of many possible realisations of
  \(R_n\) as it is determined by the \(30000\) observations sampled from
  the Bernoulli distribution. Thus, the code chunk above produces a
  differnt path every time You execute it (try this!).
\item
  If the number of coin tosses \(n\) is small, we observe the proportion
  of heads to be anything but close to its theoretical value,
  \(\mu_Y = 0.5\). However, as more and more observation are included in
  the sample we find that the path stabilizes in neighbourhood of
  \(0.5\). This is the message to take away: the average of multiple
  trials shows a clear tendency to converge to its expected value as the
  sample size increases, just as claimed by the law of large numbers.
\end{itemize}

Key Concept 2.6

The Central Limit Theorem

Suppose that \(Y_1,\dots,Y_n\) are independently and identically
distributed random variables with expectation \(E(Y_i)=\mu_Y\) and
variance \(\text{Var}(Y_i)=\sigma^2_Y\), \(0<\sigma^2_Y<\infty\). The
central limit theorem states that, if the sample size \(n\) goes to
infinity, the distribution of the scaled (by \(\sqrt{n}\)) standardized
sample average
\[ \frac{\overline{Y} - \mu_Y}{\sigma_{\overline{Y}}} = \frac{\overline{Y} - \mu_Y}{\sigma_Y/\sqrt{n}} \ \]
becomes arbitrarily well approximated by the standard normal
distribution.

According to the central limit theorem, the distribution of the sample
mean \(\overline{Y}\) of the bernoulli distributed random variables
\(Y_i\), \(i=1,...,n\) is well approximated by the normal distribution
with parameters \(\mu_Y=p=0.5\) and
\(\sigma^2_{\overline{Y}} = p(1-p) = 0.25\) for large \(n\).
Consequently, for the standardized sample mean we conclude that the
ratio

\[ \frac{\overline{Y} - 0.5}{0.5/\sqrt{n}} \tag{2.6}\] should be well
approximated by the standard normal distribution \(N(0,1)\). We employ
another simulation study to demonstrate this graphically. The idea is as
follows.

Draw a large number of random samples, \(10000\) say, of size \(n\) from
the Bernoulli distribution and compute the sample averages. Standardize
the averages as shown in (2.6). Next, visualize the distribution of the
generated standardized sample averages by means of a density histogram
and compare to the standard normal distribution. Repeat this for
different sample sizes \(n\) to see how increasing the sample size \(n\)
impacts the simulated distribution of the averages.

In R, we realized this as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We start by defining that the next four subsequently generated figures
  shall be drawn in a \(2\times2\) array such that they can be easily
  compared. This is done by calling \texttt{par(mfrow\ =\ c(2,\ 2))}
  before the figures are generated.
\item
  We define the number of repetitions \texttt{reps} as \(10000\) and
  create a vector of sample sizes named \texttt{sample.sizes}. We
  consider samples of sizes \(2\), \(10\), \(50\) and \(100\).
\item
  Next, we combine two \texttt{for()} loops to simulate the data and
  plot the distributions. The inner loop generates \(10000\) random
  samples, each consisting of \texttt{n} observations that are drawn
  from the bernoulli distribution, and computes the standardized
  averages. The outer loop executes the inner loop for the different
  sample sizes \texttt{n} and produces a plot for each iteration.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Subdivide the plot panel into a 2-by-2 array}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{# Set number of repetitions and the sample sizes}
\NormalTok{reps <-}\StringTok{ }\DecValTok{10000}
\NormalTok{sample.sizes <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{)}

\CommentTok{# outer loop (loop over the sample sizes)}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in}\NormalTok{ sample.sizes) \{}
    
\NormalTok{    samplemean <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,reps) }\CommentTok{#initialize the vector of sample menas}
\NormalTok{    stdsamplemean <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,reps) }\CommentTok{#initialize the vector of standardized sample menas}

\CommentTok{# inner loop (loop over repetitions)   }
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{reps) \{}
\NormalTok{      x <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(n,}\DecValTok{1}\NormalTok{,}\FloatTok{0.5}\NormalTok{)}
\NormalTok{      samplemean[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)}
\NormalTok{      stdsamplemean[i] <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(n)}\OperatorTok{*}\NormalTok{(}\KeywordTok{mean}\NormalTok{(x)}\OperatorTok{-}\FloatTok{0.5}\NormalTok{)}\OperatorTok{/}\FloatTok{0.5}
\NormalTok{    \}}
    
\CommentTok{# plot the histogram and overlay it with the N(0,1) density for every iteration    }
    \KeywordTok{hist}\NormalTok{(stdsamplemean, }
         \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
         \DataTypeTok{breaks =} \DecValTok{40}\NormalTok{, }
         \DataTypeTok{freq =} \OtherTok{FALSE}\NormalTok{, }
         \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }
         \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.4}\NormalTok{), }
         \DataTypeTok{xlab =} \KeywordTok{paste}\NormalTok{(}\StringTok{"n ="}\NormalTok{, n), }
         \DataTypeTok{main =} \StringTok{""}
\NormalTok{         )}
    
    \KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x), }
          \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
          \DataTypeTok{col=}\StringTok{"darkred"}\NormalTok{, }
          \DataTypeTok{add =} \OtherTok{TRUE}
\NormalTok{          )}
\NormalTok{  \}  }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-41-1} \end{center}

We see that the simulated sampling distribution of the standardized
average tends to deviate strongly from the standard normal distribution
if the sample size is small, e.g.~for \(n=5\) and \(n=10\). However as
\(n\) grows, the histograms are approaching the bell shape of a standard
normal and we can be confident that the approximation works quite well
as seen for \(n=100\).

\section{Exercises}\label{exercises}

\subsubsection*{1. Sampling}\label{sampling}
\addcontentsline{toc}{subsubsection}{1. Sampling}

Suppose you are the lottery fairy in a weekly lottery, where \(6\) out
of \(49\) unique numbers are drawn.

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Draw the winning numbers for this week.
\end{itemize}

 set.seed(123) \# Draw the winning numbers for this week

 set.seed(123) \# Draw the winning numbers for this week sample(1:49,
size = 6)

 test\_output\_contains(``set.seed(123);sample(1:49, 6)'')

\textbf{Hints:}

\begin{itemize}
\item
  You can use the function sample() to draw random numbers.
\item
  The set of elements to be sampled from is \(\{1,...,49\}\).
\item
  In sample() you can specify the sample size using the argument size.
\end{itemize}

\subsubsection*{2. Probability Density
Function}\label{probability-density-function}
\addcontentsline{toc}{subsubsection}{2. Probability Density Function}

Consider a random variable \(X\) with probability density function (pdf)

\[f_X(x)=\frac{x}{4}e^{-\frac{x^2}{8}},\quad x\geq 0.\]

\textbf{Instructions:}

\begin{itemize}
\item
  Define the pdf from above as a function f(). Use exp() for the natural
  exponential function.
\item
  Check whether the function you have defined is indeed a pdf.
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Define the pdf

\# Integrate f over the domain

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Define the pdf f \textless{}- function(x)\{x/4*exp(-x\^{}2/8)\}

\# Integrate f over the domain integrate(f, 0, Inf)\$value

\begin{verbatim}
<code data-type="sct">
\end{verbatim}

test\_function\_definition(``f'', function\_test = \{
test\_expression\_result(``f(1)'') test\_expression\_result(``f(4)'')
test\_expression\_result(``f(10)'') test\_expression\_result(``f(100)'')
\}) test\_function(`integrate', args = c(`f', `lower', `upper'), eval =
c(F, T, T)) test\_output\_contains(`1', incorrect\_msg = `Did you access
the value of the integral?')

\textbf{Hints:}

\begin{itemize}
\item
  Use function(x) \{\ldots{}\} to define a function which takes the
  argument x.
\item
  In order for f() to be a pdf, its integral over the whole domain has
  to be equal to \(1\), that is, \(\int_0^\infty f_X(x)dx=1\).
\item
  integrate() performs integration in R and with \$value you can access
  the numerical value of the computed integral.
\end{itemize}

\subsubsection*{3. Expected Value and
Variance}\label{expected-value-and-variance}
\addcontentsline{toc}{subsubsection}{3. Expected Value and Variance}

In this exercise you have to compute the expected value and the variance
of the random variable \(X\) considered in the previous exercise.

The pdf f() from the previous exercise is available in your working
environment.

\textbf{Instructions:}

\begin{itemize}
\item
  Define a suitable function ex() which integrates to the expected value
  of \(X\).
\item
  Compute the expected value of \(X\). Store the result in
  expected\_value.
\item
  Define a suitable function ex2() which integrates to the expected
  value of \(X^2\).
\item
  Compute the variance of \(X\). Store the result in variance.
\end{itemize}

\begin{verbatim}
<code data-type="pre-exercise-code">
f <- function(x){(x/4)*exp(-x^2/8)}
</code>

<code data-type="sample-code">
\end{verbatim}

\# Define the function ex

\# Compute the expected value of X

\# Define the function ex2

\# Compute the variance of X

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Define the function ex ex \textless{}- function(x)\{x*f(x)\}

\# Compute the expected value of X expected\_value \textless{}-
integrate(ex, 0, Inf)\$value

\# Define the function ex2 ex2 \textless{}- function(x)\{x\^{}2*f(x)\}

\# Compute the variance of X variance \textless{}- integrate(ex2, 0,
Inf)\$value - expected\_value\^{}2

\begin{verbatim}
<code data-type="sct">
\end{verbatim}

test\_function\_definition(``ex'', function\_test = \{
test\_expression\_result(``ex(1)'') test\_expression\_result(``ex(4)'')
test\_expression\_result(``ex(10)'')
test\_expression\_result(``ex(100)'') \}) test\_function(`integrate',
args = c(`f', `lower', `upper'), eval = c(F, T, T))
test\_object(`expected\_value') test\_function\_definition(``ex2'',
function\_test = \{ test\_expression\_result(``ex2(1)'')
test\_expression\_result(``ex2(4)'')
test\_expression\_result(``ex2(10)'')
test\_expression\_result(``ex2(100)'') \}) test\_function(`integrate',
args = c(`f', `lower', `upper'), eval = c(F, T, T))
test\_object(`variance')

\textbf{Hints:}

\begin{itemize}
\item
  The expected value of \(X\) is defined as
  \(\mathbb{E}[X]=\int_0^\infty xf_X(x)dx\).
\item
  The value of an integral computed by integrate() can be obtained via
  \$value.
\item
  The variance of \(X\) is defined as
  \(Var[X]=\mathbb{E}[X^2]-\mathbb{E}[X]^2\), where
  \(\mathbb{E}[X^2]=\int_0^\infty x^2f_X(x)dx\).
\end{itemize}

\subsubsection*{4. Standard Normal Distribution
I}\label{standard-normal-distribution-i}
\addcontentsline{toc}{subsubsection}{4. Standard Normal Distribution I}

Let \(Z\sim\mathcal{N}(0, 1)\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Compute \(\phi(3)\), that is, the value of the standard normal density
  at \(c=3\).
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Compute the value of the standard normal density at c=3

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Compute the value of the standard normal density at c=3 dnorm(3)

\begin{verbatim}
</code>

<code data-type="sct">
\end{verbatim}

test\_function\_result(`dnorm')

\textbf{Hints:}

\begin{itemize}
\tightlist
\item
  Values of \(\phi(\cdot)\) can be computed using dnorm().
\end{itemize}

\subsubsection*{5. Standard Normal Distribution
II}\label{standard-normal-distribution-ii}
\addcontentsline{toc}{subsubsection}{5. Standard Normal Distribution II}

Let \(Z\sim\mathcal{N}(0, 1)\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Compute \(P(|Z|\leq 1.64)\) by using the function pnorm().
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Compute the probability

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Compute the probability pnorm(1.64)-pnorm(-1.64)

\begin{verbatim}
</code>

<code data-type="sct">
test_function("pnorm")
\end{verbatim}

test\_output\_contains(``1 - 2*pnorm(-1.64)``, incorrect\_msg =''Not
correct!``)

\textbf{Hints:}

\begin{itemize}
\item
  \(P(|Z|\leq z)\) equals \(P(-z \leq Z \leq z)\).
\item
  Probabilities of the form \(P(a \leq Z \leq b)\) can be computed as
  \(P(Z\leq b)-P(Z\leq a)=F_Z(b)-F_Z(a)\) where \(F_Z(\cdot)\) is the
  cumulative distribution function of \(Z\). Alternatively you may
  exploit the symmetry of the standard normal distribution.
\end{itemize}

\subsubsection*{6. Normal Distribution I}\label{normal-distribution-i}
\addcontentsline{toc}{subsubsection}{6. Normal Distribution I}

Let \(Y\sim\mathcal{N}(5, 25)\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Compute the 99\% quantile of the given distribution. That is find
  \(y\) such that \(\Phi(y)=0.99\).
\end{itemize}

 \# Compute the 99\% quantile of a normal distribution with mu = 5 and
sigma\^{}2 = 25.

 \# Compute the 99\% quantile of a normal distribution with mu = 5 and
sigma\^{}2 = 25. qnorm(0.99, mean = 5, sd = sqrt(25))

 test\_function\_result(`qnorm')

\textbf{Hints:}

\begin{itemize}
\item
  You can compute quantiles of the normal distribution using the
  function qnorm().
\item
  Besides the quantile to be computed you have to specify \(\mu\) and
  \(\sigma^2\). This is done via the arguments mean and sd. However note
  that sd sets the standard deviation, \textbf{not} the variance.
\end{itemize}

\subsubsection*{7. Normal Distribution II}\label{normal-distribution-ii}
\addcontentsline{toc}{subsubsection}{7. Normal Distribution II}

Let \(Y\sim\mathcal{N}(2, 12)\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Generate \(10\) random numbers from this distribution.
\end{itemize}

 set.seed(123) \# Generate 10 random numbers from the given
distribution.

 set.seed(123) \# Generate 10 random numbers from the given
distribution. rnorm(10, mean = 2, sd = sqrt(12))

 test\_output\_contains(``set.seed(123);rnorm(10, mean = 2, sd =
sqrt(12))'')

\textbf{Hints:}

\begin{itemize}
\item
  You can use rnorm() to draw random numbers from a normal distribution.
\item
  Besides the number of draws you have to specify \(\mu\) and
  \(\sigma^2\). This can be done via the arguments mean and sd, however
  note that sd requires the standard deviation and \textbf{not} the
  variance.
\end{itemize}

\subsubsection*{8. Chi-squared Distribution
I}\label{chi-squared-distribution-i}
\addcontentsline{toc}{subsubsection}{8. Chi-squared Distribution I}

Let \(W\sim\chi^2_{10}\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Plot the corresponding probability density function (pdf) using the
  function curve(). Specify the range of x-values as \([0,25]\) via the
  argument xlim.
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Plot the pdf of a chi\^{}2 random variable with df = 10

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Plot the pdf of a chi\^{}2 random variable with df = 10
curve(dchisq(x, df = 10), xlim = c(0, 25))

\begin{verbatim}
</code>

<code data-type="sct">
\end{verbatim}

test\_function(`curve', args = c(`expr', `xlim'), eval = c(NA, T))
test\_student\_typed(`dchisq')

\textbf{Hints:}

\begin{itemize}
\item
  curve() expects the function with their respective parameter(s) (here:
  degrees of freedom df) as an argument.
\item
  The range of x-values in xlim can be passed as a vector consisting of
  both endpoints.
\end{itemize}

\subsubsection*{9. Chi-squared Distribution
II}\label{chi-squared-distribution-ii}
\addcontentsline{toc}{subsubsection}{9. Chi-squared Distribution II}

Let \(X_1,X_2\) be two independent normally distributed random variables
with \(\mu=0\) and \(\sigma^2=15\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Compute \(P(X_1^2+X_2^2>10)\).
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Compute the probability

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Compute the probability pchisq(10/15, df = 2, lower.tail = F)

\begin{verbatim}
</code>

<code data-type="sct">
\end{verbatim}

test\_output\_contains(`pchisq(10/15, df = 2, lower.tail = F)',
incorrect\_msg = ``Not correct!'')

\textbf{Hints:}

\begin{itemize}
\tightlist
\item
  Note that both random variables are not \(\mathcal{N}(0,1)\) but
  \(\mathcal{N}(0,\sigma^2)\) distributed. Hence you have to scale
  appropriately. Afterwards you can use pchisq() to compute the
  probability.
\end{itemize}

\subsubsection*{10. Student t Distribution
I}\label{student-t-distribution-i}
\addcontentsline{toc}{subsubsection}{10. Student t Distribution I}

Let \(X\sim t_{10000}\) and \(Z\sim\mathcal{N}(0,1)\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Compute the \(95\%\) quantile of both distributions. What is
  noticeable?
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Compute the 95\% quantile of a t distribution with 10000 degrees of
freedom

\# Compute the 95\% quantile of a standard normal distribution

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Compute the 95\% quantile of a t distribution with 10000 degrees of
freedom qt(0.95, df = 10000)

\# Compute the 95\% quantile of a standard normal distribution
qnorm(0.95)

\# Both values are very close to each other. This is not surprising as
for sufficient large degrees of freedom the t distribution can be
approximated by the standard normal distribution.

\begin{verbatim}
<code data-type="sct">
\end{verbatim}

test\_function\_result(`qt') test\_function\_result(`qnorm')
success\_msg(`Correct! A t-distributed RV converges to a standard normal
as the df get large.')

\textbf{Hints:}

\begin{itemize}
\item
  You can use qt() and qnorm() to compute quantiles of the given
  distributions.
\item
  For the \(t\) distribution you have to specify the degrees of freedom
  df.
\end{itemize}

\subsubsection*{11. Student t Distribution
II}\label{student-t-distribution-ii}
\addcontentsline{toc}{subsubsection}{11. Student t Distribution II}

Let \(X\sim t_1\). Once the session has initialized you will see the
plot of the corresponding probability density function (pdf).

\textbf{Instructions:}

\begin{itemize}
\item
  Generate \(1000\) random numbers from this distribution and assign
  them to the variable x.
\item
  Compute the sample mean of x. Can you explain the result?
\end{itemize}

\begin{verbatim}
<code data-type="pre-exercise-code">
\end{verbatim}

curve(dt(x, df = 1), xlim = c(-4, 4))

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

set.seed(123) \# Generate 1000 random numbers from the given
distribution. Assign them to the variable x.

\# Compute the sample mean of x.

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

set.seed(123) \# Generate 1000 random numbers from the given
distribution. Assign them to the variable x. x \textless{}- rt(1000, df
= 1)

\# Compute the sample mean of x. mean(x)

\# Although a t distribution with M = 1 is, as every other t
distribution, symmetric around zero it actually has no expectation. This
explains the highly non-zero value for the sample mean.

\begin{verbatim}
<code data-type="sct">
\end{verbatim}

test\_object(`x') test\_function\_result(`mean') success\_msg(``Right!
The expectation is not defined for a t distributed RV with one degree of
freedom.'')

\textbf{Hints:}

\begin{itemize}
\item
  You can use rt() to draw random numbers from a t distribution.
\item
  Note that the t distribution is fully parameterized through the
  degree(s) of freedom. Specify them via the argument df.
\item
  To compute the sample mean of a vector you can use the function
  mean().
\end{itemize}

\subsubsection*{12. F Distribution I}\label{f-distribution-i}
\addcontentsline{toc}{subsubsection}{12. F Distribution I}

Let \(Y\sim F(10, 4)\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Plot the quantile function of the given distribution using the
  function curve().
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Plot the quantile function of the given distribution

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Plot the quantile function of the given distribution curve(qf(x, df1
= 10, df2 = 4))

\begin{verbatim}
</code>

<code data-type="sct">
\end{verbatim}

test\_function(`curve', args = `expr', eval = NA)
test\_student\_typed(`qf')

\textbf{Hints:}

\begin{itemize}
\tightlist
\item
  curve() expects the function with their respective parameters (here:
  degrees of freedom df1 and df2) as an argument.
\end{itemize}

\subsubsection*{13. F Distribution II}\label{f-distribution-ii}
\addcontentsline{toc}{subsubsection}{13. F Distribution II}

Let \(Y\sim F(4,5)\).

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Compute \(P(1<Y<10)\) by integration of the pdf.
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Compute the probability by integration

\begin{verbatim}
</code>

<code data-type="solution">
\end{verbatim}

\# Compute the probability by integration integrate(df, lower = 1, upper
= 10, df1 = 4, df2 = 5)\$value

\begin{verbatim}
</code>

<code data-type="sct">
\end{verbatim}

test\_function(`integrate', args = c(`f', `lower', `upper', `df1',
`df2')) test\_output\_contains(`integrate(df, lower = 1, upper = 10, df1
= 4, df2 = 5)\$value')

\textbf{Hints:}

\begin{itemize}
\item
  Besides the function that has to be integrated you have to specify
  lower and upper bounds.
\item
  The additional parameters of the distribution (here df1 and df2) also
  have to be passed \textbf{inside} the call of integrate().
\item
  The value of the integral can be obtained via \$value.
\end{itemize}

\chapter{A Review of Statistics using
R}\label{a-review-of-statistics-using-r}

This section reviews important statistical concepts:

\begin{itemize}
\item
  Estimation
\item
  Hypothesis testing
\item
  Confidence intervals
\end{itemize}

Since these types of statistical methods are heavily used in
econometrics, we will discuss them in the context of inference about an
unknown population mean and discuss several applications in R.

\section{Estimation of the Population
Mean}\label{estimation-of-the-population-mean}

Key Concept 3.1

Estimators and Estimates

Estimators are functions of sample data that are drawn randomly from an
unknown population. Estimates are numerical values computed by
estimators based on the sample data. Estimators are random variables
because they are functions of \emph{random} data. Estimates are
nonrandom numbers.

Think of some economic variable, for example hourly earnings of college
graduates, denoted by \(Y\). Suppose we are interested in \(\mu_Y\) the
mean of \(Y\). In order to exactly calculate \(\mu_Y\) we would have to
interview every graduated member of the working population in the
economy. We simply cannot do this for time and cost reasons. However, we
could draw a random sample from \(n\) i.i.d. observations
\(Y_1, \dots, Y_n\) and estimate \(\mu_Y\) using one of the simplest
estimators in the sense of Key Concept 3.1 one can think of:

\[ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i, \]

the sample mean of \(Y\). Then again, we could use an even simpler
estimator for \(\mu_Y\): the very first observation in the sample,
\(Y_1\). Is \(Y_1\) a good estimator? For now, assume that

\[ Y \sim \chi_{12}^2 \]

which is not too unreasonable as the measure is nonnegative and we
expect many hourly earnings to be in a range of \(5€\,\) to \(15€\).
Moreover, it is common for income distributions to be skewed to the
right.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the chi_12^2 distribution}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{12}\NormalTok{), }
      \DataTypeTok{from =} \DecValTok{0}\NormalTok{, }
      \DataTypeTok{to =} \DecValTok{40}\NormalTok{, }
      \DataTypeTok{ylab =} \StringTok{"density"}\NormalTok{, }
      \DataTypeTok{xlab =} \StringTok{"hourly earnings in Euro"}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-42-1} \end{center}

We draw a sample of \(n=100\) observations and take the first
observation \(Y_1\) as an estimate for \(\mu_Y\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# sample from the chi_12^2 distribution, keep only the first observation}
\KeywordTok{rchisq}\NormalTok{(}\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }\DataTypeTok{df =} \DecValTok{12}\NormalTok{)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.257893
\end{verbatim}

The estimate \(8.26\) is not too far away from \(\mu_Y = 12\) but it is
somewhat intuitive that we could do better: the estimator \(Y_1\)
discards a lot of information and its variance is the population
variance:

\[ \text{Var}(Y_1) = \text{Var}(Y) = 2 \cdot 12 = 24 \]

This brings us to the following question: What is a `good' estimator in
the first place? This question is tackled in Key Concepts 3.2 and 3.3

Key Concept 3.2

Bias, Consistency and Efficiency

Disirable characteristics of an estimator are unbiasedness, consitency
and Efficiency.

Unbiasedness:\\
If the mean of the sampling distribution of some estimator \(\hat\mu_Y\)
for the population mean \(\mu_Y\) equals \(\mu_Y\)
\[ E(\hat\mu_Y) = \mu_Y \] we say that the estimator is unbiased for
\(\mu_Y\). The \emph{bias} of \(\hat\mu_Y\) is \(0\):

\[ E(\hat\mu_Y) - \mu_Y = 0\]

Consistency:

We want the uncertainty of the estimator \(\mu_Y\) to decrease as the
number of observations in the sample grows. More precisely, we want the
proabability that the estimate \(\hat\mu_Y\) falls within a small
interval of the true value \(\mu_Y\) to get increasingly closer to \(1\)
as \(n\) grows. We write this as

\[ \hat\mu_Y \xrightarrow{p} \mu_Y. \]

Variance and efficiency:

We want the estimator to be efficient. Suppose we have two estimators,
\(\hat\mu_Y\) and \(\overset{\sim}{\mu}_Y\) and for some given sample
size \(n\) it holds that

\[ E(\hat\mu_Y) = E(\overset{\sim}{\mu}_Y) = \mu_Y \] but
\[\text{Var}(\hat\mu_Y) < \text{Var}(\overset{\sim}{\mu}_Y).\]

We then would prefer to use \(\hat\mu_Y\) as it has a lower variance
than \(\overset{\sim}{\mu}_Y\), meaning that \(\hat\mu_Y\) is more
\emph{efficient} in using the information provided by the observations
in the sample.

Key Concept 3.3

Efficiency of \(\overline{Y}\): The BLUE property

Let \(\hat\mu_Y\) be a linear and unbiased estimator of \(\mu_Y\) in the
fashion of

\[ \hat\mu_Y = \frac{1}{n} \sum_{i=1}^n a_i Y_i\]

with nonrandom constants \(a_i\). We see that \(\hat\mu_Y\) is a
weighted average of the \(Y_i\) and the \(a_i\) are weights. For these
type of estimators, \(\overline{Y}\) with \(a_i = 1\) for all
\(i = 1, \dots, n\) is the most efficient estimator. We say that
\(\overline{Y}\) is the BestLinear Unbiased Estimator (BLUE).

\section{Properties of the Sample
Mean}\label{properties-of-the-sample-mean}

To examine properties of the sample mean as an estimator for the
corresponding population mean, consider the following R example.

We generate a population \texttt{pop} which consists observations
\(Y_i \ , \ i=1,\dots,10000\) that origin from a normal distribution
with mean \(\mu = 10\) and variance \(\sigma^2 = 1\). To investigate how
the estimator \(\hat{\mu} = \bar{Y}\) behaves we can draw random samples
from this population and calculate \(\bar{Y}\) for each of them. This is
easily done by making use of the function \texttt{replicate()}. Its
argument \texttt{expr} is evaluated \texttt{n} times. In this case we
draw samples of sizes \(n=5\) and \(n=25\), compute the sample means and
repeat this exactly \(n=25000\) times.

For comparison purposes we store results for the estimator \(Y_1\), the
first observation in a sample for a sample of size \(5\) separately.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate a fictive population}
\NormalTok{pop <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{# sample form pop and estimate the mean}
\NormalTok{est1 <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{expr =} \KeywordTok{mean}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pop, }\DataTypeTok{size =} \DecValTok{5}\NormalTok{)), }\DataTypeTok{n =} \DecValTok{25000}\NormalTok{)}

\NormalTok{est2 <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{expr =} \KeywordTok{mean}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pop, }\DataTypeTok{size =} \DecValTok{25}\NormalTok{)), }\DataTypeTok{n =} \DecValTok{25000}\NormalTok{)}

\NormalTok{fo <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{expr =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pop, }\DataTypeTok{size =} \DecValTok{5}\NormalTok{)[}\DecValTok{1}\NormalTok{], }\DataTypeTok{n =} \DecValTok{25000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Check that \texttt{est1} and \texttt{est2} are vectors of length
\(25000\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check if object type is vector}
\KeywordTok{is.vector}\NormalTok{(est1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.vector}\NormalTok{(est2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check lengths}
\KeywordTok{length}\NormalTok{(est1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(est2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25000
\end{verbatim}

The code chunk below produces a plot of the sampling distributions of
the estimators \(\bar{Y}\) and \(Y_1\) on the basis of the \(25000\)
samples in each case. We also plot a curve depicting the density
function of the \(N(10,1)\) distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot density estimate Y_1}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(fo), }
      \DataTypeTok{col =} \StringTok{'green'}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{),}
      \DataTypeTok{xlab =} \StringTok{'estimates'}\NormalTok{,}
      \DataTypeTok{main =} \StringTok{'Sampling Distributions of Unbiased Estimators'}
\NormalTok{      )}

\CommentTok{# add density estimate for the distribution of the sample mean with n=5 to the plot}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(est1), }
     \DataTypeTok{col =} \StringTok{'steelblue'}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{bty =} \StringTok{'l'}
\NormalTok{     )}

\CommentTok{# add density estimate for the distribution of the sample mean with n=25 to the plot}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(est2), }
      \DataTypeTok{col =} \StringTok{'red2'}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{      )}

\CommentTok{# add a vertical line marking the true parameter}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \DecValTok{10}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add N(10,1) density to the plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{mean=}\DecValTok{10}\NormalTok{), }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{lty =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{     )}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"N(10,1)"}\NormalTok{,}
                  \KeywordTok{expression}\NormalTok{(Y[}\DecValTok{1}\NormalTok{]),}
                  \KeywordTok{expression}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ }\NormalTok{n}\OperatorTok{==}\DecValTok{5}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ }\NormalTok{n}\OperatorTok{==}\DecValTok{25}\NormalTok{)}
\NormalTok{                  ), }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{,}\StringTok{'green'}\NormalTok{, }\StringTok{'steelblue'}\NormalTok{, }\StringTok{'red2'}\NormalTok{),}
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-46-1} \end{center}

At first, notice how \emph{all} sampling distributions (represented by
the solid lines) are centered around \(\mu = 10\). This is evidence for
the \emph{unbiasedness} of \(Y_1\) and \(\overline{Y}\). Of course, the
theoretical density the \(N(10,1)\) distribution is centered at \(10\),
too.

Next, have a look add the spread of the sampling distributions. Several
things are remarkable:

\begin{itemize}
\item
  First, the sampling distribution of \(Y_1\) (green curve) tracks the
  density of the \(N(10,1)\) distribution (black dashed line) pretty
  closely In fact, the sampling distribution of \(Y_1\) \emph{is} the
  \(N(10,1)\) distribution. This is less surprising if You keep in mind
  that \(Y_1\) estimator does nothing but reporting an observation that
  is randomly selected from a population with \(N(10,1)\) distribution.
  Hence, \(Y_1 \sim N(10,1)\). Note that this result is invariant to the
  sample size \(n\): the sampling distribution of \(Y_1\) is always the
  population distribution, no how large the sample is.
\item
  Second, both sampling distributions of \(\overline{Y}\) show less
  dispersion than the sampling distribution of \(Y_1\). This means that
  \(\overline{Y}\) has a lower variance than \(Y_1\). In view of Key
  Concepts 3.2 and 3.3, we find that \(\overline{Y}\) is a more
  efficient estimator than \(Y_1\). In fact, one can show that this
  holds for all \(n>1\).
\item
  Third, \(\overline{Y}\) shows a behaviour that is termed consistency
  (see Key Concept 3.2). Notice that the blue and the red density curves
  are much more concentrated around \(\mu=10\) then the green one. As
  the number of observations is increased from \(1\) to \(5\), the
  sampling distribution tightens around the true parameter. This effect
  is more dominant as the sample size is increased to \(25\). This
  implies that the probability of obtaining estimates that are close to
  the true value increases with \(n\).
\end{itemize}

\BeginKnitrBlock{rmdknit}
A more precise way to express consitency of an estimator \(\hat\mu\) for
a parameter \(\mu\) is

\[ P(|\hat{\mu} - \mu|<\epsilon) \xrightarrow[n \rightarrow \infty]{p} 1 \quad \text{for any}\quad\epsilon>0.\]

This expression says that the probability of observing a deviation from
the true value \(\mu\) that is smaller than some arbitrary
\(\epsilon > 0\) converges to \(1\) as \(n\) grows. Note that
consistency does not require unbiasedness:
\EndKnitrBlock{rmdknit}

We encourage You to go ahead and modify the code. Try out different
values for the sample size and see how the sampling distribution of
\(\overline{Y}\) changes!

\subsubsection*{\texorpdfstring{\(\overline{Y}\) is the least squares
estimator of
\(\mu_Y\)}{\textbackslash{}overline\{Y\} is the least squares estimator of \textbackslash{}mu\_Y}}\label{overliney-is-the-least-squares-estimator-of-mu_y}
\addcontentsline{toc}{subsubsection}{\(\overline{Y}\) is the least
squares estimator of \(\mu_Y\)}

Assume You have some observations \(Y_1,\dots,Y_n\) on
\(Y \sim N(10,1)\) (which is unknown) and would like to find an
estimator \(m\) that predicts the observations as good as possible where
good means to choose \(m\) such that the total deviation between the
predicted value and the observed values is small. Mathematically this
means we want to find an \(m\) that minimizes

\begin{equation}
  \sum_{i=1}^n (Y_i - m)^2. \label{eq:sqm}
\end{equation}

Think of \(Y_i - m\) as the comitted mistake when predicting \(Y_i\) by
\(m\). We could just as well minimize the sum of absolute deviations
from \(m\) but minimizing the sum of squared deviations is
mathematically more convenient and leads, roughly speaking, to the same
result. That is why the estimator we are looking for is called the
\emph{least squares estimator}. As It turns out \(m = \overline{Y}\),
the estimator of \(\mu_Y=10\) is this wanted estimator.

We can show this by generating a random sample of fair size and plotting
\eqref{eq:sqm} as a function of \(m\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the function and vectorize it}
\NormalTok{sqm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(m) \{}
 \KeywordTok{sum}\NormalTok{((y}\OperatorTok{-}\NormalTok{m)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\NormalTok{sqm <-}\StringTok{ }\KeywordTok{Vectorize}\NormalTok{(sqm)}

\CommentTok{# draw random sample and compute the mean}
\NormalTok{y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10.00543
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the objective function}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{sqm}\NormalTok{(x), }
      \DataTypeTok{from =} \OperatorTok{-}\DecValTok{50}\NormalTok{, }
      \DataTypeTok{to =} \DecValTok{70}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{"m"}\NormalTok{,}
      \DataTypeTok{ylab =}\StringTok{"sqm(m)"}
\NormalTok{      )}

\CommentTok{# add vertical line at mean(y)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{mean}\NormalTok{(y), }
       \DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }
       \DataTypeTok{col =} \StringTok{"darkred"}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-47-1} \end{center}

Notice that \eqref{eq:sqm} is a quadratic function so there is only one
minimum. The plot shows that this minimum lies exactly at the sample
mean of the sample data.

\BeginKnitrBlock{rmdknit}
Some R functions can only interact with functions that take a vector as
input evaluate the function body on every values of the vector, for
example curve(). We call such functions vectorized functions and it is
often a good idea to write vectorized functions although this is
cumbersome in some cases. Having a vectorized function in R is never a
drawback since these functions work on both single values and vectors.

Let us look at the function sqm() which is nonvectorized

 sqm \textless{}- function(m) \{\\
\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}
sum((y-m)\^{}2) \#body of the function\\
\}

Providing e.g. c(1,2,3) as the argument m would cause an error since
then the operation y-m is invalid: the vecors y and m are of
incompatible dimensions. This is why we cannot use sqm() in conjunction
with curve().

Here comes Vectorize() into play. It generates a vectorized version of a
non-vectorized function.
\EndKnitrBlock{rmdknit}

\subsubsection*{Why Random Sampling is
important}\label{why-random-sampling-is-important}
\addcontentsline{toc}{subsubsection}{Why Random Sampling is important}

So far, we assumed (somtimes implicitly) that observed data
\(Y_1, \dots, Y_n\) are the result of a sampling process that satisfies
the assumption of i.i.d. random sampling. It is very important that this
assumption is fulfilled when estimating a population mean using
\(\overline{Y}\). If this is not the case, estimates are biased.

Let us fall back to \texttt{pop}, the fictive population of \(10000\)
observations and compute the population mean \(\mu_{\texttt{pop}}\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the population mean of pop}
\KeywordTok{mean}\NormalTok{(pop)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.992604
\end{verbatim}

Next we sample \(10\) observations from \texttt{pop} with
\texttt{sample()} and estimate \(\mu_{\texttt{pop}}\) using
\(\overline{Y}\) repeatedly. However this time we use a sampling scheme
that deviates from simple random sampling: instead of ensuring that each
member of the population has the same chance to end up in a sample, we
assign a higher probability of beeing sampled to the \(2500\) smallest
observations of the population by setting the argument \texttt{prop} to
a suitable vector of probability weights:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate outcome for the sample mean when the i.i.d. assumption fails}
\NormalTok{est3 <-}\StringTok{  }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =} \DecValTok{25000}\NormalTok{, }
                    \DataTypeTok{expr =} \KeywordTok{mean}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{sort}\NormalTok{(pop), }
                                  \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }
                                  \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{2500}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{7500}\NormalTok{))}
\NormalTok{                                      )}
\NormalTok{                                )}
\NormalTok{                   )}

\CommentTok{# compute the sample mean of the outcomes}
\KeywordTok{mean}\NormalTok{(est3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.443454
\end{verbatim}

Next we plot the sampling distribution of \(\overline{Y}\) for this
non-i.i.d. case an compare it to the sampling distribution when the
i.i.d. assumption holds.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sampling distribution of sample mean, i.i.d. holds, n=25}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(est2), }
      \DataTypeTok{col =} \StringTok{'red2'}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{11}\NormalTok{),}
      \DataTypeTok{xlab =} \StringTok{'estimates'}\NormalTok{,}
      \DataTypeTok{main =} \StringTok{'When the i.i.d. Assumption Fails'}
\NormalTok{     )}

\CommentTok{# sampling distribution of sample mean, i.i.d. fails, n=25}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(est3),}
      \DataTypeTok{col =} \StringTok{'steelblue'}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{      )}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\KeywordTok{expression}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ ","} \OperatorTok{~}\StringTok{ }\NormalTok{n}\OperatorTok{==}\DecValTok{25} \OperatorTok{~}\StringTok{ ", i.i.d. fails"}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ ","} \OperatorTok{~}\StringTok{ }\NormalTok{n}\OperatorTok{==}\DecValTok{25} \OperatorTok{~}\StringTok{ ", i.i.d. holds"}\NormalTok{)}
\NormalTok{                  ), }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{'red2'}\NormalTok{, }\StringTok{'steelblue'}\NormalTok{),}
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-50-1.pdf}

We find that in this case failure of the i.i.d. assumption implies that,
on average, we underestimate \(\mu_Y\) using \(\overline{Y}\): the
corresponding distribution of \(\overline{Y}\) is shifted to the left.
In other words, \(\overline{Y}\) is a \emph{biased} estimator for
\(\mu_Y\) if the i.i.d. assumption does not hold.

\section{Hypothesis Tests Concerning the Population
Mean}\label{hypothesis-tests-concerning-the-population-mean}

In this section we briefly review concepts in hypothesis testing and
discuss how to conduct hypothesis tests in R. We focus on drawing
inference about an unkown population mean.

\subsubsection*{About Hypotheses and Hypothesis
Testing}\label{about-hypotheses-and-hypothesis-testing}
\addcontentsline{toc}{subsubsection}{About Hypotheses and Hypothesis
Testing}

In a significance test we want to exploit the information contained in a
random sample as evidence in favour or against a hypothesis.
Essentially, hypotheses are simple question that can be answered by
`yes' or `no'. When conducting a hypothesis test we always deal with two
different hypotheses:

\begin{itemize}
\item
  The null hypothesis, denoted \(H_0\) is the hypothesis we are
  interested in testing
\item
  The alternative hypothesis, denoted \(H_1\), is the hypothesis that
  holds if the null hypothesis is false
\end{itemize}

The null hypothesis that the population mean of \(Y\) equals the value
\(\mu_{Y,0}\) is written down as

\[ H_0: E(Y) = \mu_{Y,0}. \]

The alternative hypothesis states what holds if the null hypothesis is
false. Often the alternative hypothesis chosen is the most general one,

\[ H_1: E(Y) \neq \mu_{Y,0}, \]

meaning that \(E(Y)\) may be anything else but the value as the null
hypothesis. This is called a two-sided alternative.

For brevity, we will only consider the case of a two-sided alternative
in the subsequent sections of this chapter.

\subsection*{\texorpdfstring{\(p\)-Value}{p-Value}}\label{p-value}
\addcontentsline{toc}{subsection}{\(p\)-Value}

Assume that the null hypothesis is \emph{true}. The \(p\)-value is the
probability of drawing data and observing a corresponding test
statistics that is at least as adverse to what is stated under the null
hypothesis as the test statistic actually computed using the sample
data.

In context of population mean and sample mean, this definition can be
stated mathematically in the following way:

\begin{equation}
p \text{-value} = P_{H_0}\left[ \lvert \overline{Y} - \mu_{Y,0} \rvert > \lvert \overline{Y}^{act} - \mu_{Y,0} \rvert \right] \label{eq:pvalue}
\end{equation}

In \eqref{eq:pvalue}, \(\overline{Y}^{act}\) is the acutally computed mean
of the random sample. Visualized, the \(p\)-value is the area in the
part of tails of the distribution of \(\overline{Y}\) that lies beyond

\[ \mu_{Y,0} \pm \lvert \overline{Y}^{act} - \mu_{Y,0} \rvert. \]

Consequently, in order to compute the \(p\)-value as in \eqref{eq:pvalue},
knowledge about the sampling distribution of \(\overline{Y}\) when the
null hypothesis is true is required. However in most cases the sampling
distribution of \(\overline{Y}\) is unkown. Furtunately, due to the
large-sample normal approximation (see chapter 3) we know that under the
null hypothesis

\[ \overline{Y} \sim N(\mu_{Y,0}, \, \sigma^2_{\overline{Y}}) \ \ , \ \ \sigma^2_{\overline{Y}} = \frac{\sigma_Y^2}{n} \]

and thus

\[ \frac{\overline{Y} - \mu_{Y,0}}{\sigma_Y/\sqrt{n}} \sim N(0,1). \]

So in large samples, the \(p\)-value can be computed \emph{without}
knowledge about the sampling distribution of \(\overline{Y}\).

\subsection*{\texorpdfstring{Calculating the \(p\)-Value When
\(\sigma_Y\) Is
Known}{Calculating the p-Value When \textbackslash{}sigma\_Y Is Known}}\label{calculating-the-p-value-when-sigma_y-is-known}
\addcontentsline{toc}{subsection}{Calculating the \(p\)-Value When
\(\sigma_Y\) Is Known}

For now, let us assume that \(\sigma_{\overline{Y}}\) is known. Then we
can rewrite \eqref{eq:pvalue} as

\begin{align}
p \text{-value} =& \, P_{H_0}\left[ \left\lvert \frac{\overline{Y} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert > \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert \right] \\
=& \, 2 \cdot \Phi \left[ - \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}}  \right\rvert\right].  \label{eq:pvaluenorm1}
\end{align}

so the \(p\)-value can be seen as the area in the tails of the
\(N(0,1)\) distribution that lies beyond

\begin{equation}
\pm \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert \label{eq:pvaluenorm2}
\end{equation}

Whew, that was a lot of theory. Now we use R to visualize what is stated
in \eqref{eq:pvaluenorm1} and \eqref{eq:pvaluenorm2}. The next code chunck
replicates figure 3.1 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal density on the domain [-4,4]}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x),}
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{),}
      \DataTypeTok{main =} \StringTok{'Calculating a p-value'}\NormalTok{,}
      \DataTypeTok{yaxs =} \StringTok{'i'}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{'z'}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{''}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{axes =} \StringTok{'F'}
\NormalTok{      )}

\CommentTok{# add x-axis}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }
     \DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.5}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{1.5}\NormalTok{), }
     \DataTypeTok{padj =} \FloatTok{0.75}\NormalTok{,}
     \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\KeywordTok{expression}\NormalTok{(}\OperatorTok{-}\KeywordTok{frac}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y)}\OperatorTok{^}\StringTok{"act"}\OperatorTok{~-}\ErrorTok{~}\KeywordTok{bar}\NormalTok{(mu)[Y,}\DecValTok{0}\NormalTok{],sigma[}\KeywordTok{bar}\NormalTok{(Y)])),}
                \DecValTok{0}\NormalTok{,}
                \KeywordTok{expression}\NormalTok{(}\KeywordTok{frac}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y)}\OperatorTok{^}\StringTok{"act"}\OperatorTok{~-}\ErrorTok{~}\KeywordTok{bar}\NormalTok{(mu)[Y,}\DecValTok{0}\NormalTok{],sigma[}\KeywordTok{bar}\NormalTok{(Y)])))}
\NormalTok{     )}

\CommentTok{# shade p-value/2 region in left tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{,}\OperatorTok{-}\FloatTok{1.5}\NormalTok{,}\FloatTok{0.01}\NormalTok{),}\OperatorTok{-}\FloatTok{1.5}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{,}\OperatorTok{-}\FloatTok{1.5}\NormalTok{,}\FloatTok{0.01}\NormalTok{)),}\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'steelblue'}
\NormalTok{        )}

\NormalTok{## shade p-value/2 region in right tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{6}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'steelblue'}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-51-1.pdf}

\subsection*{Sample Variance, Sample Standard Deviation and Standard
Error}\label{sample-variance-sample-standard-deviation-and-standard-error}
\addcontentsline{toc}{subsection}{Sample Variance, Sample Standard
Deviation and Standard Error}

If \(\sigma^2_Y\) is unknown, it must be estimated. This can be done
efficiently using the sample variance

\begin{equation}
s_y^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2.
\end{equation}

Furthermore

\begin{equation}
s_y = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2}.
\end{equation}

is a suitable estimator for the standard deviation of \(Y\). In R,
\(s_y\) is implemented in the function \texttt{sd()}, see \texttt{?sd}.

Using R we can get a notion that \(s_y\) is a consistent estimator for
\(\sigma_Y\), that is

\[ s_Y \overset{p}{\longrightarrow} \sigma_Y. \]

The idea here is to generate a large number of samples \(Y_1,\dots,Y_n\)
where \(Y\sim N(10,10)\), estimate \(\sigma_Y\) using \(s_y\) and
investigate how the distribution of \(s_Y\) changes as \(n\) grows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# vector of sample sizes}
\NormalTok{n <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{5000}\NormalTok{, }\DecValTok{2000}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{500}\NormalTok{)}

\CommentTok{# sample observations, estimate using sd() and plot estimated distributions}
\NormalTok{s2_y <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10000}\NormalTok{, }\DataTypeTok{expr =} \KeywordTok{sd}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n[}\DecValTok{1}\NormalTok{], }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(s2_y),}
     \DataTypeTok{main =} \KeywordTok{expression}\NormalTok{(}\StringTok{'Sampling Distributions of'} \OperatorTok{~}\StringTok{ }\NormalTok{s[y]),}
     \DataTypeTok{xlab =} \KeywordTok{expression}\NormalTok{(s[y]),}
     \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{     )}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\KeywordTok{length}\NormalTok{(n)) \{}
\NormalTok{  s2_y <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10000}\NormalTok{, }\DataTypeTok{expr =} \KeywordTok{sd}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n[i],}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)))}
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(s2_y), }
        \DataTypeTok{col=}\NormalTok{i, }
        \DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\KeywordTok{expression}\NormalTok{(n}\OperatorTok{==}\DecValTok{10000}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(n}\OperatorTok{==}\DecValTok{5000}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(n}\OperatorTok{==}\DecValTok{2000}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(n}\OperatorTok{==}\DecValTok{1000}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(n}\OperatorTok{==}\DecValTok{500}\NormalTok{)}
\NormalTok{       ), }
       \DataTypeTok{col =} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-52-1} \end{center}

The plot shows that the distribution of \(s_Y\) tightens around the true
value \(\sigma_Y = 10\) as \(n\) increases.

The function that estimates the standard deviation of an estimator is
called the \emph{standard error of the estimator}. Key Concept 3.4
summarizes the terminology in the context of the sample mean.

Key Concept 3.4

The Standard Error of \(\overline{Y}\)

Take an i.i.d. sample \(Y_1, \dots, Y_n\). The mean of \(Y\) can be
consistently estimated using \(\overline{Y}\), the sample mean of the
\(Y_i\). Since \(\overline{Y}\) is a random variable, it has a sampling
distribution with variance \(\frac{\sigma_Y^2}{n}\).

The standard error of \(\overline{Y}\), denoted \(SE(\overline{Y})\) is
an estimator of the standard deviation \(\overline{Y}\):

\[ SE(\overline{Y}) = \hat\sigma_{\overline{Y}} = \frac{s_Y}{\sqrt{n}} \]

The caret (\^{}) over \(\sigma\) indicates that
\(\hat\sigma_{\overline{Y}}\) is an estimator for
\(\sigma_{\overline{Y}}\).

As an example to underpin Key Concept 3.4, consider a sample of
\(n=100\) i.i.d. observations of the bernoulli distributed variable
\(Y\) with success probability \(p=0.1\) and thus \(E(Y)=p=0.1\) and
\(\text{Var}(Y)=p(1-p)\). \(E(Y)\) can be estimated by \(\overline{Y}\)
which then has variance

\[ \sigma^2_{\overline{Y}} = p(1-p)/n = 0.0009 \]

and standard deviation

\[ \sigma_{\overline{Y}} = \sqrt{p(1-p)/n} = 0.03. \]

In this case the standard error of \(\overline{Y}\) is given as

\[ SE(\overline{Y}) = \sqrt{\overline{Y}(1-\overline{Y})/n} \]

Let verify whether \(\overline{Y}\) and \(SE(\overline{Y})\) estimate
the respective true values on average.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# draw 10000 samples of size 100 and estimate the mean of Y and}
\CommentTok{# estimate the standard error of the sample mean}

\NormalTok{mean_estimates <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}
\NormalTok{se_estimates <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
\NormalTok{  s <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }
              \DataTypeTok{size =} \DecValTok{100}\NormalTok{,  }
              \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
              \DataTypeTok{replace =}\NormalTok{ T}
\NormalTok{              )}
\NormalTok{  mean_estimates[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(s)}
\NormalTok{  se_estimates[i] <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(s)}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(s))}\OperatorTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{\}}

\KeywordTok{mean}\NormalTok{(mean_estimates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.099693
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(se_estimates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.02953467
\end{verbatim}

Both estimators seem to be unbiased for the true parameters.

\subsection*{\texorpdfstring{Calculating the \(p\)-value When
\(\sigma_Y\) is
Unknown}{Calculating the p-value When \textbackslash{}sigma\_Y is Unknown}}\label{calculating-the-p-value-when-sigma_y-is-unknown}
\addcontentsline{toc}{subsection}{Calculating the \(p\)-value When
\(\sigma_Y\) is Unknown}

When \(\sigma_Y\) is unkown, the \(p\)-value for a hypothesis test about
\(\mu_Y\) using \(\overline{Y}\) can be computed by replacing
\(\sigma_{\overline{Y}}\) in \eqref{eq:pvaluenorm1} by the standard error
\(SE(\overline{Y}) = \hat\sigma_Y\). Then,

\[ p\text{-value} = 2\cdot\Phi\left(-\left\lvert \frac{\overline{Y}^{act}-\mu_{Y,0}}{SE(\overline{Y})} \right\rvert \right). \]

This is easily done in R:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sample and estimate, compute standard error and make a hypothesis}
\NormalTok{samplemean_act <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}
  \KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }
         \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.1}\NormalTok{), }
         \DataTypeTok{replace =}\NormalTok{ T, }
         \DataTypeTok{size =} \DecValTok{100}
\NormalTok{         )}
\NormalTok{  )}

\NormalTok{SE_samplemean <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(samplemean_act }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{-}\NormalTok{samplemean_act)}\OperatorTok{/}\DecValTok{100}\NormalTok{)}

\NormalTok{mean_h0 <-}\StringTok{ }\FloatTok{0.1} \CommentTok{#true null hypothesis}

\CommentTok{# compute the pvalue}
\NormalTok{pvalue <-}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\KeywordTok{abs}\NormalTok{(samplemean_act}\OperatorTok{-}\NormalTok{mean_h0)}\OperatorTok{/}\NormalTok{SE_samplemean)}
\NormalTok{pvalue}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5382527
\end{verbatim}

\subsection*{\texorpdfstring{The
\(t\)-statistic}{The t-statistic}}\label{the-t-statistic}
\addcontentsline{toc}{subsection}{The \(t\)-statistic}

In hypothesis testing, the standardized sample average

\begin{equation}
t = \frac{\overline{Y} - \mu_{Y,0}}{SE(\overline{Y})} \label{eq:tstat}
\end{equation}

is called \(t\)-statistic. This \(t\)-statistic has an important role
when testing hypothesis about \(\mu_Y\). It is a prominent example of a
test statistic.

Implicitly, we already have computed a \(t\)-statistic for
\(\overline{Y}\) in the previous code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute a t-statistic for the sample mean}
\NormalTok{tstatistic <-}\StringTok{ }\NormalTok{(samplemean_act }\OperatorTok{-}\StringTok{ }\NormalTok{mean_h0) }\OperatorTok{/}\StringTok{ }\NormalTok{SE_samplemean}
\NormalTok{tstatistic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6154575
\end{verbatim}

Using R we can show that if \(\mu_{Y,0}\) equals the true value, that is
the null hypothesis is true, \eqref{eq:tstat} is approximately distributed
\(N(0,1)\) when \(n\) is large.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# initialize empty vector for t-statistics}
\NormalTok{tstatistics <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}

\CommentTok{# set sample size}
\NormalTok{n <-}\StringTok{ }\DecValTok{300}

\CommentTok{# simulate 10000 t-statistics}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
\NormalTok{  s <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }
              \DataTypeTok{size =}\NormalTok{ n,  }
              \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
              \DataTypeTok{replace =}\NormalTok{ T}
\NormalTok{              )}
\NormalTok{  tstatistics[i] <-}\StringTok{ }\NormalTok{(}\KeywordTok{mean}\NormalTok{(s)}\OperatorTok{-}\FloatTok{0.1}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(s)}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(s))}\OperatorTok{/}\NormalTok{n))}
\NormalTok{\}}

\CommentTok{# plot density and compare to N(0,1) density}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(tstatistics),}
     \DataTypeTok{xlab =} \StringTok{'t-statistic'}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{'Distribution of the t-statistic when n=300'}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{),}
     \DataTypeTok{col =} \StringTok{'steelblue'}
\NormalTok{     )}

\CommentTok{# N(0,1) density (dashed)}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x), }
      \DataTypeTok{add =}\NormalTok{ T, }
      \DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lwd=} \DecValTok{2}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-56-1.pdf}

Judging from the plot, the normal approximation works reasonably well
for the chosen sample size. This normal approximation has already been
used in the definition of the \(p\)-value, see \eqref{eq:tstat}.

\subsection*{Hypothesis Testing with a Prespecified Significance
Level}\label{hypothesis-testing-with-a-prespecified-significance-level}
\addcontentsline{toc}{subsection}{Hypothesis Testing with a Prespecified
Significance Level}

Key Concept 3.5

The Terminology of Hypothesis Testing

In hypothesis testing, two types of mistakes are possible:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The null hypothesis \emph{is} rejected although it is true
  (\(\alpha\)-error / type-I-error)
\item
  The null hypothesis \emph{is not} rejected although it is false
  (\(\beta\)-error / type-II-error)
\end{enumerate}

The significance level of the test is the probability to commit a
type-I-error we are willing to accept in advance. E.g. using a
prespecified significance level of \(0.05\), we reject the null
hypothesis if and only if the \(p\)-value is less than \(0.05\). The
significance level is chosen before the test is conducted.

An equivalent procedure is to reject the null hypothesis if the test
statistic observed is, in absolute value terms, larger than the critical
value of the test statistic. The critical value is determined by the
significance level chosen and defines two disjoint sets of values which
are called acceptance region and rejection region. The acceptance region
contains all values of the test statistic for which the test does not
reject while the rejection region contains all the values for which the
test does reject.

The \(p\)-value is the probability that, in repeated sampling under the
same conditions, meaning i.i.d. sampling, the same null hypothesis and
the same sample size, a test statistic is observed that provides just as
much evidence against the null hypothesis as the test statistic actually
observed.

The actual probability that the test rejects the true null hypothesis is
called the size of the test. In an ideal setting, the size does not
exceed the significance level.

The probability that the test correctly rejects a false null hypothesis
is called power.

Reconsider \texttt{pvalue} computed further above:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check whether p-value < 0.05}
\NormalTok{pvalue }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

The condition is not fulfilled so we do not reject the null hypotheis
(remember that the null hypothesis is true in this example).

When working with a \(t\)-statistic instead, it is equivalent to apply
the following rule:

\[ \text{Reject } H_0 \text{ if } \lvert t^{act} \rvert > 1.96 \]

We reject the null hypothesis at the significance level of \(5\%\) if
the computed \(t\)-statistic lies beyond the critical value of 1.96 in
absolute value terms. \(1.96\) is the \(0.05\)-quantile of the standard
normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check the critical value}
\KeywordTok{qnorm}\NormalTok{(}\DataTypeTok{p =} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -1.644854
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check whether the null is rejected using the t-statistic computed further above}
\KeywordTok{abs}\NormalTok{(tstatistic) }\OperatorTok{>}\StringTok{ }\FloatTok{1.96}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

As when using the \(p\)-value, we cannot reject the null hypothesis
using the corresponding \(t\)-statistic. Key Concept 3.6 summarizes the
procedure of performing a two-sided hypothesis about the population mean
\(E(Y)\).

Key Concept 3.6

Testing the Hypothesis \(E(Y) = \mu_{Y,0}\) Against the Alternative
\(E(Y) \neq \mu_{Y,0}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate \(\mu_{Y}\) using \(\overline{Y}\) and compute the standard
  error of \(\overline{Y}\), \(SE(\overline{Y})\).
\item
  Compute the \(t\)-statistic.
\item
  Compute the \(p\)-value and reject the null hypothesis at the \(5\%\)
  level of significance if the \(p\)-value is smaller than \(0.05\) or
  equivalently, if
\end{enumerate}

\[ \left\lvert t^{act} \right\rvert > 1.96. \]

\subsection*{One-sided Alternatives}\label{one-sided-alternatives}
\addcontentsline{toc}{subsection}{One-sided Alternatives}

Sometimes we are interested in finding evidence that the mean is bigger
or smaller than the some value hypothesized under the null. One can come
up with many examples here but, to stick to the book, take the presumed
wage differential between good and less educated working individuals.
Since we hope that this differential exists, a relevant alternative (to
the null hypothesis that there is no wage differential) is that good
educated individuals earn more, i.e.~that the average hourly wage for
this group, \(\mu_Y\) is \emph{bigger} than \(\mu_{Y,0}\) the know
average wage of less educated workers.

This is an example of a \emph{right-sided test} and the hypotheses pair
is chosen as

\[ H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs} \ \ H_1: \mu_Y > \mu_{Y,0}. \]

We reject the null hypothesis if the computed test-statistic is larger
than the critical value \(1.64\), the \(0.95\)-quantile of the
\(N(0,1)\) distribution. This ensures that \(1-0.95=5\%\) probability
mass remains in the area to the right of the critical value. Similar as
before we can visualize this in R using the function \texttt{polygon()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal density on the domain [-4,4]}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x),}
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{),}
      \DataTypeTok{main =} \StringTok{'Rejection Region of a Right-Sided Test'}\NormalTok{,}
      \DataTypeTok{yaxs =} \StringTok{'i'}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{'t-statistic'}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{''}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{axes =} \StringTok{'F'}
\NormalTok{)}

\CommentTok{# add x-axis}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }
     \DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{1.64}\NormalTok{,}\DecValTok{4}\NormalTok{), }
     \DataTypeTok{padj =} \FloatTok{0.5}\NormalTok{,}
     \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{''}\NormalTok{,}\DecValTok{0}\NormalTok{,}\KeywordTok{expression}\NormalTok{(Phi}\OperatorTok{^-}\DecValTok{1}\OperatorTok{~}\NormalTok{(.}\DecValTok{95}\NormalTok{)}\OperatorTok{==}\FloatTok{1.64}\NormalTok{),}\StringTok{''}\NormalTok{)}
\NormalTok{)}

\CommentTok{# shade rejection region in right tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.64}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\FloatTok{1.64}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{4}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\FloatTok{1.64}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'darkred'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-59-1.pdf}

In an analogously manner for the \emph{left-sided test} we have

\[ H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs.} \ \ H_1: \mu_Y < \mu_{Y,0}. \]

The null is rejected if the observed test statistic falls short of the
critical value which, for a test at the \(0.05\) level of significance,
is given by \(-1.64\), the \(0.05\)-quantile of the \(N(0,1)\)
distribution. \(5\%\) probability mass lies to the left of the critical
value.

It is straight forward to adapt the code chunk above to the case of a
left-sided test. We only have to fiddle with the color shading and the
tick marks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal density on the domain [-4,4]}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x),}
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{),}
      \DataTypeTok{main =} \StringTok{'Rejection Region of a Left-Sided Test'}\NormalTok{,}
      \DataTypeTok{yaxs =} \StringTok{'i'}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{'t-statistic'}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{''}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{axes =} \StringTok{'F'}
\NormalTok{)}

\CommentTok{# add x-axis}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }
     \DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{0}\NormalTok{,}\OperatorTok{-}\FloatTok{1.64}\NormalTok{,}\DecValTok{4}\NormalTok{), }
     \DataTypeTok{padj =} \FloatTok{0.5}\NormalTok{,}
     \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{''}\NormalTok{,}\DecValTok{0}\NormalTok{,}\KeywordTok{expression}\NormalTok{(Phi}\OperatorTok{^-}\DecValTok{1}\OperatorTok{~}\NormalTok{(.}\DecValTok{05}\NormalTok{)}\OperatorTok{==-}\FloatTok{1.64}\NormalTok{),}\StringTok{''}\NormalTok{)}
\NormalTok{)}

\CommentTok{# shade rejection region in right tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\OperatorTok{-}\FloatTok{1.64}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\OperatorTok{-}\FloatTok{1.64}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\OperatorTok{-}\FloatTok{1.64}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'darkred'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-60-1.pdf}

\section{Confidence intervals for the Population
Mean}\label{confidence-intervals-for-the-population-mean}

As stressed before, we will never estimate the exact value of a the
population mean of \(Y\) using a random sample. However, we can compute
confidence intervals for the population mean. In general, a confidence
interval for a unkown parameter is a set of values that contains the
true parameter with a prespecified probability, the confidence level.
Confidence intervals are computed using the information available in the
sample. Since this information is the result of a random process,
confidence intervals are random variables themselves.

Key Concept 3.7 shows how to compute confidence intervals for the
unknown population mean \(E(Y)\).

Key Concept 3.6

Confidence Intervals for the Population Mean

A \(95\%\) confidence interval for \(\mu_Y\) is a random variable that
contains the true \(\mu_Y\) in \(95\%\) of all possible random samples.
When \(n\) is large we can use the normal approximation. Then, \(99\%\),
\(95\%\), \(90\%\) confidence intervals are

\begin{align}
&99\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 2.58 \times SE(\overline{Y}) \right\}. \\
&95\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 1.96 \times SE(\overline{Y}) \right\}. \\
&90\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 1.64 \times SE(\overline{Y}) \right\}.
\end{align}

These confidence intervals are sets of null hypotheses we cannot reject
in a two-sided hypothesis test at the given level of confidence.

Now consider the following statements.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The interval
  \[ \left\{ \overline{Y} \pm 1.96 \times SE(\overline{Y}) \right\} \]
  covers the true value of \(\mu_Y\) with a probability of \(95\%\).
\item
  We have computed \(\overline{Y} = 5.1\) and \(SE(\overline{Y}=2.5\) so
  the interval
  \[ \left\{ 5.1  \pm 1.96 \times 2.5 \right\} = \left[0.2,10\right] \]
  covers the true value of \(\mu_Y\) with a probability of \(95\%\).
\end{enumerate}

While 1. is right (this is exactly in line with the definition above),
2. is completely wrong and none of Your lecturers wants to read such a
sentence in a term paper, written exam or similar, believe us. The
difference is that, while 1. is the definition of a random variable, 2.
is one possible \emph{outcome} of this random variable so there is no
meaning in making any probabilistic statement about it. Either the
computed interval \emph{does cover} \(\mu_Y\) or it \emph{does not}!

In R, testing hypothesis about the mean of a population on the basis of
a random sample is very easy due to functions like \texttt{t.test()}
from the stats package. It procudes an object of type list. Luckily, one
of the most simple ways to use \texttt{t.test()} is when You want to
obtain a \(95\%\) confidence interval for some population mean. We start
by generating some random data and calling \texttt{t.test()} in
conjunction with \texttt{ls()} to obtain a breakdown of the output
components.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# generate some sample data}
\NormalTok{sampledata <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}

\CommentTok{# checke type}
\KeywordTok{typeof}\NormalTok{(}\KeywordTok{t.test}\NormalTok{(sampledata))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# display list elements produced by t.test}
\KeywordTok{ls}\NormalTok{(}
  \KeywordTok{t.test}\NormalTok{(sampledata)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "alternative" "conf.int"    "data.name"   "estimate"    "method"     
## [6] "null.value"  "p.value"     "parameter"   "statistic"
\end{verbatim}

Though we find that many items are reported, at the moment we are
interested in computing a \(95\%\) confidence set for the mean.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(sampledata)}\OperatorTok{$}\StringTok{"conf.int"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  9.306651 12.871096
## attr(,"conf.level")
## [1] 0.95
\end{verbatim}

This tells us that the \(95\%\) confidence interval is

\[ \left[9.31, 12.87\right]. \]

In this example, the computed interval does cover the true \(\mu_Y\)
which we know to be \(10\).

Let us have a look at the whole standard output produced by
\texttt{t.test()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(sampledata)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  sampledata
## t = 12.346, df = 99, p-value < 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##   9.306651 12.871096
## sample estimates:
## mean of x 
##  11.08887
\end{verbatim}

We see that \texttt{t.test()} not only computes a \(95\%\) confidence
interval but automatically conducts a two-sided significance test of the
hypothesis \(H_0: \mu_Y = 0\) at the level of \(5\%\) and reports
relevant parameters thereof: the alternative hypothesis, the estimated
mean, the resulting \(t\)-statistic, the degrees of freedom of the
underlying \(t\) distribution (\texttt{t.test()} does not perform the
normal approximation) and the corresponding \(p\)-value. Very
convenient!

In this example, we come to the conclusion that the population mean
\emph{is not} significantly different from \(0\) at the level of \(5\%\)
(which is correct), since \(\mu_Y = 0\) is element of the \(95\%\)
confidence interval

\[ 0 \in \left[-0.27,0.12\right]. \] We come to an equivalent result
when using the \(p\)-value rejection rule:

\[ p = 0.456 > 0.05 \]

\section{Comparing Means from Different
Populations}\label{comparing-means-from-different-populations}

Suppose You are interested in the means of two different populations,
denote them \(\mu_1\) and \(\mu_2\). More specifically You are
interested whether these population means are different from each other
and plan an using a hypothesis test to verifiy this on the basis of
independent sample data from both populations. A suitable pair of
hypotheses then is

\begin{equation}
H_0: \mu_1 - \mu_2 = d_0 \ \ \text{vs.} \ \ H_1: \mu_1 - \mu_2 \neq d_0 \label{eq:hypmeans}
\end{equation}

where \(d_0\) denotes the hypothesized difference in means. The book
teaches us that \(H_0\) can be tested with the \(t\)-statistic

\begin{equation}
t=\frac{(\overline{Y}_1 - \overline{Y}_2) - d_0}{SE(\overline{Y}_1 - \overline{Y}_2)} \label{eq:tstatmeans}
\end{equation}

where

\begin{equation}
SE(\overline{Y}_1 - \overline{Y}_2) = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}.
\end{equation}

This is called a two sample \(t\)-test. For large \(n_1\) and \(n_2\),
\eqref{eq:tstatmeans} is standard normal distributed under the null
hypothesis. Anlog to the simple \(t\)-test we can compute confidence
intervals for the true difference in population means:

\[ (\overline{Y}_1 - \overline{Y}_2) \pm 1.96 \times SE(\overline{Y}_1 - \overline{Y}_2) \]

is a \(95\%\) confidence interval for \(d\). In R, Hypotheses as in
\eqref{eq:hypmeans} can be tested with \texttt{t.test()}, too. Note that
\texttt{t.test()} chooses \(d_0 = 0\) by default. This can be changed by
setting the argument \texttt{mu} accordingly.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# draw data from two different populations with equal mean}
\NormalTok{sample_pop1 <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{sample_pop2 <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{)}

\CommentTok{# perform a two sample t-test}
\KeywordTok{t.test}\NormalTok{(sample_pop1, sample_pop2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  sample_pop1 and sample_pop2
## t = 0.872, df = 140.52, p-value = 0.3847
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.338012  6.028083
## sample estimates:
## mean of x mean of y 
## 11.088874  9.243838
\end{verbatim}

We find that the two sample \(t\)-test does not reject the (true) null
hypothesis that \(d_0 = 0\).

\section{An Application to the Gender Gap of
Earnings}\label{an-application-to-the-gender-gap-of-earnings}

In this section discusses how to reproduce the results presented in the
box `\emph{The Gender Gap of Earnings of College Graduates in the United
States}' in the book.

In order to reproduce table 3.1 You need to download the replication
data which is hosted by Pearson and can be found and downloaded
\href{http://wps.aw.com/aw_stock_ie_3/178/45691/11696965.cw/index.html}{here}.
Download the data for chapter three as an excel spreadsheet
(cps\_ch3.xlsx). This data set contains data that ranges from \(1992\)
to \(2008\) and earnings are reported in prices of \(2008\). There are
several ways to import the .xlsx-files into R. Our suggestion is the
function \texttt{read\_excel()} from the readxl package. The package is
not part of R's standard distribution and has to be installed manually.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install and load the readxl package}
\NormalTok{## install.packages('readxl')}
\KeywordTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

You are now ready to import the data set. Make sure You use the correct
path to the downloaded file! In our example, the file is saved in a
subfolder (data) of the working directory. If You are not sure what Your
current working directory is, use \texttt{getwd()}, see also
\texttt{?getwd()}. This will give You the path that points to the place
R is currently looking for files.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# import the data into R}
\NormalTok{cps <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(}\DataTypeTok{path =} \StringTok{'data/cps_ch3.xlsx'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, install and load the package dyplr. This package provides some
handy functions that simplify data wrangling a lot. It makes use of the
\texttt{\%\textgreater{}\%} operator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install and load the dplyr package}
\NormalTok{## install.packages('dplyr')}
\KeywordTok{library}\NormalTok{(}\StringTok{'dplyr'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

First, get an overview over the data set. Next, use
\texttt{\%\textgreater{}\%} and some functions from the \texttt{dplyr}
package to group the observations by gender and year and compute
descriptive statistics for both groups.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Get an overview of the data structure}
\KeywordTok{head}\NormalTok{(cps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   a_sex  year ahe08
##   <dbl> <dbl> <dbl>
## 1    1. 1992.  17.2
## 2    1. 1992.  15.3
## 3    1. 1992.  22.9
## 4    2. 1992.  13.3
## 5    1. 1992.  22.1
## 6    2. 1992.  12.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# group data by gender and year and compute the mean, standard deviation}
\CommentTok{# and number of observations for each group}
\NormalTok{avgs <-}\StringTok{ }\NormalTok{cps }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{group_by}\NormalTok{(a_sex, year) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{summarise}\NormalTok{(}\KeywordTok{mean}\NormalTok{(ahe08), }
                  \KeywordTok{sd}\NormalTok{(ahe08), }
                  \KeywordTok{n}\NormalTok{()}
\NormalTok{                  )}

\CommentTok{# print results to the console}
\KeywordTok{print}\NormalTok{(avgs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 5
## # Groups:   a_sex [?]
##    a_sex  year `mean(ahe08)` `sd(ahe08)` `n()`
##    <dbl> <dbl>         <dbl>       <dbl> <int>
##  1    1. 1992.          23.3       10.2   1594
##  2    1. 1996.          22.5       10.1   1379
##  3    1. 2000.          24.9       11.6   1303
##  4    1. 2004.          25.1       12.0   1894
##  5    1. 2008.          25.0       11.8   1838
##  6    2. 1992.          20.0        7.87  1368
##  7    2. 1996.          19.0        7.95  1230
##  8    2. 2000.          20.7        9.36  1181
##  9    2. 2004.          21.0        9.36  1735
## 10    2. 2008.          20.9        9.66  1871
\end{verbatim}

With the pipe operator \texttt{\%\textgreater{}\%} we simply chain
different R functions that produce compatible input and ouput. In the
code above, we take the dataset \texttt{cps} and use it as an input for
the function \texttt{group\_by()}. The output of \texttt{group\_by} is
subsequently used as an input for \texttt{summarise()} and so forth.

Now that we have computed the statistics of interest for both genders,
we can investigate how the gap in earnings between both groups evolves
over time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split the data set by gender}
\NormalTok{male <-}\StringTok{ }\NormalTok{avgs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(a_sex }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }
\NormalTok{female <-}\StringTok{ }\NormalTok{avgs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(a_sex }\OperatorTok{==}\StringTok{ }\DecValTok{2}\NormalTok{)}

\CommentTok{# Rename columns of both splits}
\KeywordTok{colnames}\NormalTok{(male)   <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Y_bar_m"}\NormalTok{, }\StringTok{"s_m"}\NormalTok{, }\StringTok{"n_m"}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(female) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Y_bar_f"}\NormalTok{, }\StringTok{"s_f"}\NormalTok{, }\StringTok{"n_f"}\NormalTok{)}

\CommentTok{# Estimate Gender gaps, compute standard errors and confidence intervals for all dates}
\NormalTok{gap <-}\StringTok{ }\NormalTok{male}\OperatorTok{$}\NormalTok{Y_bar_m }\OperatorTok{-}\StringTok{ }\NormalTok{female}\OperatorTok{$}\NormalTok{Y_bar_f}

\NormalTok{gap_se <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(male}\OperatorTok{$}\NormalTok{s_m}\OperatorTok{^}\DecValTok{2} \OperatorTok{/}\StringTok{ }\NormalTok{male}\OperatorTok{$}\NormalTok{n_m }\OperatorTok{+}\StringTok{ }\NormalTok{female}\OperatorTok{$}\NormalTok{s_f}\OperatorTok{^}\DecValTok{2} \OperatorTok{/}\StringTok{ }\NormalTok{female}\OperatorTok{$}\NormalTok{n_f)}

\NormalTok{gap_ci_l <-}\StringTok{ }\NormalTok{gap }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\NormalTok{gap_se}

\NormalTok{gap_ci_u <-}\StringTok{ }\NormalTok{gap }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\NormalTok{gap_se}

\NormalTok{result <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(male[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], female[,}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{)], gap, gap_se, gap_ci_l, gap_ci_u)}

\CommentTok{# print results to the console}
\KeywordTok{print}\NormalTok{(result, }\DataTypeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Year Y_bar_m  s_m  n_m Y_bar_f  s_f  n_f  gap gap_se gap_ci_l gap_ci_u
## 1 1992    23.3 10.2 1594    20.0 7.87 1368 3.23  0.332     2.58     3.88
## 2 1996    22.5 10.1 1379    19.0 7.95 1230 3.49  0.354     2.80     4.19
## 3 2000    24.9 11.6 1303    20.7 9.36 1181 4.14  0.421     3.32     4.97
## 4 2004    25.1 12.0 1894    21.0 9.36 1735 4.10  0.356     3.40     4.80
## 5 2008    25.0 11.8 1838    20.9 9.66 1871 4.10  0.354     3.41     4.80
\end{verbatim}

We observe virtually the same results as the ones presented in the book.
the computed statistics suggest that there \emph{is} a gender gap in
earnings. Note that we can reject the null hypothesis that the gap is
zero for all periodes. Further, estimates of the gap and bounds of the
95\% confidence intervals indicate that the gap has been quite stable
over the recent past.

\section{Scatterplots, Sample Covariance and Sample
Correlation}\label{scatterplots-sample-covariance-and-sample-correlation}

A scatterplot represents two dimensional data, for example \(n\)
observation on \(X_i\) and \(Y_i\), by points in a cartesian coordinate
system. It is very easy to generate scatterplots using the
\texttt{plot()} function in R. Let's generate some fictional data on age
and earnings of workers and plot it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# generate data set}
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }
           \DataTypeTok{min =} \DecValTok{18}\NormalTok{, }
           \DataTypeTok{max =} \DecValTok{70}
\NormalTok{           )}
\NormalTok{Y <-}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n=}\DecValTok{100}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{15}\NormalTok{)}

\CommentTok{# plot observations}
\KeywordTok{plot}\NormalTok{(X, }
\NormalTok{     Y, }
     \DataTypeTok{type =} \StringTok{"p"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"A Scatterplot of X and Y"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Age"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Earnings"}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{19}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-70-1} \end{center}

The plot shows positive correlation between age and earnings. This in
line with the assumption that older workers earn more than those that
the joined the working population recently.

\subsubsection*{Sample Covariance and
Correlation}\label{sample-covariance-and-correlation}
\addcontentsline{toc}{subsubsection}{Sample Covariance and Correlation}

By now You should be familiar with the concepts of variance and
covariance. If not, we recommend You to work Your way through chapter 2
of the book (again).

As for the variance, covariance and correlation of two variables are
properties that relate to the (unknown) joint probability distribution
of these variable. Just as the individual population variances of both
variables, we can estimate covariance and correlation by means of
suitable estimators using a random sample \((X_i,Y_i)\),
\(i=1,\dots,n\).

The sample covariance

\[ s_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) \]

is an estimator for the population variance of \(X\) and \(Y\) whereas
the sample correlation

\[ r_{XY} = \frac{s_{XY}}{s_Xs_Y} \] can be used to estimate the
population correlation, a standardized measure for the strength of the
linear relationship between \(X\) and \(Y\). See chapter 3.7 in the book
for a more detailed treatment of these estimators.

As for variance and standard deviation, these estimators are implemented
as R functions in the stats package. We can use them to estimate
population covariance and population correlation the fictional data on
age and earnings.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute sample covariance of X and Y}
\KeywordTok{cov}\NormalTok{(X,Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 213.934
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute sample correlation between X and Y}
\KeywordTok{cor}\NormalTok{(X,Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.706372
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# equivalent way to compute the sample correlation}
\KeywordTok{cov}\NormalTok{(X,Y)}\OperatorTok{/}\NormalTok{(}\KeywordTok{sd}\NormalTok{(X) }\OperatorTok{*}\StringTok{ }\KeywordTok{sd}\NormalTok{(Y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.706372
\end{verbatim}

The estimates indicate that \(X\) and \(Y\) are moderately correlated.

The next code chunk uses the function \texttt{mvnorm()} from package
MASS to generate bivariate example data with different degree of
correlation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'MASS'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     select
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# positive correlation (0.81)}
\NormalTok{example1 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{mu =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{Sigma =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{),}
                    \DataTypeTok{empirical =} \OtherTok{TRUE}
\NormalTok{                    )}

\CommentTok{# negative correlation (-0.81)}
\NormalTok{example2 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{mu =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{Sigma =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{),}
                    \DataTypeTok{empirical =} \OtherTok{TRUE}
\NormalTok{                    )}

\CommentTok{# no correlation }
\NormalTok{example3 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{mu =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{Sigma =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{),}
                    \DataTypeTok{empirical =} \OtherTok{TRUE}
\NormalTok{                    )}

\CommentTok{# no correlation (quadratic relationship)}
\NormalTok{X <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\FloatTok{0.01}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\OperatorTok{-}\NormalTok{X}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(X))}

\NormalTok{example4 <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(X,Y)}

\CommentTok{# estimate}

\NormalTok{## Plots}

\CommentTok{# divide plot area as 2-by-2 array}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\KeywordTok{plot}\NormalTok{(example1, }\DataTypeTok{col=}\StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{20}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{'X'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Y'}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Correlation = 0.81"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(example2, }\DataTypeTok{col=}\StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{20}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{'X'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Y'}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Correlation = -0.81"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(example3, }\DataTypeTok{col=}\StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{20}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{'X'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Y'}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Correlation = 0"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(example4, }\DataTypeTok{col=}\StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{20}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{'X'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Y'}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Correlation = 0"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-72-1} \end{center}

\section{Exercises}\label{exercises-1}

\subsubsection*{1. Biased \ldots{}}\label{biased}
\addcontentsline{toc}{subsubsection}{1. Biased \ldots{}}

Consider the following alternative estimator

\[\widetilde{Y}=\frac{1}{n-1}\sum\limits_{i=1}^n Y_i\]

In this exercise we want to illustrate that this estimator is biased.

\textbf{Instructions:}

\begin{itemize}
\item
  Define a function for the estimator by yourself and name it Y\_tilde.
\item
  Randomly draw 5 observations from the \(\mathcal{N}(10, 25)\)
  distribution and compute an estimate with Y\_tilde(). Repeat this
  procedure 10000 times and store the results in est\_biased.
\item
  Plot a histogram of est\_biased.
\item
  Add a red vertical line at \(\mu=10\) using the function abline().
\end{itemize}

 \# Define a function for the estimator

\# Compute repeatedly estimates and store the results in est\_biased
set.seed(123)

\# Plot a histogram of est\_biased

\# Add a red vertical line at mu = 10

 \# Define a function for the estimator Y\_tilde \textless{}-
function(x)\{sum(x)/(length(x)-1)\}

\# Compute repeatedly estimates and store the results in est\_biased
set.seed(123) est\_biased \textless{}- replicate(n = 10000, expr =
Y\_tilde(rnorm(5, 10, 5)))

\# Plot a histogram of est\_biased hist(est\_biased)

\# Add a red vertical line at mu = 10 abline(v = 10, col = ``red'')

 test\_function\_definition(`Y\_tilde', function\_test = \{
test\_expression\_result(``Y\_tilde(1:10)'')
test\_expression\_result(``Y\_tilde(1:25)'')
test\_expression\_result(``Y\_tilde(seq(0, 1, 0.1))'') \})
test\_object(`est\_biased') test\_function(`hist', args = `x')
test\_function(`abline', args = c(`v', `col'))

\textbf{Hints:}

\begin{itemize}
\item
  To compute the sum of a vector you can use sum(), to get the length of
  a vector you can use length().
\item
  Use the function replicate() to compute repeatedly estimates of random
  samples. With the arguments expr and n you can specify the operation
  and how often it has to be replicated.
\item
  A histogram can be plotted with the function hist().
\item
  The point on the x-axis as well as the color for the vertical line can
  be specified via the arguments v and col.
\end{itemize}

\subsubsection*{2. \ldots{} but consistent
estimator}\label{but-consistent-estimator}
\addcontentsline{toc}{subsubsection}{2. \ldots{} but consistent
estimator}

Consider again the estimator from the previous exercise which is
available in your environment as Y\_tilde(). Do the same procedure as in
the previous exercise but now increase the number of observations to
draw from 5 to 1000. What do you note? What can you say about this
estimator?

\textbf{Instructions:}

\begin{itemize}
\item
  Randomly draw 1000 observations from the \(\mathcal{N}(10, 25)\)
  distribution and compute an estimate with Y\_tilde(). Repeat this
  procedure 10000 times and store the results in est\_consistent.
\item
  Plot a histogram of est\_consistent.
\item
  Add a red vertical line at \(\mu=10\) using the function abline().
\end{itemize}

 Y\_tilde \textless{}- function(x)\{sum(x)/(length(x)-1)\}

 \# Compute repeatedly estimates and store the results in
est\_consistent set.seed(123)

\# Plot a histogram of est\_biased

\# Add a red vertical line at mu = 10

 \# Compute repeatedly estimates and store the results in
est\_consistent set.seed(123) est\_consistent \textless{}- replicate(n =
10000, expr = Y\_tilde(rnorm(1000, 10, 5)))

\# Plot a histogram of est\_consistent hist(est\_consistent)

\# Add a red vertical line at mu = 10 abline(v = 10, col = ``red'')

 test\_object(`est\_consistent') test\_function(`hist', args = `x')
test\_function(`abline', args = c(`v', `col'))

\textbf{Hints:}

\begin{itemize}
\item
  Use the function replicate() to compute repeatedly estimates of random
  samples. With the arguments expr and n you can specify the operation
  and how often it has to be replicated.
\item
  A histogram can be plotted with the function hist().
\item
  The point on the x-axis as well as the color for the vertical line can
  be specified via the arguments v and col.
\end{itemize}

\subsubsection*{3. Efficiency of an
estimator}\label{efficiency-of-an-estimator}
\addcontentsline{toc}{subsubsection}{3. Efficiency of an estimator}

In this exercise we want to illustrate the result that the sample mean

\[\hat{\mu}_Y=\sum\limits_{i=1}^{n}a_iY_i\] with equal weighting scheme
\(a_i=\frac{1}{n}\) for \(i=1,...,n\) is the best linear unbiased
estimator (BLUE) of \(\mu_Y\).

As an alternative consider the estimator

\[\tilde{\mu}_Y=\sum\limits_{i=1}^{n}b_iY_i\]

where \(b_i\) gives the first \(\frac{n}{2}\) observations a higher
weighting than the second \(\frac{n}{2}\) observations.

The respective weighting vector is already defined and available as w in
your working environment.

\textbf{Instructions:}

\begin{itemize}
\item
  Verify that \(\tilde{\mu}\) is unbiased.
\item
  Define the alternative estimator as a function mu\_tilde().
\item
  Randomly draw 100 observations from the \(\mathcal{N}(5, 10)\)
  distribution and compute an estimate with both estimators. Repeat this
  procedure 10000 times and store the results in est\_bar and
  est\_tilde.
\item
  Compute the sample variances of est\_bar and est\_tilde. What can you
  say about both estimators?
\end{itemize}

 \# Verify that the alternative estimator is unbiased n \textless{}- 100
w \textless{}- c(rep((1+0.5)/n, n/2), rep((1-0.5)/n, n/2))

\# Define the alternative estimator mu\_tilde

\# Compute repeatedly estimates for both estimators and store the
results in est\_bar and est\_tilde set.seed(123)

\# Compute the sample variances for est\_bar and est\_tilde

 \# Verify that the alternative estimator is unbiased n \textless{}- 100
w \textless{}- c(rep((1+0.5)/n, n/2), rep((1-0.5)/n, n/2)) sum(w)

\# Define the alternative estimator mu\_tilde mu\_tilde \textless{}-
function(x)\{sum(w*x)\}

\# Compute repeatedly estimates for both estimators and store the
results in est\_bar and est\_tilde set.seed(123) est\_bar \textless{}-
replicate(expr = mean(rnorm(100, 5, 10)), n = 10000) est\_tilde
\textless{}- replicate(expr = mu\_tilde(rnorm(100, 5, 10)), n = 10000)

\# Compute the sample variances for est\_bar and est\_tilde
var(est\_bar) var(est\_tilde)

 test\_function\_result(`sum') test\_function\_definition(`mu\_tilde',
function\_test = \{ test\_expression\_result(``mu\_tilde(1:100)'')
test\_expression\_result(``mu\_tilde(2:101)'') \})
test\_object(`est\_bar') test\_object(`est\_tilde')
test\_function\_result(`var', index = 1) test\_function\_result(`var',
index = 2) success\_msg(`Correct! The sample mean is more efficient
(that is, has a lower variance) than the alternative estimator.')

\textbf{Hints:}

\begin{itemize}
\item
  In order to be an unbiased estimator all weights have to sum up to 1.
\item
  Use the function replicate() to compute repeatedly estimates of random
  samples. With the arguments expr and n you can specify the operation
  and how often it has to be replicated.
\item
  To compute sample variances you can use var().
\end{itemize}

\subsubsection*{\texorpdfstring{4. Hypothesis Test --- \(t\)
Statistic}{4. Hypothesis Test --- t Statistic}}\label{hypothesis-test-t-statistic}
\addcontentsline{toc}{subsubsection}{4. Hypothesis Test --- \(t\)
Statistic}

Consider the CPS data set from Chapter 3.6 again which is available as
cps in your working environment.

We suppose that the average hourly earnings (in prices of 2012) ahe12
exceed 23.50 \(\frac{\$}{h}\) and wish to test this hypothesis at a
significance level of \(\alpha=0.05\). For that reason please do the
following:

\textbf{Instructions:}

\begin{itemize}
\item
  Compute the test statistic by hand and assign it to tstat.
\item
  Use tstat to accept or reject the null hypothesis. Please do so using
  the normal approximation.
\end{itemize}

 cps \textless{}-
read.table(``\url{http://s3.amazonaws.com/assets.datacamp.com/production/course_1276/datasets/cps_ch3.csv}'',
header = T, sep = ``;'')

 \# Compute the t statistic by hand and assign it to tstat

\# Use tstat to accept or reject the null

 \# Compute the t statistic by hand and assign it to tstat tstat
\textless{}-
(mean(cps\(ahe12)-23.5)/(sd(cps\)ahe12)/sqrt(length(cps\$ahe12)))

\# Use tstat to accept or reject the null tstat \textgreater{}
qnorm(0.95)

 test\_object(`tstat') test\_function\_result(`qnorm')
test\_student\_typed(`tstat', times = 2)
test\_or(test\_output\_contains(`T'), test\_output\_contains(`F'))

\textbf{Hints:}

\begin{itemize}
\item
  We test \(H_0:\mu_{Y_{ahe}}\leq 23.5\) vs. \(H_1:\mu_{Y_{ahe}}>23.5\).
  That is, we conduct a right-sided test.
\item
  The t statistic is defined as
  \(\frac{\bar{Y}-\mu_{Y,0}}{s_{Y}/\sqrt{n}}\) where \(s_Y\) denotes the
  sample variance.
\item
  To decide whether the null hypothesis is accepted or rejected you can
  compare the t statistic with the respective quantile of the standard
  normal distribution. Use logical operators to check for this.
\end{itemize}

\subsubsection*{\texorpdfstring{5. Hypothesis Test ---
\(p\)-value}{5. Hypothesis Test --- p-value}}\label{hypothesis-test-p-value}
\addcontentsline{toc}{subsubsection}{5. Hypothesis Test --- \(p\)-value}

Reconsider the test situation from previous exercise. cps as well as
tstat are available in your working environment.

Instead of using the t statistic as decision criterion we can also use
the respective p-value. For that reason please do the following:

\textbf{Instructions:}

\begin{itemize}
\item
  Compute the p-value by hand and assign it to pval.
\item
  Use pval to accept or reject the null hypothesis.
\end{itemize}

 cps \textless{}-
read.table(``\url{http://s3.amazonaws.com/assets.datacamp.com/production/course_1276/datasets/cps_ch3.csv}'',
header = T, sep = ``;'') tstat \textless{}-
(mean(cps\(ahe12)-23.5)/(sd(cps\)ahe12)/sqrt(length(cps\$ahe12)))

 \# Compute the p-value by hand and assign it to pval

\# Use pval to accept or reject the null

 \# Compute the p-value by hand and assign it to pval pval \textless{}-
1-pnorm(tstat)

\# Use pval to accept or reject the null pval \textless{} 0.05

 test\_object(`pval') test\_student\_typed(`pval', times = 2)
test\_or(test\_output\_contains(`T'), test\_output\_contains(`F'))

\textbf{Hints:}

\begin{itemize}
\item
  The p-value for a right-sided test can be computed as
  \(p=P(t>t^{act}|H_0)\).
\item
  We reject the null if \(p<\alpha\). Use logical operators to check for
  this.
\end{itemize}

\subsubsection*{\texorpdfstring{6. Hypothesis Test --- One Sample
\(t\)-test}{6. Hypothesis Test --- One Sample t-test}}\label{hypothesis-test-one-sample-t-test}
\addcontentsline{toc}{subsubsection}{6. Hypothesis Test --- One Sample
\(t\)-test}

In the last two exercises we discovered two ways of conducting a
hypothesis test. In practice these approaches seem very cumbersome and
so R provides the function t.test() which does most of the work
automatically for us. In fact it provides t statistics, p-values and
even confidence intervals (more on the latter in later exercises). In
addition it also uses the t instead of the normal distribution which
becomes important especially for small sample sizes.

The data set cps as well as the variable pval from Exercise 3.4 are
available in your working environment.

\textbf{Instructions:}

\begin{itemize}
\item
  Conduct the hypothesis test from previous exercises with the function
  t.test().
\item
  Extract the t statistic and p-value from the list created by t.test().
  Assign them to the variables tstat and pvalue.
\item
  Verify that using the normal approximation here is valid as well by
  computing the difference between both \(p\)-values.
\end{itemize}

 cps \textless{}-
read.table(``\url{http://s3.amazonaws.com/assets.datacamp.com/production/course_1276/datasets/cps_ch3.csv}'',
header = T, sep = ``;'') tstat \textless{}-
(mean(cps\(ahe12)-23.5)/(sd(cps\)ahe12)/sqrt(length(cps\$ahe12))) pval
\textless{}- 1-pnorm(tstat)

 \# Conduct the hypothesis test from previous exercises with t.test()

\# Extract t statistic and p-value from list created by t.test()

\# Verify that using the normal approximation here is valid as well

 \# Conduct the hypothesis test from previous exercises with t.test()
t.test(cps\$ahe12, alternative = ``greater'', mu = 23.5)

\# Extract t statistic and p-value from list created by t.test() tstat
\textless{}-
t.test(cps\(ahe12, alternative = "greater", mu = 23.5)\)statistic pvalue
\textless{}-
t.test(cps\(ahe12, alternative = "greater", mu = 23.5)\)p.value

\# Verify that using the normal approximation here is valid as well
pvalue - pval

 test\_function\_result(`t.test') test\_object(`tstat')
test\_object(`pvalue') test\_or(test\_student\_typed(`pvalue - pval'),
test\_student\_typed(`pval - pvalue'))

\textbf{Hints:}

\begin{itemize}
\item
  The type of the test as well as the null hypothesis can be specified
  via the arguments alternative and mu.
\item
  t statistic and p-value can be obtained via
  \(statistic</tt> and <tt>\)p.value, respectively.
\end{itemize}

\subsubsection*{\texorpdfstring{7. Hypothesis Test --- Two Sample
\(t\)-test}{7. Hypothesis Test --- Two Sample t-test}}\label{hypothesis-test-two-sample-t-test}
\addcontentsline{toc}{subsubsection}{7. Hypothesis Test --- Two Sample
\(t\)-test}

Consider the annual maximum sea levels at Port Pirie (Southern
Australia) and Fremantle (Western Australia) for the last 30 years.

The observations are available in the variables portpirie and fremantle
in your working environment.

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Test whether there is a significant difference in the annual maximum
  sea levels at a significance level of \(\alpha=0.05\).
\end{itemize}

 set.seed(123) portpirie \textless{}- runif(30, 3.6, 4.6) fremantle
\textless{}- runif(30, 1.2, 1.8)

 \# Conduct a two sample t-test

 \# Conduct a two sample t-test t.test(portpirie, fremantle)

 test\_or(test\_output\_contains(`t.test(portpirie, fremantle)'),
test\_output\_contains(`t.test(fremantle, portpirie)'))

\textbf{Hints:}

\begin{itemize}
\item
  We test \(H_0:\mu_{P}-\mu_{F}=0\) vs. \(H_1:\mu_{P}-\mu_{F}\ne 0\).
  That is, we conduct a two sample t-test.
\item
  For a two sample t-test t.test() expects two vectors containing the
  data.
\end{itemize}

\subsubsection*{8. Confidence Interval}\label{confidence-interval}
\addcontentsline{toc}{subsubsection}{8. Confidence Interval}

Reconsider the test situation concerning the annual maximum sea levels
at Port Pirie and Fremantle.

The variables portpirie and fremantle are again available in your
working environment.

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Construct a 95\%-confidence interval for the difference in the sea
  levels using t.test().
\end{itemize}

 set.seed(123) portpirie \textless{}- runif(30, 3.6, 4.6) fremantle
\textless{}- runif(30, 1.2, 1.8)

 \# Construct a 95\%-confidence interval using t.test()

 \# Construct a 95\%-confidence interval using t.test()
t.test(portpirie, fremantle)\$conf.int

 test\_or(test\_output\_contains(`t.test(portpirie,
fremantle)\(conf.int'), test_output_contains('t.test(fremantle, portpirie)\)conf.int'))

\textbf{Hint:}

\begin{itemize}
\tightlist
\item
  The function t.test() computes by default a confidence interval which
  is accessible via \$conf.int.
\end{itemize}

\subsubsection{9. (Co)variance and Correlation
I}\label{covariance-and-correlation-i}

Consider a random sample \((X_i, Y_i)\) for \(i=1,...,100\).

The respective vectors \(X\) and \(Y\) are already available in your
working environment as X and Y.

\textbf{Instructions:}

\begin{itemize}
\item
  Compute the variance of \(X\) using the function cov().
\item
  Compute the covariance of \(X\) and \(Y\).
\item
  Compute the correlation between \(X\) and \(Y\).
\end{itemize}

 custom\_seed(123) X \textless{}- runif(100, 900, 1000) Y \textless{}-
3*X+rnorm(100, 0, 100)

 \# Compute the variance of X

\# Compute the covariance of X and Y

\# Compute the correlation between X and Y

 \# Compute the variance of X with cov() cov(X, X)

\# Compute the covariance of X and Y cov(X, Y)

\# Compute the correlation between X and Y cor(X, Y)

 test\_function\_result(``cov'', index = 1)
test\_function\_result(``cov'', index = 2)
test\_function\_result(``cor'')

\textbf{Hints:}

\begin{itemize}
\item
  The variance is just a special case of the covariance.
\item
  cov() as well as cor() expect a vector for each variable.
\end{itemize}

\subsubsection{10. (Co)variance and Correlation
II}\label{covariance-and-correlation-ii}

In this exercise we want to examine the limitations of the correlation
as a dependency measure.

Once the session has initialized you will see the plot of 100
realizations from two random variables \(X\) and \(Y\).

The respective observations are available in the vectors X and Y in your
working environment.

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Compute the correlation between \(X\) and \(Y\). Interpret your result
  critically.
\end{itemize}

 custom\_seed(123) X \textless{}- runif(100, 0, 3) Y \textless{}-
exp(-X) + rnorm(100, 0, .05) plot(Y \textasciitilde{} X)

 \# Compute the correlation between X and Y

 \# Compute the correlation between x and y cor(X, Y) \# The correlation
is only able to quantify linear relationships which is not the case
here. test\_function\_result(``cor'') success\_msg(`Correct! The
correlation is able to capture the negative relationship between both
variables. However it only measures linear dependencies, whereas the
dependency here is clearly nonlinear (exponential).')

\textbf{Hint:}

\begin{itemize}
\tightlist
\item
  cor() expects a vector for each variable.
\end{itemize}

\hypertarget{lrwor}{\chapter{Linear Regression with One
Regressor}\label{lrwor}}

This chapter introduces the basics in linear regression and shows how to
perform regression analysis in R. In linear regression, the aim is to
model the relationship between a dependent variable \(Y\) and one or
more explanatory variables denoted as \(X_1, X_2, \dots, X_k\).
Following the book we will focus on the concept of simple linear
regression throughout the whole chapter. In simple linear regression,
there is just one explanatory variable \(X_1\). If for example a school
cuts the class sizes by hiring new teachers, that is the school lowers
the student-teacher ratios of their classes, \(X_1\), how would this
affect the performance of the students involved in a standardized
test,\(Y\)? With linear regression we can not only examine whether the
student-teacher ratio \emph{does have} an impact on the test results but
we can also learn about the \emph{direction} and the \emph{strength} of
this effect.

To start with an easy example, consider the following combinations of
average test score and the average student-teacher ratio in some
fictional school disctrics.

1

2

3

4

5

6

7

TestScore

680

640

670

660

630

660.0

635

STR

15

17

19

20

22

23.5

25

To work with these data in R we begin by creating two vectors: one for
the student-teacher ratios (\texttt{STR}) and one for test scores
(\texttt{TestScore}), both containing the data from the table above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create sample data}
\NormalTok{STR <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{22}\NormalTok{, }\FloatTok{23.5}\NormalTok{, }\DecValTok{25}\NormalTok{)}
\NormalTok{TestScore <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{680}\NormalTok{, }\DecValTok{640}\NormalTok{, }\DecValTok{670}\NormalTok{, }\DecValTok{660}\NormalTok{, }\DecValTok{630}\NormalTok{, }\DecValTok{660}\NormalTok{, }\DecValTok{635}\NormalTok{) }

\CommentTok{# Print out sample data}
\NormalTok{STR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 15.0 17.0 19.0 20.0 22.0 23.5 25.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TestScore}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 680 640 670 660 630 660 635
\end{verbatim}

If we use a simple linear regression model, we assume that the true
relationship between both variables can be represented by a straight
line, formally

\[ Y = b \cdot X + a. \]

For now, let us suppose that the true function which relates test score
and student-teacher ratio to each other is

\[TestScore = 713 - 3 \times STR.\]

If possible, it is always a good idea to visualize the data You work
with in an appropriate way. For our purpose it is suitable to use the
function \texttt{plot()} to produce a scatterplot with \texttt{STR} on
the \(X\)-axis and \texttt{TestScore} on the \(Y\) axis. An easy way to
do so is to call
\texttt{plot(y\_variable\ \textasciitilde{}\ x\_variable)} whereby
\texttt{y\_variable} and \texttt{x\_variable} are placeholders for the
vectors of observations we want to plot. Furthermore, we might want to
add the true relationship to the plot. To draw a straight line, R
provides the function \texttt{abline()}. We just have to call this
function with arguments \texttt{a} (representing the intercept) and
\texttt{b} (representing the slope) after executing \texttt{plot()} in
order to add the line to our scatterplot.

The following code reproduces figure 4.1 from the textbook.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a scatter plot of the data}
\KeywordTok{plot}\NormalTok{(TestScore }\OperatorTok{~}\StringTok{ }\NormalTok{STR)}
\CommentTok{# add the true relationship to the plot}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{a =} \DecValTok{713}\NormalTok{, }\DataTypeTok{b =} \OperatorTok{-}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-74-1} \end{center}

We find that our line does not touch any of the points although we
claimed that it represents the true relationship. The reason for this is
the core problem of statistics, \emph{randomness}. Most of the time
there are influences which cannot be explained in a purely deterministic
fashion and thus exacerbate finding the true relationship.

In order to account for these differences between observed data and the
true relationship, we extend our model from above by an \emph{error
term} \(u\) which covers these random effects. Put differently, \(u\)
accounts for all the differences between the true regression line and
the actual observed data. Beside pure randomness, these deviations could
also arise from measerment errors or, as will be discussed later, could
be the consequence of leaving out other factors that are relevant in
explaining the dependent variable. Which other factor are plausible in
our example? For one thing, the test scores might be driven by the
teachers quality and the background of the students. It is also
imaginable that in some classes, the students were lucky on the test
days and thus achieved higher scores. For now, we will summarize such
influences by an additive component:

\[ TestScore = \beta_0 + \beta_1 \times STR + \text{other factors} \]

Of course this idea is very general as it can be easily extented to
other situations that can be described with a linear model. The basic
linear regression function we will work with hence is

\[ Y_i = \beta_0 + \beta_1 X_i + u_i. \]

Key Concept 4.1 summarizes the terminology of the simple linear
regression model.

Key Concept 4.1

Terminology for the Linear Regression Model with a Single Regressor

The linear regression model is

\[Y_i = \beta_0 + \beta_1 X_1 + u_i \]

where

\begin{itemize}
\tightlist
\item
  the subscript \(i\) runs over the observations, \(i = 1\), \ldots{},
  \(n\)
\item
  \(Y_i\) is the \emph{dependent variable}, the \emph{regressand}, or
  simply the \emph{left-hand variable}
\item
  \(X_i\) is the \emph{independent variable}, the \emph{regressor}, or
  simply the \emph{right-hand variable}
\item
  \(Y = \beta_0 + \beta_1 X\) is the \emph{population regression line}
  also called the \emph{population regression function}
\item
  \(\beta_0\) is the \emph{intercept} of the population regression line
\item
  \(\beta_1\) is the \emph{slope} of the population regression line
\item
  \(u_i\) is the \emph{error term}
\end{itemize}

\section{Estimating the Coefficients of the Linear Regression
Model}\label{estimating-the-coefficients-of-the-linear-regression-model}

In practice, the intercept \(\beta_0\) and slope \(\beta_1\)of the
population regression line are unknown. Therefore, we must employ data
to estimate both unknown parameters. In the following a real world
example will be used to demonstrate how this is achieved. We want to
relate test scores to student-teacher ratios measured in californian
schools. The test score is the district-wide average of reading and math
scores for fifth graders. Again, the class size is measured as the
number of students divided by the number of teachers (the
student-teacher ratio). As for the data, the California School dataset
(\texttt{CASchools}) comes with a R package called AER, an acronym for
\href{https://cran.r-project.org/web/packages/AER/AER.pdf}{Applied
Econometrics with R}. After installing the package with
\texttt{install.packages("AER")} and attaching it with
\texttt{library("AER")} the dataset can be loaded using the
\texttt{data} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install the AER package (once)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"AER"}\NormalTok{)}

\CommentTok{# load the AER package }
\KeywordTok{library}\NormalTok{(AER)   }

\CommentTok{# load the the data set in the workspace}
\KeywordTok{data}\NormalTok{(CASchools) }
\end{Highlighting}
\end{Shaded}

Note that once a package has been installed it is available for use at
further occasions when invoked with \texttt{library()} --- there is no
need to run \texttt{install.packages("...")} again!

For several reasons it is interesting to know what kind of object we are
dealing with. \texttt{class(object\_name)} returns the type (class) of
an object. Depending on the class of an object some functions (such as
\texttt{plot()} and \texttt{summary()}) behave differently.

Let us check the class of the object \texttt{CASchools}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(CASchools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

It turns out that \texttt{CASchools} is of class \texttt{data.frame}
which is a convienient format to work with.

With help of the function \texttt{head()} we get a first overview of our
data. This function shows only the first 6 rows of the data set which
prevents an overcrowded console output.

\BeginKnitrBlock{rmdnote}
Press ctrl + L to clear the console. This command deletes any code that
has been typed in and executed by You or printed to the console by R
functions. Good news is: anything else is left untouched. You neither
loose defined variables and alike nor the code history. It is still
possible to recall previously executed R commands using the up and down
keys. If You are working in RStudio, press ctrl + Up on Your keyboard
(CMD + Up on a mac) to review a list of previously entered commands.
\EndKnitrBlock{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(CASchools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   district                          school  county grades students
## 1    75119              Sunol Glen Unified Alameda  KK-08      195
## 2    61499            Manzanita Elementary   Butte  KK-08      240
## 3    61549     Thermalito Union Elementary   Butte  KK-08     1550
## 4    61457 Golden Feather Union Elementary   Butte  KK-08      243
## 5    61523        Palermo Union Elementary   Butte  KK-08     1335
## 6    62042         Burrel Union Elementary  Fresno  KK-08      137
##   teachers calworks   lunch computer expenditure    income   english  read
## 1    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6
## 2    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5
## 3    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3
## 4    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9
## 5    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8
## 6     6.40  12.3188 86.9565       25    5580.147 10.415000 12.408759 605.7
##    math
## 1 690.0
## 2 661.9
## 3 650.9
## 4 643.5
## 5 639.9
## 6 605.4
\end{verbatim}

We find that the dataset consists of plenty of variables and most of
them are numeric.

By the way: an alternative to \texttt{class()} and \texttt{head()} is
\texttt{str()} which is deduced from `structure' and gives a
comprehensive overview of the object. Try this!

Turning back to \texttt{CASchools}, the two variables we are intersted
in (i.e.~average test score and the student-teacher ratio) are
\emph{not} included. However, it is possible to calculate both from the
provided data. To obtain the student-teacher ratios, we simply divide
the number of students by the number of teachers. The avarage test score
is the arithmetic mean of the test score for reading and the score of
the math test. The next code chunk shows how the two variables can be
constructed and how they are appended to \texttt{CASchools} which is a
data.frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute STR and append it to CASchools}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{STR <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{students}\OperatorTok{/}\NormalTok{CASchools}\OperatorTok{$}\NormalTok{teachers }

\CommentTok{# compute TestScore and append it to CASchools}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{score <-}\StringTok{ }\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{read }\OperatorTok{+}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{math)}\OperatorTok{/}\DecValTok{2}     
\end{Highlighting}
\end{Shaded}

If we ran \texttt{head(CASchools)} again we would find the two variables
of interest as additional columns named \texttt{STR} and \texttt{score}
(check this!).

Table 4.1 from the text book summarizes the distribution of test scores
and student-teacher ratios. There are several functions which can be
used to produce similar results within R:

\begin{itemize}
\item
  \texttt{mean()} (computes the arithmetic mean of the provided numbers)
\item
  \texttt{sd()} (computes the sample standard deviation)
\item
  \texttt{quantile()} (returns a vector of the specified quantiles for
  the data)
\end{itemize}

The next code chunk shows how to achieve this. First, we compute summary
statistics on the coloumns \texttt{STR} and \texttt{score} of
\texttt{CASchools}. In order to have a nice display format we gather the
computed measures in a \texttt{data.frame} object named
\texttt{DistributionSummary}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute sample averages of STR and score}
\NormalTok{avg_STR <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR) }
\NormalTok{avg_score <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score)}

\CommentTok{# compute sample standard deviations of STR and score}
\NormalTok{sd_STR <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR) }
\NormalTok{sd_score <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score)}

\CommentTok{# set up a vector of percentiles and compute the quantiles }
\NormalTok{quantiles <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.10}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\FloatTok{0.9}\NormalTok{)}
\NormalTok{quant_STR <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR, quantiles)}
\NormalTok{quant_score <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score, quantiles)}

\CommentTok{# gather everything in a data.frame }
\NormalTok{DistributionSummary <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
                                  \DataTypeTok{Average =} \KeywordTok{c}\NormalTok{(avg_STR, avg_score), }
                                  \DataTypeTok{StandardDeviation =} \KeywordTok{c}\NormalTok{(sd_STR, sd_score), }
                                  \DataTypeTok{quantile =} \KeywordTok{rbind}\NormalTok{(quant_STR, quant_score)}
\NormalTok{                                  )}

\CommentTok{# print the summary to the console}
\NormalTok{DistributionSummary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Average StandardDeviation quantile.10. quantile.25.
## quant_STR    19.64043          1.891812      17.3486     18.58236
## quant_score 654.15655         19.053347     630.3950    640.05000
##             quantile.40. quantile.50. quantile.60. quantile.75.
## quant_STR       19.26618     19.72321      20.0783     20.87181
## quant_score    649.06999    654.45000     659.4000    666.66249
##             quantile.90.
## quant_STR       21.86741
## quant_score    678.85999
\end{verbatim}

The standard distribution
(\href{https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html}{the
Base R package}) of R already contains a \texttt{summary} function which
can be applied to objects of class \texttt{data.frame}. Type and execute
\texttt{summary(STR)}!

As done for the sample data, we use \texttt{plot()} for a visual survey.
This allows us to detect specific characteristics of our data, such as
outliers which are hard to discover by looking at mere numbers. This
time we add some additional arguments to the \texttt{plot()} function.

The first argument in our call of \texttt{plot()},
\texttt{score\ \textasciitilde{}\ STR}, is again a formula that states
the dependent variable and the regressor. However, this time the two
variables are not saved in seperate vectors but are columns of
\texttt{CASchools}. Therefore, R would not find the variables without
the argument \texttt{data} beeing correctly specified. \texttt{data}
must be in accordance with the name of the \texttt{data.frame} to which
the variables belong, in this case \texttt{CASchools}. Further arguments
are used to change the appearance of the plot: while \texttt{main} adds
a title, \texttt{xlab} and \texttt{ylab} are adding custom labels to
both axes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }
     \DataTypeTok{data =}\NormalTok{ CASchools,}
     \DataTypeTok{main =} \StringTok{"Scatterplot of TestScore and STR"}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"STR (X)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test Score (Y)"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-80-1} \end{center}

The plot (figure 4.2 in the book) shows the scatterplot of all
observations on student-teacher ratio and Test score. We see that the
points are strongly scatterd and an apparent relationship cannot be
detected by only looking at them. Yet it can be assumed that both
variables are negatively correlated, that is we expect to observe lower
test scores in bigger classes.

The function \texttt{cor()} (type and execute \texttt{?cor} for further
info), can be used to compute the correlation between 2 \emph{numerical}
vectors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR, CASchools}\OperatorTok{$}\NormalTok{score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.2263627
\end{verbatim}

As the scatterplot already suggests, the correlation is negative but
rather weak.

The task we are facing now is to find a line which fits best to the
data. Of course we could simply stick with graphical inspection and
correlation analysis and then select the best fitting line by
eyeballing. However, this is pretty unscientific and prone to subjective
perception: different students would draw different regression lines. On
this account, we are interested in techniques that are more
sophisticated. Such a technique is ordinary least squares (OLS)
estimation.

\subsection*{The Ordinary Least Squares
Estimator}\label{the-ordinary-least-squares-estimator}
\addcontentsline{toc}{subsection}{The Ordinary Least Squares Estimator}

The OLS estimator chooses the regression coefficients such that the
estimated regression line is as close as possible to the observed data
points. Thereby closeness is measured by the sum of the squared mistakes
made in predicting \(Y\) given \(X\). Let \(b_0\) and \(b_1\) be some
estimators of \(\beta_0\) and \(\beta_1\). Then the sum of squared
estimation mistakes can be expressed as

\[ \sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2. \]

The OLS estimator in the simple regression model is the pair of
estimators for intercept and slope which minimizes the expression above.
The derivation of the OLS estimators for both parameters are presented
in Appendix 4.1 of the book. The results are summarized in Key Concept
4.2.

Key Concept 4.2

The OLS Estimator, Predicted Values, and Residuals

The OLS estimators of the slope \(\beta_1\) and the intercept
\(\beta_0\) in the simple linear regression model are

\begin{align}
  \hat\beta_1 & = \frac{ \sum_{i = 1}^n (X_i - \overline{X})(Y_i - \overline{Y}) } { \sum_{i=1}^n (X_i - \overline{X})^2}  \\
  \\
  \hat\beta_0 & =  \overline{Y} - \hat\beta_1 \overline{X} 
\end{align}

The OLS predicted values \(\widehat{Y}_i\) and residuals \(\hat{u}_i\)
are

\begin{align}
  \widehat{Y}_i & =  \hat\beta_0 + \hat\beta_1 X_i,\\
  \\
  \hat{u}_i & =  Y_i - \widehat{Y}_i. 
\end{align}

The estimated intercept \(\hat{\beta}_0\), the slope parameter
\(\hat{\beta}_1\), and the residuals \(\left(\hat{u}_i\right)\) are
computed from a sample of \(n\) observations of \(X_i\) and \(Y_i\),
\(i\), \(...\), \(n\). These are \emph{estimates} of the unkown true
population intercept \(\left(\beta_0 \right)\), slope
\(\left(\beta_1\right)\), and error term \((u_i)\).

We are aware that the results presented in Key Concept 4.2 are not very
intuitive at first glance. The following interactice application aims to
help You understand the mechanics of OLS. You can add observations by
clicking into the coordinate system where the data are represented by
points. If two or more observations are available, the application
computes a regression line using OLS and some statistics which are
displayed in the right panel. The results are updated as You add further
observations to the left panel. A double-click resets the application
i.e.~all data are removed.

There are many possible ways to compute \(\hat{\beta_0}\) and
\(\hat{\beta_1}\) in R. For example, we could implement the formulas
presented in Key Concept 4.2 with two of R's most basic functions:
\texttt{mean()} and \texttt{sum()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attach}\NormalTok{(CASchools) }\CommentTok{#allows to use the variables contained in CASchools directly}

\CommentTok{# compute beta_1 }
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((STR }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(STR))}\OperatorTok{*}\NormalTok{(score }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(score))) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{((STR }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(STR))}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\CommentTok{# compute beta_0}
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(score) }\OperatorTok{-}\StringTok{ }\NormalTok{beta_}\DecValTok{1} \OperatorTok{*}\StringTok{ }\KeywordTok{mean}\NormalTok{(STR)}

\CommentTok{# print the results to the console}
\NormalTok{beta_}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.279808
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta_}\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 698.9329
\end{verbatim}

Of course there are also other and even more manual ways to do the same
tasks. Luckily, OLS is one of the most widely-used estimation
techniques. Being a statistical programming language, R already contains
a built-in function named \texttt{lm()} (\textbf{l}inear \textbf{m}odel)
which can be used to carry out regression analysis.

The first argument of the function to be specified is, similar as in
\texttt{plot()}, the regression formula with the basic syntax
\texttt{y\ \textasciitilde{}\ x} where \texttt{y} is the dependent
variable and \texttt{x} the explanatory variable. The argument
\texttt{data} sets the data set to be used in the regression. We now
revisit the example from the book where the relationship between the
test scores and the class sizes is analysed. The following code uses
\texttt{lm()} to replicate the results presented in figure 4.3 in the
book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the model and assign the result to linear_model}
\NormalTok{linear_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# Print the standard output of the estimated lm object to the console }
\NormalTok{linear_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28
\end{verbatim}

Let us add the estimated regression line to the plot. This time we also
enlarge ranges of both axes by setting the arguments \texttt{xlim} and
\texttt{ylim}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the data}
\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }
     \DataTypeTok{data =}\NormalTok{ CASchools,}
     \DataTypeTok{main =} \StringTok{"Scatterplot of TestScore and STR"}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"STR (X)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test Score (Y)"}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{30}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{600}\NormalTok{, }\DecValTok{720}\NormalTok{)}
\NormalTok{     )}

\CommentTok{# add the regression line}
\KeywordTok{abline}\NormalTok{(linear_model) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-84-1} \end{center}

Did you notice that this time, we did not pass the intercept and slope
parameters to \texttt{abline}? If you call \texttt{abline} on an object
of class \texttt{lm} that only contains a single regressor variable, R
draws the regression line automatically!

\section{Measures of Fit}\label{measures-of-fit}

After estimating a linear regression, the question occurs how well that
regression line describes the data. Are the observations tightly
clustered arround the regression line, or are they spread out? Both, the
\(R^2\) and the \emph{standard error of the regression} (\(SER\))
measure how well the OLS Regression line fits the data.

\subsection*{\texorpdfstring{The \(R^2\)}{The R\^{}2}}\label{the-r2}
\addcontentsline{toc}{subsection}{The \(R^2\)}

The \(R^2\) is the fraction of sample variance of \(Y_i\) that is
explained by \(X_i\). Mathemethically, the \(R^2\) can be written as the
ratio of the explained sum of squares to the total sum of squares. The
\emph{explained sum of squares} (\(ESS\)) is the sum of squared
deviations of the predicted values, \(\hat{Y_i}\), from the average of
the \(Y_i\). The \emph{total sum of squares} (\(TSS\)) is the sum of
squared deviations of the \(Y_i\) from their average.

\begin{align}
  ESS & =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2   \\
  \\
  TSS & =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2   \\
  \\
  R^2 & = \frac{ESS}{TSS}
\end{align}

Since \(TSS = ESS + SSR\) we can also write

\[ R^2 = 1- \frac{SSR}{TSS} \]

where \(SSR\) is the sum of squared residuals, a measure for the errors
made when predicting the \(Y\) by \(X\). The \(SSR\) is defined as

\[ SSR = \sum_{i=1}^n \hat{u}_i^2. \]

\(R^2\) lies between \(0\) and \(1\). It is easy to see that a perfect
fit, i.e.~no errors made when fitting the regression line, implies
\(R^2 = 1\) since then we have \(SSR=0\). On the contrary, if our
estimated regression line does not explain any variation in the \(Y_i\),
we have \(ESS=0\) and consequently \(R^2=0\).

\subsection*{Standard Error of the
Regression}\label{standard-error-of-the-regression}
\addcontentsline{toc}{subsection}{Standard Error of the Regression}

The \emph{Standard Error of the Regression} (\(SER\)) is an estimator of
the standard deviation of the regression error \(\hat{u}_i\). As such it
measure the magnitude of a typical deviation from the regression,
i.e.~the magnitude of a typical regression error.

\[ SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{where} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2} \]

Remember that the \(u_i\) are \emph{unobserved}. That is why we use
their estimated counterparts, the residuals \(\hat{u}_i\) instead. See
chapter 4.3 of the book for a more detailed comment on the \(SER\).

\subsection*{Application to the Test Score
Data}\label{application-to-the-test-score-data}
\addcontentsline{toc}{subsection}{Application to the Test Score Data}

Both measures of fit can be obtained by using the function
\texttt{summary()} with the \texttt{lm} object provided as the only
argument. Whereas the function \texttt{lm()} only prints out the
estimated coefficients to the console, \texttt{summary} provides
additional predefined information such as the regression's \(R^2\) and
the \(SER\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod_summary <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(linear_model)}
\NormalTok{mod_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.727 -14.251   0.483  12.822  48.540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 698.9329     9.4675  73.825  < 2e-16 ***
## STR          -2.2798     0.4798  -4.751 2.78e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 18.58 on 418 degrees of freedom
## Multiple R-squared:  0.05124,    Adjusted R-squared:  0.04897 
## F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06
\end{verbatim}

The \(R^2\) in the output is called `Multiple R-squared' and has the
value \(0.051\). Hence, \(5.1 \%\) of the variance of the dependent
variable \(score\) is explained by the explanatory variable \(STR\).
That is the regression explains some of the variance but much of the
variation in test scores remains unexplained (cf.~figure 4.3 in the
book).

The \(SER\) is called `Residual standard error' and takes the value
\(18.58\). The unit of the \(SER\) is the same as the unit of the
dependent variable. In our context we can interpret the value as
follows: on average the deviation of the actual achieved test score and
the regression line is \(18.58\) points.

Now, let us check whether the \texttt{summary()} function uses the same
definitions for \(R^2\) and \(SER\) as we do by computing them manually.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute R^2 manually}
\NormalTok{SSR <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(mod_summary}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{TSS <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((score }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(score))}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{R2 <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{SSR}\OperatorTok{/}\NormalTok{TSS}

\CommentTok{# print the value to the console}
\NormalTok{R2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05124009
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute SER manually}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(CASchools)}
\NormalTok{SER <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(SSR }\OperatorTok{/}\StringTok{ }\NormalTok{(n}\OperatorTok{-}\DecValTok{2}\NormalTok{))}

\CommentTok{# print the value to the console}
\NormalTok{SER}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18.58097
\end{verbatim}

We find that the results coincide. Note that the values provided by
\texttt{summary()} are rounded to two decimal places. Can you do so
using R?

\section{The Least Squares
Assumptions}\label{the-least-squares-assumptions}

OLS performs well under a quite broad variety of different
circumstances. However, there are some assumptions which are posed on
the data which need to be satisfied in order to achieve reliable
results.

Key Concept 4.3

The Least Squares Assumptions

\[Y_i = \beta_0 + \beta_1 X_i + u_i \text{, } i = 1, ...,n\] where

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The error term \(u_i\) has conditional mean zero given \(X_i\):
  \(E(u_i|X_i) = 0\)
\item
  \((X_i,Y_i), i = 1,...,n\) are independent and identically distributed
  (i.i.d.) draws from their joint distribution
\item
  Large outliers are unlikely: \(X_i\) and \(Y_i\) have nonzero finite
  fourth moments
\end{enumerate}

\subsection*{Assumption \#1: The Error Term has Conditional Mean of
Zero}\label{assumption-1-the-error-term-has-conditional-mean-of-zero}
\addcontentsline{toc}{subsection}{Assumption \#1: The Error Term has
Conditional Mean of Zero}

This means that no matter which value we choose for \(X\), the error
term \(u\) must not show any systematic pattern and must have a mean of
\(0\). Consider the case that \(E(u) = 0\) but for low and high values
of \(X\), the error term tends to be positive and for midrange values of
\(X\) the error tends to be negative. We can use R to construct such an
example. To do so we generate our own data using R's build in random
number generators.

We will use the following functions You should be familiar with:

\begin{itemize}
\tightlist
\item
  \texttt{runif()} (generates uniformly distributed random numbers)
\item
  \texttt{rnorm()} (generates nomally distributed random numbers)
\item
  \texttt{predict()} (does predictions based on the results of model
  fitting functions like \texttt{lm()})
\item
  \texttt{lines()} (adds line segments to an existing plot)
\end{itemize}

We start by creating a vector containing values that are randomly
scattered on the domain \([-5,5]\). For our example we decide to
generate uniformly distributed random numbers. This can be done with the
function \texttt{runif()}. We also need to simulate the error term. For
this we generate normally distributed random numbers with a mean equal
to \(0\) and a variance of \(1\) using \texttt{rnorm()}. The \(Y\)
values are obtained as a quadratic function of the \(X\) values and the
error. After generating the data we estimate both a simple regression
model and a quadratic model that also includes the regressor \(X^2\).
Finally, we plot the simulated data and add a the estimated regression
line of a simple regression model as well as the predictions made with a
quadratic model to compare the fit graphically.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set a random seed to make the results reproducible}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{321}\NormalTok{)}

\CommentTok{# simulate the data }
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DataTypeTok{min =} \OperatorTok{-}\DecValTok{5}\NormalTok{, }\DataTypeTok{max =} \DecValTok{5}\NormalTok{)}
\NormalTok{u <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{5}\NormalTok{)  }
\NormalTok{## the true relation  }
\NormalTok{Y <-}\StringTok{ }\NormalTok{X}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{X }\OperatorTok{+}\StringTok{ }\NormalTok{u                }

\CommentTok{# estimate a simple regression model }
\NormalTok{mod_simple <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}

\CommentTok{# predict using a quadratic model }
\NormalTok{prediction <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{  }\KeywordTok{I}\NormalTok{(X}\OperatorTok{^}\DecValTok{2}\NormalTok{)), }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X =} \KeywordTok{sort}\NormalTok{(X)))}

\CommentTok{# plot the results}
\KeywordTok{plot}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\KeywordTok{abline}\NormalTok{(mod_simple, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{sort}\NormalTok{(X), prediction)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-87-1} \end{center}

This shows what is meant by \(E(u_i|X_i) = 0\):

Using the quadratic model (represented by the black curve) we see that
there are no systematic deviations of the observation from the predicted
relation. It is credible that the assumption is not violated when such a
model is employed. However, using a simple linear regression model we
see that the assumption is probably violated as \(E(u_i|X_i)\) varies
with the \(X_i\).

\subsection*{\texorpdfstring{Assumption \#2: All \((X_i, Y_i)\) are
Independently and Identically
Distributed}{Assumption \#2: All (X\_i, Y\_i) are Independently and Identically Distributed}}\label{assumption-2-all-x_i-y_i-are-independently-and-identically-distributed}
\addcontentsline{toc}{subsection}{Assumption \#2: All \((X_i, Y_i)\) are
Independently and Identically Distributed}

Most common sampling schemes used when collecting data from populations
produce i.i.d. samples. For example, we could use R's random number
generator to randomly select student IDs from a university's enrollment
list and record age \(X\) and earnings \(Y\) of the corresponding
students. This is a typical example of simple random sampling and
ensures that all the \((X_i,Y_i)\) are drawn randomly from the same
population.

A prominent example where the i.i.d. assumption is not fulfilled is time
series data where we have observations on the same unit over time. For
example, take \(X\) as the number of workers employed by a production
company over the course of time. Due to technological change, the
company makes job cuts periodically but there are also some
non-deterministic influences that relate to economics, politics and
alike. Using R we can simulate such a process and plot it.

We start the series with a total of 5000 workers and simulate the
reduction of employment with a simple autoregressive process that
exhibits a downward trend and has normal distributed errors:\footnote{See
  chapter 14 in the book for more on autoregressive processes and time
  series analysis in general.}

\[ employment_t = 0.98 \cdot employment_{t-1} + u_t \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\CommentTok{# initialize the employment vector}
\NormalTok{X <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{5000}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{99}\NormalTok{))}

\CommentTok{# generate a date vector}
\NormalTok{Date <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\StringTok{"1951/1/1"}\NormalTok{), }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2050/1/1"}\NormalTok{), }\StringTok{"years"}\NormalTok{)}

\CommentTok{# generate time series observations with random influences}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{100}\NormalTok{) X[i] <-}\StringTok{ }\FloatTok{0.98}\OperatorTok{*}\NormalTok{X[i}\OperatorTok{-}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{200}\NormalTok{)}

\CommentTok{#plot the results}
\KeywordTok{plot}\NormalTok{(Date, X, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Workers"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Time (t)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-88-1} \end{center}

It is evident that the observations on \(X\) cannot be independnet in
this example: the level of today's employment is correlated with
tomorrows employment level. Thus, the i.i.d. assumption is violated for
\(X\).

\subsection*{Assumption \#3: Large outliers are
unlikely}\label{assumption-3-large-outliers-are-unlikely}
\addcontentsline{toc}{subsection}{Assumption \#3: Large outliers are
unlikely}

It is easy to come up with situations where extreme observations,
i.e.~observations that deviate considerably from the usual range of the
data, may occur. Such observations are called outliers. Technically
speaking, assumption \#3 requires that \(X\) and \(Y\) have a finite
kurtosis.\footnote{See chapter 4.4 in the book.}

Common cases where we want to exclude or (if possible) correct such
outliers is when they are apperently typos, conversion errors or
measurement errors. Even if it seems that extreme observations have been
recorded correctly, it is advisable to exclude them before estimating a
model since OLS suffers from \emph{sensitivity to outliers}.

What does this mean? One can show that extreme observation receive heavy
weighting in the computation done with OLS. Therefore, outliers can lead
to strongly distorted estimates of regression coefficient. To get a
better impression of this, consider the following application where we
have placed some sample data on \(X\) and \(Y\) which are highly
correlated. The relation between \(X\) and \(Y\) seems to be explained
pretty good by the plotted regression line: all of the blue dots lie
close to the red line and we have \(R^2=0.92\).

Now go ahead and add a further observation at, say, \((18,2)\). This
clearly is an outlier. The result is quite striking: the estimated
regression line differs greatly from the one we adjudged to fit the data
well. The slope is heavily downward biased and \(R^2\) decreased to a
mere \(29\%\)! Double-click inside the coordinate system to reset the
app. Feel free to experiment. Choose different coordinates for the
outlier or add additional ones.

The following code roughly reproduces what is shown in figure 4.5 in the
book. As done above we use sample data generated using R's random number
functions \texttt{rnorm()} and \texttt{runif()}. We estimate simple
regression models based on the original data set and a modified set
where one observation is change to be an outlier and plot the results.
In order to understand the complete code You should be familiar with the
function \texttt{sort()} which sorts the entries of a numeric vector in
ascending order.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# generate the data}
\NormalTok{X <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DataTypeTok{min =} \DecValTok{30}\NormalTok{, }\DataTypeTok{max =} \DecValTok{70}\NormalTok{ ))}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{ , }\DataTypeTok{mean =} \DecValTok{200}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{50}\NormalTok{)}
\NormalTok{Y[}\DecValTok{9}\NormalTok{] <-}\StringTok{ }\DecValTok{2000}

\CommentTok{# fit model with outlier}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}

\CommentTok{# fit model without outlier}
\NormalTok{fitWithoutOutlier <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y[}\OperatorTok{-}\DecValTok{9}\NormalTok{] }\OperatorTok{~}\StringTok{ }\NormalTok{X[}\OperatorTok{-}\DecValTok{9}\NormalTok{])}

\CommentTok{# plot the results}
\KeywordTok{plot}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\KeywordTok{abline}\NormalTok{(fit)}
\KeywordTok{abline}\NormalTok{(fitWithoutOutlier, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-89-1} \end{center}

\hypertarget{tsdotoe}{\section{The Sampling Distribution of the OLS
Estimator}\label{tsdotoe}}

Because the OLS estimators \(\hat{\beta_0}\) and \(\hat{\beta_1}\) are
computed from a randomly drawn sample, the estimators themselves are
random variables with a probability distribution --- the so-called
sampling distribution of the estimators --- which describes the values
they could take over different random samples. Although the sampling
distribution of \(\hat{\beta_0}\) and \(\hat{\beta_1}\) can be
complicated when the sample size is small and generally differs with the
number of observation, \(n\), it is possible to make certain statements
about it that hold for all \(n\). In particular
\[ E(\hat{\beta_0}) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta_1}) = \beta_1,\]
that is, \(\hat\beta_0\) and \(\hat\beta_1\) are unbiased estimators of
\(\beta_0\) and \(\beta_1\), the true parameters. If the sample is
sufficiently large, by the central limit theorem the \emph{joint}
sampling distribution of the estimators is well approximated by the
bivariate normal distribution (2.1). This implies that the marginal
distributions are also normal in large samples. Core facts on the
large-sample distribution of \(\beta_0\) and \(\beta_1\) are presented
in Key Concept 4.4.

Key Concept 4.4

Large Sample Distribution of \(\hat\beta_0\) and \(\hat\beta_1\)

If the least squares assumptions in Key Concept 4.3 hold, then in large
samples \(\hat\beta_0\) and \(\hat\beta_1\) have a jointly normal
sampling distribution. The large sample normal distribution of
\(\hat\beta_1\) is \(N(\beta_1, \sigma^2_{\hat\beta_1})\), where the
variance of the distribution, \(\sigma^2_{\hat\beta_1}\), is

\[ \sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2} \tag{4.1}. \]

The large sample normal distribution of \(\hat\beta_0\) is
\(N(\beta_0, \sigma^2_{\hat\beta_0})\), where

\[ \sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{4.2} \]

\subsection*{R Simulation Study 1}\label{r-simulation-study-1}
\addcontentsline{toc}{subsection}{R Simulation Study 1}

Whether Key Koncept 4.4 really holds can be verified using R. First we
build our own population of \(100000\) observations in total. To do this
we need values for our independent variable \(X\), for the error term
\(u\), and the regression parameters \(\beta_0\) and \(\beta_1\). With
all this combined in a simple regression model, we can compute our
dependent variable \(Y\). In our example we generate the numbers
\(X_i\), \(i = 1\), \ldots{} ,\(100000\) by drawing a random sample from
a uniform distribution on the interval \([0,20]\). The realisations of
the error terms \(u_i\) are drawn from a standard normal distribution
with parameters \(\mu = 0\) and \(\sigma^2 = 100\) (note that
\texttt{rnorm()} requires \(\sigma\) as input for the argument
\texttt{sd}, see \texttt{?rnorm}). Furthermore we chose \(\beta_0 = -2\)
and \(\beta_1 = 3.5\) so the true model is

\[ Y_i = -2 + 3.5 \cdot X_i. \]

Finally, we store the results in a data.frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate data}
\NormalTok{N <-}\StringTok{ }\DecValTok{100000}
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(N, }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{20}\NormalTok{)}
\NormalTok{u <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N, }\DataTypeTok{sd =} \DecValTok{10}\NormalTok{)}

\CommentTok{# population regression}
\NormalTok{Y <-}\StringTok{ }\OperatorTok{-}\DecValTok{2} \OperatorTok{+}\StringTok{ }\FloatTok{3.5} \OperatorTok{*}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\NormalTok{u}
\NormalTok{population <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(X, Y)}
\end{Highlighting}
\end{Shaded}

From now on we will consider the previously generated data as the true
population (which of course would be \emph{unknown} in a real world
application, otherwise there would not be a reason to do draw a random
sample in the first place). The knowledge about the true population and
the true relationship between \(Y\) and \(X\) can be used to verify the
statements made in Key Concept 4.4.

First, let us calculate the true variances \(\sigma^2_{\hat{\beta_0}}\)
and \(\sigma^2_{\hat{\beta_1}}\) for a randomly drawn sample of size
\(n = 100\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set sample size}
\NormalTok{n <-}\StringTok{ }\DecValTok{100}

\CommentTok{# compute the variance of hat_beta_0}
\NormalTok{H_i <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(X) }\OperatorTok{/}\StringTok{ }\KeywordTok{mean}\NormalTok{(X}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{X}
\NormalTok{var_b0 <-}\StringTok{ }\KeywordTok{var}\NormalTok{(H_i }\OperatorTok{*}\StringTok{ }\NormalTok{u) }\OperatorTok{/}\StringTok{ }\NormalTok{(n }\OperatorTok{*}\StringTok{ }\KeywordTok{mean}\NormalTok{(H_i}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\NormalTok{ )}

\CommentTok{# compute the variance of hat_beta_1}
\NormalTok{var_b1 <-}\StringTok{ }\KeywordTok{var}\NormalTok{( ( X }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(X) ) }\OperatorTok{*}\StringTok{ }\NormalTok{u ) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{100} \OperatorTok{*}\StringTok{ }\KeywordTok{var}\NormalTok{(X)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print variances to the console}
\NormalTok{var_b0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.045066
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var_b1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03018694
\end{verbatim}

Now let us assume that we do not know the true values of \(\beta_0\) and
\(\beta_1\) and that it is not possible to observe the whole population.
However, we can observe a random sample of \(n\) observations. Then, it
would not be possible to compute the true parameters but we could obtain
estimates of \(\beta_0\) and \(\beta_1\) from the sample data using OLS.
However, we know that these estimates are outcomes of random variables
themselves since the observations are randomly sampled from the
population. Key Concept 4.4. describes their distributions for large
\(n\). When drawing a single sample of size \(n\) it is not possible to
make any statement about these distributions. Things change if we repeat
the sampling scheme many times and compute the estimates for each
sample: using such a procedure we simulate outcomes of the respective
distributions.

To achieve this in R, we employ the following approach:

\begin{itemize}
\tightlist
\item
  We assign the number of repetitions, say \(10000\), to \texttt{reps}.
  Then we initialize a matrix \texttt{fit} were the estimates obtained
  in each sampling iteration shall be stored row-wise. Thus \texttt{fit}
  has to be an array of dimensions \texttt{reps}\(\times2\).
\item
  In the next step we draw \texttt{reps} random sample of size
  \texttt{n} from the population and obtain the OLS estimates for each
  sample. The results are stored as row entries in the outcome matrix
  \texttt{fit}. This is done using a \texttt{for()} loop.
\item
  At last, we estimate variances of both coefficient estimators using
  the sampled outcomes and plot histograms of the latter. We also add
  plot of the density functions belonging to the distributions that
  follow from Key Concept 4.4. The function \texttt{bquote()} is used to
  obtain math expressions in the titels and labels of both plots. See
  \texttt{?bquote}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set repetitions and sample size}
\NormalTok{n <-}\StringTok{ }\DecValTok{100}
\NormalTok{reps <-}\StringTok{ }\DecValTok{10000}

\CommentTok{# initialize the matrix of outcomes}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ reps)}

\CommentTok{# loop sampling and estimating of the coefficients}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{reps)\{}
\NormalTok{ sample <-}\StringTok{ }\NormalTok{population[}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N, n),]}
\NormalTok{ fit[i, ] <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ sample)}\OperatorTok{$}\NormalTok{coefficients}
\NormalTok{\}}

\CommentTok{# compute variance estimates using outcomes}
\KeywordTok{var}\NormalTok{(fit[ ,}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.057089
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(fit[ ,}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03021784
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot histograms of beta_0 estimates}
\KeywordTok{hist}\NormalTok{(fit[ ,}\DecValTok{1}\NormalTok{], }
     \DataTypeTok{main =} \KeywordTok{bquote}\NormalTok{(The }\OperatorTok{~}\StringTok{ }\NormalTok{Distribution  }\OperatorTok{~}\StringTok{ }\NormalTok{of }\OperatorTok{~}\StringTok{ }\DecValTok{10000} \OperatorTok{~}\StringTok{ }\NormalTok{beta[}\DecValTok{0}\NormalTok{] }\OperatorTok{~}\StringTok{ }\NormalTok{Estimates), }
     \DataTypeTok{xlab =} \KeywordTok{bquote}\NormalTok{(}\KeywordTok{hat}\NormalTok{(beta)[}\DecValTok{0}\NormalTok{]), }
     \DataTypeTok{freq =}\NormalTok{ F)}
\CommentTok{# add true distribution to plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x,}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\KeywordTok{sqrt}\NormalTok{(var_b0)), }\DataTypeTok{add =}\NormalTok{ T, }\DataTypeTok{col=}\StringTok{"darkred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-93-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot histograms of beta_1 estimates}
\KeywordTok{hist}\NormalTok{(fit[ ,}\DecValTok{2}\NormalTok{], }
     \DataTypeTok{main =} \KeywordTok{bquote}\NormalTok{(The }\OperatorTok{~}\StringTok{ }\NormalTok{Distribution  }\OperatorTok{~}\StringTok{ }\NormalTok{of }\OperatorTok{~}\StringTok{ }\DecValTok{10000} \OperatorTok{~}\StringTok{ }\NormalTok{beta[}\DecValTok{1}\NormalTok{] }\OperatorTok{~}\StringTok{ }\NormalTok{Estimates), }
     \DataTypeTok{xlab =} \KeywordTok{bquote}\NormalTok{(}\KeywordTok{hat}\NormalTok{(beta)[}\DecValTok{1}\NormalTok{]), }
     \DataTypeTok{freq =}\NormalTok{ F)}

\CommentTok{# add true distribution to plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x,}\FloatTok{3.5}\NormalTok{,}\KeywordTok{sqrt}\NormalTok{(var_b1)), }\DataTypeTok{add =}\NormalTok{ T, }\DataTypeTok{col=}\StringTok{"darkred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-93-2} \end{center}

We are now able to say the following: first, our variance estimates are
in favour of the claims made in Key Concept 4.4 since they come close to
the computed theoretical values. Second, the histograms suggest that the
estimators distributions indeed follow normal distributions which can be
fairly approximated by the respective normal distributions stated in Key
Concept 4.4.

\subsection*{R Simulation Study 2}\label{r-simulation-study-2}
\addcontentsline{toc}{subsection}{R Simulation Study 2}

A further result implied by Key Concept 4.4 is that both estimators are
consistent i.e.~they converge in probability to their true value. This
is since their variances converge to \(0\) as \(n\) increases. We can
check this by repeating the simulation above for an increasing sequence
of sample sizes. This means we no langer assign the sample size but a
\emph{vector} of sample sizes: \texttt{n\ \textless{}-\ c(...)}. Let us
look at the distributions of \(\beta_1\). The idea here is to add an
additional call of \texttt{for()} to the code. This is done in order to
loop over the vector of sample sizes \texttt{n}. For each of the sample
sizes we carry out the same simulation as before but plot a density
estimate for the outcomes of each iteration over \texttt{n}. Notice that
we have to change \texttt{n} to \texttt{n{[}j{]}} in the inner loop to
ensure that the \texttt{j}\(^{th}\) element of \texttt{n} is used. In
the simulation, we use sample sizes \(100, 250, 1000\) and \(3000\).
Consequently we have a total of four distinct simulations using
different sample sizes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# set repetitions and the vector of sample sizes}
\NormalTok{reps <-}\StringTok{ }\DecValTok{1000}
\NormalTok{n <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{3000}\NormalTok{)}

\CommentTok{# initialize the matrix of outcomes}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ reps)}

\CommentTok{# devide the plot panel in a 2-by-2 array}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\NormalTok{#### Loop sampling and plotting ####}

\CommentTok{# outer loop over n}
\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(n)) \{}
  
  \CommentTok{# inner loop: sampling and estimating of the coefficients}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{reps)\{}
\NormalTok{    sample <-}\StringTok{ }\NormalTok{population[}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N, n[j]), ]}
\NormalTok{    fit[i, ] <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ sample)}\OperatorTok{$}\NormalTok{coefficients}
\NormalTok{  \}}
  
  \CommentTok{# draw density estimates}
  \KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(fit[,}\DecValTok{2}\NormalTok{]), }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{,}\FloatTok{4.5}\NormalTok{), }\DataTypeTok{col=}\NormalTok{j, }
       \DataTypeTok{main =} \KeywordTok{paste}\NormalTok{(}\StringTok{"n="}\NormalTok{, n[j]), }\DataTypeTok{xlab =} \KeywordTok{bquote}\NormalTok{(}\KeywordTok{hat}\NormalTok{(beta)[}\DecValTok{1}\NormalTok{]))}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-94-1} \end{center}

We find that, as \(n\) increases, the distribution of \(\hat\beta_1\)
concentrates around its mean, i.e.~its variance decreases. Put
differently, the likelihood of observering estimates close to the true
value of \(\beta_1 = 3.5\) grows as we increase the sample size. The
same behaviour could be observed if we would analyze the distribution of
\(\hat\beta_0\) instead.

\subsection*{R Simulation Study 3}\label{r-simulation-study-3}
\addcontentsline{toc}{subsection}{R Simulation Study 3}

Furthermore, (4.1) reveals that the variance of the OLS estimator for
\(\beta_1\) decreases as the variance of the \(X_i\) increases. In other
words, as we increase the amount of information provided by the
regressor, that is increasing \(Var(X)\), which is used to estimate
\(\beta_1\), we are more confident that the estimate is close to the
true value (i.e. \(Var(\hat\beta_1)\) decreases). We can visualize this
by reproducing figure 4.6 from the book. To do this, we sample \(100\)
observations \((X,Y)\) from a bivariate normal distribution with

\[E(X)=E(Y)=5,\] \[Var(X)=Var(Y)=5\] and \[Cov(X,Y)=4.\]

Formally, this is written down as

\begin{align}
  \begin{pmatrix}
    X \\
    Y \\
  \end{pmatrix}
  \overset{i.i.d.}{\sim} & \ \mathcal{N} 
  \left[
    \begin{pmatrix}
      5 \\
      5 \\
    \end{pmatrix}, \ 
    \begin{pmatrix}
      5 & 4 \\
      4 & 5 \\
    \end{pmatrix}
  \right]. \tag{4.3}
\end{align}

To carry out the random sampling, we make use of the function
\texttt{mvtnorm()} from the package \texttt{MASS} which allows to draw
random samples from multivariate normal distributions, see
\texttt{?mvtnorm}. Next, we use the \texttt{subset()} function to split
the sample into two subsets such that the first set, \texttt{set1},
consists of observations that fulfill the condition
\(\lvert X - \overline{X} \rvert > 1\) and the second set,
\texttt{set2}, includes the remainder of the sample. We then plot both
sets and use different colors to make them distinguishable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the MASS package}
\KeywordTok{library}\NormalTok{(MASS)}

\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}

\CommentTok{# simulate bivarite normal data}
\NormalTok{bvndata <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }
                \DataTypeTok{mu =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{), }
                \DataTypeTok{Sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{4}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{                ) }

\CommentTok{# assign column names / convert to data.frame}
\KeywordTok{colnames}\NormalTok{(bvndata) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\StringTok{"Y"}\NormalTok{)}
\NormalTok{bvndata <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(bvndata)}

\CommentTok{# subset the data}
\NormalTok{set1 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(bvndata, }\KeywordTok{abs}\NormalTok{(}\KeywordTok{mean}\NormalTok{(X) }\OperatorTok{-}\StringTok{ }\NormalTok{X) }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{set2 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(bvndata, }\KeywordTok{abs}\NormalTok{(}\KeywordTok{mean}\NormalTok{(X) }\OperatorTok{-}\StringTok{ }\NormalTok{X) }\OperatorTok{<=}\StringTok{ }\DecValTok{1}\NormalTok{)}

\CommentTok{# plot both data sets}
\KeywordTok{plot}\NormalTok{(set1, }\DataTypeTok{xlab =} \StringTok{"X"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{)}
\KeywordTok{points}\NormalTok{(set2, }\DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-95-1} \end{center}

It is clear that observations that are close to the sample average of
the \(X_i\) have less variance than those that are farther away. Now, if
we were to draw a line as accurately as possible through either of the
two sets it is obvious that choosing the observations indicated by the
black dots, i.e.~using the set of observations which has larger variance
than the blue ones, would result in a more precise line. Now, let us use
OLS to estimate and draw the regression lines for both sets of
observations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate both regression lines}
\NormalTok{lm.set1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ set1)}
\NormalTok{lm.set2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ set2)}

\CommentTok{# add both lines to the plot}
\KeywordTok{abline}\NormalTok{(lm.set1, }\DataTypeTok{col=}\StringTok{"green"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(lm.set2, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-96-1} \end{center}

Evidently, the green regression line does far better in describing data
sampled from the bivariate normal distribution stated in (4.3) than the
red line. This is a nice example why we are interested in a high
variance of the regressor \(X\): more variance in the \(X_i\) means more
information from which the precision of the estimation benefits.

\section{Exercises}\label{exercises-2}

\subsubsection*{1. Class Sizes and Test
Scores}\label{class-sizes-and-test-scores}
\addcontentsline{toc}{subsubsection}{1. Class Sizes and Test Scores}

A researcher wants to analyse the relationship between class size
(measured by the student-teacher ratio) and the pupils' average test
score. Therefore he measures both variables in 10 different classes and
ends up with the following results.

\begin{verbatim}
  <tr>
    <td><b>Class Size</b></td>
    <td>23</td>
    <td>19</td>
    <td>30</td>
    <td>22</td>
    <td>23</td>
    <td>29</td>
    <td>35</td>
    <td>36</td>
    <td>33</td>
    <td>25</td>
  </tr>
  <tr>
    <td><b>Test Score</b></td>
    <td>430</td>
    <td>430</td>
    <td>333</td>
    <td>410</td>
    <td>390</td>
    <td>377</td>
    <td>325</td>
    <td>310</td>
    <td>328</td>
    <td>375</td>
  </tr>
</table>
\end{verbatim}

\textbf{Instructions:}

\begin{itemize}
\item
  Create the vectors cs (the class sizea) and ts (the test scorea),
  containing the obeservations above.
\item
  Draw a scatter plot of the results using plot().
\end{itemize}

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Create both vectors

\# Draw the scatter plot

\# Compute mean, median, variance \& standard deviation of test score

\# Compute the covariance and the correlation coefficient

\begin{verbatim}
<code data-type="solution">
\end{verbatim}

\# Create both vectors cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36,
33, 25) ts \textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328,
375)

\# Draw the scatter plot plot(cs,ts)

\begin{verbatim}
<code data-type="sct">
\end{verbatim}

test\_object(``cs'') test\_object(``ts'')

test\_function(``plot'')

\subsubsection*{2. Mean, Variance, Covariance and
Correlation}\label{mean-variance-covariance-and-correlation}
\addcontentsline{toc}{subsubsection}{2. Mean, Variance, Covariance and
Correlation}

The vectors cs and ts are available in the working environment (you can
check this: type their names to the console and then press enter).

\textbf{Instructions:}

\begin{itemize}
\item
  Compute the mean, the sample variance and the sample standard
  deviation of ts.
\item
  Compute the covariance and the correlation coefficient for ts and cs.
\end{itemize}

Use the R functions presented in this chapter: mean(), sd(), cov(),
cor() and var()

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375)

\begin{verbatim}
<code data-type="sample-code">
\end{verbatim}

\# Compute mean, variance and standard deviation of test scores

\# Compute the covariance and the correlation coefficient

\begin{verbatim}
<code data-type="solution">
\end{verbatim}

mean(ts) sd(ts) var(ts) cov(ts,cs) cor(ts,cs)

\begin{verbatim}
<code data-type="sct">
\end{verbatim}

test\_predefined\_objects(``cs'') test\_predefined\_objects(``ts'')

test\_function(``mean'', args = ``x'')

test\_function(``sd'', args = ``x'')

test\_output\_contains(``cov(ts,cs)'')

test\_output\_contains(``cor(ts,cs)'')

\subsubsection*{3. Simple Linear
Regression}\label{simple-linear-regression}
\addcontentsline{toc}{subsubsection}{3. Simple Linear Regression}

The vectors cs and ts are available in the working environment.

\textbf{Instructions:}

\begin{itemize}
\item
  The function lm() is part of the package AER. Attach it using
  library().
\item
  Use lm() to estimate the regression model
  \[TestScore_i = \beta_0 + \beta_1 STR_i + u_i.\]
\item
  Obtain a stastistical summary of the model.
\end{itemize}

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) \#
attach the package AER

\# estimate the model

\# model summary

 library(AER) lm(cs \textasciitilde{} ts) summary(lm(cs
\textasciitilde{} ts))

 test\_predefined\_objects(c(``ts'',``cs''))
test\_student\_typed(``library(AER)'') test\_output\_contains(``lm(ts
\textasciitilde{} cs)'') test\_output\_contains(``summary(lm(ts
\textasciitilde{} cs))'')

\subsubsection*{4. The Model Object}\label{the-model-object}
\addcontentsline{toc}{subsubsection}{4. The Model Object}

Let's see how a model object is structured.

The vectors cs and ts as well as the model object mod from the previous
exercise are available in your workspace.

\begin{itemize}
\tightlist
\item
  Use class() to learn about the class of the object mod
\item
  mod is an object of type list with named entries. Check this using the
  function is.list().
\item
  See what information you can obtain from mod using names().
\item
  Read out an arbitrary entry of the object mod using the \$ operator.
\end{itemize}

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) \# check the class of `mod'

\begin{verbatim}
# use 'is.list()'


# check entry names of mod using 'names()'


# use the operator '$' on 'mod'
\end{verbatim}

 class(mod) is.list(mod)
mod\(fitted.values  </code>  <code data-type="sct">  test_predefined_objects("mod")  test_function("class", args="x")  test_function("is.list", args="x")  test_student_typed("mod\)``)

\subsubsection*{5. Plotting the Regression
Line}\label{plotting-the-regression-line}
\addcontentsline{toc}{subsubsection}{5. Plotting the Regression Line}

Now add a regression line to the scatter plot from a few exercises
before. You are provided with the code for the scatter plot in script.R

The object mod is available in your working environment.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) \# Add the regression line the
the scatterplot plot(cs,ts) plot(cs,ts) abline(mod)
test\_predefined\_objects(``mod'') test\_function(``plot'')
test\_function(``abline'')

\subsubsection*{6. Summary of a Model
Object}\label{summary-of-a-model-object}
\addcontentsline{toc}{subsubsection}{6. Summary of a Model Object}

Now read out and store some of the information that is contained in the
output produced by summary():

\begin{itemize}
\item
  Assign the output of summary(mod) to the variable `s'
\item
  Check entry names of `s'
\item
  Create a new variable R2 storing the regression's \(R^2\).
\end{itemize}

The object mod is available in your working environment.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) \# Assign the model summary to
the variable `s'

\begin{verbatim}
# Check names of entries in 's'


# Store the regression's R^2 in the variable 'R2'
\end{verbatim}

 s \textless{}- summary(mod) names(s) R2 \textless{}- s\$r.squared
test\_predefined\_objects(``mod'') test\_function(``names'', args =
``x'') test\_object(``s'') test\_object(``R2'')

\subsubsection*{7. Estimated Coefficients}\label{estimated-coefficients}
\addcontentsline{toc}{subsubsection}{7. Estimated Coefficients}

summary() provides information on the statistical significance of the
estimated coefficients.

Extract the named \(2\times4\) matrix with estimated coefficients,
standard errors, \(t\)-statistics and corresponding \(p\)-values from
the model summary s. Save this matrix in an object named coefs.

The objects mod and s are available in your working environment.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) s \textless{}- summary(mod) \#
Save the coefficient matrix to `coefs'

 coefs \textless{}- s\$coefficients test\_predefined\_objects(``s'')
test\_object(``coefs'')

\subsubsection*{8. Dropping the Intercept}\label{dropping-the-intercept}
\addcontentsline{toc}{subsubsection}{8. Dropping the Intercept}

So far, we have done regressions where the model consisted of an
intercept and a single regressor. In this exercise you will learn ways
to specify and to estimate regression models excluding the intercept.

Note that excluding the intercept from a regression model might be a
dodgy practice in some models as this imposes the conditional
expectation function of the dependent variable to be zero if the
regressor is zero.

\begin{itemize}
\tightlist
\item
  Use the help function on lm (?lm) to find out how to specify the
  formula argument for a regression of ts solely on cs, i.e.~a
  regression without intercept.
\item
  Estimate the regression model without intercept and store the result
  in mod\_ni.
\end{itemize}

The vectors cs, ts and the model object mod from previous exercises are
availabe in the working environment.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) s \textless{}- summary(mod) \#
Regress ts solely and cs, store the result in `mod\_ni'

 mod\_ni \textless{}- lm(ts \textasciitilde{} cs - 1) \# or: lm(ts
\textasciitilde{} cs + 0) test\_predefined\_objects(``mod'')

\begin{verbatim}
test_or(
  {
  test_student_typed("ts ~ cs + 0")
},{
  test_student_typed("ts ~ cs - 1")
}
)

test_function("lm")

test_object("mod_ni", eval=F)
\end{verbatim}

\subsubsection*{9. Regression Output: No Constant
Case}\label{regression-output-no-constant-case}
\addcontentsline{toc}{subsubsection}{9. Regression Output: No Constant
Case}

You have estimated a model without intercept. The estimated regression
function is

\[\widehat{TestScore} = \underset{(1.36)}{12.65} \times STR.\]

Convince yourself that everything is as stated above: extract the
coefficient matrix from the summary of mod\_ni and store it in a
variable named coef.

The vectors cs, ts as well as the model object mod\_ni from the previous
exercise are availabe in your working environment.

Remember: an entry of a named list can be accessed with the \$ operator.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) mod\_ni \textless{}- lm(ts
\textasciitilde{} cs - 1) \# Extract the coefficient matrix from the
model's summary and save it to `coef'

 coef \textless{}- summary(mod\_ni)\$coefficients
test\_predefined\_objects(``mod'')
test\_predefined\_objects(``mod\_ni'') test\_object(``coef'')
success\_msg(``Right! Note that there is only one coefficient estimate
(the coefficient on cs) reported by summary(mod\_ni)'')

\subsubsection*{10. Regression Output: No Constant Case ---
Ctd.}\label{regression-output-no-constant-case-ctd.}
\addcontentsline{toc}{subsubsection}{10. Regression Output: No Constant
Case --- Ctd.}

You have estimated a model without intercept. The estimated regression
function is

\[\widehat{TestScore_i} = \underset{(1.36)}{12.65} \times STR_i.\]

The coefficient matrix coef contains the estimated coefficient on
\(STR\), its standard error, the \(t\)-statistic of the significance
test and the corresponding \(p\)-value.

\begin{itemize}
\tightlist
\item
  Print the contents of coef to the console.
\item
  Convince yourself that the reported \(t\)-statistic is correct: use
  the entries of coef to compute the \(t\)-statistic and save it to
  t\_stat.
\end{itemize}

The matrix coef from the previous exercise is availabe in your working
environment.

Hints: X{[}a,b{]} returns the {[}a,b{]} element of a matrix X. The
\(t\)-statistic for this significance test is computed as
\[t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}.\]

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) coef \textless{}-
summary(mod)\$coef \# print the contents of `coef' to the console

\# compute the t-statistic manually

 print(coef) t\_stat \textless{}- coef{[}1,1{]}/coef{[}1,2{]}
test\_predefined\_objects(``coef'') test\_output\_contains(``coef'')
test\_object(``t\_stat'')

\subsubsection*{11. Two Regressions, One
Plot}\label{two-regressions-one-plot}
\addcontentsline{toc}{subsubsection}{11. Two Regressions, One Plot}

The two estimated regression equations are

\[\widehat{TestScore_i} = \underset{(1.36)}{12.65} \times STR_i\]

and

\[\widehat{TestScore_i} = \underset{(23.9606)}{567.4272} \underset{(0.8536)}{-7.1501} \times STR_i.\]

You are provided with the code line plot(cs,ts) which creates a scatter
plot of ts and cs. It must be executed before calling abline()! You may
color the regression lines by using e.g. col = ``red'' or col = ``blue''
as an additional argument to abline() for better distinguishability.

The vectors cs and ts as well as the list objects mod and mod\_ni from
previous exercises are availabe in your working environment.

Generate a scatter plot of the ts and cs and add the estimated
regression lines of mod and mod\_ni.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) mod\_ni \textless{}- lm(ts
\textasciitilde{} cs - 1) \# Plot regression lines for both models
plot(cs, ts)

 plot(cs,ts) abline(mod, col=``blue'') abline(mod\_ni, col=``red'')
test\_predefined\_objects(``mod'')
test\_predefined\_objects(``mod\_ni'') test\_function(``abline'',
index=1) test\_function(``abline'', index=2) success\_msg(``Yep, that
looks good!'')

\subsubsection*{\texorpdfstring{12. \(TSS\) and
\(SSR\)}{12. TSS and SSR}}\label{tss-and-ssr}
\addcontentsline{toc}{subsubsection}{12. \(TSS\) and \(SSR\)}

If graphical inspection does not help, researchers resort to analytic
techniques in order to detect if a model fits the data at hand good or
better than another model.

Let us go back to the simple regression model including an intercept.
The estimated regression line for mod was

\[\widehat{TestScore_i} = 567.43 - 7.15 \times STR_i, \, R^2 = 0.8976, \, SER=15.19.\]

You can check this as mod and the vectors cs and ts are available in
your working environment.

Now, please do the following:

\begin{itemize}
\tightlist
\item
  Compute \(SSR\), the sum of squared residuals, and save it to ssr
\item
  Compute \(TSS\), the total sum of squares, and save it to tss
\end{itemize}

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) \# Compute the SSR and save it
to ssr

\# Compute the TSS and save it to tss

 ssr \textless{}- sum(mod\$residuals\^{}2) tss \textless{}- 9*var(ts) \#
Note: var() computes the unbiased sample variance! =\textgreater{}
Corrected for by multiplying with (n-1) = 9
test\_predefined\_objects(``mod'') test\_object(``ssr'')
test\_object(``tss'')

\subsubsection*{\texorpdfstring{13. The \(R^2\) of a Regression
Model}{13. The R\^{}2 of a Regression Model}}\label{the-r2-of-a-regression-model}
\addcontentsline{toc}{subsubsection}{13. The \(R^2\) of a Regression
Model}

The \(R^2\) for the regression model stored in mod is \(0.8976\). You
can check this by executing summary(mod)\$r.squared in the console
below.

Remember the formula for \(R^2\):

\[R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}\]

The objects mod, tss and ssr from the previous exercise are available in
your working environment.

\begin{itemize}
\tightlist
\item
  Use ssr and tss to compute \(R^2\) manually. \emph{Round} the result
  to four decimal places and save it to R2.
\item
  Use the logical expression == to check whether your result equals the
  value mentioned above.
\end{itemize}

You can round numeric values using the function round(). See ?round.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) ssr \textless{}-
sum(mod\$residuals\^{}2) tss \textless{}- 9*var(ts) \# Compute R\^{}2,
round it and save it to `R2'

\# Check whether your result is correct using the `==' operator

 R2 \textless{}- round(1-ssr/tss,4) R2 == 0.8976
test\_predefined\_objects(``ssr'') test\_predefined\_objects(``tss'')
test\_predefined\_objects(``mod'') test\_object(``R2'')
test\_function(``round'', args=``digits'', eq\_condition=``equal'')
test\_student\_typed(``R2 == 0.8976'', not\_typed\_msg = ``Something's
wrong. Make sure you type \texttt{R2\ ==} followed by the value
mentioned above.'') success\_msg(``Great!'')

\subsubsection*{14. The Standard Error of The
Regression}\label{the-standard-error-of-the-regression}
\addcontentsline{toc}{subsubsection}{14. The Standard Error of The
Regression}

The standard error of the Regression in the simple regression model is
\[SER = \frac{1}{n-2} \sum_{i=1}^n \widehat{u}_i^2 =\sqrt{\frac{SSR}{n-2}}.\]
It mesaures the size of an average residual which is an estimate of the
magnitude of a typical regression error.

\begin{itemize}
\item
  Use summary() to obtain the \(SER\) for the regression of
  \(TestScore\) on \(STR\) stored in the model object mod. Save the
  value in the variable SER.
\item
  Use SER to compute the model's \(SSR\) and store it in SSR.
\item
  Check that SSR is indeed the model's \(SSR\) by comparing SSR to the
  result of sum(mod\$residuals\^{}2)
\end{itemize}

The model object mod and the vectors cs and ts are available in your
workspace.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) \# Obtain the SER using
`summary()' and save the value to `SER'

\# Compute the model's SSR and save it to `SSR'

\# Do the comparison

 SER \textless{}-
summary(mod)\(sigma  SSR <- SER^2*(length(ts)-2)  SSR == sum(mod\)residuals\^{}2)

 test\_object(``SER'') test\_object(``SSR'') test\_student\_typed(``SSR
== sum(mod\$residuals\^{}2)'') success\_msg(``Nice!'')

\subsubsection*{15. The Estimated Covariance
Matrix}\label{the-estimated-covariance-matrix}
\addcontentsline{toc}{subsubsection}{15. The Estimated Covariance
Matrix}

As has been discussed in Chapter 4.4, the OLS estimators
\(\widehat{\beta}_0\) and \(\widehat{\beta}_1\) can be considered random
variables since they are functions of the random error term. For two or
more random variables, the covariances and variances are summarized by
their covariance matrix. Taking the square root of the diagonal elements
of the model's estimated covariance matrix obtains the standard errors
of \(\widehat{\beta}_0\) and \(\widehat{\beta}_1\).

summary() computes an estimate of this matrix. The respective entry is
called cov.unscaled.

\begin{itemize}
\item
  Use summary() to obtain the covariance matrix estimate for the
  regression of \(TestScore\) on \(STR\) stored in the model object mod.
  Save the matrix to cov\_matrix.
\item
  Obtain the diagonal elements of cov\_matrix, compute their square root
  and assign the result to the variable SEs.
\end{itemize}

The model object mod is available in your workspace.

Hint: diag(A) returns the diagonal elements of the matrix A.

 cs \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) ts
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(ts \textasciitilde{} cs) \# obtain the matrix and save
it to `cov\_matrix'

\# compute the standard errors and assign them to the vector `SEs'

 cov\_matrix \textless{}- summary(mod)\$cov.unscaled SEs \textless{}-
sqrt(diag(cov\_matrix)) test\_object(``cov\_matrix'')
test\_object(``SEs'')

\chapter{Hypothesis Tests and Confidence Intervals in the Simple Linear
Regression
Model}\label{hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model}

In this chapter, we continue with the treatment of the simple linear
regression model. The following subsction discuss how we may use our
knowledge about the sampling distribution of the OLS estimator in order
to make statements regarding its uncertainty. These subsections cover
the following topics:

\begin{itemize}
\item
  Testing Hypotheses about regression coefficients
\item
  Confidence intervals for regression coefficients
\item
  Regression when X is a dummy variable
\item
  Heteroskedasticity and Homoskedasticity
\end{itemize}

\section{\texorpdfstring{Testing Two-Sided Hypotheses Concerning
\(\beta_1\)}{Testing Two-Sided Hypotheses Concerning \textbackslash{}beta\_1}}\label{testing-two-sided-hypotheses-concerning-beta_1}

Using the fact that \(\hat{\beta}_1\) is approximately normal
distributed in large samples (see \protect\hyperlink{tsdotoe}{Key
Concept 4.4}), testing hypotheses about the true value \(\beta_1\) can
be done with the same approach as discussed in chapter 3.2.

Key Concept 5.1

General Form of the \(t\)-Statistic

Remember from chapter 3 that a general \(t\)-statistic has the form

\[
  t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.
\]

Key Concept 5.2

Testing Hyotheses about \(\beta_1\)

For testing the hypothesis \(H_0: \beta_1 = \beta_{1,0}\), we need to
perform the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the standard error of \(\hat{\beta}_1\), \(SE(\hat{\beta}_1)\)
\end{enumerate}

\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Compute the \(t\)-statistic
\end{enumerate}

\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) }. \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Now, given a two sided alternative (\(H_1:\beta_1 \neq \beta_{1,0}\))
  we reject at the \(5\%\) level if \(|t^{act}| > 1.96\) or,
  equivalently, if the \(p\)-value is less than \(0.05\).\\
  Recall the definition of the \(p\)-value:
\end{enumerate}

\begin{align}
    p \text{-value} =& \, \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| > \left|        \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] \\
    =& \, \text{Pr}_{H_0} (|t| > |t^{act}|) \\
    =& \, 2 \cdot \Phi(-|t^{act}|)
  \end{align}

~~~~~~ The last equality holds due to the normal approximation for large
samples.

Consider again the OLS regression stored in \texttt{linear\_model} from
Chapter 4 that gave us the regression line

\[ \widehat{TestScore} \ = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times STR \ , \ R^2=0.051 \ , \ SER=18.6. \]

For testing a hypothesis about the slope parameter (the coefficient on
\(STR\)), we need \(SE(\hat{\beta}_1)\), the standard error of the
respective point estimator. As common in the literature, standard errors
are presented in parantheses below the point estimates.

As can be witnessed in Key Concept 5.1 it is rather cumbersome to
compute the standard error and thus the \(t\)-statistic by hand. The
question You should be asking Yourself right now is obvious: can we
obtain these values with minimum effort using R? Yes, we can. Let us
first use \texttt{summary()} to get a summary on the estimated
coefficients in \texttt{linear\_model}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print the summary of coefficients to the console}
\KeywordTok{summary}\NormalTok{(linear_model)}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Estimate Std. Error   t value      Pr(>|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## STR          -2.279808  0.4798255 -4.751327  2.783308e-06
\end{verbatim}

When looking at the second column of the coefficients' summary, we
discover values for \(SE(\hat\beta_0)\) and \(SE(\hat\beta_1)\). Also,
in the third column, named \texttt{t\ value}, we find \(t\)-statistics
\(t^{act}\) suitable for tests of the individual hypotheses
\(H_0: \beta_0=0\) and \(H_0: \beta_1=0\). Furthermore, the output
provides us with \(p\)-values corresponding to both tests against the
two-sided alternatives \(H_1:\beta_0\neq0\) respectively
\(H_1:\beta_1\neq0\) in the fourth column of the table.

Let us have a closer look at the test of

\[H_0: \beta_1=0 \ \ \ vs. \ \ \ H_1: \beta_1 \neq 0.\]

Using our revisited knowledge about \(t\)-statistics we find that

\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75. \]

What does this tell us about the significance of the estimated
coefficient? We reject the null hypothesis at the \(5\%\) level of
significance since \(|t^{act}| > 1.96\) that is the observed test
statistic falls into the region of rejection. Or, alternatively and
leading to the same result, we have
\(p\text{-value} = 2.78*10^{-6} < 0.05\). We conclude that the
coefficient is significantly different from zero. With other words, our
analysis provides evidence that the clase size \emph{has an influence}
on the students test scores. We say that \(\beta_1\) is significantly
different from \(0\) at the level of \(5\%\).

Note that, although the difference is negligible in the present case as
we will see later, \texttt{summary()} does not perform the normal
approximation but calculates \(p\)-values using the appropriate
\(t\)-distribution instead. Generally, the degrees of freedom are
determined in the following manner:

\[ \text{DF} = n - k - 1 \]

where \(n\) is the number of observations used to estimate the model and
\(k\) is the number of regressors, excluding the intercept. In our case,
we have \(n=420\) observations and the only regressor is \(STR\) so
\(k=1\). A sleek way to determine the model degress of freedom using R
is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# determine degrees of freedom}
\NormalTok{linear_model}\OperatorTok{$}\NormalTok{df.residual}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 418
\end{verbatim}

Hence, for the sampling distribution of \(\hat\beta_1\) we have

\[ \hat\beta_1 \sim t_{418}\] such that the \(p\)-value for a two-sided
significance test can be obtained by executing the following code:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{pt}\NormalTok{(}\OperatorTok{-}\FloatTok{4.751327}\NormalTok{, }\DataTypeTok{df =} \DecValTok{418}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.78331e-06
\end{verbatim}

The result is very close to the value provided by \texttt{summary}.
However since \(n\) is sufficiently large one could just as well use the
standard normal density to compute the \(p\)-value:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\FloatTok{4.751327}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.02086e-06
\end{verbatim}

The difference is indeed negligible. These findings tell us that, if
\(H_0: \beta_1 = 0\) is true and we were to repeat the whole process of
gathering observations and estimating the model, chances of observing a
\(\hat\beta_1 \geq |-4.75|\) are roughly \(1:359285\) --- so higher
chances than winning the lottory next saturday but still very unlikely!

Using R we may visualise how such a statement is made when using the
normal approximation. This reflects the principles depicted in figure
5.1 in the book. Do not let the following code chunk deter You: the code
is somewhat longer than the usual examples and looks unappealing but
there is \textbf{a lot} of repetition since color shadings and
annotations are added on both tails of the normal distribution. We
recommend You to execute the code step by step in order to see how the
graph is augmented with the annotations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot the standard normal on the domain [-6,6]}
\NormalTok{t <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{,}\FloatTok{0.01}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ t, }
     \DataTypeTok{y =} \KeywordTok{dnorm}\NormalTok{(t, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
     \DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{yaxs =} \StringTok{"i"}\NormalTok{, }
     \DataTypeTok{axes =}\NormalTok{ F, }
     \DataTypeTok{ylab =} \StringTok{""}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Calculating the p-Value of a Two-Sided Test When t^act = -4.75"}\NormalTok{, }
     \DataTypeTok{cex.lab =} \FloatTok{0.7}
\NormalTok{     )}

\NormalTok{tact <-}\StringTok{ }\OperatorTok{-}\FloatTok{4.75}

\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\OperatorTok{-}\FloatTok{1.96}\NormalTok{,}\FloatTok{1.96}\NormalTok{,}\OperatorTok{-}\NormalTok{tact,tact), }\DataTypeTok{cex.axis=}\FloatTok{0.7}\NormalTok{)}

\CommentTok{# Shade the critical regions using polygon()}

\NormalTok{## critical region in left tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{,}\OperatorTok{-}\FloatTok{1.96}\NormalTok{,}\FloatTok{0.01}\NormalTok{),}\OperatorTok{-}\FloatTok{1.96}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{,}\OperatorTok{-}\FloatTok{1.96}\NormalTok{,}\FloatTok{0.01}\NormalTok{)),}\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'orange'}
\NormalTok{        )}

\NormalTok{## critical region in right tail}

\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.96}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\FloatTok{1.96}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{6}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\FloatTok{1.96}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'orange'}
\NormalTok{        )}

\CommentTok{# Add arrows and texts indicating critical regions and the p-value}
\KeywordTok{arrows}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\OperatorTok{-}\FloatTok{2.5}\NormalTok{, }\FloatTok{0.02}\NormalTok{, }\DataTypeTok{length =} \FloatTok{0.1}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(}\FloatTok{3.5}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\FloatTok{0.02}\NormalTok{, }\DataTypeTok{length =} \FloatTok{0.1}\NormalTok{)}

\KeywordTok{arrows}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{, }\FloatTok{0.16}\NormalTok{, }\OperatorTok{-}\FloatTok{4.75}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DataTypeTok{length =} \FloatTok{0.1}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FloatTok{0.16}\NormalTok{, }\FloatTok{4.75}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DataTypeTok{length =} \FloatTok{0.1}\NormalTok{)}

\KeywordTok{text}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{,}\FloatTok{0.22}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{paste}\NormalTok{(}\StringTok{"0.025="}\NormalTok{,}\KeywordTok{expression}\NormalTok{(alpha),}\StringTok{"/2"}\NormalTok{,}\DataTypeTok{sep =} \StringTok{""}\NormalTok{), }\DataTypeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{3.5}\NormalTok{,}\FloatTok{0.22}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{paste}\NormalTok{(}\StringTok{"0.025="}\NormalTok{,}\KeywordTok{expression}\NormalTok{(alpha),}\StringTok{"/2"}\NormalTok{,}\DataTypeTok{sep =} \StringTok{""}\NormalTok{), }\DataTypeTok{cex =} \FloatTok{0.7}\NormalTok{)}

\KeywordTok{text}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\FloatTok{0.18}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"-|"}\NormalTok{,t[act],}\StringTok{"|"}\NormalTok{)), }\DataTypeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{5}\NormalTok{,}\FloatTok{0.18}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"|"}\NormalTok{,t[act],}\StringTok{"|"}\NormalTok{)), }\DataTypeTok{cex =} \FloatTok{0.7}\NormalTok{)}

\CommentTok{# Add ticks indicating critical values at the 0.05-level, t^act and -t^act }
\KeywordTok{rug}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.96}\NormalTok{,}\FloatTok{1.96}\NormalTok{), }\DataTypeTok{ticksize  =} \FloatTok{0.145}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{rug}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{tact,tact), }\DataTypeTok{ticksize  =} \OperatorTok{-}\FloatTok{0.0451}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-101-1} \end{center}

The \(p\)-Value is the area under the curve to left of \(-4.75\) plus
the area under the curve to the right of \(4.75\). As we already know
from the calculations above, this value is very small.

\section{Confidence Intervals for Regression
Coefficients}\label{confidence-intervals-for-regression-coefficients}

As we already know, estimates of the regression coefficients \(\beta_0\)
and \(\beta_1\) are afflicted with sampling uncertainty, see chapter 4.
Therefore, we will \emph{never} estimate the exact true value of these
parameters from sample data in an empirical application. However, we may
construct confidence intervals for the intercept and the slope
parameter.

A \(95\%\) confidence interval for \(\beta_i\) has two equivalent
definitions:

\begin{itemize}
\tightlist
\item
  The interval is the set of values for which a hypothesis test to the
  level of \(5\%\) cannot be rejected.
\item
  The interval has a probability of \(95\%\) to contain the true value
  of \(\beta_i\). So in \(95\%\) of all samples that could be drawn, the
  confidence interval will cover the true value of \(\beta_i\).
\end{itemize}

We also say that the interval has a confidence level of \(95\%\). The
idea is summarized in Key Concept 5.3.

Key Concept 5.3

A Confidence Interval for \(\beta_i\)

Imagine You could draw all possible random samples of given size. The
interval that contains the true value \(\beta_i\) in \(95\%\) of all
samples is given by the expression

\[ \text{KI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]

Equivalently, this interval can be seen as the set of null hypotheses
for which a \(5\%\) two-sided hypothesis test does not reject.

\subsection*{R Simulation Study 5.1}\label{r-simulation-study-5.1}
\addcontentsline{toc}{subsection}{R Simulation Study 5.1}

To get a better understanding of confidence intervals we will conduct
another simulation study. For now, assume that we are confronted with
the following sample of \(n=100\) observations on a single variable
\(Y\) where

\[ Y_i \overset{i.i.d}{\sim} N(5,25) \ \ \forall \ i = 1, \dots, 100.\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}

\CommentTok{# generate and plot the sample data}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }
           \DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }
           \DataTypeTok{sd =}\DecValTok{5}
\NormalTok{           )}

\KeywordTok{plot}\NormalTok{(Y, }
     \DataTypeTok{pch=}\DecValTok{19}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-102-1} \end{center}

We assume that the data is generated by the model

\[ Y_i = \mu + \epsilon_i \]

where \(\mu\) is the unknown constant and we know that
\(\epsilon_i \overset{i.i.d.}{\sim} N(0,25)\). In this model, the OLS
estimator for \(\mu\) is given by

\[ \hat\mu = \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i \]

(try to verify this!) i.e.~the sample average of the \(Y_i\). It further
holds that

\[ SE(\hat\mu) = \frac{\sigma_{\epsilon}}{\sqrt{n}} = \frac{5}{\sqrt{100}}. \]

A large sample \(95\%\) confidence interval for \(\mu\) is then given by

\begin{equation} 
KI^{\mu}_{0.95} = \left[\hat\mu - 1.96 \times \frac{5}{\sqrt{100}} \ , \ \hat\mu + 1.96 \times \frac{5}{\sqrt{100}}  \right]. \label{eq:KI}
\end{equation}

It is fairly easy to compute this interval in R by hand. The following
code chunck generates a named vector containing the interval bounds:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind}\NormalTok{(}
  \DataTypeTok{CIlower =} \KeywordTok{mean}\NormalTok{(Y) }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{10}\NormalTok{, }
  \DataTypeTok{CIupper =} \KeywordTok{mean}\NormalTok{(Y) }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{10} 
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       CIlower  CIupper
## [1,] 4.502625 6.462625
\end{verbatim}

Nowing that \(\mu = 5\) we see that our example covers the true value
for the present sample. As opposed to real world examples, we can use R
to get a better understanding of confidence intervals by repeatedly
sampling data, estimating \(\mu\) and computing the confidence interval
for \(\mu\) as in \eqref{eq:KI}.

The procedure is as follows:

\begin{itemize}
\tightlist
\item
  We initialize the vectors \texttt{lower} and \texttt{upper} in which
  the simulated interval boundaries are to be saved. We want to simulate
  \(10000\) intervals so both vectors are set to have this length.
\item
  We use a \texttt{for()} loop to sample \(100\) observations from the
  \(N(5,25)\) distribution and compute \(\hat\mu\) as well as the
  boundaries of the confidence interval in every iteration of the loop.
\item
  At last we join \texttt{lower} and \texttt{upper} in an array.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# initialize vectors of lower and upper interval boundaries}
\NormalTok{lower <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}
\NormalTok{upper <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}

\CommentTok{# loop sampling / estimation / CI}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
\NormalTok{  Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =}\DecValTok{5}\NormalTok{)}
\NormalTok{  lower[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Y) }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{10}
\NormalTok{  upper[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Y) }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{10}
\NormalTok{\}}

\CommentTok{# join vectors of interval boundaries}
\NormalTok{CIs <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(lower, upper)}
\end{Highlighting}
\end{Shaded}

According to Key Concept 5.3 we expect that the fraction of the
\(10000\) simulated intervals saved in the array \texttt{CIs} that
contain the true value \(\mu=5\) should be roughly \(95\%\). We can
check this using logical operators.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(CIs[,}\DecValTok{1}\NormalTok{] }\OperatorTok{<=}\StringTok{ }\DecValTok{5} \OperatorTok{&}\StringTok{ }\DecValTok{5} \OperatorTok{<=}\StringTok{ }\NormalTok{CIs[,}\DecValTok{2}\NormalTok{])}\OperatorTok{/}\DecValTok{10000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9487
\end{verbatim}

The simulation shows that the fraction of intervals covering \(\mu=5\),
i.e.~those intervals for which \(H_0: \mu = 5\) cannot be rejected is
close to the theoretical value of \(95\%\).

Let us draw a plot of the first \(100\) simulated confidence intervals
and indicate those which \emph{do not} cover the true value of \(\mu\).
We do this by adding horizonal lines representing the confidence
intervals on top of each other.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# identify intervals not covering mu}
\CommentTok{# (4 intervals out of 100)}
\NormalTok{ID <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\OperatorTok{!}\NormalTok{(CIs[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OperatorTok{<=}\StringTok{ }\DecValTok{5} \OperatorTok{&}\StringTok{ }\DecValTok{5} \OperatorTok{<=}\StringTok{ }\NormalTok{CIs[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{]))}

\CommentTok{# initialize the plot}
\KeywordTok{plot}\NormalTok{(}\DecValTok{0}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{7}\NormalTok{), }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{), }
     \DataTypeTok{ylab =} \StringTok{"Sample"}\NormalTok{, }
     \DataTypeTok{xlab =} \KeywordTok{expression}\NormalTok{(mu), }
     \DataTypeTok{main =} \StringTok{"Confidence Intervals: Correct H0"}\NormalTok{)}

\CommentTok{# setup color vector}
\NormalTok{colors <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{gray}\NormalTok{(}\FloatTok{0.6}\NormalTok{), }\DecValTok{100}\NormalTok{)}
\NormalTok{colors[ID] <-}\StringTok{ "red"}

\CommentTok{# draw reference line at mu=5}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\DecValTok{5}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}

\CommentTok{# add horizontal bars representing the CIs}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{) \{}
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{c}\NormalTok{(CIs[j,}\DecValTok{1}\NormalTok{], CIs[j,}\DecValTok{2}\NormalTok{]), }
        \KeywordTok{c}\NormalTok{(j,j), }
        \DataTypeTok{col =}\NormalTok{ colors[j], }
        \DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-106-1} \end{center}

We find that for the first 100 samples, the true null hypthesis is
rejected in four cases so these intervals do not cover \(\mu=5\). We
have indicated the intervals which lead to a rejection of the true null
hypothesis by red color.

Let us now turn back to the example of test scores and class sizes.The
regression model from chapter 4 is stored in \texttt{linear\_model}. An
easy way to get \(95\%\) confidence intervals for \(\beta_0\) and
\(\beta_1\), the coefficients on \texttt{(intercept)} and \texttt{STR},
is to use the function \texttt{confint()}. We only have to provide a
fitted model object as the argument \texttt{object} to this function.
The confidence level is set to \(95\%\) by default but can be modified
by setting the argument \texttt{level}, see \texttt{?confint}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(}\DataTypeTok{object =}\NormalTok{ linear_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 2.5 %     97.5 %
## (Intercept) 680.32312 717.542775
## STR          -3.22298  -1.336636
\end{verbatim}

Let us check if the calculation is done as we expect it to be. For
\(\beta_1\), that is the coefficient on \texttt{STR}, according to the
formula presented above the interval borders are computed as

\[  -2.279808 \pm 1.96 \times 0.4798255 \, \Rightarrow \, \text{KI}_{0.95}^{\beta_1} = \left[ -3.22, -1.34 \right]  \]

so this actually leads to the same interval. Obviously, this interval
\emph{does not} contain the value zero what, as we have already seen in
the previous section, leads to rejection of the null hypothesis
\(\beta_{1,0} = 0\).

\section{Regression when X is a Binary
Variable}\label{regression-when-x-is-a-binary-variable}

Instead of using a continuous regressor \(X\), we might be interested in
running the regression

\[ Y_i = \beta_0 + \beta_1 D_i + u_i \tag{5.2} \]

where \(D_i\) is binary variable, a so-called \emph{dummy variable}. For
example, we may define \(D_i\) in the following way:

\[ D_i = \begin{cases}
        1 \ \ \text{if $STR$ in $i^{th}$ school district < 20} \\
        0 \ \ \text{if $STR$ in $i^{th}$ school district $\geq$ 20} \\
      \end{cases} \tag{5.3} \]

The regression model now is

\[ TestScore_i = \beta_0 + \beta_1 D_i + u_i. \tag{5.4} \]

Let us see how these data look like in a scatter plot:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create the dummy variable as defined above using a for loop}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(CASchools)) \{}
  \ControlFlowTok{if}\NormalTok{ (CASchools}\OperatorTok{$}\NormalTok{STR[i] }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{) \{ }
\NormalTok{    CASchools}\OperatorTok{$}\NormalTok{D[i] <-}\StringTok{ }\DecValTok{1}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      CASchools}\OperatorTok{$}\NormalTok{D[i] <-}\StringTok{ }\DecValTok{0}
\NormalTok{    \}}
\NormalTok{  \}}

\CommentTok{# Plot the data}
\KeywordTok{plot}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{D, CASchools}\OperatorTok{$}\NormalTok{score,          }\CommentTok{# provide the data to be ploted}
     \DataTypeTok{pch=}\DecValTok{20}\NormalTok{,                                }\CommentTok{# use filled circles as plot symbols}
     \DataTypeTok{cex=}\FloatTok{0.5}\NormalTok{,                               }\CommentTok{# set size of plot symbols to 0.5}
     \DataTypeTok{col=}\StringTok{"Steelblue"}\NormalTok{,                       }\CommentTok{# set the symbols' color to "Steelblue"}
     \DataTypeTok{xlab=}\KeywordTok{expression}\NormalTok{(D[i]),                 }\CommentTok{# Set title and axis names}
     \DataTypeTok{ylab=}\StringTok{"Test Score"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Dummy Regression"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-109-1} \end{center}

We see that with \(D\) as the regressor, it is not useful to think of
\(\beta_1\) as a slope parameter since \(D_i \in \{0,1\}\), i.e.~we only
observe two discrete values instead of a continuum of regressor values
lying (in some range) on the real line. Simply put, there is no
continuous line depicting the conditional expectation function
\(E(TestScore_i | D_i)\) since this function is solely defined for
\(X\)-positions \(0\) and \(1\).

Therefore, the interpretation of the coefficients in our regression
model is as follows:

\begin{itemize}
\item
  \(E(Y_i | D_i = 0) = \beta_0\) so \(\beta_0\) is the expected test
  score in districts where \(D_i=0\) i.e.~where \(STR\) is below \(20\).
\item
  \(E(Y_i | D_i = 1) = \beta_0 + \beta_1\) or, using the result above,
  \(\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)\). Thus, \(\beta_1\)
  is the difference in group specific expectations, i.e.~the difference
  in expected test score between districts with \(STR < 20\) and those
  with \(STR \geq 20\).
\end{itemize}

We will now use R to estimate the dummy regression model as defined by
equations (5.2) - (5.3) .

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the dummy regression model}
\NormalTok{dummy_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{D, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\KeywordTok{summary}\NormalTok{(dummy_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ D, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -50.496 -14.029  -0.346  12.884  49.504 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  650.077      1.393 466.666  < 2e-16 ***
## D              7.169      1.847   3.882  0.00012 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 18.74 on 418 degrees of freedom
## Multiple R-squared:  0.0348, Adjusted R-squared:  0.0325 
## F-statistic: 15.07 on 1 and 418 DF,  p-value: 0.0001202
\end{verbatim}

One can see that the expected test score in districts with \(STR < 20\)
(\(D_i = 1\)) is predicted to be \(650.1 + 7.17 = 657.27\) while
districs with \(STR \geq 20\) (\(D_i = 0\)) are expected to have an
average test score of only \(650.1\).

Group specific predictions can be added to the plot by execution of the
following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add group specific predictions to the plot}
\KeywordTok{points}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CASchools}\OperatorTok{$}\NormalTok{D, }
       \DataTypeTok{y =} \KeywordTok{predict}\NormalTok{(dummy_model), }
       \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
       \DataTypeTok{pch =} \DecValTok{20}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

Here we use the function \texttt{predict()} to obtain estimates of the
group specific means. The red dots represent these sample group
averages. Accordingly, \(\hat{\beta}_1 = 7.17\) can be seen as the
difference in group averages.

By inspection of the output generated with
\texttt{summary(dummy\_model)} we may also find an answer to the
question whether there is a statistically significant difference in
group means. This in turn would support the hypothesis that students
perform differently when they are taught in small classes rather than in
large groups. We can assess this by a two-tailed test of the hypothesis
\(H_0: \beta_1 = 0\). Conviniently the \(t\)-statistic and the
corresponding \(p\)-value for this test are computed defaultly by
\texttt{summary()}!

Since \texttt{t\ value} \(= 3.88 > 1.96\) we reject the null hypothesis
at the \(5\%\) level of significance. The same conclusion can be made
when using the \(p\)-value which reports significance to the
\(0.00012\%\) level.

As done with \texttt{linear\_model}, we may alternatively use the
\texttt{confint()} function to compute a \(95\%\) confidence interval
for the true difference in means and see if the hypothesised value is an
element of this confidence set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confidence intervals for coefficients in the dummy regression}
\KeywordTok{confint}\NormalTok{(dummy_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  2.5 %    97.5 %
## (Intercept) 647.338594 652.81500
## D             3.539562  10.79931
\end{verbatim}

We reject the hypothesis that there is no difference between group means
at the \(5\%\) significance level since \(\beta_{1,0} = 0\) lies outside
of \([3.54, 10.8]\), the \(95\%\) confidence interval for the
coefficient on \(D\).

\section{Heteroskedasticity and
Homoskedasticity}\label{heteroskedasticity-and-homoskedasticity}

All inference made in the previous chapters relies on the assumption
that the error variance does not vary as regressor values change. But
this will not necessarily be the case in most empirical applications.

Key Concept 5.4

Heteroskedasticity and Homoskedasticity

\begin{itemize}
\item
  We say that the error term of our regression model is homoskedastic if
  the variance of the conditional distribution of \(u_i\) given \(X_i\),
  \(Var(u_i|X_i=x)\), is constant \emph{for all} observations in our
  sample \[ \text{Var}(u_i|X_i=x) = \sigma^2 \ \forall \ i=1,\dots,n. \]
\item
  If instead there is dependence of the conditional variance of \(u_i\)
  on \(X_i\), the error term is said to be heteroskedastic. We then
  write
  \[ \text{Var}(u_i|X_i=x) = \sigma_i^2 \ \forall \ i=1,\dots,n. \]
\item
  Homoskedasticity is a \emph{special case} of heteroskedasticity.
\end{itemize}

For a better understanding of heteroskedasticity, we generate some
bivariate heteroskedastic data, estimate a linear regression model and
then use boxplots to depict the conditional distributions of the
residuals.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load scales package for custom color opacities}
\KeywordTok{library}\NormalTok{(scales)}

\CommentTok{# Genrate some heteroskedastic data}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
\NormalTok{x <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{25}\NormalTok{),}\DataTypeTok{each=}\DecValTok{25}\NormalTok{)}
\NormalTok{e <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{12}\NormalTok{)                }
\NormalTok{i <-}\StringTok{ }\KeywordTok{order}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DataTypeTok{max=}\KeywordTok{dnorm}\NormalTok{(e, }\DataTypeTok{sd=}\DecValTok{12}\NormalTok{))) }
\NormalTok{y <-}\StringTok{ }\DecValTok{720} \OperatorTok{-}\StringTok{ }\FloatTok{3.3} \OperatorTok{*}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{e[}\KeywordTok{rev}\NormalTok{(i)]}

\CommentTok{# Estimate the model }
\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x)}

\CommentTok{# Plot the data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }
     \DataTypeTok{y=}\NormalTok{y, }
     \DataTypeTok{main=}\StringTok{"An Example of Heteroskedasticity"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Student-Teacher Ratio"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test Score"}\NormalTok{,}
     \DataTypeTok{cex =} \FloatTok{0.5}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{27}\NormalTok{), }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{600}\NormalTok{,}\DecValTok{710}\NormalTok{)}
\NormalTok{     )}

\CommentTok{# Add the regression line to the plot}
\KeywordTok{abline}\NormalTok{(mod, }\DataTypeTok{col=}\StringTok{"darkred"}\NormalTok{)}

\CommentTok{# Add boxplots to the plot}
\KeywordTok{boxplot}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }
        \DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }
        \DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{25}\NormalTok{), }
        \DataTypeTok{col =} \KeywordTok{alpha}\NormalTok{(}\StringTok{"gray"}\NormalTok{, }\FloatTok{0.4}\NormalTok{), }
        \DataTypeTok{border =} \StringTok{"black"}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-113-1} \end{center}

For this artificial data it is straightforward to see that we face
unequal conditional error variances. Specifically, we observe that the
variance in test scores (and therefore the variance of the errors
committed) \emph{increases} with the student teacher ratio.

\subsection*{A Real-World Example for
Heteroskedasticity}\label{a-real-world-example-for-heteroskedasticity}
\addcontentsline{toc}{subsection}{A Real-World Example for
Heteroskedasticity}

Think about the economic value of education: if there would not be an
expected economic value-added to receiving education at university, You
probably would not be reading this script right now. A starting point to
empirical verification of such a relation exists is to have data on
individuals that are in an employment relationship. More precisely, we
need data on wages and education in order to work with a model like

\[ wage_i = \beta_0 + \beta_1 \cdot education_i + u_i. \]

What can be presumed about this relation? It is likely that, on average,
higher educated workers earn more money than workers with less education
so we expect to estimate an upward sloping regression line. Also it
seems plausible that workers with better education are more likely to
meet the requirements for the well-paid jobs. However, workers with low
education will have no shot at those well-paid jobs. Therefore it seems
plausible that the distribution of earnings spreads out as education
increases. In other words: we expect that there is heteroskedasticity!

To verify this empirically we may use real data on hourly earnings and
the number of years of education of employees. Such data can be found in
\texttt{CPSSWEducation}. This data set is part of the package
\texttt{AER} and stems from the Current Population Survey (CPS) which is
conducted periodically by the \href{http://www.bls.gov/}{Bureau of Labor
Statistics} in the US.

The subsequent code chunks demonstrate how to load the data into R and
how to produce a plot in the fashion of figure 5.3 in the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package and attach data}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(}\StringTok{"CPSSWEducation"}\NormalTok{)}
\KeywordTok{attach}\NormalTok{(CPSSWEducation)}

\CommentTok{# get an overview}
\KeywordTok{summary}\NormalTok{(CPSSWEducation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age          gender        earnings        education    
##  Min.   :29.0   female:1202   Min.   : 2.137   Min.   : 6.00  
##  1st Qu.:29.0   male  :1748   1st Qu.:10.577   1st Qu.:12.00  
##  Median :29.0                 Median :14.615   Median :13.00  
##  Mean   :29.5                 Mean   :16.743   Mean   :13.55  
##  3rd Qu.:30.0                 3rd Qu.:20.192   3rd Qu.:16.00  
##  Max.   :30.0                 Max.   :97.500   Max.   :18.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate a simple regression model}
\NormalTok{labor_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(earnings }\OperatorTok{~}\StringTok{ }\NormalTok{education)}

\CommentTok{# plot observations and add the regression line}
\KeywordTok{plot}\NormalTok{(education, }
\NormalTok{     earnings, }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{150}\NormalTok{)}
\NormalTok{     )}

\KeywordTok{abline}\NormalTok{(labor_model, }\DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-114-1} \end{center}

From inspecting the plot we can tell that the mean of the distribution
of earnings increases with the level of education. This is also
suggested by formal analysis: the estimated regression model stored in
\texttt{labor\_mod} asserts that there is a positive relation between
years of education and earnings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{labor_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = earnings ~ education)
## 
## Coefficients:
## (Intercept)    education  
##      -3.134        1.467
\end{verbatim}

The estimated regression equation states that, on average, an additional
year of education increases a workers hourly earnings by about
\(\$ 1.47\). Once more we use \texttt{confint()} to obtain a \(95\%\)
confidence interval for both regression coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(labor_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 2.5 %    97.5 %
## (Intercept) -5.015248 -1.253495
## education    1.330098  1.603753
\end{verbatim}

Since the interval is \([1.33, 1.60]\) we can reject the hypothesis that
the coefficient on \texttt{education} is zero at the \(5\%\) level.

What is more, the plot indicates that there is heteroskedasticity: if we
assume the regression line to be a reasonably good representation of the
conditional mean function \(E(earnings_i\vert education_i)\), the
dispersion of hourly earnings around that function cleary increases with
the level of education, i.e.~the variance of the distribution of
earnings increases. In other words: the variance of the residuals
increases with the years of education so that the regression errors are
heteroskedastic. This example makes a case that it is doubtful to assume
homoskedasticity in many economic applications.

\subsection*{Should We Care About
Heteroskedasticity?}\label{should-we-care-about-heteroskedasticity}
\addcontentsline{toc}{subsection}{Should We Care About
Heteroskedasticity?}

To answer this question, let us see how the variance of \(\hat\beta_1\)
is computed under the assumption of homoskedasticity. In this case we
have

\[ \sigma^2_{\hat\beta_1} = \frac{\sigma^2_u}{n \cdot \sigma^2_X} \tag{5.5} \]

which is a simplified version of the general equation (4.1) presented in
Key Concept 4.4. See Appendix 5.1 of the book for details on the
derivation. The \texttt{summary()} function in R estimates (5.5) by

\[ \overset{\sim}{\sigma}^2_{\hat\beta_1} = \frac{SER^2}{\sum_{i=1}^n (X_i - \overline{X})^2} \ \ \text{where} \ \ SER=\frac{1}{n-2} \sum_{i=1}^n \hat u_i^2. \]

Thus \texttt{summary()} estimates the \emph{homoskedasticity-only}
standard error

\[ \sqrt{ \overset{\sim}{\sigma}^2_{\hat\beta_1} } = \sqrt{ \frac{SER^2}{\sum(X_i - \overline{X})^2} }. \]

This in fact is an estimator for the standard deviation of the estimator
\(\hat{\beta}_1\) that is \emph{inconsistent} for the true value
\(\sigma^2_{\hat\beta_1}\) when there is heteroskedasticity. The
implication is that \(t\)-statistics computed in the manner of Key
Concept 5.1 do not have a standard normal distribution, even in large
samples. This issue may invalidate inference drawn when using the
previously treated tools for hypothesis testing: we should be cautious
when making statements about the significance of regression coefficients
on the basis of \(t\)-statistics as computed by \texttt{summary()} or
confidence intervals produced by \texttt{confint()} if it is doubtful
for the assumption of homoskedasticity to hold!

We will now use R to compute the homoskedasticity-only standard error
estimate for \(\hat{\beta}_1\) in the test score regression model
\texttt{linear\_model} by hand and see if it matches the value produced
by \texttt{summary()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Store model summary in 'mod'}
\NormalTok{model <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(linear_model)}

\CommentTok{# Extract the standard error of the regression from model summary}
\NormalTok{SER <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{sigma}

\CommentTok{# Compute the variation in 'size'}
\NormalTok{V <-}\StringTok{ }\NormalTok{(}\KeywordTok{nrow}\NormalTok{(CASchools)}\OperatorTok{-}\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{var}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR)}

\CommentTok{# Compute the standard error of the slope parameter's estimator and print it}
\NormalTok{SE.beta_}\FloatTok{1.}\NormalTok{hat <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(SER}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{V)}
\NormalTok{SE.beta_}\FloatTok{1.}\NormalTok{hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4798255
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Use logical operators to see if the value computed by hand matches the one provided # in mod$coefficients. Round estimates to four decimal places}

\KeywordTok{round}\NormalTok{(model}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{], }\DecValTok{4}\NormalTok{) }\OperatorTok{==}\StringTok{ }\KeywordTok{round}\NormalTok{(SE.beta_}\FloatTok{1.}\NormalTok{hat, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Indeed, the estimated values are equal.

\subsection*{Computation of Heteroskedasticity-Robust Standard
Errors}\label{computation-of-heteroskedasticity-robust-standard-errors}
\addcontentsline{toc}{subsection}{Computation of
Heteroskedasticity-Robust Standard Errors}

Cosistent estimation of \(\sigma_{\hat{\beta}_1}\) under
heteroskedasticity is granted when the following \emph{robust} estimator
is used.

\[ SE(\hat{\beta}_1) = \sqrt{ \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \tag{5.6} \]

Standard error estimates computed this way are also referred to as
\href{https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors}{Eicker-Huber-White
standard errors}. It can be quite cumbersome to do this calculation by
hand. Luckily, there are R function for that purpose. A convenient one,
named \texttt{vcovHC()} is part of the \texttt{sandwich} package. This
function can compute a variety of standard error estimators. The one
brought forward in (5.6) is computed when the argument \texttt{type} is
set to \texttt{"HC0"}.

Let us now compute robust standard error estimates for the coefficients
in \texttt{linear\_model}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the sandwich package}
\KeywordTok{library}\NormalTok{(sandwich)}

\CommentTok{# compute robust standard error estimates}
\NormalTok{vcov <-}\StringTok{ }\KeywordTok{vcovHC}\NormalTok{(linear_model, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{)}
\NormalTok{vcov}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             (Intercept)        STR
## (Intercept)  106.908469 -5.3383689
## STR           -5.338369  0.2685841
\end{verbatim}

The output of \texttt{vcovHC()} is the variance-covariance matrix of
coefficient estimates. We are interested in the square root of the
diagonal elements of this matrix since these values are the standard
error estimates we seek.

\BeginKnitrBlock{rmdknit}
When we have k \textgreater{} 1 regressors, writing down the equations
for a regression model becomes very messy. A more convinient way to
denote and estimate so-called multiple regression models is matrix
algebra. This is why functions like vcovHC() produce matrices. In the
simple linear regression model, the variances and covariances of the
coefficient estimators can be gathered in the variance-covariance matrix

\begin{equation}
\text{Var}
  \begin{pmatrix}
    \hat\beta_0 \\
    \hat\beta_1
  \end{pmatrix} = 
\begin{pmatrix}
  \text{Var}(\hat\beta_0) & \text{Cov}(\hat\beta_0,\hat\beta_1) \\
\text{Cov}(\hat\beta_0,\hat\beta_1) & \text{Var}(\hat\beta_1)
\end{pmatrix}
\end{equation}

which is a symmetric matrix. So vcovHC() gives us
\(\widehat{\text{Var}}(\hat\beta_0)\),
\(\widehat{\text{Var}}(\hat\beta_1)\) and
\(\widehat{\text{Cov}}(\hat\beta_0,\hat\beta_1)\) but most of the time
we are interested in the diagonal elements of the estimated matrix.
\EndKnitrBlock{rmdknit}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the square root of the diagonal elements in vcov}
\NormalTok{robust_se <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(vcov))}
\NormalTok{robust_se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)         STR 
##   10.339655    0.518251
\end{verbatim}

Now assume we want to generate a coefficient summary as provided by
\texttt{summary()} but with \emph{robust} standard error estimates for
the coefficient estimators, robust \(t\)-statistics and corresponding
\(p\)-values for the regression model \texttt{linear\_model}. This can
be done using \texttt{coeftest()} from the package \texttt{lmtest}, see
\texttt{?coeftest}. Further we specify in the argument \texttt{vcov.}
that \texttt{vcov}, the Eicker-Huber-White estimate of the variance
matrix we have computed before should be used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We invoke the function `coeftest()` on our model}
\KeywordTok{coeftest}\NormalTok{(linear_model, }\DataTypeTok{vcov. =}\NormalTok{ vcov)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 698.93295   10.33966  67.597 < 2.2e-16 ***
## STR          -2.27981    0.51825  -4.399 1.382e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that values reported in the column \texttt{Std.\ Error} equal the
ones received using \texttt{sqrt(diag(vcov))}.

How severe are the implications of using homoskedasticity-only standard
errors in the presence of heteroskedasticity? The answer is: it depends.
As mentioned above we may face the risk of drawing wrong conclusions
when conducting significance tests. Let us illustrate this by generating
another example of a heteroskedastic data set and use it to estimate a
simple regression model. We take

\[ Y_i = \beta_1 \cdot X_i + u_i \ \ , \ \ u_i \overset{i.i.d.}{\sim} N(0,0.36 \cdot X_i^2)  \]

with \(\beta_1=1\) as the data generating process. The assumption of
homoskedasticity is violated since the variance of the errors is a
non-linear increasing function of \(X_i\) but the errors have zero mean
and are i.i.d. such that the assumptions made in Key Concept 4.3 are not
violated. As before, the true conditional mean function we are
interested in estimating is

\[ E(Y_i\vert X_i) = \beta_1 X_i. \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{21}\NormalTok{)}

\CommentTok{# generate heteroskedastic data }
\NormalTok{X <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{1000}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{mean =}\NormalTok{ X, }\DataTypeTok{sd =} \FloatTok{0.6}\OperatorTok{*}\NormalTok{X)}

\CommentTok{# estimate a simple regression model}
\NormalTok{reg <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\end{Highlighting}
\end{Shaded}

We plot the data and add the regression line.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the data}
\KeywordTok{plot}\NormalTok{(X, Y, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }\DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{)}

\CommentTok{# add the regression line to the plot}
\KeywordTok{abline}\NormalTok{(reg, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-122-1} \end{center}

The plot clearly shows that the data are heteroskedastic as the variance
of \(Y\) grows with \(X\). We continue by conducting a significance test
of the (true) null hypothesis \(H_0: \beta_1 = 1\) twice, once using the
homoskedasticity-only standard error formula and once with the robust
version (5.6). An idiomatic way to do this in R is the function
\texttt{linearHypothesis()} from the package \texttt{car}, see
\texttt{?linearHypothesis}. It allows to test linear hypotheses about
parameters in linear models in a similar way as done with a
\(t\)-statistic and offers various robust covariance matrix estimators.
We test by comparing the tests' \(p\)-values to the significance level
of \(5\%\).

\BeginKnitrBlock{rmdknit}
linearHypothesis() computes a test statistic that follows an \(F\)
distribution under the null hypothesis. We will not loose too much words
on the theory behind it at this time. In general, the core idea of the
\(F\) test is to compare the fit of different models. When testing a
hypothesis about a \emph{single} coefficient using a \(F\) test, one can
show that the test statistic is simply the square of the corresponding
\(t\)-statistic:

\[ F = t^2 = \frac{\hat\beta_i - \beta_{i,0}}{SE(\hat\beta_i)} \sim F_{1,n-k-1}  \]

In linearHypothesis(), the hypothesis must be provided as a
\emph{string}. The function returns an object of class anova which
contains further information on the test that can be accessed using the
\$ operator. For example, we can obtain the test's \(p\)-value by adding
\$`Pr(\textgreater{}F)' right behind the function call.
\EndKnitrBlock{rmdknit}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test using default standard error}
\KeywordTok{linearHypothesis}\NormalTok{(reg, }\DataTypeTok{hypothesis.matrix =} \StringTok{"X = 1"}\NormalTok{)}\OperatorTok{$}\StringTok{'Pr(>F)'}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test using robust standard error}
\KeywordTok{linearHypothesis}\NormalTok{(reg, }\DataTypeTok{hypothesis.matrix =} \StringTok{"X = 1"}\NormalTok{, }\DataTypeTok{white.adjust =} \StringTok{"hc0"}\NormalTok{)}\OperatorTok{$}\StringTok{'Pr(>F)'}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

This is a good example of what can go wrong if we do not care for
heteroskedasticity: for the data set at hand the default method rejects
the null hypothesis \(\beta_1 = 1\) although it is true. Using the
robust standard error though the test does not reject the null. Of
course we could argue that this is just a coincidence and both tests are
equally well in maintaining the type I error rate of \(5\%\). This can
be further investigated by computing Monte Carlo estimates of the
rejection frequencies of both tests on the basis of a large number of
random samples. We proceed as follows:

\begin{itemize}
\tightlist
\item
  initialize vectors \texttt{t} and \texttt{t.rob} as type
  \texttt{numeric} with length \(10000\).
\item
  Using a \texttt{for()} loop, we generate \(10000\) heteroskedastic
  random samples of size \(1000\), estimate the regression model and
  check whether the tests wrongly reject the null at the level of
  \(5\%\) using comparison operators. The results are stored in the
  respective vectors \texttt{t} and \texttt{t.rob}.
\item
  After the simulation, we compute the fraction of rejections for both
  tests.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# initialize vectors t and t.rob}
\NormalTok{t <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}
\NormalTok{t.rob <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}

\CommentTok{# loop sampling and estimation}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
  
  \CommentTok{# sample data}
\NormalTok{  X <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{1000}
\NormalTok{  Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{mean =}\NormalTok{ X, }\DataTypeTok{sd =} \FloatTok{0.6}\OperatorTok{*}\NormalTok{X)}

  \CommentTok{# estimate regression model}
\NormalTok{  reg <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}

  \CommentTok{# homoskedasdicity-only significance test}
\NormalTok{  t[i] <-}\StringTok{ }\KeywordTok{linearHypothesis}\NormalTok{(reg, }\StringTok{"X = 1"}\NormalTok{)}\OperatorTok{$}\StringTok{'Pr(>F)'}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}

  \CommentTok{# robust significance test}
\NormalTok{  t.rob[i] <-}\StringTok{ }\KeywordTok{linearHypothesis}\NormalTok{(reg, }\StringTok{"X = 1"}\NormalTok{, }\DataTypeTok{white.adjust =} \StringTok{"hc0"}\NormalTok{)}\OperatorTok{$}\StringTok{'Pr(>F)'}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}

\NormalTok{\}}

\CommentTok{# compute fraction of rejections}
\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{t =} \KeywordTok{sum}\NormalTok{(t), }\DataTypeTok{t.rob =} \KeywordTok{sum}\NormalTok{(t.rob)) }\OperatorTok{/}\StringTok{ }\DecValTok{10000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           t  t.rob
## [1,] 0.0762 0.0524
\end{verbatim}

The results show that we face an increased risk of falsely rejecting the
null using the homoskedasticity-only standard error for the testing
problem at hand: with the common standard error estimator, \(7.62\%\) of
all tests reject the null hypothesis falsely. In contrast, with the
robust test statistic we are close to the nominal level of \(5\%\).

\section{The Gauss-Markov Theorem}\label{the-gauss-markov-theorem}

When estimating regression models, we know that the results of the
estimation procedure are outcomes of a random process. However, when
using unbiased estimators, at least on average, we estimate the true
parameter. When comparing different unbiased estimators, it is therefore
interesting to know which one has the highest precision: being aware
that the likelihood of estimating the \emph{exact} value of the
parameter of interest is \(0\) in an empirical application, we want to
make sure that the likelihood of obtaining an estimate very close to the
true value is as high as possible. We want to use the estimator with the
lowest variance of all unbiased estimators. The Gauss-Markov theorem
states that the OLS estimator has this property under certain
conditions.

Key Concept 5.5

The Gauss-Markov Theorem for \(\hat{\beta}_1\)

Suppose that the assumptions made in Key Concept 4.3 hold \emph{and}
that the errors are \emph{homoskedastic}. The OLS estimator is the best
(in the sense of smallest variance) linear conditionally unbiased
estimator (BLUE) in this setting.

Let us have a closer look at what this means:

\begin{itemize}
\item
  Estimators of \(\beta_1\) that are linear functions of the
  \(Y_1, \dots, Y_n\) and that are unbiased conditionally on the
  regressor \(X_1, \dots, X_n\) can be written as
  \[ \overset{\sim}{\beta}_1 = \sum_{i=1}^n a_i Y_i \] where the \(a_i\)
  are weights that are allowed to depend on the \(X_i\) but \emph{not}
  on the \(Y_i\).
\item
  We already know that \(\overset{\sim}{\beta}_1\) has a sampling
  distribution: \(\overset{\sim}{\beta}_1\) is a linear function of the
  \(Y_i\) which are random variables. If now
  \[ E(\overset{\sim}{\beta}_1 | X_1, \dots, X_n) = \beta_1 \] we say
  that \(\overset{\sim}{\beta}_1\) is a linear unbiased estimator of
  \(\beta_1\), conditionally on the \(X_1, \dots, X_n\).
\item
  We may ask if \(\overset{\sim}{\beta}_1\) is also the \emph{best}
  estimator in this class, i.e.~the most efficient one of all linear
  unbiased estimators where ``most efficient'' means smallest variance.
  The weights \(a_i\) play an important role here and it turns out that
  OLS uses just the right weights to have the BLUE property.
\end{itemize}

\subsection*{R Simulation Study: BLUE
Estimator}\label{r-simulation-study-blue-estimator}
\addcontentsline{toc}{subsection}{R Simulation Study: BLUE Estimator}

Consider the case of a regression of \(Y_i,\dots,Y_n\) only on a
constant. Here, the \(Y_i\) are assumed to be a random sample from a
population with mean \(\mu\) and variance \(\sigma^2\). We know that the
OLS estimator in this model is simply the sample mean:

\begin{equation}
\hat{\beta}_1 = \overline{\beta}_1 = \sum_{i=1}^n \underbrace{\frac{1}{n}}_{=a_i} Y_i \label{eq:bluemean}
\end{equation}

Clearly, each observation is weighted by

\[a_i = \frac{1}{n}.\]

and We also know that
\(\text{Var}(\hat{\beta}_1)=\text{Var}(\hat\beta_1)=\frac{\sigma^2}{n}\).

We will now use R for a simulation study that illustrates what happens
to the variance of \eqref{eq:bluemean} if different weights
\[ w_i = \frac{1 \pm \epsilon}{n} \] are assigned to either half of the
sample \(Y_1, \dots, Y_n\) instead of using \(\frac{1}{n}\), the weights
implied by OLS.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set sample size and number of repititions}
\NormalTok{n <-}\StringTok{ }\DecValTok{100}      
\NormalTok{reps <-}\StringTok{ }\FloatTok{1e5}

\CommentTok{# Choose epsilon and create a vector of weights as defined above}
\NormalTok{epsilon <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{w <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{((}\DecValTok{1}\OperatorTok{+}\NormalTok{epsilon)}\OperatorTok{/}\NormalTok{n,n}\OperatorTok{/}\DecValTok{2}\NormalTok{), }
       \KeywordTok{rep}\NormalTok{((}\DecValTok{1}\OperatorTok{-}\NormalTok{epsilon)}\OperatorTok{/}\NormalTok{n,n}\OperatorTok{/}\DecValTok{2}\NormalTok{) }
\NormalTok{      )}

\CommentTok{# Draw a random sample y_1,...,y_n from the standard normal distribution, }
\CommentTok{# use both estimators 1e5 times and store the result in thr vectors ols and }
\CommentTok{# weightedestimator}

\NormalTok{ols <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, reps)}
\NormalTok{weightedestimator <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, reps)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{reps) \{}
\NormalTok{  y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{  ols[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(y)}
\NormalTok{  weightedestimator[i] <-}\StringTok{ }\KeywordTok{crossprod}\NormalTok{(w,y)}
\NormalTok{\}}

\CommentTok{# Plot kernel density estimates of the estimators' distributions }

\CommentTok{# OLS}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(ols), }
     \DataTypeTok{col =} \StringTok{"purple"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{3}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Density of OLS and Weighted Estimator"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Estimates"}
\NormalTok{     )}

\CommentTok{# Weighted}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(weightedestimator), }
      \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{3}
\NormalTok{      ) }

\CommentTok{# Add a dashed line at 0 and a legend to the plot}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{"OLS"}\NormalTok{,}\StringTok{"Weighted"}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"purple"}\NormalTok{,}\StringTok{"steelblue"}\NormalTok{), }
       \DataTypeTok{lwd =} \DecValTok{3}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-125-1} \end{center}

What conclusion can we draw from the result?

\begin{itemize}
\tightlist
\item
  Both estimators seem to be unbiased: the means of their estimated
  distributions are zero.
\item
  The \texttt{weightedestimator} is less efficient than the \texttt{ols}
  estimator: there is higher dispersion when weights are
  \(w_i = \frac{1 \pm 0.8}{100}\) instead of \(w_i=\frac{1}{100}\) as
  required by the OLS solution.
\end{itemize}

Hence, our simulation results confirm what is stated by the Gauss-Markov
Theorem.

\section{Using the t-Statistic in Regression When the Sample Size Is
Small}\label{using-the-t-statistic-in-regression-when-the-sample-size-is-small}

The three OLS assumptions discussed in \protect\hyperlink{lrwor}{chapter
4} (see Key Concept 4.3) are the foundation results on the large sample
distribution of the OLS estimators in the simple regression model. What
can be said about the distribution of the estimators and their
\(t\)-statistics when the sample size is small and the population
distribution of the data is unkown? Provided that the three least
squares assumptions hold and the errors are normally distributed and
homoskedastic (we refer to these conditions as the homoskedastic normal
regression assumptions), we have normally distributed estimators and
\(t\)-distributed test tatistics. Recall the
\protect\hyperlink{thetdist}{definition} of a \(t\)-distributed variable

\[ \frac{Z}{\sqrt{W/m}} \sim t_m\]

where \(Z\) is a standard normal random variable, \(W\) is \(\chi^2\)
distributed with \(m\) degrees of freedom and \(Z\) and \(W\) are
independent. See section 5.6 in the book for a more detailed discussion
of the small sample distribution of \(t\)-statistics in regression.

Let us simulate the distribution of regression \(t\)-statistics based on
a large number of small random samples (\(n=20\)) and compare their
simulated distributions to their theoretical distribution which sould be
\(t_{18}\), the \(t\)-distribution with \(18\) degrees of freedom
(recall that \(\text{DF}=n-k-1\)).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# initialize vectors}
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}

\CommentTok{# loop sampling / estimation / t statistics}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}

\NormalTok{  X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{)}
\NormalTok{  Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{20}\NormalTok{, }\DataTypeTok{mean =}\NormalTok{ X)}
\NormalTok{  reg <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X))}
\NormalTok{  beta_}\DecValTok{0}\NormalTok{[i] <-}\StringTok{ }\NormalTok{(reg}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{0}\NormalTok{)}\OperatorTok{/}\NormalTok{(reg}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\NormalTok{  beta_}\DecValTok{1}\NormalTok{[i] <-}\StringTok{ }\NormalTok{(reg}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{(reg}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}

\NormalTok{\}}

\CommentTok{# plot distributions and compare with t_18 density function}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\CommentTok{# plot simulated density of beta_0}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(beta_}\DecValTok{0}\NormalTok{), }
     \DataTypeTok{lwd=} \DecValTok{2}\NormalTok{ , }
     \DataTypeTok{main =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{widehat}\NormalTok{(beta)[}\DecValTok{0}\NormalTok{]), }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{    )}

\CommentTok{# add t_18 density to the plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{18}\NormalTok{), }
      \DataTypeTok{add =}\NormalTok{ T, }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lty =} \DecValTok{2}
\NormalTok{      )}

\CommentTok{# plot simulated density of beta_1}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(beta_}\DecValTok{1}\NormalTok{), }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{main =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{widehat}\NormalTok{(beta)[}\DecValTok{1}\NormalTok{]), }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\NormalTok{     )}

\CommentTok{# add t_18 density to the plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{18}\NormalTok{), }
      \DataTypeTok{add =}\NormalTok{ T, }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lty =} \DecValTok{2}
\NormalTok{      ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-126-1} \end{center}

The outcomes are consistent with our expectations: the empirical
distributions of both estimators seem to track the \(t_{18}\)
distribution quite closely.

\section{Exercises}\label{exercises-3}

\subsubsection*{1. Testing Two Null Hypotheses
Individually}\label{testing-two-null-hypotheses-individually}
\addcontentsline{toc}{subsubsection}{1. Testing Two Null Hypotheses
Individually}

Consider the estimated regression line

\[ \widehat{TestScore} = \underset{(23.96)}{567.43} - \underset{(0.85)}{7.15} \times STR, \, R^2 = 0.8976, \, SER=15.19 \]

with standard errors in parentheses.

\textbf{Instructions:}

\begin{itemize}
\tightlist
\item
  Compute the \(p\)-value for a \(t\)-test of the hypothesis that the
  intercept is zero. Save the result to p\_int
\item
  Compute the \(p\)-value for a \(t\)-test of the hypothesis that the
  coefficient of STR is zero. Save the result to p\_STR
\end{itemize}

\textbf{Hint:} both hypotheses can be tested individually using a
two-sided test. Use pnorm() to obtain cumulated probabilities for
standard normally distributed outcomes, see ?pnorm

 \# Compute the p-value for the first significance test and save it to
p\_int

\# Compute the p-value for the second significance test and save it to
p\_cs

 t\_int \textless{}- 567.43/23.9606 p\_int \textless{}-
2\emph{(1-pnorm(abs(t\_int))) t\_STR \textless{}- 7.15/0.8536 p\_STR
\textless{}- 2}(1-pnorm(abs(t\_STR))) test\_object(``p\_int'')
test\_object(``p\_STR'')

\subsubsection*{2. Two Null Hypotheses You Can't Reject, Can
You?}\label{two-null-hypotheses-you-cant-reject-can-you}
\addcontentsline{toc}{subsubsection}{2. Two Null Hypotheses You Can't
Reject, Can You?}

\[\widehat{TestScore} = \underset{(23.96)}{567.43} - \underset{(0.85)}{7.15} \times STR, \, R^2 = 0.8976, \,SER=15.19\]

Can you reject the null hypotheses discussed in the previous code
exercise using individual \(t\)-tests at the \(5\%\) significance level?

The variables t\_int and t\_STR are the \(t\)-statistics. Both are
available in your working environment.

\textbf{Instructions}

\begin{itemize}
\tightlist
\item
  Gather t\_int and t\_STR in a vector test and use logical operators to
  check whether the corresponding rejection rule applies.
\end{itemize}

\textbf{Hints:}

\begin{itemize}
\tightlist
\item
  Both tests are two-sided \(t\)-tests. Key Concept 5.2 recaps how a
  two-sided \(t\)-test is conducted.
\item
  Use qnorm() to obtain normal critical values.
\end{itemize}

 t\_int \textless{}- 567.43/23.9606 t\_STR \textless{}- 7.15/0.8536 \#
check wether the t-tests reject

 test \textless{}- c(t\_int, t\_STR) \# entry is `TRUE' if a test
rejects abs(test) \textgreater{}= qnorm(0.95)

 test\_or(\{ test \textless{}- c(t\_int,t\_STR) \},\{ test \textless{}-
c(t\_STR, t\_int) \}) test\_or(\{ test\_student\_typed(``abs(test)
\textgreater{}= qnorm(0.95)'') \},\{ test\_student\_typed(``abs(test)
\textless{}= qnorm(0.95)'') \})

\subsubsection*{3. Confidence Intervals}\label{confidence-intervals}
\addcontentsline{toc}{subsubsection}{3. Confidence Intervals}

mod, an object of class lm which contains the estimated model
\[\widehat{TestScore} = \underset{(23.96)}{567.43} - \underset{(0.85)}{7.15} \times STR, \, R^2 = 0.8976, \,SER=15.19\]
is available in your working environment.

\textbf{Instructions}

\begin{itemize}
\tightlist
\item
  Compute 90\% confidence intervals for both coefficients.
\end{itemize}

\textbf{Hints:}

Use the function confint(), see ?confint. The argument level sets the
confidence level to be used.

 STR \textless{}- c(23, 19, 30, 22, 23, 29, 35, 36, 33, 25) TS
\textless{}- c(430, 430, 333, 410, 390, 377, 325, 310, 328, 375) mod
\textless{}- lm(TS \textasciitilde{} STR) \# compute 90\% confidence
intervals for the model coefficients

 confint(mod, level = 0.9)

 test\_function(``confint'', args = c(``object'',``level''))

\subsubsection*{4. A Confidence Interval for the Mean
I}\label{a-confidence-interval-for-the-mean-i}
\addcontentsline{toc}{subsubsection}{4. A Confidence Interval for the
Mean I}

Consider the model \[Y_i = \beta_1 + u_i\] where
\(Y_i \sim \mathcal{N}(\mu, \sigma^2)\). Following the discussion
preceding equation \eqref{eq:KI}, a \(95\%\) confidence interval for the
mean of the \(Y_i\) can be computed as

\[KI^{\mu}_{0.95} = \left[\hat\mu - 1.96 \times \frac{\sigma}{\sqrt{n}} \ , \ \hat\mu + 1.96 \times \frac{\sigma}{\sqrt{n}} \right].\]

\textbf{Instructions}

\begin{itemize}
\tightlist
\item
  Sample \(n=100\) observations from a normal distribution with variance
  100 and mean 10.
\item
  Use the sample to estimate \(\beta_1\). Save the estimate in mu\_hat.
\item
  Assume that \(\sigma^2 = 100\) is known. Replace the zeros in the code
  below to obtain a \(95\%\) confidence interval for the mean of the
  \(Y_i\).
\end{itemize}

\textbf{Hints:}

Use the function confint(), see ?confint. The argument level sets the
confidence level to be used.

 \# random seed for reproducibility set.seed(1)

\# sample the observations

\# estimate the mean, assign the estimate to `mu\_hat'

\# compute the 95\% confidence interval using the formula above CI
\textless{}- c( ``lower'' = 0, ``upper'' = 0 )

 set.seed(1) mu\_hat \textless{}- mean(rnorm(100, mean = 10, sd = 10))
CI \textless{}- c( ``lower'' = mu\_hat - 1.96, ``upper'' = mu\_hat +
1.96 ) test\_object(``CI'') test\_object(``mu\_hat'')
test\_object(``CI'')

\subsubsection*{5. A Confidence Interval for the Mean
II}\label{a-confidence-interval-for-the-mean-ii}
\addcontentsline{toc}{subsubsection}{5. A Confidence Interval for the
Mean II}

For historical reasons, some R functions that we use to obtain
statistical inference on model parameters, among them confint() and
summary(), rely on the \(t\)-distribution instead of using the
large-sample normal approximation. This is why for small sample sizes
(and hence small degrees of freedom), \(p\)-values and confidence
intervals reported by these functions deviate from those computed using
critical values or cumulative probabilities of the standard normal
distribution.

The \(95\%\) confidence interval for the mean in the previous exercise
is \([9.1289, 13.0489]\).

100 observations sampled from a normal distribution with \(\mu=10\) and
\(\sigma^2=100\) have been assigned to the vector s and are available in
your workspace.

\textbf{Instructions}

\begin{itemize}
\tightlist
\item
  Set up a suitable regression model to estimate the mean of the
  observations in s. Then use confint() to compute a \(95\%\) confidence
  interval for the mean.
\end{itemize}

(Check that the result is different from the interval reported above.)

 set.seed(1); s \textless{}- rnorm(100, mean = 10, sd = 10) \# use a
regression to obtain an estimate of the mean

\# compute the 95\% CI

 set.seed(1) confint(lm(s \textasciitilde{} 1))
test\_predefined\_objects(``s'') test\_output\_contains(``confint(lm(s
\textasciitilde{} 1))'')

\subsubsection*{6. Regression on a Dummy
I}\label{regression-on-a-dummy-i}
\addcontentsline{toc}{subsubsection}{6. Regression on a Dummy I}

Chapter 5.3 discusses regression when \(X\) is a dummy variables where
we have used a for() loop to generate a binary variable indicating
whether a schooling district in the CASchools data set has a
student-teacher ration below 20. Though it is instructive to use a loop
for this, there are ways to achieve the same with less code.

A dataframe DF with 100 observations of a variable X is available in
your working environment.

\textbf{Instructions}

\begin{itemize}
\item
  Use ifelse() to generate a binary vector dummy indicating whether the
  observations in X are positive.
\item
  Append dummy to the dataframe DF.
\end{itemize}

 set.seed(1) DF \textless{}- data.frame(``X'' = rnorm(100)) \# generate
the dummy vector `dummy' using `ifelse()'

\# append `dummy' to `DF'

 dummy \textless{}- ifelse(DF\(X > 0, 1, 0)  DF\)dummy \textless{}-
dummy test\_object(``dummy'') test\_function(``ifelse'')
test\_object(``DF'')

\subsubsection*{7. Regression on a Dummy
II}\label{regression-on-a-dummy-ii}
\addcontentsline{toc}{subsubsection}{7. Regression on a Dummy II}

A dataframe DF with 100 observations on Y and the binary variable D from
the previous exercise are available in your working environment.

\textbf{Instructions}

\begin{itemize}
\item
  Compute the group-specific sample means of the observations in Y. Save
  the mean of observations in Y where dummy == 1 to mu\_Y\_D1 and assign
  the mean of those observations with D == 0 to mu\_Y\_D0.
\item
  Use lm() to regress Y on D i.e.~estimate the coefficients in the model
  \[\widehat{Y}_i = \beta_0 + \beta_1 \times D_i + u_i.\]
\end{itemize}

Check that the estimates of the coefficients \(\beta_0\) and \(\beta_1\)
reflect specific sample means. Can you tell which?

 set.seed(1) DF \textless{}- data.frame(``Y'' = rnorm(100))
DF\(D <- ifelse(DF\)Y \textgreater{} 0, 1, 0) \# compute group-specific
sample means of `Y'

\# Regress `Y' on `D'

 mu\_Y\_D1 \textless{}- mean(DF\(Y[DF\)D ==1{]}) mu\_Y\_D0 \textless{}-
mean(DF\(Y[DF\)D ==0{]}) lm(Y \textasciitilde{} dummy, data = DF)
test\_object(``mu\_Y\_D1'') test\_object(``mu\_Y\_D1'') test\_or( \{
test\_function(``lm'', args = ``formula'') \},\{ ex() \%\textgreater{}\%
override\_solution(``lm(DF\(Y ~ DF\)dummy)'') \%\textgreater{}\%
check\_function(``lm'') \%\textgreater{}\% check\_arg(``formula'') \} )

\subsubsection*{8. Regression on a Dummy
III}\label{regression-on-a-dummy-iii}
\addcontentsline{toc}{subsubsection}{8. Regression on a Dummy III}

In this exercise, you have to visualize some of the results from the
dummy regression model \[\widehat{Y}_i = -0.66 + 1.43 \times D_i\]
estimated in the previous exercise.

A dataframe DF with 100 observations on X and the binary variable dummy
as well as the model object dummy\_mod from the previous exercise are
available in your working environment.

\textbf{Instructions}

\begin{itemize}
\item
  Start by drawing a visually appealing plot of the observations on
  \(Y\) and \(D\) based on the code chunk provided in Script.R. Replace
  the ??? by the correct expressions!
\item
  Add the regression line to the plot.
\end{itemize}

 set.seed(1) DF \textless{}- data.frame(``Y'' = rnorm(100))
DF\(D <- ifelse(DF\)Y \textgreater{} 0, 1, 0) dummy\_mod \textless{}-
lm(Y \textasciitilde{} D, data = DF) \# Replace the `???' by the correct
values plot(x = ??? , y = ???, pch = 20, cex = 1, col = `Steelblue',
xlab = expression(D{[}i{]}), ylab = `Test Score', main = `Dummy
Regression' )

\# add the regression line

 plot(x = DF\(D , y = DF\)Y, pch = 20, cex = 1, col = `Steelblue', xlab
= expression(D{[}i{]}), ylab = `Test Score', main = `Dummy Regression'
)\\
abline(dummy\_mod) test\_predefined\_objects(c(``DF'',``dummy\_mod''))
test\_function(``abline'') test\_function(``plot'', args =
c(``x'',``y'')) success\_msg(``Nice! Clearly, a line is not a suitable
way to think of this model!'')

\subsubsection*{9. Gender Wage Gap I}\label{gender-wage-gap-i}
\addcontentsline{toc}{subsubsection}{9. Gender Wage Gap I}

CPS1985 is a cross-section data set originating from the May 1985
\emph{Current Population Survey} by the \emph{US Census Bureau} and,
among others things, containes observations on wage and gender of
employees.

CPS1985 is part of the package AER.

\textbf{Instructions}

\begin{itemize}
\item
  Load the package AER and attach the data set CPS1985.
\item
  Estimate the dummy regression model
  \[wage_i = \beta_0 + \beta_1 \cdot female_i + u_i\] where

  \begin{align*}
    female = 
    \begin{cases}
      1, & \text{if employee is female} \\
      0, & \text{if employee is male}.
    \end{cases}
  \end{align*}

  Save the result in wage\_mod.
\end{itemize}

 \# Load the package and attach the data set

\# Perform the regression

 library(AER) data(CPS1985) dummy\_mod \textless{}- lm(wage
\textasciitilde{} gender, data = CPS1985) test\_function(``library'')
test\_function(``data'') test\_or(\{ ex() \%\textgreater{}\%
override\_solution(``attach(CPS1985); dummy\_mod \textless{}- lm(wage
\textasciitilde{} gender)'') \%\textgreater{}\%
check\_object(``dummy\_mod'') \},\{ ex() \%\textgreater{}\%
override\_solution(``dummy\_mod \textless{}-
lm(CPS1985\(wage ~ CPS1985\)gender)'') \%\textgreater{}\%
check\_object(``dummy\_mod'') \},\{ ex() \%\textgreater{}\%
override\_solution(``dummy\_mod \textless{}- lm(wage \textasciitilde{}
gender, data = CPS1985)'') \%\textgreater{}\%
check\_object(``dummy\_mod'') \} ) success\_msg(``Well done!'')

\subsubsection*{10. Gender Wage Gap II}\label{gender-wage-gap-ii}
\addcontentsline{toc}{subsubsection}{10. Gender Wage Gap II}

The wage regression from the previous exercise yields
\[\widehat{wage}_i = 9.995 - 2.116 \cdot female_i.\]

The model object dummy\_mod is available in your working environment.

\textbf{Instructions}

\begin{itemize}
\tightlist
\item
  Test the hypothesis that the coefficient on \(female_i\) is zero. This
  would imply that there is no gender wage gap. Use White's (1980)
  heteroskedasticity-robust estimator.
\end{itemize}

\textbf{Hint:}

\begin{itemize}
\item
  vcovHC() computes heteroskedasticity-robust estimates of the
  covariance matrix of the coefficient estimators for the model
  supplied, see ?vcovHC.
\item
  coeftest() performs significance tests for the coefficients in model
  objects. A covariance matrix can be supplied using the argument vcov..
\end{itemize}

 library(AER) data(CPS1985) dummy\_mod \textless{}- lm(wage
\textasciitilde{} gender, data = CPS1985) \# test whether the gender gap
is significantly different from zero \# use robust standard errors

 coeftest(dummy\_mod, vcov. = vcovHC(dummy\_mod, type = ``HC0'')) \# or
linearHypothesis(dummy\_mod, ``genderfemale=0'', vcov. =
vcovHC(dummy\_mod, type = ``HC0''))
test\_predefined\_objects(``dummy\_mod'') test\_or(\{
test\_function(``coeftest'', args = c(``x'', ``vcov.'')) \},\{
test\_function(``linearHypothesis'', args = c(``model'', ``vcov.'',
``hypothesis.matrix'')) \},\{ f \textless{}- ex() \%\textgreater{}\%
override\_solution(`linearHypothesis(dummy\_mod, ``genderfemale'', vcov.
= vcovHC(dummy\_mod, type = ``HC0''))') \%\textgreater{}\%
check\_function(``linearHypothesis'') f \%\textgreater{}\%
check\_arg(``model'') f \%\textgreater{}\% check\_arg(``vcov.'') f
\%\textgreater{}\% check\_arg(``hypothesis.matrix'') \})
success\_msg(``Right! The hypothesis that there is no wage gap is
rejected at any common level.'')

\subsubsection*{11. Heteroskedasticity-Robust Standard
Errors}\label{heteroskedasticity-robust-standard-errors}
\addcontentsline{toc}{subsubsection}{11. Heteroskedasticity-Robust
Standard Errors}

In the simple regression model, the covariance matrix of the coefficient
estimators is denoted

\begin{equation}
\text{Var}
  \begin{pmatrix}
    \hat\beta_0 \\
    \hat\beta_1
  \end{pmatrix} = 
\begin{pmatrix}
  \text{Var}(\hat\beta_0) & \text{Cov}(\hat\beta_0,\hat\beta_1) \\
\text{Cov}(\hat\beta_0,\hat\beta_1) & \text{Var}(\hat\beta_1)
\end{pmatrix}
\end{equation}

The function vcovHC can be used to obtain estimates of this matrix for a
model object of interest.

gender\_mod, a model object containing the wage regression dealt with in
exercises 9 and 10 is available in your working environment.

\textbf{Instructions}

\begin{itemize}
\tightlist
\item
  Compute robust standard errors of the type HC0 for the coefficients
  estimators in the model object gender\_mod. Store the standard errors
  in a vector named rob\_SEs.
\end{itemize}

\textbf{Hint:}

\begin{itemize}
\item
  The standard errors we seek can be obtained by taking the square root
  of the diagonal elements of the estimated covariance matrix.
\item
  diag(A) returns the diagonal elements of the matrix A.
\end{itemize}

 library(AER) data(CPS1985) dummy\_mod \textless{}- lm(wage
\textasciitilde{} gender, data = CPS1985) \# compute robust standard
errors and save them in rob\_SEs

 rob\_SEs \textless{}- sqrt(diag(vcovHC(dummy\_mod, typ = ``HC0'')))
test\_predefined\_objects(``dummy\_mod'') test\_object(``rob\_SEs'')
success\_msg(``Nice!'')

\subsubsection*{12. Robust Confidence
Intervals}\label{robust-confidence-intervals}
\addcontentsline{toc}{subsubsection}{12. Robust Confidence Intervals}

The function confint() computes confidence intervals for regression
models using homoskedasticity-only standard errors such that this
function is not an option when there is heteroskedasticity.

The function Rob\_CI() is meant to compute and report
heteroskedasticity-robust confidence intervals for both model
coefficients in a simple regression model.

gender\_mod, a model object containing the wage regression dealt with in
the previous exercises is available in your working environment.

\textbf{Instructions}

\begin{itemize}
\item
  Complete the code of Rob\_CI() given in Script.R such that lower and
  upper bounds of \(95\%\) confidence intervals are returned.
\item
  Use the function to obtain \(95\%\) confidence intervals for the model
  coefficients in dummy\_mod.
\end{itemize}

 library(AER) data(CPS1985) dummy\_mod \textless{}- lm(wage
\textasciitilde{} gender, data = CPS1985) \# complete the function below

Rob\_CI \textless{}- function(model) \{ SEs \textless{}- ??? lower
\textless{}- model\(coef - ???  upper <- model\)coef + ??? return(
cbind(``Lower'' = lower, ``Upper'' = upper) ) \}

 Rob\_CI \textless{}- function(model) \{ SEs \textless{}-
sqrt(diag(vcovHC(model, type = ``HC0''))) lower \textless{}-
model\(coef - 1.96 * SEs  upper <- model\)coef + 1.96 * SEs return(
cbind(``Lower'' = lower, ``Upper'' = upper) ) \}
test\_function\_definition(``Rob\_CI'', function\_test =
test\_expression\_result(``Rob\_CI(dummy\_mod)''))
test\_output\_contains(``Rob\_CI(dummy\_mod)'') success\_msg(``Nice!'')

\subsubsection*{13. A Small Simulation Study
I}\label{a-small-simulation-study-i}
\addcontentsline{toc}{subsubsection}{13. A Small Simulation Study I}

Consider the data generating process

\begin{align*}
  X_i \sim& \, \mathcal{U}[2,10],\\
  e_i \sim& \, N(0, X_i),\\
  Yi =& \, \beta_1 X_i + e_i  
\end{align*}

where \(\mathcal{U}[0,10]\) denotes the uniform distribution on the
interval \([0,10]\) and \(\beta_1=2\).

Notice that the errors \(e_i\) are heteroskedastic since the variance is
a linear function of \(X_i\).

\textbf{Instructions}

\begin{itemize}
\tightlist
\item
  Write a function DGP\_OLS that generates a sample \((X_i,Y_i)\),
  \(i=1,…,100\) and returns the OLS estimate of \(\beta_1\).
\end{itemize}

\textbf{Hints}

runif() can be used to obtain random samples form a uniform
distribution, see ?runif

 \# write the function set.seed(1) \# write the function DGP\_OLS
\textless{}- function() \{ X \textless{}- runif(100,2,10) Y \textless{}-
X + rnorm(100, sd = sqrt(X)) return( c(``beta\_1\_hat'' =
sum(X*Y)/sum(X\^{}2)) ) \} test\_function\_definition(``DGP\_OLS'',
function\_test = test\_expression\_result(``DGP\_OLS()'') )
success\_msg(``Well Done!'')

\subsubsection*{14. A Small Simulation Study
II}\label{a-small-simulation-study-ii}
\addcontentsline{toc}{subsubsection}{14. A Small Simulation Study II}

The function DGP\_OLS() from the previous exercise is available in your
working environment.

\textbf{Instructions}

\begin{itemize}
\item
  Use replicate() to generate a sample of \(1000\) OLS estimates
  \(\widehat{\beta}_1\) using the function DGP\_OLS. Store the estimates
  in a vector named estimates.
\item
  Estimate the variance of the OLS estimates. Store the result in
  est\_var\_OLS.
\end{itemize}

 DGP\_OLS \textless{}- function() \{ X \textless{}- runif(100,2,10) Y
\textless{}- X + rnorm(100, sd = sqrt(X)) return( c(``beta\_1\_hat'' =
sum(X*Y)/sum(X\^{}2)) ) \} set.seed(1) \# Generate 1000 estimates of
beta\_1 using `GDP\_OLS()', store them in `estimates'

\# Estimate the variance of the estimates

 set.seed(1) \# Generate 1000 estimates of beta\_1 using `GDP\_OLS()',
store them in `estimates' estimates \textless{}- replicate(1000,
DGP\_OLS()) \# Estimate the variance of the estimates est\_var\_OLS
\textless{}- var(estimates) test\_predefined\_objects(``DGP\_OLS'')
test\_object(``estimates'') test\_object(``est\_var\_OLS'')

\subsubsection*{15. A Small Simulation Study
III}\label{a-small-simulation-study-iii}
\addcontentsline{toc}{subsubsection}{15. A Small Simulation Study III}

The Gauss-Markov Theorem states that the OLS estimator in linear
regression models is no longer the most efficient estimator (in the
sense of minimum variance) among the conditionally unbiased linear
estimators, that is the OLS estimator loses the BLUE property, under
heteroskedasticity.

It turns out that OLS applied to the weighted observations
\((w_i X_i, w_i Y_i)\) where \(w_i=\frac{1}{\sigma_i}\) yields the BLUE
estimator under heteroskedasticity. This estimator is called the
\emph{weighted least squares} (WLS) estimator so that its variance is
smaller than the variance of the OLS estimator.

The function DGP\_OLS() and the estimated variance est\_var\_OLS from
the previous exercises are available in your working environment.

\textbf{Instructions}

\begin{itemize}
\item
  Write a function DGP\_WLS() that generates \(100\) samples using the
  DGP introduced in exercise 13 and returns the WLS estimate of
  \(\beta_1\). Treat \(\sigma_i\) as known, i.e.~set
  \(w_i=\frac{1}{X_i}\).
\item
  Repeat exercise 14 using DGP\_WLS(). Store the estimated variance in
  est\_var\_GLS.
\item
  Compare the estimated variances est\_var\_OLS and est\_var\_GLS using
  logical operators (\textless{} or \textgreater{}).
\end{itemize}

\textbf{Hints}

\begin{itemize}
\item
  DGP\_WLS() can be obtained using a modified code of DGP\_OLS().
\item
  Remember that functions are objects and you may print the code of a
  function to the console.
\end{itemize}

 set.seed(1) DGP\_OLS \textless{}- function() \{ X \textless{}-
runif(100,2,10) Y \textless{}- X + rnorm(100, sd = sqrt(X)) return(
c(``beta\_1\_hat'' = sum(X*Y)/sum(X\^{}2)) ) \} estimates \textless{}-
replicate(1000, DGP\_OLS()) est\_var\_OLS \textless{}- var(estimates)
set.seed(1) \# Define the function `DGP\_GLS()'

\# estimate the variance, assign the value to `var\_est\_GLS'

\# compare the estimated variances

 set.seed(1) \# Define the function `DGP\_GLS()' DGP\_GLS \textless{}-
function() \{ X \textless{}- runif(100,2,10) Y \textless{}- X +
rnorm(100, sd = sqrt(X)) w \textless{}- 1/sqrt(X) return(
c(``beta\_1\_hat'' = sum(w\^{}2\emph{X}Y)/sum((w*X)\^{}2)) ) \} \#
estimate the variance, assign the value to `var\_est\_GLS' est\_var\_GLS
\textless{}- var(replicate(1000, DGP\_GLS()))

\# compare the estimated variances est\_var\_GLS \textless{}
est\_var\_OLS
test\_predefined\_objects(c(``DGP\_OLS'',``est\_var\_OLS''))
test\_function\_definition(``DGP\_GLS'', function\_test =
test\_expression\_result(``DGP\_OLS()'')

) test\_object(``est\_var\_GLS'') test\_or(\{
test\_student\_typed(``est\_var\_GLS \textless{} est\_var\_OLS'') \},\{
test\_student\_typed(``est\_var\_GLS \textgreater{} est\_var\_OLS'')
\},\{ test\_student\_typed(``est\_var\_OLS \textgreater{}
est\_var\_GLS'') \},\{ test\_student\_typed(``est\_var\_OLS \textless{}
est\_var\_GLS'') \}) success\_msg(``Nice! This shows that the GLS
estimator indeed has lower variance than OLS.'')

\chapter{Regression Models with Multiple
Regressors}\label{regression-models-with-multiple-regressors}

In what follows we introduce linear regression models that use more than
just one explanatory variable and discuss important key concepts in
multiple regression. As we broaden our scope beyond the relationship of
only two variables (the dependent and a single regressor), some
potential issues arise, such as \emph{multicollinearity} and
\emph{omitted variable bias} (OVB). In particular, this chapter deals
with omitted variables and their hazard to causal interpretation of
OLS-estimated coefficients.

Naturally, we will introduce estimation of multiple regression models
using R. We will also advocate thoughtful usage of multiple regression
models by simulation studies in R that demonstrate the consequences of
using highly correlated regressors or a misspecified model.

\section{Omitted Variable Bias}\label{omitted-variable-bias}

Previous analysis of the relationship between test score and class size
discussed in Chapters 4 and 5 has a major flaw: we ignored other
potentially important determinants of test scores that vary with our
regressor. The influences of those variables were collected in the error
term. This might induce an estimation bias, i.e.~the mean of the OLS
estimator's sampling distribution is no longer equal to the true mean
and we measure a wrong effect on test scores of a unit change in the
student-teacher ratio. This is called omitted variable bias, see Key
Concept 6.1.

Key Concept 6.1

Omitted Variable Bias in Regression with a Single Regressor

Omitted variable bias is the bias in the OLS estimator that arises when
the regressor, \(X\), is \emph{correlated} with an omitted variable. For
omitted variable bias to occur, two conditions must be fulfilled:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(X\) is correlated with the omitted variable.
\item
  The omitted variable is a determinant of the dependent variable \(Y\).
\end{enumerate}

Together, 1. and 2. result in a violation of the first OLS assumption
\(E(u_i\vert X_i) = 0\). Formally, the resulting bias can be expressed
as

\[ \hat\beta_1 \xrightarrow[]{p} \beta_1 + \rho_{Xu} \frac{\sigma_u}{\sigma_X}. \tag{6.1} \]
See Appendix 6.1 in the book for a detailed derivation. (6.1) states
that OVB is a problem that cannot be aleviated by increasing the numner
of observations used to estimate \(\beta_1\) since then \(\hat\beta_1\)
is inconsistent: OVB prevents the estimator to converge in probability
to the true parameter value. Strength and direction of the bias are
driven by \(\rho_{Xu}\), the correlation between the error term and the
regressor.

In our example of test score and class size, it is fairly easy to come
up with variables that may cause such a bias if omitted. As mentioned in
the book, a highly relevant variable could be the percentage of english
learners in the school district: it is plausible that the ability to
speak, read and write english is an important factor for successful
learning. Therefore, students that are still learning english are likely
to perform worse in the tests than native speakers are. Also, it is
conceivable that the share of english learning students is larger in
school districts where class sizes are large. Think of less well-off
urban districs were a lot of immigrants live.

Let us think about a possible bias induced by omitting the share of
english learning students (\(PctEL\)) in view of (6.1) when the
estimated regression model excludes \(PctEL\) although the true DGP is

\[ TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times PctEL \tag{6.2}\]

where \(STR\) and \(PctEL\) are correlated,

\[corr(STR,PctEL)\neq0.\]

After defining our variables in R, we compute the correlation between
\(STR\) and \(PctEL\) as well as the correlation between \(STR\) and
\(TestScore\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the AER package}
\KeywordTok{library}\NormalTok{(AER)}

\CommentTok{# load the data set}
\KeywordTok{data}\NormalTok{(CASchools)   }

\CommentTok{# define variables}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{STR <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{students}\OperatorTok{/}\NormalTok{CASchools}\OperatorTok{$}\NormalTok{teachers       }
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{score <-}\StringTok{ }\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{read }\OperatorTok{+}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{math)}\OperatorTok{/}\DecValTok{2}

\CommentTok{# compute correlations}
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR, CASchools}\OperatorTok{$}\NormalTok{score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.2263627
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR, CASchools}\OperatorTok{$}\NormalTok{english)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1876424
\end{verbatim}

The fact that \(\widehat{corr}(STR, Testscore) = -0.2264\) is cause for
concern that omitting \(PctEL\) leads to a negatively biased estimate
\(\hat\beta_1\) since then \(\rho_{Xu} < 0\) and we have \(\sigma_X>0\),
\(\sigma_u>0\) by definition. As consequence we expect \(\hat\beta_1\),
the coefficient on \(STR\), to be too large in absolute value. Put
differently, the OLS estimate of \(\hat\beta_1\) suggests that small
classes improve test scores but the estimated effect of small classes is
too strong as it captures the effect of having fewer English learners,
too.

What happens to the magnitude of \(\hat\beta_1\) if we add the variable
\(PctEL\) to the regression, that is if we estimate the model
\[ TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times PctEL + u \]

instead? And what do we expect about the sign of, \(\hat\beta_2\), the
estimated coefficient on \(PctEL\)? Following the reasoning above we
should end up with a (still) negative but larger coefficient estimate
\(\hat\beta_1\) and a negative estimate \(\hat\beta_2\).

Let us estimate both regression models and compare. Performing a
multiple regression in R is straightforward. One can simply add
additional variables to the right hand side of the \texttt{formula}
argument of the function \texttt{lm()} by using their names and the
\texttt{+} operator. Notice that \texttt{english} is the name of the
column in the data set \texttt{CASchools} which contains observations on
the share of English learning students.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate both regression models}
\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }\DataTypeTok{data =}\NormalTok{ CASchools) }
\NormalTok{mult.mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\NormalTok{english, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\NormalTok{mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mult.mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR + english, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR      english  
##    686.0322      -1.1013      -0.6498
\end{verbatim}

We find the outcomes to be consistent with our expectations. The
following section discusses some thoery on multiple regression models.

\section{The Multiple Regression
Model}\label{the-multiple-regression-model}

In a multiple regression model we extend the basic concept of the simple
regression model discussed in Chapter 4 and 5. A multiple regression
model enables us to estimate the effect on \(Y_i\) of changing a
regressor \(X_{1i}\) if the remaining regressors
\(X_{2i},X_{3i}\dots,X_{ki}\) \emph{do not vary}. In fact we already
have performed estimation of the multiple regression model (6.2) using R
in the previous section. Recall that the interpretation of the
coefficient on student-teacher ratio is the effect on test scores of a
one unit change student-teacher ratio if the percentage of English
learners is kept constant.

As in the simple regression model, we assume the true relationship
between \(Y\) and \(X_{2i},X_{3i}\dots\) to be linear. On average, this
relation is given by the population regression function

\[ E(Y_i\vert X_{1i}=x_1, X_{2i}=x_2,  X_{3i}=x_3,\dots, X_{ki}=x_k) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \dots + \beta_k x_k. \tag{6.3} \]

As in the simple regression model, this relation does not hold exactly
since there are disturbing influences to the dependent variable \(Y\) we
cannot observe as explanatory variables. Therefore we add an error term
\(u\) which represents deviations of the observations from the
population regression line to (6.3). This yields the population multiple
regression model

\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \dots + \beta_k X_{ki} + u_i \ \ , \ \ i=1,\dots,n. \tag{6.4} \]

Key Concept 6.2

The Multiple Regression Model

The multiple regression model is

\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \dots + \beta_k X_{ki} + u_i \ \ , \ \ i=1,\dots,n.  \]

Designations are similar to those in the simple regression model:

\begin{itemize}
\tightlist
\item
  \(Y_i\) is the \(i^{th}\) observation in the dependent variable.
  Observations on the \(k\) regressors are denoted by
  \(X_{1i},X_{2i},\dots,X_{ki}\) and \(u_i\) is the error term.
\item
  The average relationship between \(Y\) and the regressors is given by
  the population regression line
  \[ E(Y_i\vert X_{1i}=x_1, X_{2i}=x_2,  X_{3i}=x_3,\dots, X_{ki}=x_k) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \dots + \beta_k x_k. \]
\item
  \(\beta_0\) is the intercept; it is the expected value of \(Y\) when
  all \(X\)s equal \(0\). \(\beta_j \ , \ j=1,\dots,k\) are the
  coefficients on \(X_j \ , \ j=1,\dots,k\). \(\beta_1\) measures the
  expected change in \(Y_i\) that results from a one unit change in
  \(X_{1i}\) while holding all other regressors \(X_{ji} \ , \ j\neq1\)
  constant.
\end{itemize}

Key Concept 6.2 summarizes the core concepts of the multiple regression
model. How can we estimate the coefficients of the multiple regression
model (6.4)? We will not go to much into detail on this issue as our
focus lies on usage of R instead of the technical refinements. However
it should be pointed out that, similarly to the simple regression model,
the coefficients of the multiple regression model can be estimated using
OLS. As in the simple model, we seek to minimize the sum of squared
prediction mistakes by choosing estimates \(b_0,b_1,\dots,b_k\) for the
coefficients \(\beta_0,\beta_1,\dots,\beta_k\) such that

\[\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - b_2 X_{2i} - \dots -  b_k X_{ki})^2 \tag{6.5}\]

is minimized. Note that (6.5) is simply an extension of \(SSR\) in the
case with just one regressor and an intercept. The estimators that
minimize (6.5) are hence denoted
\(\hat\beta_0,\hat\beta_1,\dots,\hat\beta_k\) and, as in the simple
regression model, we call them the ordinary least squares estimators of
\(\beta_0,\beta_1,\dots,\beta_k\). For the predicted value of \(Y_i\)
given the regressors and the estimates
\(\hat\beta_0,\hat\beta_1,\dots,\hat\beta_k\) we have

\[ \hat{Y}_i = \hat\beta_0 + \hat\beta_1 X_{1i} + \dots +\hat\beta_k X_{ki}. \]
and, as before, the difference of \(Y_i\) and its predicted value
\(\hat{Y}_i\) is called the OLS residual of observation \(i\):
\(\hat{u} = Y_i - \hat{Y}_i\).

 For further information regarding the theory behind multiple
regression, see Chapter 18.1 in the book which inter alia presents a
derivation of the OLS estimator in the multiple regression model using
matrix notation.

Now let us jump back to the example of test scores and class sizes. The
estimated model object is \texttt{mult.mod}. As for simple regression
models we can use the \texttt{summary()} function to obtain information
on estimated coefficients and model statistics.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(mult.mod)}\OperatorTok{$}\NormalTok{coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Estimate Std. Error    t value      Pr(>|t|)
## (Intercept) 686.0322445 7.41131160  92.565565 3.871327e-280
## STR          -1.1012956 0.38027827  -2.896026  3.978059e-03
## english      -0.6497768 0.03934254 -16.515882  1.657448e-47
\end{verbatim}

So the estimated multiple regression model is

\[ \widehat{TestScore} = \underset{(7.41)}{686.03} - \underset{(0.38)}{1.10} \times STR - \underset{(0.04)}{0.65} \times PctEL \tag{6.6}.  \]

Unlike in the simple regression model where the data can be represented
by points in the two-dimensional cartesian coordinate system, we are now
dealing with three dimensions. Hence observations can be represented by
points in the three-dimensional real space, denoted \(\mathbb{R}^3\).
Therefore (6.6) is now longer a regression line but a \emph{regression
plane}. This idea extends to higher dimensions when we further expand
the number of regressors \(k\). We then say that the regression model
can be represented a hyperplane in the \(k+1\) dimensional space. It is
already hard to imagine such a space if \(k=3\) and we best stick with
the general idea that, in the multiple regression model, the dependent
variable is explained by a \emph{linear combination of the regressors}.
However, in the present case we are able to visualize the situation. The
following figure is an interactive 3D visualization of the data and the
estimated regression plane (6.6).

\hypertarget{381c145c764c}{}

We observe that the estimated regression plane fits the data reasonably
well --- at least with regard to the shape and spatial poistion of the
point cloud. The color of the markers is an indicator for the absolute
deviation from the predicted regression plane. Observations that are
coloured more reddish lie close to the regression plane while the color
shifts to blue with growing distance. An anomaly that can be seen from
the plot is that there might be heteroskedasticity: we see that the
dispersion of regression errors made, i.e.~the distance of observations
to the regression plane shows a tendency to decrease as the share of
English learning students increases.

\section{Measures of Fit in Multiple
Regression}\label{measures-of-fit-in-multiple-regression}

In multiple regression, common summary statistics are \(SER\),
\(R\)\^{}2 and the adjusted \(R^2\).

Taking the code from Section 6.3 You could simply use
\texttt{summary(mult.mod)} to print the \(SER\), \(R^2\) and
adjusted-\(R^2\). For multiple regression models the \(SER\) is computed
as

\[ SER = s_{\hat u} = \sqrt{s_{\hat u}^2} \] where, in contrast to the
simple regression model we apply a modification to the denominator of
the premultiplied factor in \(s_{\hat u}^2\) in order to accommodate for
additional regressors. Thus,

\[ s_{\hat u}^2 = \frac{1}{n-k-1} \, SSR \]

with \(k\) the number of regressors \emph{excluding} the intercept.
While \texttt{summary()} computes the \(R^2\) just as in the case of a
single regressor, it is not a reliable measure for multiple regression
models. This is due to \(R^2\) becoming larger every time an additional
regressor is added to the model since adding a regressor decreases the
\(SSR\) --- at least unless the respective estimated coefficient is
exactly zero what practically never happens to be the case (see chapter
6.4 in the book). The adjusted \(R^2\) takes this into consideration by
``punishing'' the addition of regressors using a correction factor. So
the adjusted \(R^2\) or simply \(\overline{R^2}\) is a modified version
of \(R^2\).

\[ \overline{R^2} = 1-\frac{n-1}{n-k-1} \, \frac{SSR}{TSS} \]

As You may have already suspected, \texttt{summary()} adjusts the
formula for \(SER\) by itself and it computes \(\overline{R^2}\) and of
course \(R^2\) by default, therby leaving the decision which measure to
rely on up to the user.

You can find the measures at the bottom of the output produced by
calling \texttt{summary(mult.mod)}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(mult.mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR + english, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.845 -10.240  -0.308   9.815  43.461 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 686.03224    7.41131  92.566  < 2e-16 ***
## STR          -1.10130    0.38028  -2.896  0.00398 ** 
## english      -0.64978    0.03934 -16.516  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.46 on 417 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4237 
## F-statistic:   155 on 2 and 417 DF,  p-value: < 2.2e-16
\end{verbatim}

We can also compute the measures by hand using the formulas above. Let
us check if the results coincide with the values provided by
\texttt{summary()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the components}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(CASchools)                            }\CommentTok{# number of observations (rows)}
\NormalTok{k <-}\StringTok{ }\DecValTok{2}                                          \CommentTok{# number of regressors}

\NormalTok{y_mean <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score)                 }\CommentTok{# mean of avg. test-scores}

\NormalTok{SSR <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(mult.mod)}\OperatorTok{^}\DecValTok{2}\NormalTok{)               }\CommentTok{# sum of squared residuals}
\NormalTok{TSS <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((CASchools}\OperatorTok{$}\NormalTok{score }\OperatorTok{-}\StringTok{ }\NormalTok{y_mean )}\OperatorTok{^}\DecValTok{2}\NormalTok{)       }\CommentTok{# total sum of squares}
\NormalTok{ESS <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((}\KeywordTok{fitted}\NormalTok{(mult.mod) }\OperatorTok{-}\StringTok{ }\NormalTok{y_mean)}\OperatorTok{^}\DecValTok{2}\NormalTok{)       }\CommentTok{# explained sum of squares}

\CommentTok{# compute the measures}

\NormalTok{SER <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{(n}\OperatorTok{-}\NormalTok{k}\OperatorTok{-}\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{SSR)                    }\CommentTok{# standard error of the regression}
\NormalTok{Rsq <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{(SSR }\OperatorTok{/}\StringTok{ }\NormalTok{TSS)                          }\CommentTok{# R^2}
\NormalTok{adj_Rsq <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{(n}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{(n}\OperatorTok{-}\NormalTok{k}\OperatorTok{-}\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{SSR}\OperatorTok{/}\NormalTok{TSS          }\CommentTok{# adj. R^2}

\CommentTok{# Print the measures to the console}
\KeywordTok{c}\NormalTok{(}\StringTok{"SER"}\NormalTok{ =}\StringTok{ }\NormalTok{SER, }\StringTok{"R2"}\NormalTok{ =}\StringTok{ }\NormalTok{Rsq, }\StringTok{"Adj.R2"}\NormalTok{ =}\StringTok{ }\NormalTok{adj_Rsq)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        SER         R2     Adj.R2 
## 14.4644831  0.4264315  0.4236805
\end{verbatim}

We find that the results do match. Now, what can be said about the fit
of our multiple regression model for test scores with the percentage of
english learners as an additional regressor? Does it improve on the
simple model including only an intercept and a measure of the class
size? The answer is yes: this is easily seen by comparing these measures
of fit with those for the simple regression model \texttt{mod}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# SER}
\KeywordTok{summary}\NormalTok{(mod)}\OperatorTok{$}\NormalTok{sigma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18.58097
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# R^2}
\KeywordTok{summary}\NormalTok{(mod)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05124009
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Adj. R^2}
\KeywordTok{summary}\NormalTok{(mod)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.04897033
\end{verbatim}

Including \(PctEL\) as a regressor boots the \(R^2\) from about \(5\%\)
to \(42.6\%\). Qualitatively the same is observed for \(\overline{R^2}\)
which we deem to be more reliable in view of the discussion above.
Notice that the difference between \(R^2\) and \(\overline{R^2}\) is
small since \(k=2\) and \(n\) is large. Condensed, the fit of (6.6)
improves vastly on the fit of the simple regression model with \(STR\)
as the only regressor. Comparing prediction errors we find that the
prediction precision of the multiple regression model (6.6) improves
upon the simple model as adding \(PctEL\) lowers the \(SER\) from
\(18.6\) to \(14.5\) units of test score.

 As already mentioned \(R^2\) and \(\overline{R^2}\) may be used to
quantify how good a model fits the data. However it is rarely a good
idea to maximize these measures by stuffing the model with regressors in
general. You will (hopefully) not find any serious study that does so.
Instead it is more fruitful to include regressors that improve
estimation of the causal effect of interest which is \emph{not} assessed
by means of a low \(SSR\). The issue of variable selection is covered in
Chapter 7 of this script and Chapter 7 in the book.

\section{OLS Assumptions in Multiple
Regression}\label{ols-assumptions-in-multiple-regression}

In the multiple regression model we extend the known three least squares
assumptions imposed for the simple regression model (see
\protect\hyperlink{ch4}{Chapter 4}, Key Concept 4.3) and add a fourth
assumption. These assumptions are presented in Key Concept 6.4. While we
will not go into the details of assumptions 1, 2 and 3 since their ideas
have been discussed before and are easily generalized to the case of
multiple regressors, we will devote our attention to the fourth
assumption. This forth assumption forbids the occurence of perfect
correlation between any pair of regressors.

Key Concept 6.4

The Least Squares Assumptions in the Multiple Regression Model

The multiple regression model is given by

\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_1 X_{2i} + \dots + \beta_k X_{ki} + u_i \ , \ i=1,\dots,n. \]

The OLS assumptions in the multiple regression model are an extension of
the ones made for the simple regression model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regressors \((X_{1i}, X_{2i}, \dots, X_{ki}, Y_i) \ , \ i=1,\dots,n\)
  are drawn such that the i.i.d. assumption holds.
\item
  \(u_i\) is an error term with conditional zero given the regressors,
  i.e. \[ E(u_i\vert X_{1i}, X_{2i}, \dots, X_{ki}) = 0. \]
\item
  Large outliers are unlikely, formally \(X_{1i},\dots,X_{ki}\) and
  \(Y_i\) have finite fourth moments.
\item
  No perfect multicollinearity.
\end{enumerate}

\subsection*{Multicollinearity}\label{multicollinearity}
\addcontentsline{toc}{subsection}{Multicollinearity}

If two or more regressors in a multiple regression model are
\emph{strongly} correlated we say that there is
\emph{multicollinearity}. If the correlation between two or more
regressors is perfect (correlation coeficient equals \(1\)), we refer to
the situation as \emph{perfect multicollinearity}. Perfectly
multicollinear regressors can be written as linear combinations of each
other. While strong multicollinearity in general is unpleasent as it
causes the variance of the OLS estimator to be large (we will discuss is
in more detail later), the presence of perfect multicollinearity makes
it even impossible to solve for the OLS estimator, that is the model
cannot be estimated at all.

The next section presents some examples of perfect multicollinearity and
demonstrates how R, specifically the \texttt{lm} function deals with
them.

\subsubsection*{Examples of Perfect
Multicollinearity}\label{examples-of-perfect-multicollinearity}
\addcontentsline{toc}{subsubsection}{Examples of Perfect
Multicollinearity}

How does R react if we want it to estimate a model with perfectly
correlated regressors?

If we use the \texttt{lm} function to estimate a model with a set of
regressors that suffer from perfect multicollinearity the system will
produce a warning in the first line of the coefficient section of the
output (1 not defined because of singularities) and ignore the
regressor(s) which is (are) assumed to be a linear combination of the
others. See the following example where we add another variable
\texttt{FracEL}, the fraction of Englisch learners to \texttt{CASchools}
where observations are scaled values of the observations for
\texttt{english} and use it as a regressor together with \texttt{STR}
and \texttt{english} in a multiple regression model. In this example
\texttt{english} and \texttt{FracEL} are perfectly collinear. The R code
is as follows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the fraction of english learners        }
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{FracEL <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{english}\OperatorTok{/}\DecValTok{100}

\CommentTok{# estimate the model}
\NormalTok{mult.mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{FracEL, }\DataTypeTok{data =}\NormalTok{ CASchools) }

\CommentTok{# obtain a summary of the model}
\KeywordTok{summary}\NormalTok{(mult.mod)                                                 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR + english + FracEL, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.845 -10.240  -0.308   9.815  43.461 
## 
## Coefficients: (1 not defined because of singularities)
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 686.03224    7.41131  92.566  < 2e-16 ***
## STR          -1.10130    0.38028  -2.896  0.00398 ** 
## english      -0.64978    0.03934 -16.516  < 2e-16 ***
## FracEL             NA         NA      NA       NA    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.46 on 417 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4237 
## F-statistic:   155 on 2 and 417 DF,  p-value: < 2.2e-16
\end{verbatim}

Notice that the row \texttt{FracEL} in the coefficients section of the
output consists of \texttt{NA} entries since \texttt{FracEL} was
excluded from the model.

If we were to compute OLS by hand we would run into the problem as well
but no one would be helping us out! The computation simply fails. Why is
this the case? Take the following example:

Assume You want to estimate a simple linear regression model with an
intercept and one regressor:

\[ Y_i = \beta_0 + \beta_1 X_i + u_i\]

As mentioned above, for perfect multicollinearity to be present \(X\)
has to be a linear combination of the other regressors. Since the only
other regressor is a constant (think of the right hand side of the model
equation as \(\beta_0 \times 1 + \beta_1 X_i + u_i\) so that \(\beta_1\)
is always multiplied by \(1\) for every observation), \(X\) has to be
constant as well. For \(\hat\beta_1\) we have

\[ \hat{\beta_1} =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2} = \frac{\widehat{cov}(X,Y)}{\widehat{Var}(X)}. \tag{6.7} \]

So the variance of the regressor \(X\) stands in the denominator. Since
the variance of a constant is zero, we are not able to compute this
fraction and \(\hat{\beta}_1\) remains undefined.

Note: In this special case the nominator in (6.7) equal zero, too. Can
You show that?

Let us behold two further examples where our selection of regressors
induces perfect multicollinearity. First, assume that we intend to
analyse the effect of class size on test score by using a dummy variable
that identifies ``Not very small'' classes (NVS). We define that a
school has the NVS attribute when the school's average student-teacher
ratio is at least \(12\).

\[ NVS = \begin{cases} 0 \ \ \ \text{if STR < 12} \\ 1 \ \ \ \text{otherwise} \end{cases} \]

We add the corresponding column to \texttt{CASchools} and estimate a
multiple regression model with covariates \texttt{computer} and
\texttt{english}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# if STR smaller 12, NVS = 0, else NVS = 1}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{NVS <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR }\OperatorTok{<}\StringTok{ }\DecValTok{12}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{# estimate the model}
\NormalTok{mult.mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{computer }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{NVS, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# obtain a model summary}
\KeywordTok{summary}\NormalTok{(mult.mod)                                                  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ computer + english + NVS, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -49.492  -9.976  -0.778   8.761  43.798 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 663.704837   0.984259 674.319  < 2e-16 ***
## computer      0.005374   0.001670   3.218  0.00139 ** 
## english      -0.708947   0.040303 -17.591  < 2e-16 ***
## NVS                 NA         NA      NA       NA    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.43 on 417 degrees of freedom
## Multiple R-squared:  0.4291, Adjusted R-squared:  0.4263 
## F-statistic: 156.7 on 2 and 417 DF,  p-value: < 2.2e-16
\end{verbatim}

Again, the output of \texttt{summary(mult.mod)} tells us that inclusion
of \texttt{NVS} in the regression would render the estimation
infeasible. What happened here? This is an example where we made a
logical mistake when defining the regressor \texttt{NVS}: if we had
taken a closer look at \texttt{NVS}, the redefined measure for class
size, we would have noticed that there is not a single school with
\(STR<12\) hence \(NVS\) equals one for all observations. We can check
this by printing the contents of \texttt{CASchools\$NVS} to the console
or by using the function \texttt{table}, see \texttt{?table}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{NVS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   1 
## 420
\end{verbatim}

\texttt{CASchools\$NVS} is a vector of \(420\) ones and our data set
includes \(420\) observations. This obviously violates assumption 4 of
Key Concept 6.4: remember that observations for the intercept are always
\(1\) so we have

\begin{align}
  intercept = \, & \lambda \cdot NVS \\
  \\
  \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} = \, & \lambda \cdot \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} \\   \Leftrightarrow \, & \lambda = 1.
\end{align}

Since both regressors can be written as a linear combination of each
other, we face perfect multicollinearity and R excludes \(NVS\) from the
model. Thus the message to take away is: think carefully about how
regressors You created Yourself could possibly interact with
unrecognized features of the data set!

Another example of perfect multicollinearity is known as the ``dummy
variable trap''. This may occur when multiple dummy variables are used
as regressors. A common case for this is when the dummies are used to
sort the data set into mutually exclusive categories. For example,
suppose we have spatial information that indicates whether a school is
located in the North, West, South or East of the US which allows us to
create the dummy variables

\begin{align}
  North_i =& 
  \begin{cases}
    1 \ \ \text{if located in the north} \\
    0 \ \ \text{otherwise}
  \end{cases} \\
  \\
    West_i =& 
  \begin{cases}
    1 \ \ \text{if located in the west} \\
    0 \ \ \text{otherwise}
  \end{cases} \\
  \\
    South_i =& 
  \begin{cases}
    1 \ \ \text{if located in the south} \\
    0 \ \ \text{otherwise}
  \end{cases} \\
    \\
    East_i =& 
  \begin{cases}
    1 \ \ \text{if located in the east} \\
    0 \ \ \text{otherwise}.
  \end{cases}
\end{align}

Since the directions are mutually exclusive, for every school
\(i=1,\dots,n\) we have

\[ North_i + West_i + South_i + East_i = 1. \]

We run into problems when trying to estimate a model that includes a
constant and \emph{all four} direction dummies, e.g.~in the model

\[ TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times english + \beta_3 \times North_i + \beta_4 \times West_i + \beta_5 \times South_i + \beta_6 \times East_i + u_i \tag{6.8}\]
since then for all observations \(i=1,\dots,n\) the constant term can be
written as a linear combination of the dummies:

\begin{align}
  intercept = \, & \lambda_1 \cdot (North + West + South + East) \\
  \\
  \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} = \, & \lambda \cdot \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} \\   \Leftrightarrow \, & \lambda = 1
\end{align}

and we have perfect multicollinearity. This is what is meant by the
``dummy variable trap'': not paying attention and erroneously including
all dummies and an intercept term in a regression model.

How does the \texttt{lm} function in R handle a regression like (6.8)?
To answer this we first generate some artifical categorical data and
append a new column named \texttt{directions} to \texttt{CASchools} and
see what \texttt{lm} does when asked to estimate the model above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# Generate artificial data on location}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{direction <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"West"}\NormalTok{,}\StringTok{"North"}\NormalTok{,}\StringTok{"South"}\NormalTok{,}\StringTok{"East"}\NormalTok{), }\DecValTok{420}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T)}

\CommentTok{# estimate the model}
\NormalTok{mult.mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{direction, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# obtain a model summary}
\KeywordTok{summary}\NormalTok{(mult.mod)                                                 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR + english + direction, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.018 -10.098  -0.556   9.304  42.596 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    685.67356    7.43308  92.246  < 2e-16 ***
## STR             -1.12246    0.38231  -2.936  0.00351 ** 
## english         -0.65096    0.03934 -16.549  < 2e-16 ***
## directionNorth   1.60680    1.92476   0.835  0.40431    
## directionSouth  -1.17013    2.07665  -0.563  0.57342    
## directionWest    2.44340    2.05191   1.191  0.23442    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.45 on 414 degrees of freedom
## Multiple R-squared:  0.4315, Adjusted R-squared:  0.4246 
## F-statistic: 62.85 on 5 and 414 DF,  p-value: < 2.2e-16
\end{verbatim}

Notice that R solves the problem sketched above on its own by generating
and including the dummies \texttt{directionNorth},
\texttt{directionSouth} and \texttt{directionWest} but ommitting
\texttt{directionEast}. Of course, omission of every other dummy instead
would be achieve the same. This is done by convention. Another solution
would be to exclude the intercept and include all dummies instead. Does
this mean that the information on schools located in the East is
discarded? Furtunately, this is not the case: exclusion of
\texttt{directEast} just alters the interpretation of coefficient
estimates on the remaining dummies from absolute to relative. For
example, the coefficient estimate on \texttt{directionNorth} states
that, on average, test scores in the North are about 1.61 points higher
than in the East.

A last example considers the case where a perfect linear relationship
arises from redundant regressors. Suppose we have a regressor \(PctES\),
the percentage of English speakers in the school where

\[ PctES = 100 -  PctEL\]

and both \(PctES\) and \(PctEL\) are included in a regression model. One
regressor is redundant since the other one conveys the same information.
Since this obviously is a case where the regressors can be written as
linear combination, we end up with perfect multicollinearity, again.

Let us do this in R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Percentage of english speakers }
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{PctES <-}\StringTok{ }\DecValTok{100} \OperatorTok{-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{english}

\CommentTok{# estimate the model}
\NormalTok{mult.mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{PctES, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# obtain a model summary}
\KeywordTok{summary}\NormalTok{(mult.mod)                                                 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR + english + PctES, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.845 -10.240  -0.308   9.815  43.461 
## 
## Coefficients: (1 not defined because of singularities)
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 686.03224    7.41131  92.566  < 2e-16 ***
## STR          -1.10130    0.38028  -2.896  0.00398 ** 
## english      -0.64978    0.03934 -16.516  < 2e-16 ***
## PctES              NA         NA      NA       NA    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.46 on 417 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4237 
## F-statistic:   155 on 2 and 417 DF,  p-value: < 2.2e-16
\end{verbatim}

Once more, the \texttt{lm} function refuses to estimate the full model
using OLS and excludes \texttt{PctES}.

See chapter 18.1 of the book for an explanation of perfect
multicollinearity and its consequences to the OLS estimator in general
multiple regression models using matrix notation.

\subsubsection*{Imperfect
Multicollinearity}\label{imperfect-multicollinearity}
\addcontentsline{toc}{subsubsection}{Imperfect Multicollinearity}

As opposed to perfect multicollinearity, imperfect multicollinearity is
--- to a certain extend --- not a problem at all. In fact, imperfect
multicollinearity is the reason why we are interested in estimating
multiple regression models in the first place: the OLS estimator allows
us to \emph{isolate} influences of \emph{correlated} regressors on the
dependent variable. So if it was not for these dependencies, there would
not be a reason to resort to a multiple regression approach and we could
simply work with a single-regressor model. However, reality is
complicated and this is rarely the case. Moreover, we already know that
ignoring dependencies among regressors which influence the outcome
variable has an adverse effect on estimation results (keyword: omitted
variable bias!).

So when and why is imperfect multicollinearity a problem? Suppose You
got the regression model

\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i \tag{6.9} \]

and You are interested in estimating \(\beta_1\), the effect on \(Y_i\)
of a one unit change in \(X_{1i}\) while holding \(X_{2i}\) constant.
You do not know that the true model indeed includes \(X_2\) but You
follow some reasoning and add \(X_2\) as a covariate to the model in
order to address a potential omitted variable bias. You are confident
that \(E(u_i\vert X_{1i}, X_{2i})=0\) for all observations and there is
no reason to suspect violation of the assumptions 2 and 3 made in Key
Concept 6.4. If now \(X_1\) and \(X_2\) are highly correlated, OLS has
its difficulties to estimate \(\beta_1\) precisely. That means although
\(\hat\beta_1\) is a consistent and unbiased estimator for \(\beta_1\),
it has a large variance due to \(X_2\) being included in the model. If
the errors are homoskedastic, this issue can be better understood from
inspecting the formula for the variance of \(\hat\beta_1\) in the model
(6.9) (cf.~Appendix 6.2 of the book):

\[ \sigma^2_{\hat\beta_1} = \frac{1}{n} \left( \frac{1}{1-\rho^2_{X_1,X_2}} \right) \frac{\sigma^2_u}{\sigma^2_{X_1}} \tag{6.10} \]

Firstly, notice that if \(\rho_{X_1,X_2}=0\) i.e.~if there is no
correlation between both regressors, including \(X_2\) in the model has
no influence on the variance of \(\hat\beta_1\). Secondly, if \(X_1\)
and \(X_2\) are correlated, \(\sigma^2_{\hat\beta_1}\) is inversely
proportional to \(1-\rho^2_{X_1,X_2}\) so the stronger the correlation
between \(X_1\) and \(X_2\), the smaller is \(1-\rho^2_{X_1,X_2}\) and
thus the bigger is the variance of \(\hat\beta_1\). Thirdly, (6.10)
reveals that, in any case, increasing the sample size helps to reduce
the variance of \(\hat\beta_1\). Of course, this is not limited to the
case with two regressors: in general multiple regression, imperfect
multicollinearity inflates the variance of one or more coefficient
estimators and it is an empirical question which estimators are severly
affected by this and which are not. So is the decision whether one wants
to hazard the consequences of adding a large number of covariates when
the sample size is small.

In sum, undesirable consequences of imperfect multicollinearity are not
the result of a logical error made by the researcher (as is often the
case for perfect multicollinearity) but are rather a problem that is
linked to the data used, the model to be estimated and the research
question at hand.

Let us conduct a simulation study to illustrate the issues sketched
above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We use (6.9) as the data generating process and choose
  \(\beta_0 = 5\), \(\beta_1 = 2.5\) and \(\beta_2 = 3\) and \(u_i\) is
  an error term distributed as \(\mathcal{N}(0,5)\). In a first step, we
  sample the regressor data from a bivariate normal distribution:
  \[ X_i = (X_{1i}, X_{2i}) \overset{i.i.d.}{\sim} \mathcal{N} \left[\begin{pmatrix} 0 \\ 0  \end{pmatrix}, \begin{pmatrix} 10 & 2.5 \\ 2.5 & 10 \end{pmatrix} \right] \]
  It is straightforward to see that the correlation between \(X_1\) and
  \(X_2\) in the population is rather low:
\end{enumerate}

\[ \rho_{X_1,X_2} = \frac{cov(X_1,X_2)}{\sqrt{Var(X_1)}\sqrt{Var{(X_2)}}} = \frac{2.5}{10} = 0.25 \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Next, we estimate the model (6.9) and save the estimates for
  \(\beta_1\) and \(\beta_2\). This is repeated \(10000\) times with a
  \texttt{for} loop so we end up with a large number of estimates that
  allow us to describe the distributions of \(\hat\beta_1\) and
  \(\hat\beta_2\).
\item
  We repeat steps 1 and 2 but increase the covariance between \(X_1\)
  and \(X_2\) from \(2.5\) to \(8.5\) such that the correlation between
  the regressors is high:
  \[ \rho_{X_1,X_2} = \frac{cov(X_1,X_2)}{\sqrt{Var(X_1)}\sqrt{Var{(X_2)}}} = \frac{8.5}{10} = 0.85 \]
\item
  In order to assess the effect on the precision of the estimators of
  increasing the collinearity between \(X_1\) and \(X_2\) we compute
  estimates of the variances of \(\hat\beta_1\) and \(\hat\beta_2\) and
  compare.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{library}\NormalTok{(mvtnorm)}

\CommentTok{# set number of observations}
\NormalTok{n <-}\StringTok{ }\DecValTok{50}

\CommentTok{# initialize vectors of coefficients}
\NormalTok{coefs1 <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\StringTok{"hat_beta_1"}\NormalTok{ =}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{), }\StringTok{"hat_beta_2"}\NormalTok{ =}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{))}
\NormalTok{coefs2 <-}\StringTok{ }\NormalTok{coefs1}

\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# loop sampling and estimation}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
 
  \CommentTok{# for cov(X_1,X_2) = 0.25}
\NormalTok{  X <-}\StringTok{ }\KeywordTok{rmvnorm}\NormalTok{(n, }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{100}\NormalTok{), }\DataTypeTok{sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\FloatTok{2.5}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{,}\DecValTok{10}\NormalTok{)))}
\NormalTok{  u <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\DecValTok{5}\NormalTok{)}
\NormalTok{  Y <-}\StringTok{ }\DecValTok{5} \OperatorTok{+}\StringTok{ }\FloatTok{2.5}\OperatorTok{*}\NormalTok{X[,}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\DecValTok{3}\OperatorTok{*}\NormalTok{X[,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{u}
\NormalTok{  coefs1[i,] <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X[,}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{X[,}\DecValTok{2}\NormalTok{])}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
  
  \CommentTok{# for cov(X_1,X_2) = 0.85}
\NormalTok{  X <-}\StringTok{ }\KeywordTok{rmvnorm}\NormalTok{(n, }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{100}\NormalTok{), }\DataTypeTok{sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\FloatTok{8.5}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\FloatTok{8.5}\NormalTok{,}\DecValTok{10}\NormalTok{)))}
\NormalTok{  Y <-}\StringTok{ }\DecValTok{5} \OperatorTok{+}\StringTok{ }\FloatTok{2.5}\OperatorTok{*}\NormalTok{X[,}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\DecValTok{3}\OperatorTok{*}\NormalTok{X[,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{u}
\NormalTok{  coefs2[i,] <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X[,}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{X[,}\DecValTok{2}\NormalTok{])}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}

\CommentTok{# estimate the variances}
\KeywordTok{var}\NormalTok{(coefs1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             hat_beta_1  hat_beta_2
## hat_beta_1  0.05674375 -0.01387725
## hat_beta_2 -0.01387725  0.05712459
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(coefs2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            hat_beta_1 hat_beta_2
## hat_beta_1  0.1904949 -0.1610405
## hat_beta_2 -0.1610405  0.1909056
\end{verbatim}

Since we call \texttt{var} on matrices rather than vectors, the oucomes
are variance-covariance matrices. We are interested in the variances
which are the diagonal elements. We see that due to the high
collinearity, the variances of \(\hat\beta_1\) and \(\hat\beta_2\) have
more than tripled: in Econometrics lingo we say that it has become more
difficult to estimate the true coefficients precisely.

\section{The Distribution of the OLS Estimators in Multiple
Regression}\label{the-distribution-of-the-ols-estimators-in-multiple-regression}

As in simple linear regression, different samples will produce different
values of the OLS estimators in the multiple regression model. Here
again this variation leads to uncertainty of those estimators and we
seek to describe it using their sampling distribution(s). In short, if
the assumption made in Key Concept 6.4 hold, the large sample
distribution of \(\hat\beta_0,\hat\beta_1,\dots,\hat\beta_k\) is
multivariate normal and the individual estimators themselves are also
normally distributed. Key Concept 6.5 summarizes the corresponding
statements made in Chapter 6.6 of the book. A more technical derivation
of these results can be found in Chapter 18 of the book.

Key Concept 6.5

Large-sample distribution of
\(\hat\beta_0,\hat\beta_1,\dots,\hat\beta_k\)

If the least squares assumptions in the multiple regression model (see
Key Concept 6.4) hold, then in large samples, the OLS estimators
\(\hat\beta_0,\hat\beta_1,\dots,\hat\beta_k\) are jointly normally
distributed. We also say that their joint distribution is
\emph{multivariate} normal. Further, each \(\hat\beta_j\) is distributed
as \(N(\beta_j,\sigma_{\beta_j}^2)\).

Essentially, Key Concept 6.5 states that, if the sample size is large,
we can approximate the individual sampling distributions of the
coefficient estimators by specific normal distributions and their joint
sampling distribution by a multivariate normal distribution.

How can we use R two get a notion of what the joint PDF of the
coefficient estimators in multiple regression model looks like? When
estimating some model on arbitrary data once, we would end up with a set
of point estimates that do not reveal any information on the joint
density of the estimators. However, with a large number of estimations
using repeatedly randomly sampled data from the same population we could
generate a large set of point estimates that allows us to plot an
\emph{estimate} of the joint density function.

The approach we will use is a follows:

\begin{itemize}
\tightlist
\item
  Generate \(10000\) random samples of size \(75\) using the DGP
\end{itemize}

\[ Y_i = 5 + 2.5 \cdot X_{1i} + 3 \cdot X_{2i} + u_i  \]

where the regressors \(X_{1i}\) and \(X_{2i}\) are sampled for each
observations as

\[ X_i = (X_{1i}, X_{2i}) \sim \mathcal{N} \left[\begin{pmatrix} 0 \\ 0  \end{pmatrix}, \begin{pmatrix} 10 & 2.5 \\ 2.5 & 10 \end{pmatrix} \right] \]

and

\[ u_i \sim \mathcal{N}(0,5) \]

is an error term.

\begin{itemize}
\tightlist
\item
  For each of the \(10000\) simulated sets of sample data, we estimate
  the model
\end{itemize}

\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i \] and save the
coefficient estimates \(\hat\beta_1\) and \(\hat\beta_2\).

\begin{itemize}
\tightlist
\item
  We compute a density estimate of the joint distribution of
  \(\hat\beta_1\) and \(\hat\beta_2\) in the model above using the
  function \texttt{kde2d} from the package \texttt{MASS}, see
  \texttt{?MASS}. This estimate is then plotted using the function
  \texttt{persp}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{library}\NormalTok{(mvtnorm)}

\CommentTok{# set sample size}
\NormalTok{n <-}\StringTok{ }\DecValTok{50}

\CommentTok{# initialize vector of coefficients}
\NormalTok{coefs <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\StringTok{"hat_beta_1"}\NormalTok{ =}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{), }\StringTok{"hat_beta_2"}\NormalTok{ =}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{))}

\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# loop sampling and estimation}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
\NormalTok{  X <-}\StringTok{ }\KeywordTok{rmvnorm}\NormalTok{(n, }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{100}\NormalTok{), }\DataTypeTok{sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\FloatTok{2.5}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{,}\DecValTok{10}\NormalTok{)))}
\NormalTok{  u <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\DecValTok{5}\NormalTok{)}
\NormalTok{  Y <-}\StringTok{ }\DecValTok{5} \OperatorTok{+}\StringTok{ }\FloatTok{2.5}\OperatorTok{*}\NormalTok{X[,}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\DecValTok{3}\OperatorTok{*}\NormalTok{X[,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{u}
\NormalTok{  coefs[i,] <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X[,}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{X[,}\DecValTok{2}\NormalTok{])}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}

\CommentTok{# Compute density estimate}
\NormalTok{kde <-}\StringTok{ }\KeywordTok{kde2d}\NormalTok{(coefs[,}\DecValTok{1}\NormalTok{], coefs[,}\DecValTok{2}\NormalTok{])}

\CommentTok{# plot density estimate}
\KeywordTok{persp}\NormalTok{(kde,  }\DataTypeTok{theta =} \DecValTok{310}\NormalTok{, }\DataTypeTok{phi =} \DecValTok{30}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"beta_1"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"beta_2"}\NormalTok{, }\DataTypeTok{zlab =} \StringTok{"Est. Density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-140-1} \end{center}

From the plot above we can see that the density estimate has some
similarity to a bivariate normal distribution (see \href{Ch2}{chapter
2}) though it is not very pretty and probably a little rough.
Furthermore, there is correlation between the estimators such that
\(\rho\neq0\) in (2.1). Also, the distribution's shape deviates from the
symmetric bell shape of the bivariate standard normal distribution and
has an eliptical surface area instead.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate correlation between estimators}
\KeywordTok{cor}\NormalTok{(coefs[,}\DecValTok{1}\NormalTok{], coefs[,}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.2503028
\end{verbatim}

Where does this correlation come from? Notice that, due to the way we
generated the data, there is correlation between the regressors \(X_1\)
and \(X_2\). Correlation between the regressors in a multiple regression
model always translates to correlation between the estimators (see
Appendix 6.2 of the book). In our case, the positive correlation between
\(X_1\) and \(X_2\) translates to negative correlation between
\(\hat\beta_1\) and \(\hat\beta_2\). To get a better feeling You can
vary the point of view in the subsequent smooth interactive 3D plot of
the same density estimate used for plotting with \texttt{persp}. Here
You can see that the shape of the distribution is somewhat stretched due
to \(\hat\rho=-0.20\) and it is also apparent that both estimators are
unbiased since their joint density seems to be centered close to the
true parameter vector \((\beta_1,\beta_2) = (2.5,3)\).

\hypertarget{381c6074ab3f}{}

\section{Exercises}\label{exercises-4}

For the course of this section, you will work with Boston, the Boston
Housing data set which contains 506 observations on housing values in
suburbs of Boston. The data set comes with the package MASS which is
already installed for the interactive R exercises below.

\subsubsection*{1. The Boston Housing Data
Set}\label{the-boston-housing-data-set}
\addcontentsline{toc}{subsubsection}{1. The Boston Housing Data Set}

\textbf{Instructions}

\begin{itemize}
\item
  Load both the package and the data set.
\item
  Get yourself an overview over the data set using the summary function.
  Use ?Boston for detailed information on the variables.
\item
  Estimate a simple linear regression model explaining the median house
  value (medv) by the percent of households with low socioecononomic
  status, lstat, and a constant. Save the model to bh\_mod.
\item
  Print a coefficient summary to the console that reports robust
  standard errors.
\end{itemize}

\textbf{Hint}

You only need basic functions here: library(), data(), lm() and
coeftest().

 \# attach both packages and load the data set

\chapter{Obtain an overview over the data
set}\label{obtain-an-overview-over-the-data-set}

\chapter{Estimate the simple regression
model}\label{estimate-the-simple-regression-model}

\chapter{print the summary to the
console}\label{print-the-summary-to-the-console}

 \# attach both packages and load the data set library(AER)
library(MASS) data(``Boston'') \# Obtain an overview over the data set
summary(Boston) \# Estimate the simple regression model bh\_mod
\textless{}- lm(medv \textasciitilde{} lstat, data = Boston) \# print
the summary to the console coeftest(bh\_mod, vcov. = vcovHC)
test\_function(``library'', index = 1) test\_function(``library'', index
= 2) test\_function(``data'')

test\_or(\{ test\_function(``coeftest'', args=c(``x'',``vcov.'')) \},\{
f \textless{}- ex() \%\textgreater{}\%
override\_solution(``coeftest(bh\_mod, vcov. = vcovHC)'')
\%\textgreater{}\% check\_function(`coeftest') f \%\textgreater{}\%
check\_arg(``x'') \%\textgreater{}\% check\_equal() f \%\textgreater{}\%
check\_arg(``vcov.'') \%\textgreater{}\% check\_equal() \},\{ f
\textless{}- ex() \%\textgreater{}\%
override\_solution(``coeftest(bh\_mod, vcov. = vcovHC(bh\_mod, type =
`HC0'))'') \%\textgreater{}\% check\_function(`coeftest') f
\%\textgreater{}\% check\_arg(``x'') \%\textgreater{}\% check\_equal() f
\%\textgreater{}\% check\_arg(``vcov.'') \%\textgreater{}\%
check\_equal() \},\{ f \textless{}- ex() \%\textgreater{}\%
override\_solution(``coeftest(bh\_mod, vcov. = vcovHC(bh\_mod, type =
`HC1'))'') \%\textgreater{}\% check\_function(`coeftest') f
\%\textgreater{}\% check\_arg(``x'') \%\textgreater{}\% check\_equal() f
\%\textgreater{}\% check\_arg(``vcov.'') \%\textgreater{}\%
check\_equal() \})

test\_or(\{ test\_object(``bh\_mod'') \},\{ f \textless{}- ex()
\%\textgreater{}\%
override\_solution(`lm(Boston\(medv ~ Boston\)lstat)')
\%\textgreater{}\% check\_function(`lm') f \%\textgreater{}\%
check\_arg(`formula') \%\textgreater{}\% check\_equal() \},\{ f
\textless{}- ex() \%\textgreater{}\%
override\_solution(`attach(Boston);lm(medv \textasciitilde{} lstat)')
\%\textgreater{}\% check\_function(`lm') f \%\textgreater{}\%
check\_arg(`formula') \%\textgreater{}\% check\_equal() \})

success\_msg(``Correct. Notice both model coefficients are highly
significant.'')

\subsubsection*{2. A Multiple Regression Model of Housing Prices
I}\label{a-multiple-regression-model-of-housing-prices-i}
\addcontentsline{toc}{subsubsection}{2. A Multiple Regression Model of
Housing Prices I}

Now, let us expand the idea from the previous exercise by adding
additional regressors to the model and estimate it again.

As has been discussed in chapter 6.3, adding regressors to the model
improves the fit, that is the \(SER\) decreases and the \(R^2\)
increases.

The packages AER and MASS have been loaded. The model object bh\_mod is
available in the environment.

\textbf{Instructions}

\begin{itemize}
\item
  Regress the median housing value in a destrict, medv, on the average
  age of the buildings, age, the per capita crime rate, crim, the
  percentage of individuals with low socioeconomic status, lstat, and a
  constant. Put differently, estimate the model
  \[medv_i = \beta_0 + \beta_1 lstat_i + \beta_2 age_i + \beta_3 crim_i + u_i.\]
\item
  Print a coefficient summary to the console that reports robust
  standard errors for the augmented model.
\item
  The simple regression model's \(R^2\) is stored in R2\_res. Save the
  multiple regression model's \(R^2\) to R2\_unres and check whether the
  augmented model yields a higher \(R^2\). Use \textless{} or
  \textgreater{} for the comparison.
\end{itemize}

 library(AER) library(MASS) data(``Boston'') attach(Boston) bh\_mod
\textless{}- lm(medv \textasciitilde{} lstat, data = Boston) R2\_res
\textless{}- summary(bh\_mod)\$r.squared \# Conduct the regression

\chapter{Obtain a robust coefficient
summary}\label{obtain-a-robust-coefficient-summary}

\chapter{Compare both coefficients of
determination}\label{compare-both-coefficients-of-determination}

 \# Conduct the regression mod \textless{}- lm(medv \textasciitilde{}
lstat + crim + age, data = Boston)

\chapter{Obtain a robust coefficient
summary}\label{obtain-a-robust-coefficient-summary-1}

coeftest(mod, vcov. = vcovHC)

\chapter{Compare both coefficients of
determination}\label{compare-both-coefficients-of-determination-1}

R2\_unres \textless{}- summary(mod)\$r.squared R2\_unres \textless{}
R2\_res

test\_or(\{ ex() \%\textgreater{}\% check\_function(`lm')
\%\textgreater{}\% check\_result() \},\{ ex() \%\textgreater{}\%
override\_solution(`lm(Boston\(medv ~ Boston\)lstat +
Boston\(crim + Boston\)age)') \%\textgreater{}\% check\_function(`lm')
\%\textgreater{}\% check\_result() \},\{ ex() \%\textgreater{}\%
override\_solution(`lm(medv \textasciitilde{} lstat + crim + age)')
\%\textgreater{}\% check\_function(`lm') \%\textgreater{}\%
check\_result() \})

test\_or(\{ test\_function(``coeftest'') \},\{ f \textless{}- ex()
\%\textgreater{}\%
override\_solution(`coeftest(lm(Boston\(medv ~ Boston\)lstat +
Boston\(crim + Boston\)age), vcov. = vcovHC)') \%\textgreater{}\%
check\_function(`coeftest') f \%\textgreater{}\% check\_arg(`x')
\%\textgreater{}\% check\_equal() \},\{\\
f \textless{}- ex() \%\textgreater{}\%
override\_solution(`coeftest(lm(medv \textasciitilde{} lstat + crim +
age), vcov. = vcovHC)') \%\textgreater{}\% check\_function(`coeftest') f
\%\textgreater{}\% check\_arg(`x') \%\textgreater{}\% check\_equal() \})

test\_student\_typed(`vcov. = vcovHC')

test\_predefined\_objects(`R2\_res') test\_object(`R2\_unres')

test\_or(\{ test\_student\_typed(``R2\_unres \textgreater{} R2\_res'')
\},\{ test\_student\_typed(``R2\_unres \textless{} R2\_res'') \},\{
test\_student\_typed(`R2\_res \textless{} R2\_unres') \},\{
test\_student\_typed(`R2\_res \textgreater{} R2\_unres') \})

success\_msg(``Right. Remember that R2 generally grows when a regressor
is added to the model but this does not mean that the model improves
upon a model with less regressors in any case.'')

\subsubsection*{3. A Multiple Regression Model of Housing Prices
II}\label{a-multiple-regression-model-of-housing-prices-ii}
\addcontentsline{toc}{subsubsection}{3. A Multiple Regression Model of
Housing Prices II}

Look at the regression equation below describing the previously
estimated model with White standard errors in parantheses.

\[ \widehat{medv}_i = \underset{(0.74)}{32.828} \underset{(0.08)}{-0.994} \times lstat_i \underset{(0.03)}{-0.083} \times crim_i + \underset{(0.02)}{0.038} \times age_i\]

This model is saved in bh\_mult\_mod which is available in the working
environment.

\textbf{Instructions}

As has been stressed in Chapter 6.3, it is not meaningful to use \(R^2\)
when comparing regression models with a different number of regressors.
Instead, the \(\overline{R}^2\) should be used. \(\overline{R}^2\)
adjusts for the circumstance that the \(SSR\) reduces when a regressor
is added to the model.

\begin{itemize}
\item
  Use the model object to compute the correction factor
  \(CF = \frac{n-1}{n-k-1}\) where \(n\) is the number of observations
  and \(k\) is the number of regressors, excluding the intercept. Save
  it to CF.
\item
  Use summary() to obtain \(R^2\) and \(\overline{R}^2\) for
  bh\_mult\_mod. Is is sufficient if you print both values to the
  console.
\item
  Check that \[\overline{R}^2 = 1 - (1-R^2) \cdot CF\]. Use the ==
  operator.
\end{itemize}

 library(AER) library(MASS) data(``Boston'') bh\_mult\_mod \textless{}-
lm(medv \textasciitilde{} lstat + crim + age, data = Boston) \#
Correction Factor n \textless{}- k \textless{}- CF \textless{}-

\chapter{Obtain R\^{}2 and the adj.
R\^{}2}\label{obtain-r2-and-the-adj.-r2}

\chapter{Check that the adj. R\^{}2 can be computed as stated
above}\label{check-that-the-adj.-r2-can-be-computed-as-stated-above}

 \# Correction Factor n \textless{}-
nrow(bh\_mult\_mod\(model) k <- ncol(bh_mult_mod\)model)-1 CF
\textless{}- (n-1)/(n-k-1)

\chapter{Obtain R\^{}2 and the adj.
R\^{}2}\label{obtain-r2-and-the-adj.-r2-1}

summary(bh\_mult\_mod)\(r.squared summary(bh_mult_mod)\)adj.r.squared

\chapter{Check that the adj. R\^{}2 can be computed as stated
above}\label{check-that-the-adj.-r2-can-be-computed-as-stated-above-1}

1 - (1 -
summary(bh\_mult\_mod)\(r.squared) * CF == summary(bh_mult_mod)\)adj.r.squared

 test\_predefined\_objects(``bh\_mult\_mod'') test\_object(``n'')
test\_object(``k'') test\_object(``CF'')
test\_output\_contains(`summary(bh\_mult\_mod)\(r.squared') test_output_contains('summary(bh_mult_mod)\)adj.r.squared')
ex() \%\textgreater{}\% check\_operator(``=='') \%\textgreater{}\%
check\_result() \%\textgreater{}\% check\_equal() success\_msg(``Well
done!'')

\subsubsection*{4. A Fully Fledged Model for Housing
Values?}\label{a-fully-fledged-model-for-housing-values}
\addcontentsline{toc}{subsubsection}{4. A Fully Fledged Model for
Housing Values?}

Have a look again at the description of the variables contained in the
Boston data set. Which variable would you expect to have the highest
\(p\)-value in a multiple regression model which uses \emph{all}
remaining variables as regressors to explain medv?

\textbf{Instructions}

\begin{itemize}
\item
  Regress medv on all remaining variables that you find in the Boston
  data set.
\item
  Obtain a heteroskedasticity robust summary of the coefficients.
\item
  The \(\overline{R}^2\) for the model in exercise 3 is \(0.5533\). What
  can you say about the \(\overline{R}^2\) of the large regression
  model? Does this model improve on the previous one? (no code
  submission needed)
\end{itemize}

The packages AER and MASS as well as the data set Boston are loaded to
the working environment.

\textbf{Hints}

\begin{itemize}
\item
  For brevity, use the regression formula medv \textasciitilde{}. in
  your call of lm(). This is a shortcut that specifies a regression of
  medv on all the remaining variables in the data set supplied to the
  argument data.
\item
  Use summary on both models for a comparison of the \(\overline{R}^2\).
\end{itemize}

 library(AER) library(MASS) data(Boston) \# Run a regression of `medv'
on all remaining variables in the `Boston' data set

\chapter{obtain a robust summary of the
coefficients}\label{obtain-a-robust-summary-of-the-coefficients}

\chapter{What is the model's adj.
R\^{}2?}\label{what-is-the-models-adj.-r2}

 \# Run a regression of `medv' on all remaining variables in the
`Boston' data set full\_mod \textless{}- lm(medv \textasciitilde{}.,
data = Boston)

\chapter{obtain a robust summary of the
coefficients}\label{obtain-a-robust-summary-of-the-coefficients-1}

coeftest(full\_mod, vcov. = vcovHC(full\_mod, type = ``HC0''))

\chapter{What is the model's adj.
R\^{}2?}\label{what-is-the-models-adj.-r2-1}

summary(full\_mod)\$adj.r.squared test\_object(``full\_mod'')
test\_function(``coeftest'', args=``x'') test\_student\_typed(`vcov. =
vcovHC') success\_msg(``Right. Notice that the smallest p-value is
associated with the coefficient on ptratio, the pupil-teacher ratio by
town. It is conceivable that quality of the schooling district is an
important location factor. According to the adj. R\^{}2 , the full model
does better than the model dealt with in exercises 2 and 3 which uses a
smaller subset of the variables as regressors.'')

\subsubsection*{5. Model Selection}\label{model-selection}
\addcontentsline{toc}{subsubsection}{5. Model Selection}

Maybe we can improve the model by dropping a variable?

In this exercise, you have to estimate several models, each time
dropping one of the explanatory variables used in the large regression
model of exercise 4 and compare the models' \(\overline{R}^2\)

The full regression model from the previous exercise, full\_mod, is
available in your environment.

\textbf{Instructions}

\begin{itemize}
\tightlist
\item
  You are totally free to choose a way to solve this. We recommend the
  following approach:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start by estimating a model mod\_new, say, where e.g. lstat is
  excluded from the explanatory variables. Next, access this model's
  \(\overline{R}^2\).
\item
  Compare the model's \(\overline{R}^2\) to the \(\overline{R}^2\) of
  the full model (this was about \(0.7338\)).
\item
  Repeat Steps 1 and 2 for all explanatory variables used in the full
  regression model. Save the model with the highest improvement in
  \(\overline{R}^2\) to better\_mod.
\end{enumerate}

 library(AER) library(MASS) data(Boston) full\_mod \textless{}- lm(medv
\textasciitilde{}., data = Boston) \# Find the model which fits the data
better than `full\_mod'

 \# This solution is a bit technical but efficient

\chapter{Loop estimation of models}\label{loop-estimation-of-models}

l \textless{}- list() for (i in 1:13) \{ d \textless{}- Boston{[},-i{]}
\# save each adj. R\^{}2 as a list entry in l l{[}{[}i{]}{]}
\textless{}- summary(lm(medv \textasciitilde{}., data=d))\$adj.r.squared
\}

\chapter{assign variable names to the list
entries}\label{assign-variable-names-to-the-list-entries}

names(l) \textless{}- names(Boston{[} ,1:13{]})

\chapter{select the variable whose omission leads to the highest
improvement in adj.
R\^{}2}\label{select-the-variable-whose-omission-leads-to-the-highest-improvement-in-adj.-r2}

which.max(l) \# 7th column this is ``age''

\chapter{hence a model that fits the data better
is}\label{hence-a-model-that-fits-the-data-better-is}

better\_model \textless{}- lm(medv \textasciitilde{}., data =
Boston{[},-7{]}) test\_object(``better\_model'') success\_msg(``Correct.
Notice that this model fits the data better than the full model and does
better than all other models that include 12 of the available set of 13
regressors, in terms of adj. R\^{}2. However, the increase in adj.
R\^{}2 is very small and the model is not `the best' of all conceivable
models.'')

\chapter{Hypothesis Tests and Confidence intervals in Multiple
Regression}\label{hypothesis-tests-and-confidence-intervals-in-multiple-regression}

This chapter discusses methods that allow to quantify the sampling
uncertainty inherent to the OLS estimator of coefficients in multiple
regression models. The basis for this are standard errors, hypothesis
tests and confidence intervals which, just as for the simple linear
regression model, can be computed using basic R functions. We will also
tackle the issue of testing joint hypothesis on coefficients of multiple
regression models.

\section{Hypothesis Tests and Confidence Intervals for a Single
Coefficient}\label{hypothesis-tests-and-confidence-intervals-for-a-single-coefficient}

First, we will discuss how to compute standard errors, how to test
hypotheses and how to construct confidence intervals for a single
regression coefficient \(\beta_j\) in a multiple regression model. The
basic idea is summarized in Key Concept 7.1.

Key Concept 7.1

Testing the Hypothesis \(\beta_j = \beta_{j,0}\) Against the Alternative
\(\beta_j \neq \beta_{j,0}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the standard error of \(\hat{\beta_j}\)
\item
  Compute the \(t\)-statistic,
  \[t = \frac{\hat{\beta}_j - \beta_{j,0}} {SE(\hat{\beta_j})}\]
\item
  Compute the \(p\)-value, \[p\text{-value} = 2 \Phi(-|t^{act}|)\]
\end{enumerate}

where \(t^{act}\) is the value of the \(t\)-statistic actually computed.
Reject the hypothesis at the 5\% significance level if the \(p\)-value
is less than \(0.05\) or, equivalently, if \(|t^{act}| > 1.96\). The
standard error and (typically) the \(t\)-statistic and the corresponding
\(p\)-value for testing \(\beta_j = 0\) are computed automatically by
statistical software, e.g.~by \texttt{summary()}.

It is straightforward to verify that principles of testing single
hypothesis about the significance of coefficients in the multiple
regression model are just as in in the simple regression model.

You can easily see this by inspecting the coefficient summary of the
regression model

\[ TestScore = \beta_0 - \beta_1 \times size - \beta_2 \times english + u \]

already discussed in chapter 6. Let us review this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\KeywordTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ size + english, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.845 -10.240  -0.308   9.815  43.461 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 686.03224    7.41131  92.566  < 2e-16 ***
## size         -1.10130    0.38028  -2.896  0.00398 ** 
## english      -0.64978    0.03934 -16.516  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.46 on 417 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4237 
## F-statistic:   155 on 2 and 417 DF,  p-value: < 2.2e-16
\end{verbatim}

You may check that these quantities are computed as in the simple
regression model by computing the \(t\)-statistics or \(p\)-values by
hand using the output above and R as a calculator.

For example, using the definition of the \(p\)-value for a two-sided
test as given in Key Concept 7.1, we can confirm the \(p\)-value for a
test of the hypothesis that the coeffiecient \(\beta_0\), that is the
coefficient on \texttt{(intercept)}, is zero to be approximatley zero.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{pnorm}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\FloatTok{92.566}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

Remember that, given a vector of quantiles, \texttt{pnorm()} calculates
associated probabilities for the standard normal distribution by default
which is suitable here since we approximate with the standard normal
distribution.

Key Concept 7.2

Confidence Intervals for a Single Coefficient in Multiple Regression

A \(95\%\) two-sided confidence interval for the coefficient \(\beta_j\)
is an interval that contains the true value of \(\beta_j\) with a
\(95 \%\) probability; that is, it contains the true value of
\(\beta_j\) in \(95 \%\) of all randomly drawn samples. Equivalently, it
is the set of values of \(\beta_j\) that cannot be rejected by a
\(5 \%\) two-sided hypothesis test. When the sample size is large, the
\(95 \%\) confidence interval for \(\beta_j\) is
\[\left[\hat{\beta_j}- 1.96 \times SE(\hat{\beta}_j), \hat{\beta_j} + 1.96 \times SE(\hat{\beta_j})\right].\]

\section{An Application to Test Scores and the Student-Teacher
Ratio}\label{an-application-to-test-scores-and-the-student-teacher-ratio}

Let us take a look at the regression from section 6.3 again.

Computing individual confidence intervals for the coefficients in the
multiple regression model can be done analogously as in the simple
regression model using the function \texttt{confint()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\KeywordTok{confint}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   2.5 %      97.5 %
## (Intercept) 671.4640580 700.6004311
## size         -1.8487969  -0.3537944
## english      -0.7271113  -0.5724424
\end{verbatim}

We note that \(95\%\) confidence intervals for all three coefficients
are computed.

If we want to compute confidence intervals at another level of
\(\alpha=0.1\) say, we just have to set the argument \texttt{level} in
our call of \texttt{confint()} accordingly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(model, }\DataTypeTok{level =} \FloatTok{0.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     5 %        95 %
## (Intercept) 673.8145793 698.2499098
## size         -1.7281904  -0.4744009
## english      -0.7146336  -0.5849200
\end{verbatim}

The output now reports the desired \(90\%\) confidence intervals for all
model coefficients.

Knowing how to use to make inference about the coefficients in multiple
regression models, You can now answer the following question:

Can the null hypothesis that a change in the student-teacher ratio,
\texttt{size}, has no significant influence on test scores,
\texttt{scores}, --- if we control for the percentage of students
learning English in the district, \texttt{english}, --- be rejected at
the \(10\%\) and the \(5\%\) level of significance?

The outputs above tell us that zero is not an element of the computed
confidence intervals for the coefficient of \texttt{size} such that we
can reject the null hypothesis at significance levels of \(5\%\) and
\(10\%\).

Note that rejection at the \(5\%\)-level implies rejection at the
\(10\%\) level (why?).

The same conclusion can be made when beholding the \(p\)-value for
\texttt{size}: \(0.00398 < 0.05 = \alpha\).

The \(95\%\) confidence interval tells us that we can be \(95\%\)
confident that a one-unit decrease in the student-teacher ratio has an
effect on test scores that lies in the interval with a lower bound of
\(-1.8488\) and an upper bound of \(-0.3538\).

\subsection*{Another Augmentation of the
Model}\label{another-augmentation-of-the-model}
\addcontentsline{toc}{subsection}{Another Augmentation of the Model}

What is the average effect on test scores of reducing the
student-teacher ratio when the expenditures per pupil and the percentage
of english learning pupils are held constant?

We can pursue this question by augmenting our model equation by an
additional regressor that is a measure for spendings per pupil. Using
\texttt{?CASchools} we find that \texttt{CASchools} contains the
variable expenditure which provides expenditures per student.

The model we want the estimate now is

\[ TestScore = \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times expenditure + u \]

with \(expenditure\) the total amount of expenditures per pupil in the
district (thousands of dollars).

Let us now estimate the model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scale expenditure to thousands of dollars}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{expenditure <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{expenditure}\OperatorTok{/}\DecValTok{1000}

\CommentTok{# estimate the model}
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{expenditure, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\KeywordTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ size + english + expenditure, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -51.340 -10.111   0.293  10.318  43.181 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 649.57795   15.20572  42.719  < 2e-16 ***
## size         -0.28640    0.48052  -0.596  0.55149    
## english      -0.65602    0.03911 -16.776  < 2e-16 ***
## expenditure   3.86790    1.41212   2.739  0.00643 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.35 on 416 degrees of freedom
## Multiple R-squared:  0.4366, Adjusted R-squared:  0.4325 
## F-statistic: 107.5 on 3 and 416 DF,  p-value: < 2.2e-16
\end{verbatim}

We see that the estimated effect of a one unit change in the
student-teacher ratio on test scores with expenditures per pupil and the
share of english learning pupils held constant is rather small
(\(-0.29\)). What is more, the coefficient on \(size\) is not
significantly different from zero anymore even at the level of \(10\%\)
since \(p\text{-value}=0.55\). Can You come up with an interpretation
for these findings (see chapter 7.1 of the book)? Mathematically, the
insignificance of \(\beta_1\) could be due to a larger standard error of
\(\hat{\beta}_1\) resulting from adding \(expenditure\) to the model so
that we are estimating the true coefficent on \(size\) less precisely.
This illustrates the issue of strongly correlated regressors (imperfect
multicollinearity). The correlation between \(size\) and
\(expenditures\) can be computed using \texttt{cor()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{size, CASchools}\OperatorTok{$}\NormalTok{expenditure)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.6199822
\end{verbatim}

Altogether we conclude that the new model provides no evidence that
changing the student-teacher ratio, e.g.~by hiring new teachers, has any
effect on the test scores while keeping expenditures per student and the
share of english learners constant at the same time.

\section{\texorpdfstring{Joint Hypothesis Testing Using the
\(F\)-Statistic}{Joint Hypothesis Testing Using the F-Statistic}}\label{joint-hypothesis-testing-using-the-f-statistic}

The estimated model is

\[ \widehat{TestScore} = 649.58 -0.29 \times size - 0.66 \times english + 3.87 \times expenditure. \]

Now, can we reject the hypothesis that the coefficient on \(size\) and
and the coefficient on \(expenditure\) are zero? To answer this
question, we have to resort to methods that allow joint hypothesis
testing. A joint hypothesis imposes restrictions on multiple regression
coefficients. This is different from conducting individual \(t\)-tests
where a single resitriction is imposed on a single coefficient. Chapter
7.2 of the book explains why testing the individual coefficients one at
a time is different from testing them jointly.

The homoskedasticity-only \(F\)-Statistic is given by

\[ F = \frac{(SSR_{restricted} - SSR_{unrestricted})/q}{SSR_{unrestricted} / (n-k_{unrestricted}-1)} \]

with \(SSR_{restricted}\) the sum of squared residuals from the
restricted regression, i.e.~the regression where we impose the
restriction. \(SSR_{unrestricted}\) is the sum of squared residuals from
the full model, \(q\) is the number of restriction under the null and
\(k\) is the number of regressors in the unrestricted regression.

Luckily, it is fairly easy to conduct \(F\)-tests in R. We can use the
function \texttt{linearHypothesis()} which is contained in the
\texttt{car} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the multiple regression model}
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{expenditure, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# execute the function on the model object and provide both linear restrictions }
\CommentTok{# to be tested as strings}
\KeywordTok{linearHypothesis}\NormalTok{(model, }\KeywordTok{c}\NormalTok{(}\StringTok{"size=0"}\NormalTok{, }\StringTok{"expenditure=0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## size = 0
## expenditure = 0
## 
## Model 1: restricted model
## Model 2: score ~ size + english + expenditure
## 
##   Res.Df   RSS Df Sum of Sq      F   Pr(>F)    
## 1    418 89000                                 
## 2    416 85700  2    3300.3 8.0101 0.000386 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

From the output we can infer that the \(F\)-statistic for this joint
hypothesis test is about \(8.01\) and the corresponding \(p\)-value is
\(0.0004\). Thus we can reject the null hypothesis that both
coefficients are zero at any level of significance commonly used in
practice.

Note:

The standard output of a model summary also reports an \(F\)-statistic
and the corresponding \(p\)-value. The null hypothesis belonging to this
\(F\)-test is that all of the population coefficients in the model
except for the intercept are zero, so the hypotheses pair is

\[H_0: \beta_1=0, \ \beta_2 =0, \ \beta_3 =0 \quad \text{vs.} \quad H_1: \beta_j \neq 0 \ \text{for at least one} \ j=1,2,3.\]

This is also called the ``overall'' regression \(F\)-statistic and the
null hypothesis is obviously different from testing if only \(\beta_1\)
and \(\beta_3\) are zero.

We will now check that the \(F\)-statistic belonging to the \(p\)-value
listed in the model's summary coincides with the result reported by
\texttt{linearHypothesis()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# execute the function on the model object and provide the linear restriction }
\CommentTok{# to be tested as strings}
\KeywordTok{linearHypothesis}\NormalTok{(model, }\KeywordTok{c}\NormalTok{(}\StringTok{"size=0"}\NormalTok{,}\StringTok{"english=0"}\NormalTok{,}\StringTok{"expenditure=0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## size = 0
## english = 0
## expenditure = 0
## 
## Model 1: restricted model
## Model 2: score ~ size + english + expenditure
## 
##   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
## 1    419 152110                                  
## 2    416  85700  3     66410 107.45 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Access the F-statistic from the model's summary}
\KeywordTok{summary}\NormalTok{(model)}\OperatorTok{$}\NormalTok{fstatistic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    value    numdf    dendf 
## 107.4547   3.0000 416.0000
\end{verbatim}

The test rejects the null hypothesis that the model has no power in
explaining test scores rather clearly.

\section{Confidence Sets for Multiple
Coefficients}\label{confidence-sets-for-multiple-coefficients}

Based on the \(F\)-statistic that we have previously encountered, we can
specify confidence sets. Confidence sets are analogous to confidence
intervals for single coefficients. As such, confidence sets consist of
combinations of coefficients that contain the true combination of
coefficients in, \(95\%\) say, of all cases if we could infinitely draw
random samples, just as in the univariate case. Put differently, a
confidence set is the set of coefficient combinations for which we
cannot reject a joint null hypothesis tested using a \(F\)-test.

If we consider two coefficients, their confidence set is an ellipse
which is centered around the point defined by both coefficient
estimates. Again, there is a very convenient way to plot the confidence
set for two coefficients of model objects, namely the function
\texttt{confidenceEllipse()} which is also coming with the \texttt{car}
package.

In the following, we plot the \(95\%\) confidence ellipse for the
coefficients on \texttt{size} and \texttt{expenditure} from the
regression conducted above. By specifying the additional argument
\texttt{fill}, the confidence set is colored which gives us a better
impression which set of coefficients is meant.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Draw the 95% confidence set for coefficients on size and expenditure}
\KeywordTok{confidenceEllipse}\NormalTok{(model, }
                  \DataTypeTok{fill =}\NormalTok{ T, }
                  \DataTypeTok{which.coef =} \KeywordTok{c}\NormalTok{(}\StringTok{"size"}\NormalTok{,}\StringTok{"expenditure"}\NormalTok{),}
                  \DataTypeTok{main =} \StringTok{"95% Confidence Set"}
\NormalTok{                  )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-151-1} \end{center}

We see that the ellipse is centered around \((-0.29, 3.87)\) i.e.~the
pair of coefficients estimates on \(size\) and \(expenditure\). What is
more, \((0,0)\) is not element of the \(95\%\) confidence set so that we
can reject \(H_0: \beta_1 = 0, \ \beta_3 = 0\).

\section{Model Specification for Multiple
Regression}\label{model-specification-for-multiple-regression}

Choosing a regression specifaction i.e.~selecting the variables to be
included in a regression model can be quite cumbersome. However, there
are some guidelines how to do this. The goal is clear: obtaining an
unbiased estimation of the causal effect of interest. As a starting
point, one should think about omitted variables, that is to avoid a
possible estimation bias by using suitable control variables (omitted
variables bias in the context of multiple regression is explained in Key
Concept 7.3). A second step could be to compare different regression
model specifications by measures of fit. However, as we shall see one
should not rely solely on \(\overline{R^2}\).

Key Concept 7.3

Omitted Variable Bias in Multiple Regression

Omitted variable bias is the bias in the OLS estimator that arises when
one or more included regressors are correlated with an omitted variable.
For omitted variable bias to arise, two things must be true:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  At least one of the included regressors must be correlated with the
  omitted variable.
\item
  The omitted variable must be a determinant of the dependent variable,
  \(Y\).
\end{enumerate}

We will now discuss an example were we face a potential omitted variable
bias in a multiple regression model:

Consider again the estimated regression equation

\[ \widehat{TestScore} = \underset{(8.7)}{686.0} - \underset{(0.43)}{1.10} \times size - \underset{(0.031)}{0.650} \times english. \]

We are interested in estimated the causal effect of class size on test
score. In this estimation, there might be a bias due to omitting
``outside learning opportunities'' from our regression sice a measure
like this could be a determinant of the students test scores and could
also be correlated with both regressors already included in the model
(so that both conditions of Key Concept 7.3 are fulfilled). ``outside
learning opportunities'' is a complicated concept that is difficult to
quantify. A surrogate we can consider instead is the students' economic
backgroud which should be strongly related to the outside learning
opportunities: think of wealthy parents that are able to provide time
and/or money for private tuition of there children. We thus augment the
model with the variable \texttt{lunch}, the percentage of students that
qualify for a free or subsidized lunch in school due to family incomes
below a certain threshold, and estimate the model again.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the model and print summary to console}
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\KeywordTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ size + english + lunch, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -32.849  -5.151  -0.308   5.243  31.501 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 700.14996    4.68569 149.423  < 2e-16 ***
## size         -0.99831    0.23875  -4.181 3.54e-05 ***
## english      -0.12157    0.03232  -3.762 0.000193 ***
## lunch        -0.54735    0.02160 -25.341  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 9.08 on 416 degrees of freedom
## Multiple R-squared:  0.7745, Adjusted R-squared:  0.7729 
## F-statistic: 476.3 on 3 and 416 DF,  p-value: < 2.2e-16
\end{verbatim}

Thus, the estimated regression line is

\[ \widehat{TestScore} = \underset{(4.7)}{700.15} - \underset{(0.24)}{1.00} \times size - \underset{(0.032)}{0.12} \times english + \underset{(0.022)}{0.55} \times lunch. \]

We observe no substantial changes in the conclusion about the effect of
\texttt{size}: the coefficient changes by only \(0.1\) and keeps its
significance.

Although the difference in estimated coefficients is not big in this
case, it might be a good idea to keep \texttt{lunch} in the model to
make the assumption of conditional mean independence more credible (see
chapter 7.5 of the book).

\subsection*{Model Specification in Theory and in
Practice}\label{model-specification-in-theory-and-in-practice}
\addcontentsline{toc}{subsection}{Model Specification in Theory and in
Practice}

Key Concept 7.4

\(R^2\) and \(\overline{R^2}\): What They Tell You --- and What They
Don't

The \(R^2\) and \(\overline{R^2}\) tell you whether the regressors are
good at predicting, or ``explaining'' the values of the independent
variable in the sample of data at hand. if the \(R^2\) (or
\(\overline{R^2}\)) is nearly \(1\), then the regressors produce good
prediction of the dependent variable in that sample, in the sense that
the variance of OLS residuals is small compared to the variance of the
dependent variable. If the \(R^2\) (or \(\overline{R^2}\)) is nearly
\(0\), the opposite is true.

The \(R^2\) and \(\overline{R^2}\) do not tell you whether:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An included variable is statistically significant.
\item
  The regressors are true cause of the movements in the dependent
  variable
\item
  There is omitted variable bias, or
\item
  You have chosen the most appropriate set of regressors.
\end{enumerate}

Key Concept 7.4 names some common pitfalls when using \(R^2\) and
\(\overline{R^2}\) to evaluate the predictive ability of regression
models.

For example, think of regressing \(TestScore\) on \(PLS\) which measures
the available parking lot space in thousand square feet. You are likely
to observe a significant coefficient of reasonable magnitude and
moderate to high values for \(R^2\) and \(\overline{R^2}\). The reason
for this is that parking lot space is correlated with many determinants
of the test score like location, class size, financial endowment and so
on. Although we do not have observations on \(PLS\), we can use R to
generate some relatively realistic data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Generate observations for parking lot space}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{PLS <-}\StringTok{ }\DecValTok{5} \OperatorTok{+}\StringTok{ }\FloatTok{0.6}\OperatorTok{*}\NormalTok{CASchools}\OperatorTok{$}\NormalTok{expenditure }\OperatorTok{+}\StringTok{ }\FloatTok{0.6}\OperatorTok{*}\NormalTok{CASchools}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{size}\OperatorTok{/}\DecValTok{100} \OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(CASchools), }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{# plot parking lot space against test score}
\KeywordTok{plot}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{PLS, }
\NormalTok{     CASchools}\OperatorTok{$}\NormalTok{score,}
     \DataTypeTok{xlab =} \StringTok{"Parking Lot Space"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test Score"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-153-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# regress test score on PLS}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{PLS, }\DataTypeTok{data =}\NormalTok{ CASchools))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ PLS, data = CASchools)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -50.16 -14.25   0.65  13.56  49.88 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 6.224e+02  7.715e+00  80.679  < 2e-16 ***
## PLS         9.915e-03  2.393e-03   4.144 4.13e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 18.7 on 418 degrees of freedom
## Multiple R-squared:  0.03946,    Adjusted R-squared:  0.03717 
## F-statistic: 17.17 on 1 and 418 DF,  p-value: 4.131e-05
\end{verbatim}

\(PLS\) is generated as a linear function of \(expenditure\),
\(income\), \(size\) and random disturbances. Therefore the data suggest
that there is some postive relationship between parking lot space and
test score. In fact, when estimating the model

\begin{align}
\widehat{TestScore} = \beta_0 + \beta_1 \times PLS + u \label{eq:plsmod} 
\end{align}

using \texttt{lm()} we find that the coefficient on \(PLS\) is positive
and significantly different from zero. Also \(R^2\) and
\(\overline{R^2}\) are about \(0.47\) which is a lot more than the
roughly \(0.05\) observed when regressing test score on class size only.

This suggests that increasing the parking lot space boosts a school's
test scores and that model \eqref{eq:plsmod} does even better in
explaining heterogeneity in the dependent variable than a model with
\(size\) as the only regressor. Keeping in mind how \(PLS\) is
constructed this comes of no surprise. It is evident that the high
\(R^2\) cannot be used to the conclude that the estimated relation
between parking lot space and test scores is causal: the (relatively)
high \(R^2\) is due to correlation between \(PLS\) and other
determinantes and/or control variables. Increasing parking lot space is
not an appropriate measure to generate more learning success!

\section{Analysis of the Test Score Data
Set}\label{analysis-of-the-test-score-data-set}

Chapter 6 and some of the previous sections have stressed that it is
important to include control variables in regression models if it is
plausible that there are omitted factors that cannot be measured
directly. Recall that in our example of test scores, we are interested
in estimating the causal effect of a change in the student-teacher ratio
on test scores. In what follows, we will provide an example how to use
multiple regression models in order to alleviate omitted variable bias
and we will demonstrate how to report results using R.

So far we have considered two variables that control for unobservable
student characteristics which correlate with the student-teacher ratio
and are assumed to have an impact on test scores:

\begin{itemize}
\item
  \(english\), the percentage of english learning students
\item
  \(lunch\), the share of students that qualify for a subsidized or even
  a free lunch at school
\end{itemize}

Another new variable provided with \texttt{CASchools} is \(calworks\),
the percentage of students that qualify for the CalWorks income
assistance programm. Students eligible for CalWorks live in families
with a total income below the threshold for the subsidized lunch
programm so both variables are indicators for the share of economically
disadvantaged children. Using R we can confirm that both indicators are
highly correlated:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate correlation between calworks and lunch}
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{calworks, CASchools}\OperatorTok{$}\NormalTok{lunch)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7394218
\end{verbatim}

There is no unabigious way to proceed when deciding which variable to
use. In any case it is not a good idea to use both variables as
regressors having in mind consequences of colinearity. Therefore, we
will also consider alternative model specifications.

For a start, we plot student characteristics against test scores.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}

\NormalTok{m <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{layout}\NormalTok{(m)}

\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{english, }
     \DataTypeTok{data =}\NormalTok{ CASchools, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{), }
     \DataTypeTok{main =} \StringTok{"Percentage of English language learners"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{lunch, }
     \DataTypeTok{data =}\NormalTok{ CASchools, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Percentage qualifying for reduced price lunch"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{calworks, }
     \DataTypeTok{data =}\NormalTok{ CASchools, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{), }
     \DataTypeTok{main =} \StringTok{"Percentage qualifying for income assistance"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-155-1.pdf}

We see that all relationships are negative. We can use R to estimate the
correlation coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate correlation between student characteristics and test scores}
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score, CASchools}\OperatorTok{$}\NormalTok{english)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.6441238
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score, CASchools}\OperatorTok{$}\NormalTok{lunch)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.868772
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score, CASchools}\OperatorTok{$}\NormalTok{calworks)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.6268533
\end{verbatim}

We will consider \(5\) different model equations:

\begin{align*}
  (I) \quad \widehat{TestScore} =& \, \beta_0 + \beta_1 \times size + u \\
  (II) \quad \widehat{TestScore} =& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + u \\
  (III) \quad \widehat{TestScore} =& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times lunch + u \\
  (IV) \quad \widehat{TestScore} =& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_4 \times calworks + u \\
  (V) \quad \widehat{TestScore} =& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times lunch + \beta_4 \times calworks + u
\end{align*}

The best way to communicate regression results is in a table. The
\texttt{stargazer} package is very convenient for this purpose. It
provides a function that generates professionally looking HTML and LATEX
tables that satisfy scienctific standards. One simply has to provide one
or multiple object(s) of class \texttt{lm} and the rest is done by the
function \texttt{stargazer()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the stargazer library}
\KeywordTok{library}\NormalTok{(stargazer)}

\CommentTok{# estimate different model specifications}
\NormalTok{spec1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size , }\DataTypeTok{data =}\NormalTok{ CASchools)}
\NormalTok{spec2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\NormalTok{spec3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\NormalTok{spec4 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{calworks, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\NormalTok{spec5 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\NormalTok{calworks, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# generate a Latex table using stargazer}
\KeywordTok{stargazer}\NormalTok{(spec1, spec2, spec3, spec4, spec5, }
          \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"(I)"}\NormalTok{, }\StringTok{"(II)"}\NormalTok{, }\StringTok{"(III)"}\NormalTok{, }\StringTok{"(IV)"}\NormalTok{, }\StringTok{"(V)"}\NormalTok{)}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

\% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University.
E-mail: hlavac at fas.harvard.edu \% Date and time: Thu, Apr 26, 2018 -
10:53:51 \% Requires LaTeX packages: rotating

\begin{sidewaystable}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{5}{c}{\textit{Dependent variable:}} \\ 
\cline{2-6} 
\\[-1.8ex] & \multicolumn{5}{c}{score} \\ 
 & (I) & (II) & (III) & (IV) & (V) \\ 
\\[-1.8ex] & spec1 & spec2 & spec3 & spec4 & spec5\\ 
\hline \\[-1.8ex] 
 size & $-$2.280$^{***}$ & $-$1.101$^{***}$ & $-$0.998$^{***}$ & $-$1.308$^{***}$ & $-$1.014$^{***}$ \\ 
  & (0.480) & (0.380) & (0.239) & (0.307) & (0.240) \\ 
  & & & & & \\ 
 english &  & $-$0.650$^{***}$ & $-$0.122$^{***}$ & $-$0.488$^{***}$ & $-$0.130$^{***}$ \\ 
  &  & (0.039) & (0.032) & (0.033) & (0.034) \\ 
  & & & & & \\ 
 lunch &  &  & $-$0.547$^{***}$ &  & $-$0.529$^{***}$ \\ 
  &  &  & (0.022) &  & (0.032) \\ 
  & & & & & \\ 
 calworks &  &  &  & $-$0.790$^{***}$ & $-$0.048 \\ 
  &  &  &  & (0.053) & (0.061) \\ 
  & & & & & \\ 
 Constant & 698.933$^{***}$ & 686.032$^{***}$ & 700.150$^{***}$ & 697.999$^{***}$ & 700.392$^{***}$ \\ 
  & (9.467) & (7.411) & (4.686) & (6.024) & (4.698) \\ 
  & & & & & \\ 
\hline \\[-1.8ex] 
Observations & 420 & 420 & 420 & 420 & 420 \\ 
R$^{2}$ & 0.051 & 0.426 & 0.775 & 0.629 & 0.775 \\ 
Adjusted R$^{2}$ & 0.049 & 0.424 & 0.773 & 0.626 & 0.773 \\ 
Residual Std. Error & 18.581 (df = 418) & 14.464 (df = 417) & 9.080 (df = 416) & 11.654 (df = 416) & 9.084 (df = 415) \\ 
F Statistic & 22.575$^{***}$ (df = 1; 418) & 155.014$^{***}$ (df = 2; 417) & 476.306$^{***}$ (df = 3; 416) & 234.638$^{***}$ (df = 3; 416) & 357.054$^{***}$ (df = 4; 415) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{5}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{sidewaystable}

The table states that \(scores\) is the dependent variable and that we
consider \(5\) models. We see that the coloumns of Table 7.1 contain all
the information provided by \texttt{summary()} for the regression models
\texttt{spec1} to \texttt{spec5}: the coefficient section of the table
presents coefficients estimates equipped with signifiance codes
(asterisks) and standard errors in parantheses below. Although there a
no \(t\)-statistics, it is straightforward for the reader to compute
them simply by dividing a coefficient estimate by the correspondung
standard error. In the botton of the table we find summary statistics
for each model and a legend. For an in-depth discussion of the tabular
presentation of regression results, see chapter 7.6 of the book.

What can we conclude from the model comparison?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We see that adding control variables roughly halves the coefficient on
  \texttt{size}. Also the coefficient is not very sensitive to the set
  of control variables used. The conclusion is that decreasing the
  student-teacher ratio ceteris paribus by one unit leads to an
  estimated average increase in test scores of about \(1\) point.
\item
  Adding student characteristics as controls boosts \(R^2\) and
  \(\overline{R^2}\) from \(0.049\) (\texttt{spec1}) up to \(0.773\)
  (\texttt{spec3} and \texttt{spec5}), so we can consider these
  variables as suitable predictors for test scores. Morever, the
  estimated coefficients on all control variables are consistent with
  the impressions gained from figure 7.2.
\item
  We see that the control variables are not always individually
  statistically significant: for example in \texttt{spec5} we see that
  the coefficient on \(calworks\) is not significantly different from
  zero at the level of \(5\%\) since
  \(\lvert-0.048/0.061\rvert=0.79 < 1.64\). We also observe that the
  effect on the estimate (and its standard error) of the coefficient on
  \(size\) of adding \(calworks\) to the base specification
  \texttt{spec3} is negligible. We can therefore consider
  \texttt{calworks} as a redundant control variable in this setting.
\end{enumerate}

\section{Exercises}\label{exercises-5}

\subsubsection*{Inference in the Multiple Regression Model --
t-statistics}\label{inference-in-the-multiple-regression-model-t-statistics}
\addcontentsline{toc}{subsubsection}{Inference in the Multiple
Regression Model -- t-statistics}

\textbf{Instructions}

\textbf{Hint}

 library(MASS) data(``Boston'') mod \textless{}- lm(medv
\textasciitilde{} lstat + age + crim, data = Boston)

\chapter{Nonlinear Regression
Functions}\label{nonlinear-regression-functions}

Until now we assumed the regression function to be linear, i.e.~we have
treated the slope of the regression function as a constant. This implies
that the effect on \(Y\) of a one unit change in \(X\) does not depend
on the level of \(X\). If however the effect of a change in \(X\) on
\(Y\) does depend on the value of \(X\), we have to use a nonlinear
regression function.

\section{A General Strategy for Modeling Nonlinear Regression
Functions}\label{a-general-strategy-for-modeling-nonlinear-regression-functions}

Let us have a look at an example where in fact using a nonlinear
regression function is better suited for estimation of the population
relationship between the regressor, \(X\), and the regressand, \(Y\):
the relationship between the income of schooling districts and their
test scores.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Prepare the data}
\KeywordTok{library}\NormalTok{(AER)                                                     }
\KeywordTok{data}\NormalTok{(CASchools)}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{size <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{students}\OperatorTok{/}\NormalTok{CASchools}\OperatorTok{$}\NormalTok{teachers}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{score <-}\StringTok{ }\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{read }\OperatorTok{+}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{math)}\OperatorTok{/}\DecValTok{2}       
\end{Highlighting}
\end{Shaded}

We start our analysis by computing the correlation between the two
variables.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income, CASchools}\OperatorTok{$}\NormalTok{score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7124308
\end{verbatim}

The correlation coefficient is about \(0.71\). This means that income
and test scores are positively correlated. In other words, children
whose parents have an above average income tend to achieve above average
test scores. Can we use the correlation coefficient to assess whether a
linear regression model does fit the data adequately? To answer this
question we visualize the data and add a linear regression line to the
plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Fit linear model}
\NormalTok{linear_model<-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# Plot observations}
\KeywordTok{plot}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income, CASchools}\OperatorTok{$}\NormalTok{score,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"District Income (thousands of dollars)"}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"Test Score"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Test Score vs. District Income and a Linear OLS Regression Function"}\NormalTok{)}

\CommentTok{# Add the regression line to the plot}
\KeywordTok{abline}\NormalTok{(linear_model, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-160-1} \end{center}

As Stock and Watson point out, the linear regression line seems to
overestimate the true relationship when income is very high or very low
and underestimates it in the midrange.

Fortunately, usage of the OLS is not restricted to linear functions of
the regressors. Thus we can for example model test scores as a function
of income and the square of income. The corresponding regression model
is

\[TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 Income_i^2 + u_i.\]

This equation is called the \emph{quadratic regression model}. Note,
that \(Income^2\) is treated as an additional explanatory variable.
Hence, the quadratic model is a special case of a multivariate
regression model. When fitting the model with \texttt{lm()} we have to
use the \texttt{\^{}} operator in conjunction with the function
\texttt{I()} to add the quadratic term as an additional regressor to the
\texttt{formula} argument.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit the quadratic Model}
\NormalTok{quadratic_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{income }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{2}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# Generate model summary}
\KeywordTok{summary}\NormalTok{(quadratic_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ income + I(income^2), data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -44.416  -9.048   0.440   8.347  31.639 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 607.30174    3.04622 199.362  < 2e-16 ***
## income        3.85099    0.30426  12.657  < 2e-16 ***
## I(income^2)  -0.04231    0.00626  -6.758 4.71e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 12.72 on 417 degrees of freedom
## Multiple R-squared:  0.5562, Adjusted R-squared:  0.554 
## F-statistic: 261.3 on 2 and 417 DF,  p-value: < 2.2e-16
\end{verbatim}

The output tells us that the estimated model equation is

\[\widehat{TestScore}_i = \underset{(3.05)}{607.3} + \underset{(0.30)}{3.85} \times Income_i - \underset{(0.01)}{0.0423} \times Income_i^2.\]

Notice that this estimated model equation allows us to test the
hypothesis that the relationship between test scores and district income
is linear against the alternative hypothesis that it is nonlinear. This
corresponds to testing

\[H_0: \beta_2 = 0 \ \ \text{vs.} \ \  H_1: \beta_2\neq0,\]

since \(\beta_2=0\) corresponds to a simple linear equation and
\(\beta_2\neq0\) implies a quadratic relationship. We find that
\(t=(\hat\beta_2 - 0)/SE(\hat\beta_2) = -0.0423/0.01 = -4.23\) so the
null is rejected at any common level of significance and we conclude
that the relationship is nonlinear. This is consistent with our informal
inspection of the plotted data.

We can now draw the same scatterplot as for the linear model and add the
regression line for the quadratic model. Because \texttt{abline()} can
only draw straight lines, it cannot be used for this task. A function
which can be used to draw lines without being restricted to straight
lines is \texttt{lines()}, see \texttt{?lines}. The most basic call of
\texttt{lines()} is \texttt{lines(x\_values,\ y\_values)} where
\texttt{x\_values} and \texttt{y\_values} are vectors of the same length
that provide coordinates of the points to be sequentially connected by a
line. This makes it necessary to sort the coordinate pairs according to
the X-values. Otherwise You will not get the desired result! In this
example we use the function \texttt{order()} to sort the fitted values
of \(TestScore\) according to the observations for \(income\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scatterplot of observatuib for income and TestScore}
\KeywordTok{plot}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income, CASchools}\OperatorTok{$}\NormalTok{score,}
     \DataTypeTok{col  =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"District Income (thousands of dollars)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test Score"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Linear and Quadratic OLS Regression Functions"}\NormalTok{)}

\CommentTok{#Add linear function to the plot}
\KeywordTok{abline}\NormalTok{(linear_model , }\DataTypeTok{col=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}

\CommentTok{#Add quatratic function to the plot}
\NormalTok{order_id <-}\StringTok{ }\KeywordTok{order}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income)}

\KeywordTok{lines}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CASchools}\OperatorTok{$}\NormalTok{income[order_id], }
      \DataTypeTok{y =} \KeywordTok{fitted}\NormalTok{(quadratic_model)[order_id],}
      \DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd=}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-162-1} \end{center}

We see that the quadratic model does much better in fitting the data
than the linear model.

\section{Nonlinear Functions of a Single Independent
Variable}\label{nonlinear-functions-of-a-single-independent-variable}

\subsection*{Polynomials}\label{polynomials}
\addcontentsline{toc}{subsection}{Polynomials}

The approach used to obtain a quadratic model can be generalized to
polynomial models of arbitrary order \(r\).
\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \ldots + \beta_r X_i^r + u_i\]

A cubic model (\(r=3\)) for instance can be estimated in the same way as
the quadratic model --- we just have to add \texttt{I(income\^{}3)} to
the \texttt{formula} argument in our call of \texttt{lm()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cubic_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{income }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{3}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ CASchools)}
\end{Highlighting}
\end{Shaded}

In practice the question will arise which polynomial order should be
chosen. First note that, similarly as for \(r=2\), we can test the null
hypothesis that the true relation is linear against the alternative
hypothesis that the relationship is a polynomial of degree \(r\):

\[ H_0: \beta_2=0, \ \beta_3=0,\dots,\beta_r=0 \ \ \ \text{vs.} \ \ \ H_1: \text{at least one} \ \beta_j\neq0, \ j=2,\dots,r \]

This is a joint null hypothesis with \(r-1\) restrictions so it can be
tested using the \(F\)-test presented in chapter 7. Remember that the
function \texttt{linearHypothesis()} can compute such test statistics.
If, for example, we would like to test the null hypothesis of a linear
model against the alternative of a polynomial of a maximal degree
\(r=3\) we could simply do the following:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(car)}

\CommentTok{# test the hypothesis}
\KeywordTok{linearHypothesis}\NormalTok{(cubic_model, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"I(income^2)=0"}\NormalTok{, }\StringTok{"I(income^3)=0"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ income + I(income^2) + I(income^3)
## 
##   Res.Df   RSS Df Sum of Sq      F    Pr(>F)    
## 1    418 74905                                  
## 2    416 67170  2    7735.5 23.954 1.424e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The \(p\)-value for this test is very small so that we reject the null
hypothesis. However, this does not tell us which \(r\) to choose. In
practice, one approach to determine degree of the polynomial is to use
sequential testing:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate a polynomial model for some maximum value \(r\).
\item
  Use a \(t\)-test to test whether \(\beta_r = 0\). Rejection of the
  null means that \(X^r\) belongs in the regression equation.
\item
  Acceptance of the null in step 2 means that \(X^r\) can be eliminated
  from the model. Continue by repeating step 1 with order \(r-1\) and
  test whether \(\beta_{r-1}=0\). If the test rejects, use a polynomial
  model of order \(r-1\).
\item
  If the tests from step 3 rejects, continue with the procedure until
  the coefficient on the highest power is statistically significant.
\end{enumerate}

There is no unambigous guideline how to choose \(r\) in step one.
However as Stock and Watson point out, economic data is often smooth
such that it is appropriate to choose small orders like \(2\), \(3\), or
\(4\).

We will now demonstrate how to apply sequential testing in the example
of a cubic model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(cubic_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ income + I(income^2) + I(income^3), data = CASchools)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -44.28  -9.21   0.20   8.32  31.16 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  6.001e+02  5.830e+00 102.937  < 2e-16 ***
## income       5.019e+00  8.595e-01   5.839 1.06e-08 ***
## I(income^2) -9.581e-02  3.736e-02  -2.564   0.0107 *  
## I(income^3)  6.855e-04  4.720e-04   1.452   0.1471    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 12.71 on 416 degrees of freedom
## Multiple R-squared:  0.5584, Adjusted R-squared:  0.5552 
## F-statistic: 175.4 on 3 and 416 DF,  p-value: < 2.2e-16
\end{verbatim}

The estimated cubic model stored in \texttt{cubic\_model} is

\[ \widehat{TestScore}_i = \underset{(5.83)}{600.1} + \underset{(0.86)}{5.02} \times Income + \underset{(0.03)}{-0.96} \times Income^2 + \underset{(-0.00047)}{0.00069}  \times Income^3. \]

Summary tells us that the \(t\)-statistic on \(Income^3\) is \(1.42\) so
the null that the relationship is quadratic cannot be rejected, even at
the \(10\%\) level. This is contrary to the result of the book which
reports robust standard errors throughout so we will also use robust
variance-covariance estimation to reproduce these results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the lmtest package for coeftest()}
\KeywordTok{library}\NormalTok{(lmtest)}

\CommentTok{# test the hypothesis using robust standard errors}
\KeywordTok{coeftest}\NormalTok{(cubic_model, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(cubic_model, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                Estimate  Std. Error  t value  Pr(>|t|)    
## (Intercept)  6.0008e+02  5.1021e+00 117.6150 < 2.2e-16 ***
## income       5.0187e+00  7.0735e-01   7.0950 5.606e-12 ***
## I(income^2) -9.5805e-02  2.8954e-02  -3.3089  0.001018 ** 
## I(income^3)  6.8549e-04  3.4706e-04   1.9751  0.048918 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Notice that the reported standard errors have changed. Furthermore, the
coefficient for \texttt{income\^{}3} is now significant at the \(5\%\)
level. This means we reject the hypothesis that the regression function
is quadratic against the alternative that it is cubic. Furthermore, we
can also test if the coefficients for \texttt{income\^{}2} and
\texttt{income\^{}3} are jointly significant using a robust version of
the \(F\)-test.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# robust F-test for }
\KeywordTok{linearHypothesis}\NormalTok{(cubic_model, }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(cubic_model, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{),}
                 \KeywordTok{c}\NormalTok{(}\StringTok{"I(income^2)=0"}\NormalTok{, }\StringTok{"I(income^3)=0"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ income + I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(>F)    
## 1    418                        
## 2    416  2 37.691 9.043e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

With a \(p\)-value of \texttt{9.043e-16}, i.e.~much less than \(0.05\),
the null hypothesis of linearity is rejected in favour of the
alternative that the relationship is quadratic or cubic.

\subsubsection*{Interpretation of Coefficients in Nonlinear Regression
Models}\label{interpretation-of-coefficients-in-nonlinear-regression-models}
\addcontentsline{toc}{subsubsection}{Interpretation of Coefficients in
Nonlinear Regression Models}

The coefficients in polynomial regression do not have a simple
interpretation. Think of a quadratic model. It is not helpful to think
of the coefficient on \(X\) as the expected change in \(Y\) associated
with a change in \(X\) holding the other regressors constant because one
other is \(X^2\) which changes as \(X\) is varied. This is also the case
for other deviations from linearity, for example in models where
regressors and/or the dependent variable are log-transformed. The best
way to approach this is to calculate the estimated effect on \(Y\)
associated with a change in \(X\) for one or more values of \(X\). This
idea is summarized in Key Concept 8.1.

Key Concept 8.1

The Expected Effect on \(Y\) of a Change in \(X_1\) in a Nonlinear
Regression Model

Consider the nonlinear population regression model

\[ Y_i = f(X_{1i}, X_{2i}, \dots, X_{ki}) + u_i \ , \ i=1,\dots,n,\]

where \(f(X_{1i}, X_{2i}, \dots, X_{ki})\) is the population regression
function and \(u_i\) is the error term.

The expected change in \(Y\), \(\Delta Y\), associated with the change
in \(X_1\), \(\Delta X_1\), holding \(X_2, \cdots , X_k\) constant. That
is, the expected change in \(Y\) is the difference:

\[\Delta Y = f(X_1 + \Delta X_1, X_2, \cdots, X_k) - f(X_1, X_2, \cdots, X_k).\]

The estimator of this unknown population difference is the difference
between the predicted values for these two cases. Let
\(\hat{f}(X_1, X_2, \cdots, X_k)\) be the predicted value of of \(Y\)
based on the estimator \(\hat{f}\) of the population regression
function. Then the predicted change in \(Y\) is

\[\Delta \hat{Y} = \hat{f}(X_1 + \Delta X_1, X_2, \cdots, X_k) - \hat{f}(X_1, X_2, \cdots, X_k).\]

For example, we may ask the following: what is the predicted change in
test scores associated with a one unit change (i.e. \(\$1000\)), based
on the estimated quadratic regression function

\[\widehat{TestScore} = 607.3 + 3.85 \times Income - 0.0423 \times Income^2\ ?\]

Because the regression function is quadratic, this effect depends on the
initial district income. We therefore consider two cases: an increase in
district income form \(10\) to \(11\) (i.e.~from \(\$10000\) per capita
to \(\$11000\)) and an increase in district income from \(40\) to \(41\)
(that is from \(\$40000\) to \(\$41000\)).

In order to obtain the \(\Delta \hat{Y}\) associated with a change in
income form \(10\) to \(11\), we use the following formula:

\[\Delta \hat{Y} = \left(\hat{\beta}_0 + \hat{\beta}_1 \times 11 + \hat{\beta}_2 \times 11^2\right) - \left(\hat{\beta}_0 + \hat{\beta}_1 \times 10 + \hat{\beta}_2 \times 10^2\right) \]
To compute \(\hat{Y}\) using R we may use \texttt{predict()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute and assign the quadratic model}
\NormalTok{quadriatic_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{income }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{2}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# set up data to predict}
\NormalTok{new_data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{income =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{))}

\CommentTok{# do the prediction}
\NormalTok{Y_hat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(quadriatic_model, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\CommentTok{# compute the difference}
\KeywordTok{diff}\NormalTok{(Y_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        2 
## 2.962517
\end{verbatim}

Analogously we can compute the effect of a change in \(income\) from
\(40\) to \(11\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up data to predict}
\NormalTok{new_data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{income =} \KeywordTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{41}\NormalTok{))}

\CommentTok{# do the prediction}
\NormalTok{Y_hat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(quadriatic_model, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\CommentTok{# compute the difference}
\KeywordTok{diff}\NormalTok{(Y_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         2 
## 0.4240097
\end{verbatim}

So for the quadratic model, the expected change in \(TestScore\) induced
by an increase in \(income\) from \(10\) to \(11\) is about \(2.96\)
points but an increase in \(income\) from \(40\) to \(41\) increases the
predicted score by only \(0.42\). Hence the slope of the estimated
quadratic regression function is steeper at low levels of income than at
the higher levels.

\subsection*{Logarithms}\label{logarithms}
\addcontentsline{toc}{subsection}{Logarithms}

Another way to specify a nonlinear regression function is to use the
natural logarithm of \(Y\) and/or \(X\). Logarithms convert changes in
variables into percentage changes which is convenient as many
relationships are naturally expressed in terms of percentages.

There are three different cases in which logarithms might be used.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(X\) could be transformed by taking its logarithm but \(Y\) is not.
\item
  We could transform \(Y\) to its logarithm but leave \(X\) at level.
\item
  A third case is that both \(Y\) and \(X\) are transformed to their
  logarithms. The interpretation of the regression coefficients is
  different in each case.
\end{enumerate}

\subsubsection*{\texorpdfstring{Case I: \(X\) is in logarithm, \(Y\) is
not.}{Case I: X is in logarithm, Y is not.}}\label{case-i-x-is-in-logarithm-y-is-not.}
\addcontentsline{toc}{subsubsection}{Case I: \(X\) is in logarithm,
\(Y\) is not.}

The regression model then is

\[Y_i = \beta_0 + \beta_1 \times \ln(X_i) + u_i \text{, } i=1,...,n. \]
Similar as for polynomial regression we do not have to create a new
variable by computing \(\ln(X)\). We can simply adjust the
\texttt{formula} argument of \texttt{lm()} to tell R that the
log-transformation of a variable should be used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate a level-log model}
\NormalTok{LinearLog_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# compute robust summary}
\KeywordTok{coeftest}\NormalTok{(LinearLog_model, }
         \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(LinearLog_model, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 557.8323     3.8399 145.271 < 2.2e-16 ***
## log(income)  36.4197     1.3969  26.071 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

According to the output the estimated regression function is:

\[\widehat{TestScore} = 557.8 + 36.42 \times \ln(Income).\]

Let us draw a plot of this function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# draw scatterplot}
\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{income, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{data =}\NormalTok{ CASchools,}
     \DataTypeTok{main =} \StringTok{"Linear-Log Regression Line"}\NormalTok{)}

\CommentTok{# add Linear-Log regression line}
\NormalTok{order_id  <-}\StringTok{ }\KeywordTok{order}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income)}

\KeywordTok{lines}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income[order_id],}
      \KeywordTok{fitted}\NormalTok{(LinearLog_model)[order_id], }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-168-1} \end{center}

We can interpret \(\hat{\beta}_1\) as follows: a \(1\%\) increase in
income is associated with an increase in test scores of
\(0.01 \times 36.42 = 0.36\) points. In order to get the estimated
effect of a one unit change in income (that is a change in the original
units,thousands of dollars, not in logarithms) on test scores, the
method presented in Key Concept 8.1 can be used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up new data}
\NormalTok{new_data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{income =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{41}\NormalTok{))}

\CommentTok{# predict outcomes }
\NormalTok{Y_hat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(LinearLog_model, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\CommentTok{# compute expected difference}
\NormalTok{changes <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(Y_hat, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{, }\DataTypeTok{byrow =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{changes[ ,}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{changes[ ,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.471166 0.899297
\end{verbatim}

The estimated model states that for an income increase from \(\$10,000\)
to \(\$11,000\), test scores increase by an expected amount of \(3.47\)
points. When income increases from \(\$40,000\) to \(\$41,000\), the
expected increase in test scores is only about \(0.90\) points.

\subsubsection*{\texorpdfstring{Case II: \(Y\) is in logarithm, \(X\) is
not}{Case II: Y is in logarithm, X is not}}\label{case-ii-y-is-in-logarithm-x-is-not}
\addcontentsline{toc}{subsubsection}{Case II: \(Y\) is in logarithm,
\(X\) is not}

If You want to learn about the absolute impact of an explanatory
variable on Your dependent variable, it is not recommended to
log-transform the latter. There are, however, cases where we want to
learn about \(\ln(Y)\) instead of \(Y\).

The corresponding regression then model is

\[ \ln(Y_i) = \beta_0 + \beta_1 \times X_i + u_i \ \ , \ \ i=1,...,n. \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate a log-linear model }
\NormalTok{LogLinear_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(score) }\OperatorTok{~}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# compute a robust summary}
\KeywordTok{coeftest}\NormalTok{(LogLinear_model, }
         \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(LogLinear_model, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##               Estimate Std. Error  t value  Pr(>|t|)    
## (Intercept) 6.43936234 0.00289382 2225.210 < 2.2e-16 ***
## income      0.00284407 0.00017509   16.244 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimated regression function is
\[\widehat{\ln(TestScore)} = 6.439 + 0.00284 \times income.\] Since we
are interested in \(\ln(Y)\) rather than \(Y\), we do not retransform
the dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scatterplot}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log}\NormalTok{(score) }\OperatorTok{~}\StringTok{ }\NormalTok{income, }
     \DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch=}\DecValTok{20}\NormalTok{, }
     \DataTypeTok{data =}\NormalTok{ CASchools,}
     \DataTypeTok{main =} \StringTok{"Log-Linear Regression Function"}
\NormalTok{     )}

\CommentTok{# add the Log-Linear regression line}
\NormalTok{order_id  <-}\StringTok{ }\KeywordTok{order}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income)}

\KeywordTok{lines}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income[order_id], }
      \KeywordTok{fitted}\NormalTok{(LogLinear_model)[order_id], }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-171-1} \end{center}

Note that the \(Y\)-Axis is now log-transformed.

In a log-linear model, a one-unit change in \(X\) is associated with an
estimated \(100 \times \hat\beta_1 \%\) change in \(Y\). This time we
left the \(X\) values unchanged.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# do predictions}
\NormalTok{Y_hat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(LogLinear_model, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\CommentTok{# calculate changes}
\NormalTok{changes <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(Y_hat, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{, }\DataTypeTok{byrow =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{changes[ ,}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{changes[ ,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.00284407 0.00284407
\end{verbatim}

\subsubsection*{\texorpdfstring{Case III: \(X\) and \(Y\) are in
logarithms}{Case III: X and Y are in logarithms}}\label{case-iii-x-and-y-are-in-logarithms}
\addcontentsline{toc}{subsubsection}{Case III: \(X\) and \(Y\) are in
logarithms}

The log-log regression model is

\[\ln(Y_i) = \beta_0 + \beta_1 \times \ln(X_i) + u_i \ \ , \ \ i=1,...,n. \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate the log-log model}
\NormalTok{LogLog_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(score) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# print robust summary to the console}
\KeywordTok{coeftest}\NormalTok{(LogLog_model, }
         \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(LogLog_model, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error  t value  Pr(>|t|)    
## (Intercept) 6.3363494  0.0059246 1069.501 < 2.2e-16 ***
## log(income) 0.0554190  0.0021446   25.841 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimated regression function hence is
\[\widehat{\ln(TestScore)} = 6.336 + 0.0554 \times income.\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scatterplot}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log}\NormalTok{(score) }\OperatorTok{~}\StringTok{ }\NormalTok{income, }
     \DataTypeTok{data =}\NormalTok{ CASchools,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Log-Log Regression Function"}\NormalTok{)}

\CommentTok{# plot the estimate log-log regression line}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{sort}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income), }
      \KeywordTok{fitted}\NormalTok{(LogLog_model)[}\KeywordTok{order}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income)], }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-173-1} \end{center}

In a log-log model, a \(1\%\) change in \(X\) is associated with an
estimated \(\hat\beta_1 \%\) change in \(Y\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict Y}
\NormalTok{Y_hat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(LogLog_model, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\CommentTok{# compute changes}
\NormalTok{changes <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(Y_hat, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{, }\DataTypeTok{byrow =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{changes[ ,}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{changes[ ,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.005281992 0.001368439
\end{verbatim}

Key Concept 8.2 summarizes the three logarithmic regression models.

Key Concept 8.2

Logarithms in Regression: Three Cases

Logarithms can be used to transform the dependent variable \(Y\) or the
independent variable \(X\), or both (the variable being transformed must
be positive). The following table summarizes these three cases and the
interpretation of the regression coefficient \(\beta_1\). In each case,
\(\beta_1\), can be estimated by applying OLS after taking the
logarithm(s) of the dependent and/or the independent variable.

Case

Model Specification

Interpretation of \(\beta_1\)

\((I)\)

\(Y_i = \beta_0 + \beta_1 \ln(X_i) + u_i\)

A \(1 \%\) change in \(X\) is associated with a change in \(Y\) of
\(0.01 \times \beta_1\).

\((II)\)

\(\ln(Y_i) = \beta_0 + \beta_1 X_i + u_i\)

A change in \(X\) by one unit (\(\Delta X = 1\)) is associated with a
\(100 \times \beta_1 \%\) change in \(Y\).

\((III)\)

\(\ln(Y_i) = \beta_0 + \beta_1 \ln(X_i) + u_i\)

A \(1\%\) change in \(X\) is associated with a \(\beta_1\%\) change in
\(Y\), so \(\beta_1\) is the elasticity of \(Y\) with respect to \(X\).

Of course we can also estimate a polylog model like

\[ TestScore_i = \beta_0 + \beta_1 \times \ln(income_i) + \beta_2 \times \ln(income_i)^2 + \beta_3 \times \ln(income_i)^3 + u_i \]

which models the dependent variable \(TestScore\) by a third-degree
polynomial of the log-transformed regressor \(income\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the polylog model}
\NormalTok{polyLog_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(income) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(}\KeywordTok{log}\NormalTok{(income)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(}\KeywordTok{log}\NormalTok{(income)}\OperatorTok{^}\DecValTok{3}\NormalTok{), }
                    \DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# print robust summary to the console}
\KeywordTok{coeftest}\NormalTok{(polyLog_model, }
         \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(polyLog_model, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                  Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)      486.1341    79.3825  6.1239 2.115e-09 ***
## log(income)      113.3820    87.8837  1.2901    0.1977    
## I(log(income)^2) -26.9111    31.7457 -0.8477    0.3971    
## I(log(income)^3)   3.0632     3.7369  0.8197    0.4128    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Which of the models presented here is the most suitable? Comparing by
\(\overline{R^2}\) we find that, leaving out the log-linear model, all
models have a similar fit. In the class of polynomial models, the cubic
specification has the highest \(\overline{R^2}\) whereas the linear-log
specification is the best of the log-models (check this!). Let us first
compare both models graphically by plotting the corresponding estimated
regression functions

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scatter plot}
\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{income, }
     \DataTypeTok{data =}\NormalTok{ CASchools,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Linear-Log and Cubic Regression Functions"}\NormalTok{)}

\CommentTok{# add Linear-Log regression line}
\NormalTok{order_id  <-}\StringTok{ }\KeywordTok{order}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income)}

\KeywordTok{lines}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income[order_id],}
      \KeywordTok{fitted}\NormalTok{(LinearLog_model)[order_id], }
      \DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add cubic regression line}
\KeywordTok{lines}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CASchools}\OperatorTok{$}\NormalTok{income[order_id], }
      \DataTypeTok{y =} \KeywordTok{fitted}\NormalTok{(cubic_model)[order_id],}
      \DataTypeTok{col=}\StringTok{"darkred"}\NormalTok{, }
      \DataTypeTok{lwd=}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-175-1} \end{center}

We see that both regression lines are nearly identical. Here the
linear-log model is preferable since it has a slightly higher
\(\overline{R^2}\) and is more parsimonious in terms of regressors: it
does not need higher-degree polynomials.

\section{Interactions Between Independent
Variables}\label{interactions-between-independent-variables}

There are research questions where it is interesting to know how the
effect on \(Y\) of a change in one independent variables dependends on
the value of another independent variable. For example, we could ask if
districts with many english learners benefit differentially from a
decrease in class sizes than those with few english learning students.
To answer this, we need to include an interaction term into a multiple
regression model. We will consider three cases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Interactions between two binary variables
\item
  Interactions between a binary and a continuous variable
\item
  Interactions between two continuous variables
\end{enumerate}

The following subsections discuss those cases briefly and demonstrate
how to perform such regressions using R.

\subsubsection*{Interactions Between Two Binary
Variables}\label{interactions-between-two-binary-variables}
\addcontentsline{toc}{subsubsection}{Interactions Between Two Binary
Variables}

Take two binary variables \(D_1\) and \(D_2\) and the population
regression

\[ Y_i = \beta_0 + \beta_1 \times D_{1i} + \beta_2 \times D_{2i} + u_i. \]

Now assume

\begin{align}
  Y_i=& \, \ln(Earnings_i),\\
  \\
  D_{1i} =& \,
   \begin{cases}
      1 & \text{if $i^{th}$ person has a college degree} \\
      0 & \text{else},
    \end{cases} \\
    \\
  D_{2i} =& \, 
    \begin{cases}
      1 & \text{if $i^{th}$ person is female} \\
      0 & \text{if $i^{th}$ person is male}.
    \end{cases}\\
\end{align}

By now You should know that \(\beta_1\) measures the average difference
in \(\ln(Earnings)\) between individuals with and without a college
degree and \(\beta_2\) is the gender differential in \(\ln(Earnings)\),
ceteris paribus. This model does not allow us to determine if there is a
geneder specific effect of heaving a college degree and, if so, how
strong this is. Luckily it is easy to come up with a model specification
that allows to investigate this:

\[ Y_i = \beta_0 + \beta_1 \times D_{1i} + \beta_2 \times D_{2i} + \beta_3 \times (D_{1i} \times D_{2i}) + u_i \]

\((D_{1i} \times D_{2i})\) is called an interaction term and \(\beta_3\)
measures the difference in the effect of having a college degree for
women versus men.

Key Concept 8.3

A Method for Interpreting Coefficients in Regression with Binary
Variables

Compute expected values of \(Y\) for each possible set described by the
set of binary variables. Compare the expected values. The coefficients
can be expressed either as expected values or as the difference between
at least two expected values.

Following Key Concepts 8.1 we have

\begin{align*}
  E(Y_i\vert D_{1i}=0, D_{2i} = d_2) =& \, \beta_0 + \beta_1 \times 0 + \beta_2 \times d_2 + \beta_3 \times (0 \times d_2) \\
  =& \, \beta_0 + \beta_2 \times d_2.
\end{align*}

Changing \(D_{1i}\) from \(0\) to \(1\) we obtain

\begin{align*}
  E(Y_i\vert D_{1i}=1, D_{2i} = d_2) =& \, \beta_0 + \beta_1 \times 1 + \beta_2 \times d_2 + \beta_3 \times (1 \times d_2) \\
  =& \, \beta_0 + \beta_1 + \beta_2 \times d_2 + \beta_3 d_2
\end{align*}

and hence the overall effect is

\[ E(Y_i\vert D_{1i}=1, D_{2i} = d_2) - E(Y_i\vert D_{1i}=0, D_{2i} = d_2) = \beta_1 + \beta_3 d_2 \]
so the effect is a difference of expected values.

\paragraph{Application to the student-teacher ratio and the percentage
of English
learners}\label{application-to-the-student-teacher-ratio-and-the-percentage-of-english-learners}
\addcontentsline{toc}{paragraph}{Application to the student-teacher
ratio and the percentage of English learners}

Now let

\begin{align*}
  HiSTR =& \, 
    \begin{cases}
      1, & \text{if $STR \geq 20$} \\
      0, & \text{else},
    \end{cases} \\
  \\
  HiEL =& \,
    \begin{cases}
      1, & \text{if $PctEL \geq 10\%$} \\
      0, & \text{else}.
    \end{cases}
\end{align*}

We may use R construct the variables above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Add HiSTR to CASchools}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{HiSTR <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{size }\OperatorTok{>=}\StringTok{ }\DecValTok{20}\NormalTok{)}

\CommentTok{# Add HiEL to CASchools}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{HiEL <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{english }\OperatorTok{>=}\StringTok{ }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We proceed by estimating the model

\[ TestScore_i = \beta_0 + \beta_1 \times HiSTR + \beta_2 \times HiEL + \beta_3 \times (HiSTR \times HiEL) + u_i \]

using \texttt{lm()}. There are several ways to add an interaction term
to the model \texttt{formula} argument of \texttt{lm()} but the most
intuitive is to use the product of both variables
\texttt{HiEL\ *\ HiSTR}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the model with binary interaction term}
\NormalTok{bi_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{HiSTR }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL }\OperatorTok{+}\StringTok{ }\NormalTok{HiSTR }\OperatorTok{*}\StringTok{ }\NormalTok{HiEL, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# print summary}
\KeywordTok{summary}\NormalTok{(bi_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ HiSTR + HiEL + HiSTR * HiEL, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.078 -10.679  -1.282   9.665  45.522 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  664.143      1.316 504.852  < 2e-16 ***
## HiSTR         -1.908      2.235  -0.854    0.394    
## HiEL         -18.316      2.144  -8.544 2.49e-16 ***
## HiSTR:HiEL    -3.260      3.223  -1.012    0.312    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 16.06 on 416 degrees of freedom
## Multiple R-squared:  0.2948, Adjusted R-squared:  0.2897 
## F-statistic: 57.97 on 3 and 416 DF,  p-value: < 2.2e-16
\end{verbatim}

The estimated regression model is

\[ \widehat{TestScore} = 664.1 - 1.9 \times HiSTR - 18.3 \times HiEL - 3.3 \times (HiSTR \times HiEL) \]

and it predicts that the effect of moving from a school district with a
low student-teacher ratio to a district with a high student-teacher
ratio, depending on high or low percentage of english learners is
\(-1.9-3.3\times HiEL\). So for districts with a low share of english
learners (\(HiEL = 0\)), the estimated effect is a decrease of \(1.9\)
points in test scores while for districts with a big fraction of english
learner (\(HiEL = 1\)), the predicted decrease in test scores amounts to
\(1.9 + 3.3 = 5.2\) points.

We can also use the model to estimate the mean test score for each
combination of binary variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate means for all combinations of HiSTR and HiEL}
\KeywordTok{predict}\NormalTok{(bi_model, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\StringTok{"HiSTR"}\NormalTok{=}\DecValTok{0}\NormalTok{, }\StringTok{"HiEL"}\NormalTok{=}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1 
## 664.1433
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(bi_model, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\StringTok{"HiSTR"}\NormalTok{=}\DecValTok{0}\NormalTok{, }\StringTok{"HiEL"}\NormalTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1 
## 645.8278
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(bi_model, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\StringTok{"HiSTR"}\NormalTok{=}\DecValTok{1}\NormalTok{, }\StringTok{"HiEL"}\NormalTok{=}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1 
## 662.2354
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(bi_model, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\StringTok{"HiSTR"}\NormalTok{=}\DecValTok{1}\NormalTok{, }\StringTok{"HiEL"}\NormalTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1 
## 640.6598
\end{verbatim}

\subsubsection*{Interactions Between a Continuous and a Binary
Variable}\label{interactions-between-a-continuous-and-a-binary-variable}
\addcontentsline{toc}{subsubsection}{Interactions Between a Continuous
and a Binary Variable}

Now consider a continuous variable \(X_i\), the years of working
experience of person \(i\) instead of the gender. We than have

\begin{align*}
  Y_i =& \, \ln(Earnings_i) \\
  \\
  X_i =& \, \text{working experience of person }i \\
  \\
  D_i =& \,  
    \begin{cases}
      1, & \text{if $i^{th}$ person has a college degree} \\
      0, & \text{else}.
    \end{cases}
\end{align*}

The base model thus is

\[ Y_i = \beta_0 + \beta_1 \times X_i + \beta_2 \times D_i + u_i, \]

a simple multiple regression model that allows us to estimate the
average benefit of having a college degree holding working experience
constant and the average effect on earnings of a change in working
experience holding college degree constant.

By adding the interaction term \(X_i \times D_i\) we allow the effect of
an additional year of work experience to differ for person with and
without college degree.

\[ Y_i = \beta_0 + \beta_1 \times X_i + \beta_2 \times D_i + \beta_3 \times (X_i \times D_i) + u_i  \]

Here, \(\beta_3\) measures the difference in the effect of an additional
year of work experience for college graduates versus nongraduates. A
further possible specifications is

\[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 (X_i \times D_i) + u_i. \]

This model states that the expected impact of an additional year of work
experience on earnings is the same for individuals without college
degree but differs for college graduates. All three population
regression functions can be visualized by straight lines. Key Concept
8.4 summarizes the differences.

Key Concept 8.4

Interactions Between Binary and Continuous Variables

An interaction term like \(X_i \times D_i\) (where \(X\) is continuous
and \(D\) is binary) allows for the slope to depend on the binary
variable \(D\). There are three possibilities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Different intercept and same slope:
  \[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 D_i + u_i \]
\item
  Different intercept and different slope:
  \[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 D_i + \beta_3 \times (X_i \times D_i) + u_i \]
\item
  Same intercept and different slope:
  \[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 (X_i \times D_i) + u_i \]
\end{enumerate}

The following code chunk shows how replicate the results shown in Figure
8.8 using fictional data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate fictional data}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\NormalTok{D <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T)}
\NormalTok{Y <-}\StringTok{ }\DecValTok{450} \OperatorTok{+}\StringTok{  }\DecValTok{150} \OperatorTok{*}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\DecValTok{500} \OperatorTok{*}\StringTok{ }\NormalTok{D }\OperatorTok{+}\StringTok{ }\DecValTok{50} \OperatorTok{*}\StringTok{ }\NormalTok{(X }\OperatorTok{*}\StringTok{ }\NormalTok{D) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{200}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{300}\NormalTok{)}

\CommentTok{# divide plotting area}
\NormalTok{m <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{layout}\NormalTok{(m)}

\CommentTok{# Estimate models and plot regression lines}

\CommentTok{# 1.(base model)}
\KeywordTok{plot}\NormalTok{(X,}\KeywordTok{log}\NormalTok{(Y),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Different Intercepts, Same Slope"}
\NormalTok{)}
\NormalTok{mod1_coef <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\NormalTok{D)}\OperatorTok{$}\NormalTok{coefficients}

\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(mod1_coef[}\DecValTok{1}\NormalTok{], mod1_coef[}\DecValTok{2}\NormalTok{]), }
       \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{       )}

\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(mod1_coef[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{mod1_coef[}\DecValTok{3}\NormalTok{], mod1_coef[}\DecValTok{2}\NormalTok{]), }
       \DataTypeTok{col =} \StringTok{"green"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{       )}
              
       
\CommentTok{# 2. (base model + interaction term)}
\KeywordTok{plot}\NormalTok{(X,}\KeywordTok{log}\NormalTok{(Y),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Different Intercepts, Different Slopes"}
\NormalTok{)}

\NormalTok{mod2_coef <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\NormalTok{D }\OperatorTok{+}\StringTok{ }\NormalTok{X}\OperatorTok{:}\NormalTok{D)}\OperatorTok{$}\NormalTok{coefficients}

\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(mod2_coef[}\DecValTok{1}\NormalTok{], mod2_coef[}\DecValTok{2}\NormalTok{]), }
       \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{)}

\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(mod2_coef[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{mod2_coef[}\DecValTok{3}\NormalTok{], mod2_coef[}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{mod2_coef[}\DecValTok{4}\NormalTok{]), }
       \DataTypeTok{col =} \StringTok{"green"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{)}

\CommentTok{# 3. (omission of D as regressor + interaction term)}
\KeywordTok{plot}\NormalTok{(X,}\KeywordTok{log}\NormalTok{(Y),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Same Intercept, Different Slopes"}
\NormalTok{)}

\NormalTok{mod3_coef <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\NormalTok{X}\OperatorTok{:}\NormalTok{D)}\OperatorTok{$}\NormalTok{coefficients}

\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(mod3_coef[}\DecValTok{1}\NormalTok{], mod3_coef[}\DecValTok{2}\NormalTok{]), }
       \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{)}

\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(mod3_coef[}\DecValTok{1}\NormalTok{], mod3_coef[}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{mod3_coef[}\DecValTok{3}\NormalTok{]), }
       \DataTypeTok{col =} \StringTok{"green"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-179-1} \end{center}

\paragraph{Application to the student-teacher ratio and the percentage
of English
learners}\label{application-to-the-student-teacher-ratio-and-the-percentage-of-english-learners-1}
\addcontentsline{toc}{paragraph}{Application to the student-teacher
ratio and the percentage of English learners}

Using a model specification like 2. in Key Concept 8.3 we may answer the
question whether the effect on test scores of decreasing the
student-teacher ratio depends on the whether there are many or few
English learners. We estimate the regression model

\[ \widehat{TestScore_i} = \beta_0 + \beta_1 \times size_i + \beta_2 \times HiEL_i + \beta_2 (size_i \times HiEL_i) + u_i. \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the model}
\NormalTok{bci_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL }\OperatorTok{+}\StringTok{ }\NormalTok{size }\OperatorTok{*}\StringTok{ }\NormalTok{HiEL, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# print summary to console}
\KeywordTok{summary}\NormalTok{(bci_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ size + HiEL + size * HiEL, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -37.356 -10.790  -0.841   9.911  46.457 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 682.2458    10.5109  64.908   <2e-16 ***
## size         -0.9685     0.5398  -1.794   0.0735 .  
## HiEL          5.6391    16.7177   0.337   0.7360    
## size:HiEL    -1.2766     0.8441  -1.512   0.1312    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 15.88 on 416 degrees of freedom
## Multiple R-squared:  0.3103, Adjusted R-squared:  0.3054 
## F-statistic:  62.4 on 3 and 416 DF,  p-value: < 2.2e-16
\end{verbatim}

The estimated regression model is

\[ \widehat{TestScore} = 682.2 - 0.97 \times size + 5.6 \times HiEL - 1.28 \times (size \times HiEL). \]

We find that the estimated regression line for districts with a low
fraction of English learners (\(HiEL_i=0\)) is

\[ \widehat{TestScore} = 682.2 - 0.97\times size_i. \]

For districts with a high fraction of English learners we have

\begin{align} 
  \widehat{TestScore} =& \, 682.2 + 5.6 - 0.97\times size_i - 1.28 \times size_i \\
   =& \, 687.8 - 2.25 \times size_i
\end{align}

so the predicted increase in test scores following a reduction of the
student-teacher ratio by \(1\) is about \(0.97\) points in districts
where the fraction of English learners is low but \(2.25\) in districts
with a high share of English learners. The difference between these
effects is \(1.28\), the magnitude of the coefficient on the interaction
term \(size \times HiEL\).

The next code chunk draws both lines belonging to the model. In order to
make observations with \(HiEL = 0\) distinguishable from those with
\(HiEL = 0\), we assign different colors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# identify observations PctEL >= 10}
\NormalTok{id <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{english }\OperatorTok{>=}\StringTok{ }\DecValTok{10}

\CommentTok{# plot observations with HiEL = 0 as red dots}
\KeywordTok{plot}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{size[}\OperatorTok{!}\NormalTok{id], CASchools}\OperatorTok{$}\NormalTok{score[}\OperatorTok{!}\NormalTok{id],}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{""}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"class size (student-teacher ratio)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"test score"}
\NormalTok{     )}

\CommentTok{# plot observations with HiEL = 1 as green dots}
\KeywordTok{points}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{size[id], CASchools}\OperatorTok{$}\NormalTok{score[id],}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"green"}
\NormalTok{     )}

\CommentTok{# read out estimated coefficients of bci_model}
\NormalTok{coefs <-}\StringTok{ }\NormalTok{bci_model}\OperatorTok{$}\NormalTok{coefficients}

\CommentTok{# Draw the estimated regression line for HiEL = 0}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(coefs[}\DecValTok{1}\NormalTok{], coefs[}\DecValTok{2}\NormalTok{]),}
       \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{       )}

\CommentTok{# Draw the estimated regression line for HiEL = 1}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(coefs[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{coefs[}\DecValTok{3}\NormalTok{], coefs[}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{coefs[}\DecValTok{4}\NormalTok{]),}
       \DataTypeTok{col =} \StringTok{"green"}\NormalTok{, }
       \DataTypeTok{lwd =} \FloatTok{1.5} 
\NormalTok{       )}

\CommentTok{# Add a legend to the plot}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \DataTypeTok{pch=}\KeywordTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{20}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"green"}\NormalTok{), }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"HiEL = 0"}\NormalTok{, }\StringTok{"HiEL = 1"}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-181-1} \end{center}

\subsubsection*{Interactions Between Two Continuous
Variables}\label{interactions-between-two-continuous-variables}
\addcontentsline{toc}{subsubsection}{Interactions Between Two Continuous
Variables}

If we have a regression model with \(Y\) the log earnings and two
continuous regressors \(X_1\), the years of work experience, and
\(X_2\), the years of schooling, a simple multiple regression model
cannot be used to estimate the effect on wages of an additional year of
work experience depending on a given level of schooling. This effect can
be assessed by including the interaction term \((X_{1i} \times X_{2i})\)
in the model:

\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 \times (X_{1i} \times X_{2i}) + u_i \]

Following Key Concept 8.1 we find that the effect on \(Y\) of a change
on \(X_1\) given \(X_2\) is
\[ \frac{\Delta Y}{\Delta X_1} = \beta_1 + \beta_3 X_2. \]

In the earnings example, a positive \(\beta_3\) implies that the effect
on log earnings of an additional year of work experience grows linearly
with years of schooling.

Vice versa we have
\[ \frac{\Delta Y}{\Delta X_2} = \beta_2 + \beta_3 X_1. \] as the effect
on log earnings of an additional year of schooling holding work
experience constant.

Altogether we find that \(\beta_3\) measures the effect of a unit
increase in \(X_1\) and \(X_2\) beyond the effects of increasing \(X_1\)
alone and \(X_2\) alone by one unit. The overall change in \(Y\) is thus

\begin{align}
Y_i = (\beta_1 + \beta_3 X_2) \Delta X_1 + (\beta_2 + \beta_3 X_1) \Delta X_2 + \beta_3\Delta X_1 \Delta X_2. \label{eq:generalinteraction}
\end{align}

Key Concept 8.5 summarizes interactions between two regressors in
multiple regression.

Key Concept 8.5

Interactions in Multiple Regression

The interaction term between the two regressors \(X_1\) and \(X_2\) is
given by their product \(X_1 \times X_2\). Adding this interaction term
as a regressor to the model
\[ Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + u_i \] allows the effect
of change on \(X_2\) to depend on the value of \(X_1\) and vice versa.
Thus the coefficient \(\beta_3\) in the model
\[ Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 \times X_2) + u_i \]
measures the effect of a one-unit increase in both \(X_1\) and \(X_2\)
above and beyond the sum of both individual effects. This holds for
continuous and binary regressors.

\paragraph{Application to the student-teacher ratio and the percentage
of English
learners}\label{application-to-the-student-teacher-ratio-and-the-percentage-of-english-learners-2}

We will now examine the interaction between student-teacher ratio and
the percentage of english learners which both are continuous variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate regression model including the interaction between 'PctEL' and 'size'}
\NormalTok{cci_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{english}\OperatorTok{:}\NormalTok{size, }\DataTypeTok{data =}\NormalTok{ CASchools) }

\CommentTok{# print a summary to the console}
\KeywordTok{summary}\NormalTok{(cci_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ size + english + english:size, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.836 -10.226  -0.343   9.796  43.447 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  686.338527   9.402603  72.995   <2e-16 ***
## size          -1.117018   0.482537  -2.315   0.0211 *  
## english       -0.672912   0.437985  -1.536   0.1252    
## size:english   0.001162   0.021905   0.053   0.9577    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.48 on 416 degrees of freedom
## Multiple R-squared:  0.4264, Adjusted R-squared:  0.4223 
## F-statistic: 103.1 on 3 and 416 DF,  p-value: < 2.2e-16
\end{verbatim}

The estimated model equation is
\[ \widehat{TestScore} = 686.3 - 1.12 \times STR - 0.67 \times PctEL + 0.0012(STR\times PctEL). \]

For the interpretation, let us consider some quartiles of \(PctEL\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{english)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   1.941   8.778  15.768  22.970  85.540
\end{verbatim}

According to \eqref{eq:generalinteraction}, if \(PctEL\) is at its median
\(8.78\) the slope of the regression function relating test scores and
the student teacher ratio is predicted to be
\(-1.12 + 0.0012 \times 8.78 = -1.11\). This means that increasing
\(STR\) by one unit deteriorates test scores by estimated \(1.11\)
points. For the \(75\%\) quantile the estimated change on \(TestScore\)
of a one-unit increase in \(STR\) is estimated by
\(-1.12 + 0.0012 \times 23.0 = -1.09\) so the slope is somewhat lower.
The interpretation is that for a school district with \(23\%\) english
learners, a reduction of the studtent-teacher ratio by one unit is
expected to increase the test scores by only \(1.09\) points.

Note, however, that the output produced by summary indicates that the
difference of the effect for the median and the \(75\%\) quantile is not
statistically significant. The \(p\)-value for the test
\(H_0: \beta_3 = 0\) cannot be rejected at the \(5\%\) level of
significance. Using robust standard errors, one can show that
\(\beta_3\) is not significantly different from zero even at the level
of \(10\%\) (verify this using R!).

\subsubsection*{Example: The Demand for Economic
Journals}\label{example-the-demand-for-economic-journals}
\addcontentsline{toc}{subsubsection}{Example: The Demand for Economic
Journals}

In this section we replicate the empirical example
\textit{The Demand for Economic Journals} presented at pages 336 - 337
of the book. The central question is: how elastic is the demand by
libraries for economic journals? The idea here is to analyze the
relationship between the number of subscription to a journal at U.S.
libraries and the journal's subscription price. The study uses the
dataset \texttt{Journals} wich is provided with the \texttt{AER} package
and contains observations for \(180\) economic journals for the year
\(2000\). You can use the help function (\texttt{?Journals}) to get more
information on the data after loading the package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package and the dataset}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(}\StringTok{"Journals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We measure the price as ``price per citation'' and have to compute
journal age and the number of character manually. For consistency with
the book we also rename the variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define and rename variables}
\NormalTok{Journals}\OperatorTok{$}\NormalTok{PricePerCitation <-}\StringTok{ }\NormalTok{Journals}\OperatorTok{$}\NormalTok{price}\OperatorTok{/}\NormalTok{Journals}\OperatorTok{$}\NormalTok{citations}
\NormalTok{Journals}\OperatorTok{$}\NormalTok{Age <-}\StringTok{ }\DecValTok{2000} \OperatorTok{-}\StringTok{ }\NormalTok{Journals}\OperatorTok{$}\NormalTok{foundingyear}
\NormalTok{Journals}\OperatorTok{$}\NormalTok{Characters <-}\StringTok{ }\NormalTok{Journals}\OperatorTok{$}\NormalTok{charpp }\OperatorTok{*}\StringTok{ }\NormalTok{Journals}\OperatorTok{$}\NormalTok{pages}\OperatorTok{/}\DecValTok{10}\OperatorTok{^}\DecValTok{6}
\NormalTok{Journals}\OperatorTok{$}\NormalTok{Subscriptions <-}\StringTok{ }\NormalTok{Journals}\OperatorTok{$}\NormalTok{subs}
\end{Highlighting}
\end{Shaded}

Note that the range of ``price per citation'' is quite large:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute summary statistics for price per citation}
\KeywordTok{summary}\NormalTok{(Journals}\OperatorTok{$}\NormalTok{PricePerCitation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
##  0.005223  0.464495  1.320513  2.548455  3.440171 24.459459
\end{verbatim}

The lowest price observed is a mere \(0.5\)¢ per citation while the
highest price is more than \(20\)¢ per citation.

After loading and preparing the data, we estimate the four different
model specifications. All models are log-log models. This is useful
because it allows us to directly interpret the ceofficients as
elasticities, see Key Concept 8.2. \((I)\) is a simple linear model. To
aleviate a possible omitted variable bias, \((II)\) augments \((I)\) by
the covariates \(\ln(Age)\) and \(\ln(Characters)\). The largest model
\((III)\) attempts to capture nonlinearities in the relationship of
\(\ln(Subscriptions)\) and \(\ln(PricePerCitation)\) using a cubic
regression function of \(\ln(PricePerCitation)\) and also adds the
interaction term \((PricePerCitation \times Age)\) while specfication
\((IV)\) does not include the cubic term.

\begin{align*}
  (I)\quad \ln(Subscriptions_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + u_i \\
  \\
  (II)\quad \ln(Subscriptions_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_4 \ln(Age_i) + \beta_6 \ln(Characters_i) + u_i \\
  \\
  (III)\quad \ln(Subscriptions_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_2 \ln(PricePerCitation_i)^2 \\
  +& \, \beta_3 \ln(PricePerCitation_i)^3 + \beta_4 \ln(Age_i) + \beta_5 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] \\ +& \, \beta_6 \ln(Characters_i) + u_i \\
  \\
  (IV)\quad \ln(Subscriptions_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_4 \ln(Age_i) + \beta_5 + \beta_6 \ln(Characters_i) + u_i
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate models (I) - (IV)}
\NormalTok{Journals_mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(Subscriptions) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(PricePerCitation), }\DataTypeTok{data =}\NormalTok{ Journals)}

\NormalTok{Journals_mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(Subscriptions) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(PricePerCitation) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(Age) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{log}\NormalTok{(Characters), }\DataTypeTok{data =}\NormalTok{ Journals)}

\NormalTok{Journals_mod3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(Subscriptions) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(PricePerCitation) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(}\KeywordTok{log}\NormalTok{(PricePerCitation)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{I}\NormalTok{(}\KeywordTok{log}\NormalTok{(PricePerCitation)}\OperatorTok{^}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(Age) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(Age)}\OperatorTok{:}\KeywordTok{log}\NormalTok{(PricePerCitation) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{log}\NormalTok{(Characters), }\DataTypeTok{data =}\NormalTok{ Journals)}

\NormalTok{Journals_mod4 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(Subscriptions) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(PricePerCitation) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(Age) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{log}\NormalTok{(Age)}\OperatorTok{:}\KeywordTok{log}\NormalTok{(PricePerCitation) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(Characters), }\DataTypeTok{data =}\NormalTok{ Journals)}
\end{Highlighting}
\end{Shaded}

Using \texttt{summary()}, we obtain the following estimated model
equations:

\begin{align*}
  (I)\quad \widehat{\ln(Subscriptions_i)} =& \, 4.77 - 0.53 \ln(PricePerCitation_i) \\
  \\
  (II)\quad \widehat{\ln(Subscriptions_i)} =& \, 3.21 - 0.41 \ln(PricePerCitation_i) + 0.42 \ln(Age_i) + 0.21 \ln(Characters_i) \\
  \\
  (III)\quad \widehat{\ln(Subscriptions_i)} =& \, 3.41 - 0.96 \ln(PricePerCitation_i) + 0.02 \ln(PricePerCitation_i)^2 \\
  +& \, 0.004 \ln(PricePerCitation_i)^3 + 0.37 \ln(Age_i) \\
  +& \, 0.16 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] \\ +& \, 0.23 \ln(Characters_i) \\
  \\
  (IV)\quad \widehat{\ln(Subscriptions_i)} =& \, 3.43 - 0.90 \ln(PricePerCitation_i) + 0.37 \ln(Age_i) \\ 
  +&  \, 0.14 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] + 0.23 \ln(Characters_i)
\end{align*}

It is of interest whether the coefficients the nonlinear transformations
of \(\ln(PricePerCitation)\) in Model \((III)\) are statistically
significant. To answer this, we use a \(F\)-Test:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# F-Test for significance of cubic terms}
\KeywordTok{linearHypothesis}\NormalTok{(Journals_mod3, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"I(log(PricePerCitation)^2)=0"}\NormalTok{,}\StringTok{"I(log(PricePerCitation)^3)=0"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## I(log(PricePerCitation)^2) = 0
## I(log(PricePerCitation)^3) = 0
## 
## Model 1: restricted model
## Model 2: log(Subscriptions) ~ log(PricePerCitation) + I(log(PricePerCitation)^2) + 
##     I(log(PricePerCitation)^3) + log(Age) + log(Age):log(PricePerCitation) + 
##     log(Characters)
## 
##   Res.Df    RSS Df Sum of Sq      F Pr(>F)
## 1    175 82.738                           
## 2    173 82.500  2    0.2379 0.2494 0.7795
\end{verbatim}

We cannot reject the null hypothesis \(H_0: \beta_3=\beta_4=0\) in model
\((III)\).

We will now use the \texttt{stargazer()} function to generate a verbose
tabular representation of all four estimated models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the stargazer package}
\KeywordTok{library}\NormalTok{(stargazer)}

\CommentTok{# generate a Latex table using stargazer}
\KeywordTok{stargazer}\NormalTok{(Journals_mod1, Journals_mod2, Journals_mod3, Journals_mod4,}
          \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"(I)"}\NormalTok{, }\StringTok{"(II)"}\NormalTok{, }\StringTok{"(III)"}\NormalTok{, }\StringTok{"(IV)"}\NormalTok{)}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

Dependent variable:

log(Subscriptions)

(I)

(II)

(III)

(IV)

log(PricePerCitation)

-0.533***

-0.408***

-0.961***

-0.899***

(0.036)

(0.042)

(0.189)

(0.162)

I(log(PricePerCitation)2)

0.017

(0.024)

I(log(PricePerCitation)3)

0.004

(0.007)

log(Age)

0.424***

0.373***

0.374***

(0.090)

(0.089)

(0.089)

log(Characters)

0.206*

0.235**

0.229**

(0.107)

(0.106)

(0.105)

log(PricePerCitation):log(Age)

0.156***

0.141***

(0.055)

(0.045)

Constant

4.766***

3.207***

3.408***

3.434***

(0.056)

(0.314)

(0.318)

(0.315)

Observations

180

180

180

180

R2

0.557

0.613

0.635

0.634

Adjusted R2

0.555

0.607

0.622

0.626

Residual Std. Error

0.750 (df = 178)

0.705 (df = 176)

0.691 (df = 173)

0.688 (df = 175)

F Statistic

224.037*** (df = 1; 178)

93.009*** (df = 3; 176)

50.149*** (df = 6; 173)

75.749*** (df = 4; 175)

Note:

\emph{p\textless{}0.1; \textbf{p\textless{}0.05; }}p\textless{}0.01

The subsequent code chunk reproduces figure 8.9 of the book:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# divide plotting area}
\NormalTok{m <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{layout}\NormalTok{(m)}

\CommentTok{# scatterplot}
\KeywordTok{plot}\NormalTok{(Journals}\OperatorTok{$}\NormalTok{PricePerCitation, }
\NormalTok{     Journals}\OperatorTok{$}\NormalTok{Subscriptions, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Subscriptions"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"ln(Price per ciation)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"(a)"}
\NormalTok{     )}

\CommentTok{# log-log scatterplot and estimated regression line (I)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log}\NormalTok{(Journals}\OperatorTok{$}\NormalTok{PricePerCitation), }
     \KeywordTok{log}\NormalTok{(Journals}\OperatorTok{$}\NormalTok{Subscriptions), }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"ln(Subscriptions)"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"ln(Price per ciation)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"(b)"}
\NormalTok{     )}

\KeywordTok{abline}\NormalTok{(Journals_mod1,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{       )}

\CommentTok{# log-log scatterplot and regression lines (IV) for Age = 5 and Age = 80}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log}\NormalTok{(Journals}\OperatorTok{$}\NormalTok{PricePerCitation), }
     \KeywordTok{log}\NormalTok{(Journals}\OperatorTok{$}\NormalTok{Subscriptions), }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"ln(Subscriptions)"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"ln(Price per ciation)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"(c)"}
\NormalTok{     )}

\NormalTok{JM4C <-Journals_mod4}\OperatorTok{$}\NormalTok{coefficients}

\CommentTok{# Age = 80}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(JM4C[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{JM4C[}\DecValTok{3}\NormalTok{] }\OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(}\DecValTok{80}\NormalTok{), }
\NormalTok{                JM4C[}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{JM4C[}\DecValTok{5}\NormalTok{] }\OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(}\DecValTok{80}\NormalTok{)}
\NormalTok{                ),}
       \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{)}

\CommentTok{# Age = 5}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(JM4C[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{JM4C[}\DecValTok{3}\NormalTok{] }\OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(}\DecValTok{5}\NormalTok{), }
\NormalTok{                JM4C[}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{JM4C[}\DecValTok{5}\NormalTok{] }\OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{                ),}
       \DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{,}
       \DataTypeTok{lwd =} \FloatTok{1.5}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-191-1.pdf}

As can be seen from plots (a) and (b), the relation between
subscriptions and the citation price is inverse and nonlinear and
log-transforming both variables makes it approximately linear. Plot (c)
shows that the price elasticity of journal subscriptions depends on the
journal's age: the red line shows the estimated relationship for
\(Age=80\) while the green line represents the prediction from model
\((IV)\) for \(Age=5\).

Which conclusion can be made?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We conclude that the demand for journals is more elastic for young
  journals than for old journals.
\item
  For model \((III)\) we cannot reject the null hypothesis that the
  coefficients on \(\ln(PricePerCitation)^2\) and
  \(\ln(PricePerCitation)^3\) are both zero using an \(F\)-test. This is
  evidence supporting a linear relation between log-subscriptions and
  log-price.
\item
  Demand is greater for Journals with more characters, holding price and
  age constant.
\end{enumerate}

Hence we have found evidence, that the price elasticity of demand for
economic journals depends on the age of the journal. Altogether our
estimates suggest that the demand is very inelastic, i.e.~the
librararies' demand for economic journals is quite insensitve to the
price: using model \((IV)\), even for a young journal (\(Age=5\)) we
estimate the price elasticity to be
\(-0,899+0,374\times\ln(5)+0,141\times\left[\ln(1)\times\ln(5)\right] \approx -0.3\)
so that a one percent increase in price is predicted to reduce the
demand by only \(0.3\) percent.

This finding comes at no surprise since providing the most recent
research is a necessity for libraries.

\section{Nonlinear Effects on Test Scores of the Student-Teacher
Ratio}\label{nonlinear-effects-on-test-scores-of-the-student-teacher-ratio}

In this section we will discuss three specific questions about the
relatin between test scores and the student-teacher ratio:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Does the effect on test scores of decreasing the student-teacher ratio
  depend on the fraction of english learners when we control for
  economic idiosyncracies of different districts?
\item
  Does this effect depend on the the student-teacher ratio?
\item
  How strong is the effect of decreasing the student-teacher ratio (by
  two students per teacher) if we take into account economic
  characteristics and nonlinearities?
\end{enumerate}

Two answer these questions we will consider a total of seven models,
some of which are nonlinear regression specifications of the types that
have been discussed before. As measures for the students' economic
backgrounds, we will additionally consider the regressors \(lunch\) and
\(\ln(income)\). We use the logarithm of \(income\) because the analysis
discussed in chapter 8.2 showed that the nonlinear relationship between
\(income\) and \(TestScores\) is approximately logarithmic. We do not
include expenditures per pupil (\(expenditure\)) because doing so allows
the expenditures to vary with the student-teacher ratio, see chapter
7.2.

\paragraph{Nonlinear Regression Models of Test
Scores}\label{nonlinear-regression-models-of-test-scores}
\addcontentsline{toc}{paragraph}{Nonlinear Regression Models of Test
Scores}

The considered model specifications are:

\begin{align}
 \widehat{TestScore_i} =& \, \beta_0 + \beta_1 size_i + \beta_4 english_i + \beta_9 lunch_i + u_i \\
 \\
 \widehat{TestScore_i} =& \, \beta_0 + \beta_1 size_i + \beta_4 english_i + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
 \\
  \widehat{TestScore_i} =& \, \beta_0 + \beta_1 size_i + \beta_5 HiEL_i + \beta_6 (HiEL_i\times size_i) + u_i \\
  \\
  \widehat{TestScore_i} =& \, \beta_0 + \beta_1 size_i + \beta_5 HiEL_i + \beta_6 (HiEL_i\times size_i) + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  \\
  \widehat{TestScore_i} =& \, \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_5 HiEL_i + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  \\
  \widehat{TestScore_i} =& \, \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_3 size_i^3 + \beta_5 HiEL_i + \beta_6 (HiEL\times size) \\ &\,+ \beta_7 (HiEL_i\times size_i^2) + \beta_8 (HiEL_i\times size_i^3) + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  \\
  \widehat{TestScore_i} =& \, \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_3 size_i^3 + \beta_4 english + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i
\end{align}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate all models}
\NormalTok{TestScore_mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\NormalTok{TestScore_mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }\DataTypeTok{data =}\NormalTok{ CASchools)}

\NormalTok{TestScore_mod3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL}\OperatorTok{:}\NormalTok{size, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\NormalTok{TestScore_mod4 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL}\OperatorTok{:}\NormalTok{size }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }
    \DataTypeTok{data =}\NormalTok{ CASchools)}

\NormalTok{TestScore_mod5 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(size}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(size}\OperatorTok{^}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }
    \DataTypeTok{data =}\NormalTok{ CASchools)}

\NormalTok{TestScore_mod6 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(size}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(size}\OperatorTok{^}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL}\OperatorTok{:}\NormalTok{size }\OperatorTok{+}\StringTok{ }
\StringTok{    }\NormalTok{HiEL}\OperatorTok{:}\KeywordTok{I}\NormalTok{(size}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL}\OperatorTok{:}\KeywordTok{I}\NormalTok{(size}\OperatorTok{^}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }\DataTypeTok{data =}\NormalTok{ CASchools)}

\NormalTok{TestScore_mod7 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{size }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(size}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(size}\OperatorTok{^}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{log}\NormalTok{(income), }\DataTypeTok{data =}\NormalTok{ CASchools)}
\end{Highlighting}
\end{Shaded}

Next, we may use \texttt{summary()} to assess the models. Using
\texttt{stargazer()} we may also obtain a LATEX-based tabular
representation of all regression outputs and which is more convenient
for comparison of the models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate a latex table of regression outputes}
\KeywordTok{stargazer}\NormalTok{(TestScore_mod1, TestScore_mod2, TestScore_mod3, TestScore_mod4, TestScore_mod5, }
\NormalTok{    TestScore_mod6, TestScore_mod7, }\DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"(1)"}\NormalTok{, }\StringTok{"(2)"}\NormalTok{, }\StringTok{"(3)"}\NormalTok{, }\StringTok{"(4)"}\NormalTok{, }
        \StringTok{"(5)"}\NormalTok{, }\StringTok{"(6)"}\NormalTok{, }\StringTok{"(7)"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Dependent variable:

score

(1)

(2)

(3)

(4)

(5)

(6)

(7)

size

-1.00***

-0.73***

-0.97*

-0.53*

64.34**

83.70***

65.29**

(0.24)

(0.23)

(0.54)

(0.30)

(25.46)

(29.69)

(25.48)

english

-0.12***

-0.18***

-0.17***

(0.03)

(0.03)

(0.03)

I(size2)

-3.42***

-4.38***

-3.47***

(1.29)

(1.51)

(1.29)

I(size3)

0.06***

0.07***

0.06***

(0.02)

(0.03)

(0.02)

lunch

-0.55***

-0.40***

-0.41***

-0.42***

-0.42***

-0.40***

(0.02)

(0.03)

(0.03)

(0.03)

(0.03)

(0.03)

log(income)

11.57***

12.12***

11.75***

11.80***

11.51***

(1.74)

(1.77)

(1.73)

(1.75)

(1.73)

HiEL

5.64

5.50

-5.47***

816.08*

(16.72)

(9.14)

(1.03)

(434.61)

size:HiEL

-1.28

-0.58

-123.28*

(0.84)

(0.46)

(66.35)

I(size2):HiEL

6.12*

(3.35)

I(size3):HiEL

-0.10*

(0.06)

Constant

700.15***

658.55***

682.25***

653.67***

252.05

122.35

244.81

(4.69)

(7.68)

(10.51)

(8.89)

(165.82)

(192.18)

(165.93)

Observations

420

420

420

420

420

420

420

R2

0.77

0.80

0.31

0.80

0.80

0.80

0.80

Adjusted R2

0.77

0.79

0.31

0.79

0.80

0.80

0.80

Residual Std. Error

9.08 (df = 416)

8.64 (df = 415)

15.88 (df = 416)

8.63 (df = 414)

8.56 (df = 413)

8.55 (df = 410)

8.57 (df = 413)

F Statistic

476.31*** (df = 3; 416)

405.36*** (df = 4; 415)

62.40*** (df = 3; 416)

325.80*** (df = 5; 414)

277.21*** (df = 6; 413)

185.78*** (df = 9; 410)

276.52*** (df = 6; 413)

Note:

\emph{p\textless{}0.1; \textbf{p\textless{}0.05; }}p\textless{}0.01

Let us summarize what can be concluded from the table above:

First of all, it is apparent that the student-teacher ratio is
statistically significant in all seven models. Adding \(\ln(income)\) to
model (1) we find that the corresponding coefficient is statistically
significant at the level of \(1\%\) while all other coefficients remain
at their significance level. Furthermore, the estimate for the
coefficient on \(size\) is roughly \(0.27\) points larger which could be
a sign of attenuated omitted variable bias. We consider this a reason to
include \(\ln(income)\) as a regressor in other models, too.

Regressions (3) and (4) are regression that aim to assees the effect of
allowing for an interaction between \(size\) and \(HiEL\), without and
with economic control variables. In both modesl, the interaction term
and the dummy are not statistically significant. Thus, even with
economic control variables we cannot the reject the null hypothesis,
that the effect of the student-teacher ratios on test scores is the same
for districts with high and districts with low share of english learning
students.

Regression (5) includes a cubic term for the student-teacher ratio and
omittes the interaction between \(size\) and \(HiEl\). The estimation
results indicate that there is a nonlinear effect of the student-teacher
ratio on test scores. (Can You verify this using an \(F\)-test of
\(H_0: \beta_2=\beta_3=0\)?)

Consequently, regression (6) further explores whether the fraction of
english learners is also accountable for the effect of the
student-teacher ration by using \(HiEL \times size\) and the
interactions \(HiEL \times size^2\) and \(HiEL \times size^3\). All
individual \(t\)-tests indicate that that there are significant effects.
We check this using a robust \(F\)-test of
\(H_0: \beta_6=\beta_7=\beta_8=0\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check joint significance of interactions}
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_mod6, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"size:HiEL=0"}\NormalTok{, }\StringTok{"I(size^2):HiEL=0"}\NormalTok{, }\StringTok{"I(size^3):HiEL=0"}\NormalTok{),}
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_mod6, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## size:HiEL = 0
## I(size^2):HiEL = 0
## I(size^3):HiEL = 0
## 
## Model 1: restricted model
## Model 2: score ~ size + I(size^2) + I(size^3) + HiEL + HiEL:size + HiEL:I(size^2) + 
##     HiEL:I(size^3) + lunch + log(income)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(>F)  
## 1    413                    
## 2    410  3 2.6903 0.04597 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We find that the null can be rejected at the level of \(5\%\) and
conclude that the regression function differs for districts with high
and low percentage of english learners.

Specification (7) uses a continuous measure for the share of english
learners instead of a dummy variable (and thus omitts the interaction
terms). We observe only small changes to the coefficient estimates on
the other regressors and thus conclude that the results observed for
specification (5) are not sensitive to the way the percentage of english
learners is measured.

We continue by reproducing figure 8.10 of the book using R for
interpretation of the nonlinear specifications (2), (5) and (7).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scatterplot}
\KeywordTok{plot}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{size, }
\NormalTok{     CASchools}\OperatorTok{$}\NormalTok{score, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{,}\DecValTok{28}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{600}\NormalTok{,}\DecValTok{740}\NormalTok{),}
     \DataTypeTok{pch=}\DecValTok{20}\NormalTok{, }
     \DataTypeTok{col=}\StringTok{"gray"}\NormalTok{, }
     \DataTypeTok{xlab=}\StringTok{"Student-Teacher Ratio"}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"Test Score"}\NormalTok{)}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"top"}\NormalTok{, }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Linear Regression (2)"}\NormalTok{,}\StringTok{"Cubic Regression (5)"}\NormalTok{, }\StringTok{"Cubic Regression (7)"}\NormalTok{),}
       \DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{,}
       \DataTypeTok{ncol =} \DecValTok{3}\NormalTok{,}
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{,}\StringTok{"black"}\NormalTok{)}
\NormalTok{       )}

\CommentTok{# data for use with predict()}
\NormalTok{new_data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"size"}\NormalTok{=}\KeywordTok{seq}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{24}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                       \StringTok{"english"}\NormalTok{=}\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{english),}
                       \StringTok{"lunch"}\NormalTok{=}\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{lunch),}
                       \StringTok{"income"}\NormalTok{=}\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income),}
                       \StringTok{"HiEL"}\NormalTok{=}\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{HiEL)}
\NormalTok{                       )}

\CommentTok{# add estimated regression function for model (2)}
\NormalTok{fitted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(TestScore_mod2, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\KeywordTok{lines}\NormalTok{(new_data}\OperatorTok{$}\NormalTok{size, }
\NormalTok{      fitted,}
      \DataTypeTok{lwd=}\FloatTok{1.5}\NormalTok{,}
      \DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}

\CommentTok{# add estimated regression function for model (5)}
\NormalTok{fitted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(TestScore_mod5, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\KeywordTok{lines}\NormalTok{(new_data}\OperatorTok{$}\NormalTok{size, }
\NormalTok{      fitted, }
      \DataTypeTok{lwd=}\FloatTok{1.5}\NormalTok{,}
      \DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}

\CommentTok{# add estimated regression function for model (7)}
\NormalTok{fitted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(TestScore_mod7, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\KeywordTok{lines}\NormalTok{(new_data}\OperatorTok{$}\NormalTok{size, }
\NormalTok{      fitted, }
      \DataTypeTok{col=}\StringTok{"black"}\NormalTok{,}
      \DataTypeTok{lwd=}\FloatTok{1.5}\NormalTok{,}
      \DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-196-1} \end{center}

Note that for the figure above all regressors except for \(size\) are
set to their sample averages. We see that the cubic regressions (5) and
(7) are almost identical. They indicate that the relation between test
scores and the student-teacher ratio has only a small amount of
nonlinearity since they do not deviate much from the regression function
of (2).

The next code chunk reproduces figure 8.11 of the book. We use
\texttt{plot()} and \texttt{points()} to color observations depending on
\(HiEL\). Again, the regression lines are drawn based on predictions
using average sample averages of all regressors except for \(size\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scatterplot}

\CommentTok{# observations with HiEL = 0}
\KeywordTok{plot}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{size[CASchools}\OperatorTok{$}\NormalTok{HiEL}\OperatorTok{==}\DecValTok{0}\NormalTok{], }
\NormalTok{     CASchools}\OperatorTok{$}\NormalTok{score[CASchools}\OperatorTok{$}\NormalTok{HiEL}\OperatorTok{==}\DecValTok{0}\NormalTok{], }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{,}\DecValTok{28}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{600}\NormalTok{,}\DecValTok{730}\NormalTok{),}
     \DataTypeTok{pch=}\DecValTok{20}\NormalTok{, }
     \DataTypeTok{col=}\StringTok{"gray"}\NormalTok{, }
     \DataTypeTok{xlab=}\StringTok{"Student-Teacher Ratio"}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"Test Score"}\NormalTok{)}

\CommentTok{# observations with HiEL = 1}
\KeywordTok{points}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{size[CASchools}\OperatorTok{$}\NormalTok{HiEL}\OperatorTok{==}\DecValTok{1}\NormalTok{], }
\NormalTok{       CASchools}\OperatorTok{$}\NormalTok{score[CASchools}\OperatorTok{$}\NormalTok{HiEL}\OperatorTok{==}\DecValTok{1}\NormalTok{],}
       \DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{,}
       \DataTypeTok{pch=}\DecValTok{20}\NormalTok{)}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"top"}\NormalTok{, }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Regression (6) with HiEL=0"}\NormalTok{, }\StringTok{"Regression (6) with HiEL=1"}\NormalTok{),}
       \DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{,}
       \DataTypeTok{ncol =} \DecValTok{2}\NormalTok{,}
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"green"}\NormalTok{,}\StringTok{"red"}\NormalTok{)}
\NormalTok{       )}

\CommentTok{# data for use with predict()}
\NormalTok{new_data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"size"}\NormalTok{=}\KeywordTok{seq}\NormalTok{(}\DecValTok{12}\NormalTok{,}\DecValTok{28}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                       \StringTok{"english"}\NormalTok{=}\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{english),}
                       \StringTok{"lunch"}\NormalTok{=}\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{lunch),}
                       \StringTok{"income"}\NormalTok{=}\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{income),}
                       \StringTok{"HiEL"}\NormalTok{=}\DecValTok{0}
\NormalTok{                       )}

\CommentTok{# add estimated regression function for model (6) with HiEL=0}
\NormalTok{fitted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(TestScore_mod6, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\KeywordTok{lines}\NormalTok{(new_data}\OperatorTok{$}\NormalTok{size, }
\NormalTok{      fitted, }
      \DataTypeTok{lwd=}\FloatTok{1.5}\NormalTok{,}
      \DataTypeTok{col=}\StringTok{"green"}\NormalTok{)}

\CommentTok{# add estimated regression function for model (6) with HiEL=1}
\NormalTok{new_data}\OperatorTok{$}\NormalTok{HiEL=}\DecValTok{1}

\NormalTok{fitted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(TestScore_mod6, }\DataTypeTok{newdata =}\NormalTok{ new_data)}

\KeywordTok{lines}\NormalTok{(new_data}\OperatorTok{$}\NormalTok{size, }
\NormalTok{      fitted, }
      \DataTypeTok{lwd=}\FloatTok{1.5}\NormalTok{,}
      \DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-197-1} \end{center}

From the regression output we see that (6) finds statistically
significant coefficients on the interaction terms \(HiEL:size\),
\(HiEL:size^2\) and \(HiEL:size^3\) i.e.~there is evidence that the
nonlinear relationship connecting test scores and student-teacher ration
depends on the amount of English learning students in the district.
However, the figure shows that this difference is not/ of practical
importance and is a good example of why one should be careful when
inpreting nonlinear models: though the two regression functions look
different, we see that the slope of both functions, that is for both
districts with high and districs with low share of English learning
students, is almost identical for student-teacher ratios between \(17\)
and \(23\). Since this range includes almost \(90\%\) of all
observations, we can be confident about this.

One might be tempted to object since both functions show opposing slopes
for student-teacher ratios below \(15\) and beyond \(24\). Two things
can be set aginst that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The data has only few observations with low and high values of the
  student-teacher ratio so there is only little information to be
  exploited when estimating the model. This means the estimated function
  is less precise in the extremes of the dataset
\item
  Such a behaviour is a typical caveat when using cubic functions since
  they generally show extreme behaviour for extreme observations. Think
  of the graph of \(f(x) = x^3\).
\end{enumerate}

We thus conclude that there is no dependence of the relation between
class size and test scores on the percentage of English learners in the
district.

\subsubsection*{Summary}\label{summary}
\addcontentsline{toc}{subsubsection}{Summary}

We are now able to answer the three question posed at the beginning of
this section.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Considering the linear models, the percentage of English learners has
  only little influence on the effect of test scores of changing the
  student-teacher ratio. This result keeps valid if we control for
  economic background of the students. While the cubic specification (6)
  provides evidence, the strength of the effect is negligible.
\item
  When controlling for the students' economic backgroumd we find
  evidence of nonlinearities in the relationship between student-teacher
  ratio and test scores.
\item
  The linear specification (2) predicts that a reduction of the
  student-teacher ratio by two students per teacher leads to an
  improvement in test scores of about \(-0.73 \times (-2) = 1.46\)
  points. Since the model is linear, this effect is independent of the
  class size. Asssume that the student-teacher ratio is \(20\). For
  example, model (5) predicts that the reduction increases test scores
  by
  \[64.33*18+18^2*(-3.42)+18^3*(0.059) - (64.33*20+20^2*(-3.42)+20^3*(0.059)) \approx 3.3\]
  points. if the ratio is \(22\), a reduction to \(20\) leads to a
  predicted improvement in test scores of
  \[64.33*20+20^2*(-3.42)+20^3*(0.059) - (64.33*22+22^2*(-3.42)+22^3*(0.059)) \approx 2.4\]
  points. This suggests that the effect is stronger in smaller classes.
\end{enumerate}

\chapter{Assessing Studies Based on Multiple
Regression}\label{assessing-studies-based-on-multiple-regression}

The majority of this chapter of the book is of a theoretical nature.
Therefore this section briefly reviews the concepts of internal and
external validity in general and discusses examples of threats to
internal and external validity of multiple regression models. We will
disuss consequences of

\begin{itemize}
\tightlist
\item
  Misspecification of the functional form of the regression function
\item
  Measurement errors
\item
  Missing data and sample selection
\item
  Simultaneous causality
\end{itemize}

as well as sources of inconsitency of OLS standard errors. We also
review concerns of internal validity and external validity in the
context of forecasting using regression models.

The chapter closes with an application with R where we assess whether
results found by multiple regression using the \texttt{CASchools} data
can be generalized to school districs of another federal state of the
United States.

For a more detailed treatment of these topics we encourage you to work
through chapter 9 of the book.

\section{Internal and External
Validity}\label{internal-and-external-validity}

Key Concept 9.1

Internal and External Validity

We say a statistical analysis has \textbf{internal} validity if the
statistical inferences made about causal effects are valid for the
considered population.

An analysis is said to have \textbf{external} validity if inferences and
conclusion are valid for the studies population and can be generalized
to other populations and settings.

\subsubsection*{Threats to Internal
Validity}\label{threats-to-internal-validity}
\addcontentsline{toc}{subsubsection}{Threats to Internal Validity}

For internal validity to exist, there are two conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The estimator of the causal effect, that is the coefficient(s) of
  interest, needs to be unbiased and consistent.
\item
  Statistical Inference is valid, that is hypotheses tests should have
  the desired significance level and confidence intervals should have
  the desired confidence level.
\end{enumerate}

In multiple regression, we estimate the model coefficients using OLS.
Thus for condition 1. to be fulfilled we need the OLS estimator to be
unbiased and consistent. For the second condition to be valid, the
standard errors must be valid such that hypothesis testing and
computation of confidence intervals yield results that are trustworthy.
Remember that a necessary condition for conditions 1. and 2. to be
fulfilled is that the assumptions of Key Concept 6.4 hold.

\subsubsection*{Threats to External
Validity}\label{threats-to-external-validity}
\addcontentsline{toc}{subsubsection}{Threats to External Validity}

External validity might be invalid

\begin{itemize}
\item
  if there are \textbf{differences between the considered populations}.
\item
  if there are \textbf{differences in the settings} of the considered
  populations, e.g.~the legal framework, the time of the investigation
  etc.
\end{itemize}

\section{Threats to Internal Validity of Multiple Regression
Analysis}\label{threats-to-internal-validity-of-multiple-regression-analysis}

This section lists five sources that cause the OLS estimator in
(multiple) regression models to be biased and inconsistent for the
causal effect of interest and discusses possible remedies. Note that all
five sources arise from violation of the first least squares assumption
in Key Concept 6.4.

This sections treats:

\begin{itemize}
\item
  Omitted variable Bias
\item
  Misspecification of the functional form
\item
  Measurement errors
\item
  Missing data and sample selection
\item
  Simultaneous causality bias
\end{itemize}

Beside these threats for consistency of the coefficient estimation, we
will also briefly discuss sources of inconsistent estimation of OLS
standard errors.

\subsubsection*{Omitted Variable Bias}\label{omitted-variable-bias-1}
\addcontentsline{toc}{subsubsection}{Omitted Variable Bias}

Key Concept 9.2

Omitted Variable Bias: Should I include More Variables in My Regression?

Inclusion of additional variables reduces the risk of omitted variable
bias but may increase the variance of the estimator of the coefficient
of interest.

We present some guidelines that help deciding whether to include an
additional variable:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Specify the coefficient(s) of interest.
\item
  Identify the most important potential sources of omitted variable bias
  by using knowlegde available \emph{before} estimating the model. You
  should end up with a base specification and a set of regressors that
  are questionable.
\item
  Use different model specifactions to test whether questionable
  regressors have coefficients different from zero.
\item
  Use tables to provide ``full disclosure'' of your results i.e.~present
  different model specifications that do both support your argument and
  enable the reader to see the effect of including questionable
  regressors.
\end{enumerate}

By now you should be aware of omitted variable bias and its
consequences. Key Concept 9.2 gives some guidelines on how to proceed if
there are control variables that possibly allow to reduce an omitted
variable bias. If including additional variables to mitigate the bias is
not an option because there are no adequate controls, there are
different approaches to solve the problem:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Usage of Panel data (discussed in Chapter 10)
\item
  Usage of Instrumental variables regression (discussed in Chapter 12)
\item
  Usage of a randomized control experiment (discussed in Chapter 13)
\end{enumerate}

\subsubsection*{Misspecification of the Functional Form of the
Regression
Function}\label{misspecification-of-the-functional-form-of-the-regression-function}
\addcontentsline{toc}{subsubsection}{Misspecification of the Functional
Form of the Regression Function}

If the population regression function is nonlinear but the regression
model is linear, we say that the functional form of the regression
function is misspecified. This leads to a bias of the OLS estimator.

Key Concept 9.3

Functional Form Misspecification

We say a regression suffers from misspecification of the functional form
when the functional form of the estimated regression model differs from
the functional form of the population regression function. Functional
form misspecification leads to biased and inconsistent coefficient
estimators. A way to detect functional form misspecification is to plot
the estimated regression function and the data. This may also be helpful
to choose the correct functional form.

It is easy to come up with an example case of misspecification of the
functional form:

Consider the case where the population regression function is
\[ Y_i = - X_i^2 \] but the estimated model is
\[ Y_i = \beta_0 + \beta_1 X_i + u_i \] so that the regression function
is misspecified.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}

\CommentTok{# generate data set}
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{, }\OperatorTok{-}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\NormalTok{X}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{# plot the data}
\KeywordTok{plot}\NormalTok{(X, Y, }
     \DataTypeTok{main =} \StringTok{"Misspecification of Functional Form"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}
\NormalTok{     )}

\CommentTok{# estimate and plot the regression function}
\NormalTok{ms_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\NormalTok{ms_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ X)
## 
## Coefficients:
## (Intercept)            X  
##     8.11363     -0.04684
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{abline}\NormalTok{(ms_mod, }
       \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{,}
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-199-1} \end{center}

It is evident that regression errors are relatively small for
observations close to \(X=-3\) and \(X=3\) but increase for \(X\) values
closer to zero and even more for regressor values beyond \(-4\) and
\(4\). Consequences are drastic: the intercept is estimated to be
\(8.1\) and for the slope parameter we obtain an estimate close to zero
which is obviously wrong. This issue does not dissapear as the number of
observations is inreased because OLS is biased \emph{and} inconsistent
due to the misspecification of the regression function.

\subsubsection*{Measurement Error and Errors-in-Variables
Bias}\label{measurement-error-and-errors-in-variables-bias}
\addcontentsline{toc}{subsubsection}{Measurement Error and
Errors-in-Variables Bias}

Key Concept 9.4

Errors-in-Variable Bias

When independent variables are measure imprecisely, we speak of
Errors-in-variables bias. This bias does not disappear if the sample
size is large. If the measurement error has mean zero and is independent
of the affected variable, the OLS estimator of the respective
coefficient is biased towards zero.

Suppose you are measuring data on the single regressor \(X_i\)
incorrectly so that there is a measurement error and you observe
\(\overset{\sim}{X}_i\) instead of \(X_i\). Then, instead of estimating
the population the regression model
\[ Y_i = \beta_0 + \beta_1 X_i + u_i \] you end up estimating

\begin{align*}
  Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + \underbrace{\beta_1 (X_i - \overset{\sim}{X}_i) + u_i}_{=v_i} \\
  Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + v_i
\end{align*}

where \(\overset{\sim}{X}_i\) and the error term \(v_i\) are correlated.
Thus OLS would be biased and inconsitent for the true \(\beta_1\) in
this example.

One can show that direction and strength of the bias depend on the
correlation between the observed regressor, \(\overset{\sim}{X}_i\), and
the measurement error, \((X_i - \overset{\sim}{X}_i)\). The correlation
depends on the type of the measurement error.

The classical measurement error model assumes that the error, \(w_i\),
has zero mean and that the error is uncorrelated with the variable,
\(X_i\), and the error term of the poulation regression model, \(u_i\):

\begin{equation}
  \overset{\sim}{X}_i = X_i + w_i \ \ , \ \ corr(w_i,u_i)=0 \ \ , \ \ corr(w_i,X_i)=0 
\end{equation}

Then it holds that

\begin{equation}
  \widehat{\beta}_1 \xrightarrow{p}{\frac{\sigma_{X}^2}{\sigma_{X}^2 + \sigma_{w}^2}} \label{eq:cmembias}
\end{equation}

Which implies inconsistency as \(\sigma_{X}^2, \sigma_{w}^2 > 0\) such
that the fraction in \eqref{eq:cmembias} is smaller than \(1\). Note that,
there are two extreme cases: first, if there is no measurement error,
\(\sigma_{w}^2=0\) such that
\(\widehat{\beta}_1 \xrightarrow{p}{\beta_1}\). Second, if
\(\sigma_{w}^2 \gg \sigma_{X}^2\) we have
\(\widehat{\beta}_1 \xrightarrow{p}{0}\). This is the case if the
measurement error is so large that there essentially is no information
on \(X\).

The most obvious way to deal with errors-in-variables bias is to use an
accurately measured \(X\). If this not possible, instrumental variables
regression can be an option. One might also deal with the issue by using
a mathematical model of the measurement error and correct the estimates.
For example, if it is plausible that the classical measurement error
model applies and if there is information that can be used to estimate
the ratio in equation \eqref{eq:cmembias}, one could compute an estimate
that corrects for the downwoard bias.

For example, consider two bivariate normal distributed random variables
\(X,Y\). It is a well known result, that the conditional expectation
function of \(Y\) given \(X\) has the form

\begin{align}
  E(Y\vert X) = E(Y) + \rho_{X,Y} \frac{\sigma_{Y}}{\sigma_{X}}\left[X-E(X)\right]. \label{eq:bnormexpfn} 
\end{align}

Thus for

\begin{align}
  (X, Y) \sim \mathcal{N}\left[\begin{pmatrix}50\\ 100\end{pmatrix},\begin{pmatrix}10 & 5 \\ 5 & 10 \end{pmatrix}\right] \label{eq:bvnormd}
\end{align}

according to \eqref{eq:bnormexpfn}, the population regression function is

\begin{align*}
  Y_i =& \, 100 + 0.5 (X - 50) \\
      =& \, 75 + 0.5 X.
\end{align*}

Suppose you gather data on \(X\) and \(Y\) but that can you only measure
\(\overset{\sim}{X_i} = X_i + w_i\) with
\(w_i \overset{i.i.d.}{\sim} \mathcal{N}(0,\sqrt{10})\). Since the
\(w_i\) are purely random, there is no correlation between the \(X_i\)
and the \(w_i\) so that we have a case of the classical measurement
error model. We can illustrate this using R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# load the mvtnorm package and simulate data}
\KeywordTok{library}\NormalTok{(mvtnorm)}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \KeywordTok{rmvnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{100}\NormalTok{), }
          \DataTypeTok{sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{))}
\NormalTok{          )}
\NormalTok{  )}

\CommentTok{# set columns names}
\KeywordTok{colnames}\NormalTok{(dat) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\StringTok{"Y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We now estimate a simple linear regression of \(Y\) on \(X\) using this
sample data and run the same regression again but this time add i.i.d.
\(\mathcal{N}(0,\sqrt{10})\) errors to \(X\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the model (without measurement error)}
\NormalTok{nme_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ dat)}

\CommentTok{# estimate the model (measurement error in X)}
\NormalTok{dat}\OperatorTok{$}\NormalTok{X <-}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{X }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{sd =} \KeywordTok{sqrt}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\NormalTok{me_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ dat)}

\CommentTok{# print estimated coefficients to console}
\NormalTok{nme_mod}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           X 
##  76.3002047   0.4755264
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{me_mod}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           X 
##   87.276004    0.255212
\end{verbatim}

We visualize the estimation results and compare with the population
regression function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot sample data}
\KeywordTok{plot}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X, dat}\OperatorTok{$}\NormalTok{Y, }
     \DataTypeTok{pch=}\DecValTok{20}\NormalTok{, }
     \DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"X"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Y"}
\NormalTok{     )}

\CommentTok{# add population regression function}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(}\DecValTok{75}\NormalTok{,}\FloatTok{0.5}\NormalTok{), }
       \DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{,}
       \DataTypeTok{lwd  =} \FloatTok{1.5}
\NormalTok{      )}

\CommentTok{# add estimated regression functions}
\KeywordTok{abline}\NormalTok{(nme_mod, }
       \DataTypeTok{col =} \StringTok{"purple"}\NormalTok{,}
       \DataTypeTok{lwd  =} \FloatTok{1.5}
\NormalTok{       )}
\KeywordTok{abline}\NormalTok{(me_mod, }
       \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{,}
       \DataTypeTok{lwd  =} \FloatTok{1.5}
\NormalTok{       )}

\CommentTok{# add legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkgreen"}\NormalTok{, }\StringTok{"purple"}\NormalTok{, }\StringTok{"darkred"}\NormalTok{), }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Population"}\NormalTok{, }\StringTok{"No Errors"}\NormalTok{, }\StringTok{"Errors"}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-202-1} \end{center}

Notice that in the situation without measurement error, the estimated
regression function is close to the population regression function.
Things are different when we use the error afflicted data on \(X\): both
the estimate for the intercept and the estimate for the coefficient on
\(X\) differ considerably from results obtained using the ``clean'' data
on \(X\). In particular \(\widehat{\beta}_1 = 0.255\). This is evidence
for the downward bias. We are in the comfortable situation to know
\(\sigma_X^2\) and \(\sigma^2_w\). This allows us to correct for bias
using \eqref{eq:cmembias}. Using this information we obtain a
biased-corrected estimate
\[\frac{\sigma_X^2 + \sigma_w^2}{\sigma_X^2} \widehat{\beta}_1 = \frac{10+10}{10} 0.255 = 0.51\]
which is fairly close to the true coefficient in the population
regression function, \(\beta_1=0.5\).

\subsubsection*{Missing Data and Sample
Selection}\label{missing-data-and-sample-selection}
\addcontentsline{toc}{subsubsection}{Missing Data and Sample Selection}

Key Concept 9.5

Sample Selection Bias

When the sampling process influences the availability of data and when
there is a relation of this sampling process to the dependend variable
that goes beyond the dependence on the regressors, we say that there is
a sample selection bias. This bias is due to correlation between one ore
more regressors and the error term. This implies both bias and
inconsistency of the OLS estimator.

There are three cases of Sample selection but only one of which poses a
threat to internal validity of a regression study. The three cases are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Data are missing at random.
\item
  Data are missing based on the value of a regressor.
\item
  Data are missing due to a selection process that is releated to the
  dependent variable.
\end{enumerate}

Let us jump back to the example of variables \(X\) and \(Y\) distributed
as stated in equation \eqref{eq:bvnormd} and illustrate all three cases
using R.

If data are missing at random, this is nothing but loosing observations.
For example, loosing \(50\%\) of sample would be the same as never
having seen the (randomly chosen) half of the sample observed. This does
not introduce an estimation bias.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# simulate data}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \KeywordTok{rmvnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{100}\NormalTok{), }
          \DataTypeTok{sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{))}
\NormalTok{          )}
\NormalTok{  )}

\KeywordTok{colnames}\NormalTok{(dat) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\StringTok{"Y"}\NormalTok{)}

\CommentTok{# mark 500 randomly selected observations}
\NormalTok{id <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{, }\DataTypeTok{size =} \DecValTok{500}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X[}\OperatorTok{-}\NormalTok{id], }
\NormalTok{     dat}\OperatorTok{$}\NormalTok{Y[}\OperatorTok{-}\NormalTok{id], }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"X"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{)}

\KeywordTok{points}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X[id], }
\NormalTok{       dat}\OperatorTok{$}\NormalTok{Y[id], }
       \DataTypeTok{col =} \StringTok{"gray"}\NormalTok{, }
       \DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}

\CommentTok{# add population regression function}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(}\DecValTok{75}\NormalTok{,}\FloatTok{0.5}\NormalTok{), }
       \DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{,}
       \DataTypeTok{lwd  =} \FloatTok{1.5}
\NormalTok{      )}

\CommentTok{# add estimated regression function for full sample}
\KeywordTok{abline}\NormalTok{(nme_mod)}

\CommentTok{# estimate model case 1, add regression line}
\NormalTok{dat <-}\StringTok{ }\NormalTok{dat[}\OperatorTok{-}\NormalTok{id,]}

\NormalTok{c1_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(dat}\OperatorTok{$}\NormalTok{Y }\OperatorTok{~}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ dat)}
\KeywordTok{abline}\NormalTok{(c1_mod, }\DataTypeTok{col =} \StringTok{"purple"}\NormalTok{)}

\CommentTok{# add legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkgreen"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"purple"}\NormalTok{), }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Population"}\NormalTok{, }\StringTok{"Full sample"}\NormalTok{, }\StringTok{"500 obs. randomly selected"}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-203-1} \end{center}

The gray dots are the \(500\) discarded observations. The estimation
results when using the remaining observations deviate only marginally
from the results obtained using the full sample.

Selecting data randomly based on the value of a regressor has also the
effect of reducing the sample size and does not introduce estimation
bias. We will now drop observations with \(X > 45\), reestimate the
model and compare.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# simulate data}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \KeywordTok{rmvnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{100}\NormalTok{), }
          \DataTypeTok{sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{))}
\NormalTok{          )}
\NormalTok{  )}

\KeywordTok{colnames}\NormalTok{(dat) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\StringTok{"Y"}\NormalTok{)}

\CommentTok{# mark observations}
\NormalTok{id <-}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{X }\OperatorTok{>=}\StringTok{ }\DecValTok{45}

\KeywordTok{plot}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X[}\OperatorTok{-}\NormalTok{id], }
\NormalTok{     dat}\OperatorTok{$}\NormalTok{Y[}\OperatorTok{-}\NormalTok{id], }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"X"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{)}

\KeywordTok{points}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X[id], }
\NormalTok{       dat}\OperatorTok{$}\NormalTok{Y[id], }
       \DataTypeTok{col =} \StringTok{"gray"}\NormalTok{, }
       \DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}

\CommentTok{# add population regression function}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(}\DecValTok{75}\NormalTok{,}\FloatTok{0.5}\NormalTok{), }
       \DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{,}
       \DataTypeTok{lwd  =} \FloatTok{1.5}
\NormalTok{      )}

\CommentTok{# add estimated regression function for full sample}
\KeywordTok{abline}\NormalTok{(nme_mod)}

\CommentTok{# estimate model case 1, add regression line}
\NormalTok{dat <-}\StringTok{ }\NormalTok{dat[}\OperatorTok{-}\NormalTok{id,]}

\NormalTok{c2_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(dat}\OperatorTok{$}\NormalTok{Y }\OperatorTok{~}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ dat)}
\KeywordTok{abline}\NormalTok{(c2_mod, }\DataTypeTok{col =} \StringTok{"purple"}\NormalTok{)}

\CommentTok{# add legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkgreen"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"purple"}\NormalTok{), }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Population"}\NormalTok{, }\StringTok{"Full sample"}\NormalTok{, }\StringTok{"Obs. with X <= 45"}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-204-1} \end{center}

Note that although we dropped more than \(90\%\) of all observations,
the estimated regression function is very close to the line estimated
based on the full sample.

In the third case we face sample selection bias. We can illustrate this
by using the selection procedure
\texttt{dat{[}which(dat\$X\ \textless{}=\ 55\ \&\ dat\$Y\ \textgreater{}=\ 100){]}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# simulate data}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \KeywordTok{rmvnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{100}\NormalTok{), }
          \DataTypeTok{sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{))}
\NormalTok{          )}
\NormalTok{  )}

\KeywordTok{colnames}\NormalTok{(dat) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\StringTok{"Y"}\NormalTok{)}

\CommentTok{# mark observations}
\NormalTok{id <-}\StringTok{ }\KeywordTok{which}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X }\OperatorTok{<=}\StringTok{ }\DecValTok{55} \OperatorTok{&}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{Y }\OperatorTok{>=}\StringTok{ }\DecValTok{100}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X[}\OperatorTok{-}\NormalTok{id], }
\NormalTok{       dat}\OperatorTok{$}\NormalTok{Y[}\OperatorTok{-}\NormalTok{id], }
       \DataTypeTok{col =} \StringTok{"gray"}\NormalTok{, }
       \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
       \DataTypeTok{xlab =} \StringTok{"X"}\NormalTok{,}
       \DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{)}

\KeywordTok{points}\NormalTok{(dat}\OperatorTok{$}\NormalTok{X[id], }
\NormalTok{     dat}\OperatorTok{$}\NormalTok{Y[id], }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}
\NormalTok{     )}

\CommentTok{# add population regression function}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{coef =} \KeywordTok{c}\NormalTok{(}\DecValTok{75}\NormalTok{,}\FloatTok{0.5}\NormalTok{), }
       \DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{,}
       \DataTypeTok{lwd  =} \FloatTok{1.5}
\NormalTok{      )}

\CommentTok{# add estimated regression function for full sample}
\KeywordTok{abline}\NormalTok{(nme_mod)}

\CommentTok{# estimate model case 1, add regression line}
\NormalTok{dat <-}\StringTok{ }\NormalTok{dat[id,]}

\NormalTok{c3_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(dat}\OperatorTok{$}\NormalTok{Y }\OperatorTok{~}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ dat)}
\KeywordTok{abline}\NormalTok{(c3_mod, }\DataTypeTok{col =} \StringTok{"purple"}\NormalTok{)}

\CommentTok{# add legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkgreen"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"purple"}\NormalTok{), }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Population"}\NormalTok{, }\StringTok{"Full sample"}\NormalTok{, }\StringTok{"X <= 55 & Y >= 100"}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-205-1} \end{center}

We see that the selection process leads to biased estimation results.
There are methods that allow to correct for sample selection bias.
However, these methods are beyond the scope of the book and therefore
not considered here. The concept of sample selection bias is summarized
in Key Concept 9.5.

\subsubsection*{Simultaneous Causality}\label{simultaneous-causality}
\addcontentsline{toc}{subsubsection}{Simultaneous Causality}

Key Concept 9.6

Simultaneous Causality Bias

So far we have assumed that the changes in the independent variable
\(X\) are responsible for changes in the dependent variable \(Y\). When
the reverse is also true, say that there is simultaneous causality
between \(X\) and \(Y\). This reverse causality leads to correlaltion
between \(X\) and the error in the population regression of interest
such that the coefficient on \(X\) is estimated with a bias.

Suppose we are interested in estimating the effect of a \(20\%\)
increase of cigarettes prices on ciggarettes consumption in the United
States using a multiple regression model. This may be investigated using
the dataset \texttt{CigarettesSW} which is part of the \texttt{AER}
package. \texttt{CigarettesSW} is a panel data set on cigarette
consumption for all 48 continental US States from 1985-1995 and provides
data on economic indicators and average local prices, taxes and per
capita pack consumption.

After loading the dataset, we pick observations for the year 1995 and
plot logarithmns of average federal and local excise taxes,
\texttt{tax}, against pack consumtion, \texttt{packs}, and estimate a
simple linear regression model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(}\StringTok{"CigarettesSW"}\NormalTok{)}
\NormalTok{c1995 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(CigarettesSW, year }\OperatorTok{==}\StringTok{ "1995"}\NormalTok{)}

\CommentTok{# estimate model}
\NormalTok{cigcon_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(packs) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(price), }\DataTypeTok{data =}\NormalTok{ c1995)}
\NormalTok{cigcon_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log(packs) ~ log(price), data = c1995)
## 
## Coefficients:
## (Intercept)   log(price)  
##      10.850       -1.213
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot estimated regression line and data}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log}\NormalTok{(c1995}\OperatorTok{$}\NormalTok{price), }\KeywordTok{log}\NormalTok{(c1995}\OperatorTok{$}\NormalTok{packs),}
     \DataTypeTok{xlab =} \StringTok{"ln(Price)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"ln(Consumption)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Demand for Cigarettes"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}
\NormalTok{     )}

\KeywordTok{abline}\NormalTok{(cigcon_mod, }
       \DataTypeTok{col=}\StringTok{"darkred"}\NormalTok{, }
       \DataTypeTok{lwd=}\FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-206-1.pdf}

Remember from chapter 8 that, due to the log-log specification, in the
population regression the coefficient on the logarithm of price is
interpreted as the price elasticity of consumption. The estimated
coefficient suggest that a \(1\%\) increase in cigarettes prices reduces
cigarette consumption by about \(1.2\%\), on average. Have we estimated
a demand curve? The answer is no: this is a classic example of
simultaneous causality (see Key Concept 9.6) since the observations are
market equilibria which are determined by both changes in supply and
changes in demand such that the price is correlated with the error term
and the OLS estimator is biased. We can neither estimate a demand nor a
supply curve consistently using this approach.

We will return to this issue (and the dataset) in chapter 12 which
presents instrumental variables regression, an approach that allows
consistent estimation when there is simultaneous causality.

\subsubsection{Sources of Inconsistency of OLS Standard
Errors}\label{sources-of-inconsistency-of-ols-standard-errors}

There are two central threats to computation of consistent OLS standard
errors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Heteroskedasticity: implications of heteroskedasticiy have been
  discussed in chapter 5. Heteroskedasticity robust standard errors as
  computed by the function \texttt{vcovHC()} from the package
  \texttt{sandwich} produce valid standard errors unter
  heteroskedasticity.
\item
  Serial correlation: if the population regression error is correlated
  across observations, we speak of serial correlation. This happens
  often in applications were repeated observations are used like in
  panel data studies. As for heteroskedasticity, the \texttt{vcovHC()}
  package can be used to obtain valid standard errors when there is
  serial correlation.
\end{enumerate}

Inconsistently computed standard errors will produce invalid hypothesis
tests and wrong confidence intervals. E.g. when testing the null
hypothesis that some model coefficient is zero, we cannot trust the
outcome anymore because the test may fail to have the significance level
of \(5\%\) due to a wrongly computed standard error.

Key Concept 9.7 summarizes all discussed threats to internal validity.

Key Concept 9.7

Threats to Internal Validity of a Regression Study

The five primary threats to internal validity of a multiple regression
study are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Omitted variables
\item
  Misspecefication of functional form
\item
  Errors in variables (due to measurement error in regressors)
\item
  Sample selection
\item
  Simultaneous causality
\end{enumerate}

All these threats lead to failure of the first least squares assumption
\[E(u_i\vert X_{1i},\dots ,X_{ki}) \neq 0\] so that the OLS estimator is
biased \emph{and} inconsistent.

Furthermore, if one does not adjust for heteroskedasticity
\emph{and}/\emph{or} serial correlation, if present, incorrect standard
errors may be a threat to internal validity of the study.

\section{Internal and External Validity When the Regression is Used for
Forecasting}\label{internal-and-external-validity-when-the-regression-is-used-for-forecasting}

Recall the regression of test scores on the student-teacher ratio
(\(STR\)) performed in chapter 4:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{linear_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\NormalTok{linear_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28
\end{verbatim}

The estimated regression function was

\[ \widehat{TestScore} = 698.9 - 2.28 \times STR.\]

The book gives the example of a parent moving to a metropolitan area who
plans to choose where to live based on quality of local schools. A
school disctrics average test score is an adequate mesaure for quality.
However, the parent has information on the student-teacher ratio only
such that test scores need to be predicted. Although we have established
that there is omitted variable bias in this model due to omisssion of
variables like student learning opportunities outside school, the share
of english learners and so on, \texttt{linear\_model} may in fact be
useful for the parent:

The parent does not care if the coefficien on \(STR\) has causal
interpretation, she wants \(STR\) to explain as much variation in test
scores as possible. Therefore, despite the fact that
\texttt{linear\_model} cannot be used to estimate the causal effect of a
change in \(STR\) on test scores, it can be considered a \emph{reliable
predictor} of test scores in general.

Thus, the threats to internal validity as summarized in Key Concept 9.7
are negligible for the parent. This is, as instanced in the book,
different for a superindent who has been tasked to take measures that
increase test scores: she requires a more reliable model that does not
suffer from the threats listed in Key Concept 9.7.

\section{Example: Test Scores and Class
Size}\label{example-test-scores-and-class-size}

This section discusses internal and external validity of results gained
from analyzing the California test score data using regression models.

\subsubsection*{External Validity of the
Study}\label{external-validity-of-the-study}
\addcontentsline{toc}{subsubsection}{External Validity of the Study}

External validity of the California analysis means that its results can
be generalized. If this is possible depends on the population and the
setting. Following the book we conduct the same analysis using data for
fourth graders in \(220\) public school districts in Massechusetts in
1998. Just as \texttt{CASchools}, the data set \texttt{MASchools} is
part of the \texttt{AER} package. Use the help function
(\texttt{?MASchools}) to get info on the definitions of all variables
contained.

We start by loading the data set and proceed by computing some summary
statistics.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"MASchools"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(MASchools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    district         municipality           expreg       expspecial   
##  Length:220         Length:220         Min.   :2905   Min.   : 3832  
##  Class :character   Class :character   1st Qu.:4065   1st Qu.: 7442  
##  Mode  :character   Mode  :character   Median :4488   Median : 8354  
##                                        Mean   :4605   Mean   : 8901  
##                                        3rd Qu.:4972   3rd Qu.: 9722  
##                                        Max.   :8759   Max.   :53569  
##                                                                      
##      expbil           expocc          exptot        scratio      
##  Min.   :     0   Min.   :    0   Min.   :3465   Min.   : 2.300  
##  1st Qu.:     0   1st Qu.:    0   1st Qu.:4730   1st Qu.: 6.100  
##  Median :     0   Median :    0   Median :5155   Median : 7.800  
##  Mean   :  3037   Mean   : 1104   Mean   :5370   Mean   : 8.107  
##  3rd Qu.:     0   3rd Qu.:    0   3rd Qu.:5789   3rd Qu.: 9.800  
##  Max.   :295140   Max.   :15088   Max.   :9868   Max.   :18.400  
##                                                  NA's   :9       
##     special          lunch          stratio          income      
##  Min.   : 8.10   Min.   : 0.40   Min.   :11.40   Min.   : 9.686  
##  1st Qu.:13.38   1st Qu.: 5.30   1st Qu.:15.80   1st Qu.:15.223  
##  Median :15.45   Median :10.55   Median :17.10   Median :17.128  
##  Mean   :15.97   Mean   :15.32   Mean   :17.34   Mean   :18.747  
##  3rd Qu.:17.93   3rd Qu.:20.02   3rd Qu.:19.02   3rd Qu.:20.376  
##  Max.   :34.30   Max.   :76.20   Max.   :27.00   Max.   :46.855  
##                                                                  
##      score4          score8          salary         english       
##  Min.   :658.0   Min.   :641.0   Min.   :24.96   Min.   : 0.0000  
##  1st Qu.:701.0   1st Qu.:685.0   1st Qu.:33.80   1st Qu.: 0.0000  
##  Median :711.0   Median :698.0   Median :35.88   Median : 0.0000  
##  Mean   :709.8   Mean   :698.4   Mean   :35.99   Mean   : 1.1177  
##  3rd Qu.:720.0   3rd Qu.:712.0   3rd Qu.:37.96   3rd Qu.: 0.8859  
##  Max.   :740.0   Max.   :747.0   Max.   :44.49   Max.   :24.4939  
##                  NA's   :40      NA's   :25
\end{verbatim}

It is fairly easy to replicate key components of table 9.1 of the book
using \texttt{R}. To be consistent with variable names used in the
\texttt{CASchools} data set, we do some formatting beforehand.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Customized variables in MASchools}
\NormalTok{MASchools}\OperatorTok{$}\NormalTok{score <-}\StringTok{ }\NormalTok{MASchools}\OperatorTok{$}\NormalTok{score4 }
\NormalTok{MASchools}\OperatorTok{$}\NormalTok{STR <-}\StringTok{ }\NormalTok{MASchools}\OperatorTok{$}\NormalTok{stratio}

\CommentTok{# Reproduce Table 9.1 of the book}
\NormalTok{vars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"score"}\NormalTok{, }\StringTok{"STR"}\NormalTok{, }\StringTok{"english"}\NormalTok{, }\StringTok{"lunch"}\NormalTok{, }\StringTok{"income"}\NormalTok{)}

\KeywordTok{cbind}\NormalTok{(}
  \DataTypeTok{CA_mean =} \KeywordTok{sapply}\NormalTok{(CASchools[ ,vars], mean),}
  \DataTypeTok{CA_sd   =} \KeywordTok{sapply}\NormalTok{(CASchools[ ,vars], sd),}
  \DataTypeTok{MA_mean =} \KeywordTok{sapply}\NormalTok{(MASchools[ ,vars], mean),}
  \DataTypeTok{MA_sd   =} \KeywordTok{sapply}\NormalTok{(MASchools[ , vars], sd)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           CA_mean     CA_sd    MA_mean     MA_sd
## score   654.15655 19.053347 709.827273 15.126474
## STR      19.64043  1.891812  17.344091  2.276666
## english  15.76816 18.285927   1.117676  2.900940
## lunch    44.70524 27.123381  15.315909 15.060068
## income   15.31659  7.225890  18.746764  5.807637
\end{verbatim}

The summary statistics reveal that the average test score is higher for
school districts in Massachusetts. Notice that the test is somewhat
different from that used in California (the Massachusetts test score
also includes results for the school subject `science') so a direct
comparison of test scores is not appropriate. We also see that, on
average, classes are smaller in Massachusetts than in California and
that the average district income, average percentage of english learners
as well as the average share of students receiving subsidized lunch
differ considerably from the averages computed for California. There are
also notable differences in the observed dispersion of variables.

Following the book we examine the relationship between district income
and test scores in Massachusetts as we have done before in chapter 8 for
the California data and reproduce figure 9.2 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate linear model}
\NormalTok{Linear_model_MA <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =}\NormalTok{ MASchools)}
\NormalTok{Linear_model_MA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ income, data = MASchools)
## 
## Coefficients:
## (Intercept)       income  
##     679.387        1.624
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate linear-log model}
\NormalTok{Linearlog_model_MA <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }\DataTypeTok{data =}\NormalTok{ MASchools) }
\NormalTok{Linearlog_model_MA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ log(income), data = MASchools)
## 
## Coefficients:
## (Intercept)  log(income)  
##      600.80        37.71
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate Cubic model}
\NormalTok{cubic_model_MA <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\KeywordTok{I}\NormalTok{(income) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{3}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ MASchools)}
\NormalTok{cubic_model_MA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ I(income) + I(income^2) + I(income^3), data = MASchools)
## 
## Coefficients:
## (Intercept)    I(income)  I(income^2)  I(income^3)  
##  600.398531    10.635382    -0.296887     0.002762
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot data}
\KeywordTok{plot}\NormalTok{(MASchools}\OperatorTok{$}\NormalTok{income, MASchools}\OperatorTok{$}\NormalTok{score,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"District income"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test score"}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{620}\NormalTok{,}\DecValTok{780}\NormalTok{))}

\CommentTok{# add estimated regression line for the linear model}
\KeywordTok{abline}\NormalTok{(Linear_model_MA, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add estimated regression function for Linear-log model}
\NormalTok{order_id  <-}\StringTok{ }\KeywordTok{order}\NormalTok{(MASchools}\OperatorTok{$}\NormalTok{income)}

\KeywordTok{lines}\NormalTok{(MASchools}\OperatorTok{$}\NormalTok{income[order_id],}
      \KeywordTok{fitted}\NormalTok{(Linearlog_model_MA)[order_id], }
      \DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add estimated cubic regression function}
\KeywordTok{lines}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ MASchools}\OperatorTok{$}\NormalTok{income[order_id], }
      \DataTypeTok{y =} \KeywordTok{fitted}\NormalTok{(cubic_model_MA)[order_id],}
      \DataTypeTok{col =} \StringTok{"orange"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{) }

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Linear"}\NormalTok{,}\StringTok{"Linear-Log"}\NormalTok{,}\StringTok{"Cubic"}\NormalTok{),}
       \DataTypeTok{lty =} \DecValTok{1}\NormalTok{,}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}\StringTok{"darkgreen"}\NormalTok{,}\StringTok{"orange"}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-211-1} \end{center}

The plot indicates that the cubic specification fits the data best.
Interstingly, this is different from the \texttt{CASchools} data where
the pattern of nonlinearity is better described by the linear-log
specification.

We continue by estimating the most of the model specifications used for
analysis of the \texttt{CASchools} data set in chapter 8 and use
\texttt{stargazer()} to generate a tabular representation of the
regression results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Add HiEL to 'MASchools'}
\NormalTok{MASchools}\OperatorTok{$}\NormalTok{HiEL <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(MASchools}\OperatorTok{$}\NormalTok{english }\OperatorTok{>}\StringTok{ }\KeywordTok{median}\NormalTok{(MASchools}\OperatorTok{$}\NormalTok{english))}

\CommentTok{# estimate model specifications from table 9.2 of the book}
\NormalTok{TestScore_MA_mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }\DataTypeTok{data =}\NormalTok{ MASchools)}

\NormalTok{TestScore_MA_mod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }\DataTypeTok{data =}\NormalTok{ MASchools)}

\NormalTok{TestScore_MA_mod3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\NormalTok{income }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{3}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ MASchools)}

\NormalTok{TestScore_MA_mod4 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(STR}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(STR}\OperatorTok{^}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{english }\OperatorTok{+}\StringTok{ }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }
\StringTok{    }\NormalTok{income }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{3}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ MASchools)}

\NormalTok{TestScore_MA_mod5 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL}\OperatorTok{:}\NormalTok{STR }\OperatorTok{+}\StringTok{ }
\StringTok{    }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =}\NormalTok{ MASchools)}

\NormalTok{TestScore_MA_mod6 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(income}\OperatorTok{^}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL }\OperatorTok{+}\StringTok{ }\NormalTok{HiEL}\OperatorTok{:}\NormalTok{STR }\OperatorTok{+}\StringTok{ }
\StringTok{    }\NormalTok{lunch }\OperatorTok{+}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =}\NormalTok{ MASchools)}

\CommentTok{# robust estimation of variance-covariance matrices}
\KeywordTok{library}\NormalTok{(sandwich)}

\NormalTok{rob_se <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod1, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))), }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod2, }
    \DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))), }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod3, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))), }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod4, }
    \DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))), }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod5, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))), }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod6, }
    \DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))))}

\CommentTok{# generate table with stargazer()}

\KeywordTok{library}\NormalTok{(stargazer)}

\KeywordTok{stargazer}\NormalTok{(Linear_model_MA, TestScore_MA_mod2, TestScore_MA_mod3, TestScore_MA_mod4, }
\NormalTok{    TestScore_MA_mod5, TestScore_MA_mod6, }\DataTypeTok{type =} \StringTok{"latex"}\NormalTok{, }\DataTypeTok{se =}\NormalTok{ rob_se, }\DataTypeTok{object.names =} \OtherTok{TRUE}\NormalTok{, }
    \DataTypeTok{model.numbers =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"(I)"}\NormalTok{, }\StringTok{"(II)"}\NormalTok{, }\StringTok{"(III)"}\NormalTok{, }\StringTok{"(IV)"}\NormalTok{, }
        \StringTok{"(V)"}\NormalTok{, }\StringTok{"(VI)"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University.
E-mail: hlavac at fas.harvard.edu \% Date and time: Thu, Apr 26, 2018 -
10:54:09 \% Requires LaTeX packages: rotating

\begin{sidewaystable}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{6}{c}{\textit{Dependent variable:}} \\ 
\cline{2-7} 
\\[-1.8ex] & \multicolumn{6}{c}{score} \\ 
 & (I) & (II) & (III) & (IV) & (V) & (VI) \\ 
\hline \\[-1.8ex] 
 STR & $-$1.718$^{***}$ & $-$0.689$^{**}$ & $-$0.641$^{**}$ & 12.426 & $-$1.018$^{***}$ & $-$0.672$^{**}$ \\ 
  & (0.499) & (0.270) & (0.268) & (14.010) & (0.370) & (0.271) \\ 
  & & & & & & \\ 
 I(STR$\hat{\mkern6mu}$2) &  &  &  & $-$0.680 &  &  \\ 
  &  &  &  & (0.737) &  &  \\ 
  & & & & & & \\ 
 I(STR$\hat{\mkern6mu}$3) &  &  &  & 0.011 &  &  \\ 
  &  &  &  & (0.013) &  &  \\ 
  & & & & & & \\ 
 english &  & $-$0.411 & $-$0.437 & $-$0.434 &  &  \\ 
  &  & (0.306) & (0.303) & (0.300) &  &  \\ 
  & & & & & & \\ 
 HiEL &  &  &  &  & $-$12.561 &  \\ 
  &  &  &  &  & (9.793) &  \\ 
  & & & & & & \\ 
 lunch &  & $-$0.521$^{***}$ & $-$0.582$^{***}$ & $-$0.587$^{***}$ & $-$0.709$^{***}$ & $-$0.653$^{***}$ \\ 
  &  & (0.078) & (0.097) & (0.104) & (0.091) & (0.073) \\ 
  & & & & & & \\ 
 log(income) &  & 16.529$^{***}$ &  &  &  &  \\ 
  &  & (3.146) &  &  &  &  \\ 
  & & & & & & \\ 
 income &  &  & $-$3.067 & $-$3.382 & $-$3.867 & $-$3.218 \\ 
  &  &  & (2.353) & (2.491) & (2.488) & (2.306) \\ 
  & & & & & & \\ 
 I(income$\hat{\mkern6mu}$2) &  &  & 0.164$^{*}$ & 0.174$^{*}$ & 0.184$^{**}$ & 0.165$^{*}$ \\ 
  &  &  & (0.085) & (0.089) & (0.090) & (0.085) \\ 
  & & & & & & \\ 
 I(income$\hat{\mkern6mu}$3) &  &  & $-$0.002$^{**}$ & $-$0.002$^{**}$ & $-$0.002$^{**}$ & $-$0.002$^{**}$ \\ 
  &  &  & (0.001) & (0.001) & (0.001) & (0.001) \\ 
  & & & & & & \\ 
 STR:HiEL &  &  &  &  & 0.799 &  \\ 
  &  &  &  &  & (0.555) &  \\ 
  & & & & & & \\ 
 Constant & 739.621$^{***}$ & 682.432$^{***}$ & 744.025$^{***}$ & 665.496$^{***}$ & 759.914$^{***}$ & 747.364$^{***}$ \\ 
  & (8.607) & (11.497) & (21.318) & (81.332) & (23.233) & (20.278) \\ 
  & & & & & & \\ 
\hline \\[-1.8ex] 
Observations & 220 & 220 & 220 & 220 & 220 & 220 \\ 
R$^{2}$ & 0.067 & 0.676 & 0.685 & 0.687 & 0.686 & 0.681 \\ 
Adjusted R$^{2}$ & 0.063 & 0.670 & 0.676 & 0.675 & 0.675 & 0.674 \\ 
Residual Std. Error & 14.646 (df = 218) & 8.686 (df = 215) & 8.607 (df = 213) & 8.626 (df = 211) & 8.621 (df = 212) & 8.637 (df = 214) \\ 
F Statistic & 15.616$^{***}$ (df = 1; 218) & 112.284$^{***}$ (df = 4; 215) & 77.232$^{***}$ (df = 6; 213) & 57.803$^{***}$ (df = 8; 211) & 66.023$^{***}$ (df = 7; 212) & 91.560$^{***}$ (df = 5; 214) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{6}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{sidewaystable}

We continue by reproducing the \(F\)-statistics and \(p\)-values testing
exclusion of groups of variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# F-test model (3)}
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_MA_mod3, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"I(income^2)=0"}\NormalTok{,}\StringTok{"I(income^3)=0"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod3, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + english + lunch + income + I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(>F)    
## 1    215                        
## 2    213  2 7.7448 0.0005664 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# F-tests model (4)}
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_MA_mod4, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"STR=0"}\NormalTok{,}\StringTok{"I(STR^2)=0"}\NormalTok{, }\StringTok{"I(STR^3)=0"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod4, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## STR = 0
## I(STR^2) = 0
## I(STR^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income + 
##     I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(>F)  
## 1    214                    
## 2    211  3 2.8565 0.03809 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_MA_mod4, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"I(STR^2)=0"}\NormalTok{, }\StringTok{"I(STR^3)=0"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod4, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## I(STR^2) = 0
## I(STR^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income + 
##     I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(>F)
## 1    213                 
## 2    211  2 0.4463 0.6406
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_MA_mod4, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"I(income^2)=0"}\NormalTok{,}\StringTok{"I(income^3)=0"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod4, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income + 
##     I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(>F)    
## 1    213                        
## 2    211  2 7.7487 0.0005657 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# F-tests model (5)}
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_MA_mod5, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"STR=0"}\NormalTok{,}\StringTok{"STR:HiEL=0"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod5, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## STR = 0
## STR:HiEL = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + HiEL + HiEL:STR + lunch + income + I(income^2) + 
##     I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(>F)  
## 1    214                   
## 2    212  2 4.0062 0.0196 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_MA_mod5, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"I(income^2)=0"}\NormalTok{,}\StringTok{"I(income^3)=0"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod5, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + HiEL + HiEL:STR + lunch + income + I(income^2) + 
##     I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F   Pr(>F)   
## 1    214                      
## 2    212  2 5.8468 0.003375 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_MA_mod5, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"HiEL=0"}\NormalTok{,}\StringTok{"STR:HiEL=0"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod5, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## HiEL = 0
## STR:HiEL = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + HiEL + HiEL:STR + lunch + income + I(income^2) + 
##     I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(>F)
## 1    214                 
## 2    212  2 1.5835 0.2077
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# F-test Model (6)}
\KeywordTok{linearHypothesis}\NormalTok{(TestScore_MA_mod6, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"I(income^2)=0"}\NormalTok{,}\StringTok{"I(income^3)=0"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(TestScore_MA_mod6, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + lunch + income + I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F   Pr(>F)   
## 1    216                      
## 2    214  2 6.5479 0.001737 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that, in terms of \(\overline{R^2}\), specification (3) which
uses a cubic to model the relationship between district income and test
scores indeed does perform better the the linear-log specification (2).
Using different \(F\)-tests on models (4) and (5), we cannot reject the
hypothesis that there is no nonlinear relationship between student
teacher ratio and test score and also that the share of english learners
has an influence on the relationship of interest. Furthermore,
regression (6) shows that percentage of english learners can be omitted
as a regressor. Because changes in specification made in regressions (4)
to (6) do not lead to substantially different results than those of
regression (3), we choose model (3) as the most suitable specification.

In compariosn to the California data, we observe the following results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Controlling for the students background characteristics in model
  specification (2) reduces the coefficient of interest (student-teacher
  ratio) by roughly \(60\%\). The estimated coefficients are close.
\item
  The coefficient on student-teacher ratio is always significantly
  different from zero at the level of \(1\%\) for both data sets. This
  holds for all model specifications under consideration in both
  studies.
\item
  The share of english learners in the school district has no important
  influence on the impact on test score of a change in the
  student-teacher ratio in both studies.
\end{enumerate}

The biggest difference is that, in contrast to the California results,
we do not find evience of a nonlinear relationship between test scores
and the student teacher ratio for the Massachusetts data since the
correspoding \(F\)-tests on model (4) do not reject.

As pointed out by the authors, the test scores for California and
Massachusetts have different units because the tests are different. Thus
the estimated coefficients on the student-teacher ratio in both
regressions cannot be compared before standardizing test scores to the
same units as
\[\frac{Testscore - \overline{TestScore}}{\sigma_{TestScore}}\] for all
observations in both datasets and reruning the regressions of interest
using the standardized data. One can show that the coefficient on
student-teacher ratio in the regression using standardized test scores
is the coefficient of the original regression devided by the standard
deviation of test scores.

For model (3) of the Massachusetts data, the estimated coeffiecient on
the student-teacher ratio is \(-0.64\). A reduction of the
student-teacher ratio by two students is predicted to increase test
scores by \(-2 \cdot (-0.64) = 1.28\) points. Thus we can compute the
effect of a reduction of student-teacher ratio by two students on the
standardized test scores as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TestScore_MA_mod3}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{] }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(MASchools}\OperatorTok{$}\NormalTok{score) }\OperatorTok{*}\StringTok{ }\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        STR 
## 0.08474001
\end{verbatim}

Thus for Massachussetts, the predicted increase of test scores due to a
reduction of the student-teacher ratio by two students is \(0.085\)
standard deviation of the distribution of the observed distribution of
test scores.

Beholding the linear specification (2) for California, the estimated
coefficient on the student-teacher ratio is \(-0.73\) so the predicted
increase of test scores induced by a reduction of the student-teacher
ratio by two students is \(-0.73 * (-2) = 1.46\). We use \texttt{R} to
compute the predicted change in standard deviation units:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TestScore_mod2}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{] }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score) }\OperatorTok{*}\StringTok{ }\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        STR 
## 0.07708103
\end{verbatim}

This shows that the the predicted increase of test scores due to a
reduction of the student-teacher ratio by two students is \(0.077\)
standard deviation of the distribution of the observed distribution of
test scores for the California data.

In terms of standardized test scores, the predicted change is
essentially the same for school districts in California and
Massachusetts.

Altogether, the results are in favour of the presumption that inference
made using data on Californian elementary school districts is externally
valid.

\subsubsection*{Internal Validity of the
Study}\label{internal-validity-of-the-study}
\addcontentsline{toc}{subsubsection}{Internal Validity of the Study}

External validity of the study \textbf{does not} ensure their internal
validity. Though the model specification choosen improves upon a simple
linear regression model, internal validity may still be violated due to
some of the threats listed in Key Concept 9.7 threats. These threats
are:

\begin{itemize}
\item
  Omitted variable bias
\item
  Misspecification of functional form
\item
  Errors in variables
\item
  Sample selection issues
\item
  Simultaneous causality
\item
  Heteroskedasticity
\item
  Correlation of errors across observations
\end{itemize}

Consult the book for a in depth discussion of these threats in view of
the test score studies.

\subsubsection*{Summary}\label{summary-1}
\addcontentsline{toc}{subsubsection}{Summary}

We have found that there \emph{is} a statistically significant effect of
the student-teacher ratio on test scores though it is quite small.
However, it remains unclear if we have indeed estimated the causal
effect of inerest since --- although our approach includes control
variables, takes into account nonlinearities in the population
regression function and statistical inference is made using robust
standard errors --- the results might still be biased for example if
there are omitted factors which we have not considered. Thus
\emph{internal validity} of the study remains questionable. As we have
concluded from comparison with the analysis of the Massachusetts
dataset, this result is \emph{externally valid}.

The following chapters adress techniques that can be remedies to all the
threats to internal validity named in Key Concept 9.7 if multiple
regression alone is insufficient. This includes regression using panel
data and approaches that employ instrumental variables.

\chapter{Regression with Panel Data}\label{regression-with-panel-data}

Regression using panel data may mititgate omitted variable bias when
there is no information on variables available that correlate with both
the regressors of interest and the independent variable if these
variables are constant in the time dimension or across entities.
Therefore, provided that panel data is availabe, panel regression
methods may improve upon multiple regression models which, as discussed
in chapter 9, produce results that are not internally valid in such a
setting.

This chapter covers the following topics:

\begin{itemize}
\tightlist
\item
  Notation for panel data
\item
  Fixed effects regression using time and/or entity fixed effects
\item
  Computation of standard errors in fixed effects regression models
\end{itemize}

Following the book, for applications we make use of the data set
\texttt{Fatalities} from the \texttt{AER} package which is a panel data
set reporting annually state level observations on US traffic fatalities
for 1982 through 1988. The applications are concered with the question
if there are effects of alcohol taxes and drunk driving laws on road
fatalities and, if present, \emph{how strong} these effects are.

For this purpose we will introduce a convenient \texttt{R} function that
enables to estimate linear panel regression models, the function
\texttt{plm()} which comes with the package \texttt{plm}. Usage of
\texttt{plm()} is very similar as for the \texttt{lm()} function which
we have used throughout the previous five chapters for estimation of
simple and multiple regression models.

\section{Panel Data}\label{panel-data}

Key Concept 10.1

Notation for Panel Data

In contrast to cross-section data where we have observations on \(n\)
subjects (entities), panel data has observations on \(n\) entities at
\(T\geq2\) time periods. This is denoted

\[(X_{it},Y_{it}), \ i=1,\dots,n \ \ \ \text{and} \ \ \ t=1,\dots,T. \]
The index \(i\) refers to the entity beeing observed while \(t\) refers
to the time period.

Sometimes panel data is also called longitudinal data as it adds a
temporal dimension the what we call cross-sectional data. Let us have a
glimpse at the data set \texttt{Fatalities} by checking its structure
and listing the first few observations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package and data}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(Fatalities)}

\CommentTok{# obtain dimension and inspect structure}
\KeywordTok{is.data.frame}\NormalTok{(Fatalities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(Fatalities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 336  34
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(Fatalities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    336 obs. of  34 variables:
##  $ state       : Factor w/ 48 levels "al","az","ar",..: 1 1 1 1 1 1 1 2 2 2 ...
##  $ year        : Factor w/ 7 levels "1982","1983",..: 1 2 3 4 5 6 7 1 2 3 ...
##  $ spirits     : num  1.37 1.36 1.32 1.28 1.23 ...
##  $ unemp       : num  14.4 13.7 11.1 8.9 9.8 ...
##  $ income      : num  10544 10733 11109 11333 11662 ...
##  $ emppop      : num  50.7 52.1 54.2 55.3 56.5 ...
##  $ beertax     : num  1.54 1.79 1.71 1.65 1.61 ...
##  $ baptist     : num  30.4 30.3 30.3 30.3 30.3 ...
##  $ mormon      : num  0.328 0.343 0.359 0.376 0.393 ...
##  $ drinkage    : num  19 19 19 19.7 21 ...
##  $ dry         : num  25 23 24 23.6 23.5 ...
##  $ youngdrivers: num  0.212 0.211 0.211 0.211 0.213 ...
##  $ miles       : num  7234 7836 8263 8727 8953 ...
##  $ breath      : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 1 1 1 ...
##  $ jail        : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 2 2 2 ...
##  $ service     : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 2 2 2 ...
##  $ fatal       : int  839 930 932 882 1081 1110 1023 724 675 869 ...
##  $ nfatal      : int  146 154 165 146 172 181 139 131 112 149 ...
##  $ sfatal      : int  99 98 94 98 119 114 89 76 60 81 ...
##  $ fatal1517   : int  53 71 49 66 82 94 66 40 40 51 ...
##  $ nfatal1517  : int  9 8 7 9 10 11 8 7 7 8 ...
##  $ fatal1820   : int  99 108 103 100 120 127 105 81 83 118 ...
##  $ nfatal1820  : int  34 26 25 23 23 31 24 16 19 34 ...
##  $ fatal2124   : int  120 124 118 114 119 138 123 96 80 123 ...
##  $ nfatal2124  : int  32 35 34 45 29 30 25 36 17 33 ...
##  $ afatal      : num  309 342 305 277 361 ...
##  $ pop         : num  3942002 3960008 3988992 4021008 4049994 ...
##  $ pop1517     : num  209000 202000 197000 195000 204000 ...
##  $ pop1820     : num  221553 219125 216724 214349 212000 ...
##  $ pop2124     : num  290000 290000 288000 284000 263000 ...
##  $ milestot    : num  28516 31032 32961 35091 36259 ...
##  $ unempus     : num  9.7 9.6 7.5 7.2 7 ...
##  $ emppopus    : num  57.8 57.9 59.5 60.1 60.7 ...
##  $ gsp         : num  -0.0221 0.0466 0.0628 0.0275 0.0321 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list first few observations}
\KeywordTok{head}\NormalTok{(Fatalities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   state year spirits unemp   income   emppop  beertax baptist  mormon
## 1    al 1982    1.37  14.4 10544.15 50.69204 1.539379 30.3557 0.32829
## 2    al 1983    1.36  13.7 10732.80 52.14703 1.788991 30.3336 0.34341
## 3    al 1984    1.32  11.1 11108.79 54.16809 1.714286 30.3115 0.35924
## 4    al 1985    1.28   8.9 11332.63 55.27114 1.652542 30.2895 0.37579
## 5    al 1986    1.23   9.8 11661.51 56.51450 1.609907 30.2674 0.39311
## 6    al 1987    1.18   7.8 11944.00 57.50988 1.560000 30.2453 0.41123
##   drinkage     dry youngdrivers    miles breath jail service fatal nfatal
## 1    19.00 25.0063     0.211572 7233.887     no   no      no   839    146
## 2    19.00 22.9942     0.210768 7836.348     no   no      no   930    154
## 3    19.00 24.0426     0.211484 8262.990     no   no      no   932    165
## 4    19.67 23.6339     0.211140 8726.917     no   no      no   882    146
## 5    21.00 23.4647     0.213400 8952.854     no   no      no  1081    172
## 6    21.00 23.7924     0.215527 9166.302     no   no      no  1110    181
##   sfatal fatal1517 nfatal1517 fatal1820 nfatal1820 fatal2124 nfatal2124
## 1     99        53          9        99         34       120         32
## 2     98        71          8       108         26       124         35
## 3     94        49          7       103         25       118         34
## 4     98        66          9       100         23       114         45
## 5    119        82         10       120         23       119         29
## 6    114        94         11       127         31       138         30
##    afatal     pop  pop1517  pop1820  pop2124 milestot unempus emppopus
## 1 309.438 3942002 208999.6 221553.4 290000.1    28516     9.7     57.8
## 2 341.834 3960008 202000.1 219125.5 290000.2    31032     9.6     57.9
## 3 304.872 3988992 197000.0 216724.1 288000.2    32961     7.5     59.5
## 4 276.742 4021008 194999.7 214349.0 284000.3    35091     7.2     60.1
## 5 360.716 4049994 203999.9 212000.0 263000.3    36259     7.0     60.7
## 6 368.421 4082999 204999.8 208998.5 258999.8    37426     6.2     61.5
##           gsp
## 1 -0.02212476
## 2  0.04655825
## 3  0.06279784
## 4  0.02748997
## 5  0.03214295
## 6  0.04897637
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# summarize variables 'state' and 'year'}
\KeywordTok{summary}\NormalTok{(Fatalities[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      state       year   
##  al     :  7   1982:48  
##  az     :  7   1983:48  
##  ar     :  7   1984:48  
##  ca     :  7   1985:48  
##  co     :  7   1986:48  
##  ct     :  7   1987:48  
##  (Other):294   1988:48
\end{verbatim}

We find that the data set consists of 336 observations on 34 variables.
Notice that the variable \texttt{state} is a factor variable with 48
levels (one for each of the 48 contigous US states). The varaible
\texttt{year} is also a factor variable that has 7 levels identifying
the time period when the observation was made which gives us
\(7\times48 = 336\) observations in total. Since all variables are
observed for all entities and time periods we say the the panel is
\textbf{balanced}. If there were missing data for at least one entities
in at least one time period we would call the panel \textbf{unbalanced}.

\subsubsection*{Example: Traffic Deaths and Alcohol
Taxes}\label{example-traffic-deaths-and-alcohol-taxes}
\addcontentsline{toc}{subsubsection}{Example: Traffic Deaths and Alcohol
Taxes}

Coming to the question of how the alcohol taxes and traffic fatalities
are related, we start by reproducing figure 10.1 of the book. First we
estimate simple regressions using data for years 1982 and 1988 that
model the relationship between beer tax (adjusted for 1988 dollars) and
the traffic fatility rate. Beforehand we define the latter as the number
of fatalitites per 10000 inhabitants and choose subsets of the
observations made for years 1982 and 1988. Afterwards, we plot both
subsets of data and the corresponding estimated regression functions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define fatality rate}
\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{fatal_rate <-}\StringTok{ }\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{fatal }\OperatorTok{/}\StringTok{ }\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{pop }\OperatorTok{*}\StringTok{ }\DecValTok{10000}

\CommentTok{# subset data}
\NormalTok{Fatalities1982 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(Fatalities, year }\OperatorTok{==}\StringTok{ "1982"}\NormalTok{)}
\NormalTok{Fatalities1988 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(Fatalities, year }\OperatorTok{==}\StringTok{ "1988"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate simple regression models using 1982 and 1988 data}
\KeywordTok{library}\NormalTok{(lmtest)}
\KeywordTok{library}\NormalTok{(sandwich)}

\NormalTok{fatal1982_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }\DataTypeTok{data =}\NormalTok{ Fatalities1982)}
\NormalTok{fatal1988_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }\DataTypeTok{data =}\NormalTok{ Fatalities1988)}

\KeywordTok{coeftest}\NormalTok{(fatal1982_mod, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatal1982_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  2.01038    0.14957 13.4408   <2e-16 ***
## beertax      0.14846    0.13261  1.1196   0.2687    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coeftest}\NormalTok{(fatal1988_mod, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatal1988_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)  1.85907    0.11461 16.2205 < 2.2e-16 ***
## beertax      0.43875    0.12786  3.4314  0.001279 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimated regression functions are:

\begin{align*}
  \widehat{FatalityRate} =& \, \underset{(0.15)}{2.01} + \underset{(0.13)}{0.15} \times BeerTax \quad (1982 \text{ data}), \\
  \widehat{FatalityRate} =& \, \underset{(0.11)}{1.86} + \underset{(0.13)}{0.44} \times BeerTax \quad (1988 \text{ data})
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot observations of interest and add estimated regression line for 1982 data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Fatalities1982}\OperatorTok{$}\NormalTok{beertax, }
     \DataTypeTok{y =}\NormalTok{ Fatalities1982}\OperatorTok{$}\NormalTok{fatal_rate, }
     \DataTypeTok{xlab =} \StringTok{"Beer tax (in 1988 dollars)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Fatality rate (fatalities per 10000)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Traffic Fatality Rates and Beer Taxes in 1982"}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{4.5}\NormalTok{),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}

\KeywordTok{abline}\NormalTok{(fatal1982_mod, }\DataTypeTok{lwd=}\FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-220-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot observations of interest and add estimated regression line for 1988 data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Fatalities1988}\OperatorTok{$}\NormalTok{beertax, }
     \DataTypeTok{y =}\NormalTok{ Fatalities1988}\OperatorTok{$}\NormalTok{fatal_rate, }
     \DataTypeTok{xlab =} \StringTok{"Beer tax (in 1988 dollars)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Fatality rate (fatalities per 10000)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Traffic Fatality Rates and Beer Taxes in 1988"}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{4.5}\NormalTok{),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}

\KeywordTok{abline}\NormalTok{(fatal1988_mod, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-220-2.pdf}

In both plots, each point represents observations of beer tax and
fatality rate for a given state in the respective year. The regression
results indicate a postive relationship between beer tax and fatality
rate for both years wherby the estimated coefficient on beer tax for the
1988 data is almost three times as large as for the 1988 data set. This
is contrary to our expectations: alcohol taxes are supposed to
\emph{lower} the rate of traffic fatalities. As known from chapter 6,
this is possibly due to omitted variable bias since both models do not
include any covariates (e.g.~economic conditions) which could be
corrected for using a multiple regression approach. However, both models
cannot account for omitted \emph{unobservable} factors that differ from
state to state but can be assumed constant over the observation span
like the populations attitude towas drunk driving. As shown in the next
section, panel data allow us to hold such factors constant.

\hypertarget{PDWTTP}{\section{\texorpdfstring{Panel Data with Two Time
Periods: ``Before and Afer''
Comparisons}{Panel Data with Two Time Periods: Before and Afer Comparisons}}\label{PDWTTP}}

Suppose there are only \(T=2\) time periods \(t=1982,1988\). This allows
us to analyze differences in changes of the the fatality rate from year
1982 to 1988. We start by considering the population regression function
\[FatalityRate_{it} = \beta_0 + \beta_1 BeerTax_{it} + \beta_2 Z_{i} + u_{it}\]
where the \(Z_i\) are state specific characteristics that differ between
states but are \emph{constant over time}. For \(t=1982,1988\) we have

\begin{align*}
  FatalityRate_{i1982} =&\, \beta_0 + \beta_1 BeerTax_{i1982} + \beta_2 Z_i + u_{i1982}, \\
  FatalityRate_{i1988} =&\, \beta_0 + \beta_1 BeerTax_{i1988} + \beta_2 Z_i + u_{i1988}.
\end{align*}

We can eliminate the \(Z_i\) by regressing the difference in the
fatality rate between 1988 and 1982 on the difference in beer tax
between those years:
\[FatalityRate_{i1988} - FatalityRate_{i1982} = \beta_1 (BeerTax_{i1988} - BeerTax_{i1982}) + u_{i1988} - u_{i1982}\]
Using this regression model we can obtain an estimate for \(\beta_1\)
without worrying about a possible bias due to omission of the \(Z_i\)
since these influences are eliminated from the model. Let us use
\texttt{R} to estimate a regression based on the differenced data and
plot the estimated regression function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# differences }
\NormalTok{diff_fatal_rate <-}\StringTok{ }\NormalTok{Fatalities1988}\OperatorTok{$}\NormalTok{fatal_rate }\OperatorTok{-}\StringTok{ }\NormalTok{Fatalities1982}\OperatorTok{$}\NormalTok{fatal_rate}
\NormalTok{diff_beertax <-}\StringTok{ }\NormalTok{Fatalities1988}\OperatorTok{$}\NormalTok{beertax }\OperatorTok{-}\StringTok{ }\NormalTok{Fatalities1982}\OperatorTok{$}\NormalTok{beertax}
\CommentTok{# estimate regression on differenced data}
\NormalTok{fatal_diff_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(diff_fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{diff_beertax)}

\KeywordTok{coeftest}\NormalTok{(fatal_diff_mod, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fatal_diff_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##               Estimate Std. Error t value Pr(>|t|)   
## (Intercept)  -0.072037   0.065355 -1.1022 0.276091   
## diff_beertax -1.040973   0.355006 -2.9323 0.005229 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We obtain the OLS estimated regression function

\[\widehat{FatalityRate_{i1988} - FatalityRate_{i1982}} = \underset{(0.065)}{-0.072} \underset{(0.36)}{-1.04} \times (BeerTax_{i1988}-BeerTax_{i1982}).\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot differenced data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ diff_beertax, }
     \DataTypeTok{y =}\NormalTok{ diff_fatal_rate, }
     \DataTypeTok{xlab =} \StringTok{"Change in beer tax (in 1988 dollars)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Change in fatality rate (fatalities per 10000)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Changes in Traffic Fatality Rates and Beer Taxes in 1982-1988"}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.6}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.5}\NormalTok{, }\DecValTok{1}\NormalTok{),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}

\CommentTok{# add regression line to plot}
\KeywordTok{abline}\NormalTok{(fatal_diff_mod, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-222-1} \end{center}

The intercept allows for a change in the mean fatality rate in the time
between 1982 and 1988 in the absence of a change in the beer tax. We
observe that the estimated coefficient on beer tax is now negative and
significantly different from zero at the level of \(5\%\). Its
interpretation is that raising the beer tax by \(\$1\) causes trafic
fatalities to decrease by \(1.04\) per \(10000\) people. This is rather
large as the average fatality rate is approximately \(2\) persons per
\(10000\) people.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute mean fatality rate over all states for all time periods}
\KeywordTok{mean}\NormalTok{(Fatalities}\OperatorTok{$}\NormalTok{fatal_rate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.040444
\end{verbatim}

Again, this outcome is likely to be a consequence of omitting factors
that influence the fatality rate, are correlated with the beer tax
\emph{and} change over time. The message is that we need to be more
careful and control for such factors before drawing conclusion about the
effect of a raise in beer taxes.

Further, note that the approach presented in this section discards
information for years \(1983\) to \(1987\). A method that allows to use
data for more than \(T=2\) time periods and allows us to add control
variables is the fixed effects regression approach.

\section{Fixed Effects Regression}\label{fixed-effects-regression}

Consider the panel regression model

\[Y_{it} = \beta_0 + \beta_1 X_{it} + \beta_2 Z_i +  u_{it}\]

where the \(Z_i\) are unobserved time-invariant heterogeneities across
entities \(i=1,\dots,n\). We are interested in estimating \(\beta_1\),
the effect on \(Y\) of a change in \(X\) holding constant \(Z\). Letting
\(\alpha_i = \beta_0 + \beta_2 Z_i\) we obtain

\begin{align}
Y_{it} = \alpha_i + \beta_1 X_{it} + u_{it} \label{eq:femodel}.
\end{align}

Having invidual specific intercepts \(\alpha_i\), \(i=1,\dots,n\) where
each of which can be understood as the fixed effect of entity \(i\),
\eqref{eq:femodel} is called the \emph{fixed effects regression model}.
The variation in the \(\alpha_i\), \(i=1,\dots,n\) comes from the
\(Z_i\). \eqref{eq:femodel} can be rewritten as a regression model
containing \(n-1\) dummy regressors and a constant:

\begin{align}
Y_{it} = \beta_i + \beta_1 X_{it} + \gamma_2 D2_i + \gamma_3 D3_i + \cdots + \gamma_n Dn_i + u_{it} \label{eq:drmodel}.
\end{align}

Model \eqref{eq:drmodel} has \(n\) different intercepts --- one for every
entity. Models \eqref{eq:femodel} and \eqref{eq:drmodel} are equivalnt
representations of the fixed effects regression model.

The model can be generalized to contain more that just one determinant
of \(Y\) that are correlated with \(X\) and change over time. Key
Concept 10.2 presents the generalized fixed effects regression model.

Key Concept 10.2

The Fixed Effects Regression Model

The fixed effects regression model is

\begin{align}
Y_{it} = \beta_1 X_{1,it} + \cdots + \beta_k X_{k,it} + \alpha_i + u_{it} \label{eq:gfemodel}
\end{align}

with \(i=1,\dots,n\) and \(t=1,\dots,T\). The \(\alpha_i\) are
entity-specific intercepts that capture heterogeneities across entities.
An equivalent representation of this model is given by

\begin{align}
Y_{it} = \beta_0 + \beta_1 X_{1,it} + \cdots + \beta_k X_{k,it} + \gamma_2 D2_i + \gamma_3 D3_i + \cdots + \gamma_n Dn_i  + u_{it} \label{eq:gdrmodel}
\end{align}

where the \(D2_i,D3_i,\dots,Dn_i\) are dummy variables.

\subsection*{Estimation and Inference}\label{estimation-and-inference}
\addcontentsline{toc}{subsection}{Estimation and Inference}

Software packages use a so-called ``entity-demeaned'' OLS algorithm that
is computationally more efficient than estimating regression models with
\(k+n\) regressors as need for models \eqref{eq:gfemodel} and
\eqref{eq:gdrmodel}.

Taking averages on both sides of \eqref{eq:femodel} we obtain

\begin{align*}
\frac{1}{n} \sum_{i=1}^n Y_{it} =& \, \beta_1 \frac{1}{n} \sum_{i=1}^n X_{it} + \frac{1}{n} \sum_{i=1}^n a_i + \frac{1}{n} \sum_{i=1}^n u_{it} \\
\overline{Y} =& \, \beta_1 \overline{X}_i + \alpha_i + \overline{u}_i 
\end{align*}

substraction from \eqref{eq:femodel} gives us

\begin{align}
Y_{it} - \overline{Y}_i =& \, \beta_1(X_{it}-\overline{X}_i) + (u_{it} - \overline{u}_i) \\
\overset{\sim}{Y}_{it} =& \, \beta_1 \overset{\sim}{X}_{it} + \overset{\sim}{u}_{it}. \label{eq:edols}
\end{align}

In this regression model, the OLS estimate of the parameter of interest
\(\beta_1\) is equal to the estimate obtained using \eqref{eq:drmodel} ---
without the need to estimate \(n-1\) dummies and an intercept.

Thus, we have two ways to estimate \(\beta_1\) in the fixed effects
regression:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  OLS regression of the dummy regression model \eqref{eq:drmodel}
\item
  OLS regression using the entity demeaned data \eqref{eq:edols}
\end{enumerate}

Provided the fixed effects regression assumptions stated in Key Concept
10.3 hold, the sampling distribution of the OLS estimator in the fixed
effects regression model has a normal distribution in large samples. The
variance can be estimated and we can compute standard errors,
\(t\)-statistics and confidence intervals for model coefficients. In the
next section, we will see how to estimate a fixed effects model using
\texttt{R} and how to obtain a model summary that makes use
heteroskedasticity robust standard errors. Thereby we leave aside
complicated formulas of the estimators. See chapter 10.5 and appendix
10.2 for a discussion of the theoretical aspects.

\subsection*{Application to Traffic
Deaths}\label{application-to-traffic-deaths}
\addcontentsline{toc}{subsection}{Application to Traffic Deaths}

Following Key Concept 10.2, the simple fixed effects model for
estimation of the relation between traffic fatality rates and the beer
taxes is

\begin{align}
\widehat{FatalityRate}_{it} = \beta_1 BeerTax_{it} + StateFixedEffects + u_{it}, \label{eq:fatsemod}
\end{align}

the regression of the traffic fatalaty rate on beer tax and 48 binary
regressor --- one for each state.

We can simply use the function \texttt{lm()} to obtain an estimate for
\(\beta_1\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fatal_fe_lm_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Fatalities)}
\NormalTok{fatal_fe_lm_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = fatal_rate ~ beertax + state - 1, data = Fatalities)
## 
## Coefficients:
## beertax  stateal  stateaz  statear  stateca  stateco  statect  statede  
## -0.6559   3.4776   2.9099   2.8227   1.9682   1.9933   1.6154   2.1700  
## statefl  statega  stateid  stateil  statein  stateia  stateks  stateky  
##  3.2095   4.0022   2.8086   1.5160   2.0161   1.9337   2.2544   2.2601  
## statela  stateme  statemd  statema  statemi  statemn  statems  statemo  
##  2.6305   2.3697   1.7712   1.3679   1.9931   1.5804   3.4486   2.1814  
## statemt  statene  statenv  statenh  statenj  statenm  stateny  statenc  
##  3.1172   1.9555   2.8769   2.2232   1.3719   3.9040   1.2910   3.1872  
## statend  stateoh  stateok  stateor  statepa  stateri  statesc  statesd  
##  1.8542   1.8032   2.9326   2.3096   1.7102   1.2126   4.0348   2.4739  
## statetn  statetx  stateut  statevt  stateva  statewa  statewv  statewi  
##  2.6020   2.5602   2.3137   2.5116   2.1874   1.8181   2.5809   1.7184  
## statewy  
##  3.2491
\end{verbatim}

As discussed in the previous section, it is also possible to estimate
\(\beta_1\) by

\[\overset{\sim}{FatalityRate} = \beta_1 \overset{\sim}{BeerTax}_{it} + u_{it},  \]

an OLS regression based on entity demeaned data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# demeaned data}
\NormalTok{Fatalities_demeaned <-}\StringTok{ }\KeywordTok{with}\NormalTok{(Fatalities,}
            \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{fatal_rate =}\NormalTok{ fatal_rate }\OperatorTok{-}\StringTok{ }\KeywordTok{ave}\NormalTok{(fatal_rate, state),}
            \DataTypeTok{beertaxs =}\NormalTok{ beertax }\OperatorTok{-}\StringTok{ }\KeywordTok{ave}\NormalTok{(beertax, state)))}

\CommentTok{# estimate regression}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Fatalities_demeaned))}
\end{Highlighting}
\end{Shaded}

Equivalently it possible to use \texttt{plm()} from the package with the
same name. If not already done, install the package \texttt{plm} using
\texttt{install.packages("plm")} and attach it by calling
\texttt{library(plm)}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install and load the 'plm' package}
\NormalTok{## install.packages("plm")}
\KeywordTok{library}\NormalTok{(plm)}
\end{Highlighting}
\end{Shaded}

As for \texttt{lm()} we have to specify the regression formula and the
data to be used in our call of \texttt{plm()}. Additionally, it is
required to pass a vector of names of entity and time id variables to
the \texttt{index} argument. For \texttt{Fatalities}, the id variable
for entities is named \texttt{state} and the time id variable is
\texttt{year}. Since the fixed effects estimator is also called the
\emph{within} estimator, we set \texttt{model\ =\ "within"}. The
\texttt{coeftest()} function allows to obtain significance based on
robust standard errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the fixed effects regression with plm()}
\NormalTok{fatal_fe_mod <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }
                    \DataTypeTok{data =}\NormalTok{ Fatalities,}
                    \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }
                    \DataTypeTok{model =} \StringTok{"within"}\NormalTok{)}

\CommentTok{# summary using robust standard errors}
\KeywordTok{coeftest}\NormalTok{(fatal_fe_mod, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatal_fe_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##         Estimate Std. Error t value Pr(>|t|)  
## beertax -0.65587    0.28880  -2.271  0.02388 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Agian, we find that the estimated coefficient is \(-0.6559\). Notice
that \texttt{plm()} uses the entity-demeaned OLS algorithm as no
coefficients for dummy variables are reported. The estimated regression
function is

\begin{align}
\widehat{FatalityRate} = \underset{(0.29)}{-0.66} \times BeerTax + StateFixedEffects. \label{eq:efemod}
\end{align}

We conclude that the coefficient on \(BeerTax\) is nagative and highly
significant. The interpretation is that the estimated reduction in
traffic fatalities due to a \(\$1\) increase in the real beer tax is
\(0.66\) per \(10000\) people which is still pretty high. Though
including state fixed effects eliminates the risk of a bias due to
omitted factors that vary across states but not over time, we suspect
that there are other omitted varibales that vary over time and thus
cause a bias.

\section{Regression with Time Fixed
Effects}\label{regression-with-time-fixed-effects}

Controlling for variables that are constant across entities but vary
over time can be realized by including time fixed effects. If there are
\textbf{only time fixed effects}, the fixed effects regression model
becomes
\[Y_{it} = \beta_0 + \beta_1 X_{it} + \delta_2 B2_t + \cdots + \delta_T BT_t + u_{it},\]
where only \(T-1\) dummies are included (\(B1\) is omitted) since the
model includes an intercept. This model eliminates only omitted
variables bias from unobserved varaibles that evolve over time but are
constant across entities.

In some applications it is meaningful to include entity and time fixed
effects. The \textbf{entity and time fixed effects} regression model is
\[Y_{it} = \beta_0 + \beta_1 X_{it} + \gamma_2 D2_i + \cdots + \gamma_n DT_i + \delta_2 B2_t + \cdots + \delta_T BT_t + u_{it} .\]
The combined model allows to eliminate bias from unobservables that
change over time but are constant over entities and controls for factors
that differ across entities but are constant over time. Such models can
be estimated using and OLS algorithm that is implemented in R.

The following code chunk shows how to estimate the combined entity and
time fixed effects model of the relation between fatalities and beer
tax,
\[FatalityRate_{it} = \beta_1 BeerTax_{it} + StateEffects + TimeFixedEffects + u_{it}\]
using both, \texttt{lm()} and \texttt{plm()}. It is straightforward
estimate this regression with \texttt{lm()} since it is just an
extension of \eqref{eq:fatsemod}. We only have to adjust the
\texttt{formula} argument by adding the additional regressor
\texttt{year} for time fixed effects. In our call of \texttt{plm()} we
set another argument \texttt{effect\ =\ "twoways"} for inclusion of
entity \emph{and} time dummies.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate combind time and entity fixed effects regression model}
\CommentTok{# via lm()}
\NormalTok{fatal_tefe_lm_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Fatalities)}
\NormalTok{fatal_tefe_lm_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = fatal_rate ~ beertax + state + year - 1, data = Fatalities)
## 
## Coefficients:
##  beertax   stateal   stateaz   statear   stateca   stateco   statect  
## -0.63998   3.51137   2.96451   2.87284   2.02618   2.04984   1.67125  
##  statede   statefl   statega   stateid   stateil   statein   stateia  
##  2.22711   3.25132   4.02300   2.86242   1.57287   2.07123   1.98709  
##  stateks   stateky   statela   stateme   statemd   statema   statemi  
##  2.30707   2.31659   2.67772   2.41713   1.82731   1.42335   2.04488  
##  statemn   statems   statemo   statemt   statene   statenv   statenh  
##  1.63488   3.49146   2.23598   3.17160   2.00846   2.93322   2.27245  
##  statenj   statenm   stateny   statenc   statend   stateoh   stateok  
##  1.43016   3.95748   1.34849   3.22630   1.90762   1.85664   2.97776  
##  stateor   statepa   stateri   statesc   statesd   statetn   statetx  
##  2.36597   1.76563   1.26964   4.06496   2.52317   2.65670   2.61282  
##  stateut   statevt   stateva   statewa   statewv   statewi   statewy  
##  2.36165   2.56100   2.23618   1.87424   2.63364   1.77545   3.30791  
## year1983  year1984  year1985  year1986  year1987  year1988  
## -0.07990  -0.07242  -0.12398  -0.03786  -0.05090  -0.05180
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# via plm()}
\NormalTok{fatal_tefe_mod <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }
                      \DataTypeTok{data =}\NormalTok{ Fatalities,}
                      \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }
                      \DataTypeTok{model =} \StringTok{"within"}\NormalTok{, }
                      \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{)}

\KeywordTok{coeftest}\NormalTok{(fatal_tefe_mod, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fatal_tefe_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##         Estimate Std. Error t value Pr(>|t|)  
## beertax -0.63998    0.35015 -1.8277  0.06865 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Notice that since we exclude the intercept, \texttt{lm()} does estimate
coefficients for \((n-1) + (T-1) = 47 + 6 = 53\) binary variables!
Agina, \texttt{plm()} does only report the estimated coefficient on
\(BeerTax\).

The estimated regression function is

\begin{align}
\widehat{FatalityRate} =  \underset{(0.35)}{-0.64} \times BeerTax + StateEffects + TimeFixedEffects. \label{eq:cbnfemod}
\end{align}

With an estimate of \(-0.66\), the result is close to the estimated
coefficient for the regression model including only entity fixed
effects. Unsurprisingly, the coefficient is less precisely estimated as
before but significantly different from zero at the level of \(10\%\).

In view of \eqref{eq:efemod} and \eqref{eq:cbnfemod} we conclude that the
estimated relationship between traffic fatalities and the real beer tax
is not affected by omitted variable bias due to factors that are
constant either over time or across states.

\section{The Fixed Effects Regression Assumptions and Standard Errors
for Fixed Effects
Regression}\label{the-fixed-effects-regression-assumptions-and-standard-errors-for-fixed-effects-regression}

This section focusses on the entity fixed effects regression model and
presents model assumptions that need to hold in order for OLS to produce
unbiased estimates that are asymptotically normally distributed for
large \(n\). These assumptions are an extension of the assumptions made
for the multiple regression model (see Key Concept 6.4) and are given in
Key Concept 10.3. We will also briefly discuss standard errors in fixed
effects regression models which differ from standard errors in multiple
regression as the regression error can exhibit \textbf{serial
correlation} in panel models.

Key Concept 10.3

The Fixed Effects Regression Assumptions

In the fixed effects regression model
\[ Y_{it} = \beta_1 X_{it} + \alpha_i + u_{it} \ \ , \ \ i=1,\dots,n, \ t=1,\dots,T, \]
we assume that

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  the error term \(u_{it}\) has conditional mean zero, that is
  \(E(u_{it}|u_{i1}, u_{i2},\dots, u_{iT})\).
\item
  \((X_{i1}, X_{i2}, \dots, X_{i3}, u_{i1}, \dots, u_{iT})\),
  \(i=1,\dots,n\) are i.i.d. draws from their joint distribution.
\item
  Large outliers are unlikely i.e. \((X_{it}, u_{it})\) have nonzero
  finite fourth moments.
\item
  there is no perfect multicollinearity.
\end{enumerate}

When there are multiple regressors, \(X_{it}\) is replaced by
\(X_{1,it}, X_{2,it}, \dots, X_{k,it}\).

The first assumption is that the error is uncorrelated with \emph{all}
observations on variable \(X\) for entity \(i\) over time. If this
assumption is violated, we have omitted variables bias. The second
assumption ensures that variables are i.i.d. across entities
\(i=1,\dots,n\). Notice that this \emph{does not} require observations
to be uncorrelated \emph{within} an entity. We say that the \(X_{it}\)
are allowed to be \textbf{autocorrelated} or \textbf{serially
correlated} within entities. This is a common property of time series
data. The same is allowed for errors \(u_{it}\). Consult chapter 10.5
for a detailed explanation why autocorrelation is a plausible
characteristic in panel applications. The second assumption is granted
if entities are selected by simple random sampling. The third and fourth
assumptions are analagous to the multiple regression assumptions made in
Key Concept 6.4.

\subsubsection*{Standard Errors for Fixed Effects
Regression}\label{standard-errors-for-fixed-effects-regression}
\addcontentsline{toc}{subsubsection}{Standard Errors for Fixed Effects
Regression}

Similar as for heteroskedasticity, autocorrelation invalidates usual of
regression errors invalidates usual standard error fomulas and also the
way heteroskedasticity-robust standard errors are computed since these
formulas are derived under the assumption that there is no
autocorrelation. In the context of heteroskedasticity \emph{and}
autocorrelation so-called \textbf{heteroskedasticity and
autocorrelation-consistent (HAC) standard errors} need to be used.
\textbf{Clustered standard errors} belong to these type of standard
errors. They allow for heteroskedasticity \emph{and} autocorrelated
errors within an entity but \emph{not for any kind of correlation
across} entities.

As shown in the examples throughout this chapter, it is fairly easy to
specify usage of clustered standard errors in regression summaries
procuced by function like \texttt{coeftest()} in conjunction with
\texttt{vcovHC()} from the \texttt{sandwich} package. Conveniently,
\texttt{vcovHC()} recognizes panel model objects and computes clusterd
standard errors by default.

The regressions conducted during this chapter are a good examples why
usage of clustered standard errors is crucial in fixed effects models.
For example, consider the entity and time fixed effects regression model
for fatalities. Since \texttt{fatal\_tefe\_lm\_mod} is an object of
class \texttt{lm}, \texttt{coeftest()} does not compute clustered
standard errros but uses robust standard errors that are only valid in
the absence of autocorrelated errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check class of the model (lm) object}
\KeywordTok{class}\NormalTok{(fatal_tefe_lm_mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "lm"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Summary based on heteroskedasticity-robust standard errors (no adjustment for autocorrelation)}
\KeywordTok{coeftest}\NormalTok{(fatal_tefe_lm_mod, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fatal_tefe_lm_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))[}\DecValTok{1}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Estimate Std. Error    t value   Pr(>|t|) 
## -0.6399800  0.2547149 -2.5125346  0.0125470
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check class of the (plm) model object}
\KeywordTok{class}\NormalTok{(fatal_tefe_mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "plm"        "panelmodel"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Summary based on clusterd standard errors (adjustment for autocorrelation + heteroskedasticity)}
\KeywordTok{coeftest}\NormalTok{(fatal_tefe_mod, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fatal_tefe_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##         Estimate Std. Error t value Pr(>|t|)  
## beertax -0.63998    0.35015 -1.8277  0.06865 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The outcomes deviate rather strongly: imposing no autocorrelation we
obtain a standard error of \(0.25\), implying significance of
\(\beta_1\), the coefficient on \(BeerTax\) at the level of \(5\%\),
whereas the clusterd standard error (\(0.35\)) leads to acceptance of
the hypothesis \(H_0: \beta_1 = 0\) at the same level of significance
(see \eqref{eq:cbnfemod}). Consult Appendix 10.2 of the book for insights
on the computation of clustered standard errors.

\section{Drunk Driving Laws and Traffic
Deaths}\label{drunk-driving-laws-and-traffic-deaths}

There are two major sources of omitted variable bias that are not
accounted for by all of our models of the relation between trafic
fatalitites and beer taxes that we have considered so far: economic
conditions and driving laws. Fortunately, \texttt{Fatalities} has
recordings on state specific legal drinking age (\texttt{drinkage}),
punishment (\texttt{jail}, \texttt{service}) and various economic
indicators like unemployment rate (\texttt{unemp}) and per capita income
(\texttt{income}). We may use this data to extend the preceeding
analysis.

At first, we define variables according to the regression results
presented in table 10.1 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Discretize the minimum legal drinking age}
\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{drinkagec <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(Fatalities}\OperatorTok{$}\NormalTok{drinkage,}
                            \DataTypeTok{breaks =} \DecValTok{18}\OperatorTok{:}\DecValTok{22}\NormalTok{, }
                            \DataTypeTok{include.lowest =} \OtherTok{TRUE}\NormalTok{, }
                            \DataTypeTok{right =} \OtherTok{FALSE}
\NormalTok{                            )}

\CommentTok{# Set minimum drinking age [21,22] to be the baseline level}
\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{drinkagec <-}\StringTok{ }\KeywordTok{relevel}\NormalTok{(Fatalities}\OperatorTok{$}\NormalTok{drinkagec, }\StringTok{"[21,22]"}\NormalTok{)}

\CommentTok{# Mandadory jail or community service?}
\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{punish <-}\StringTok{ }\KeywordTok{with}\NormalTok{(Fatalities, }\KeywordTok{factor}\NormalTok{(jail }\OperatorTok{==}\StringTok{ "yes"} \OperatorTok{|}\StringTok{ }\NormalTok{service }\OperatorTok{==}\StringTok{ "yes"}\NormalTok{, }
                                             \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{)}
\NormalTok{                                             )}
\NormalTok{                          )}

\CommentTok{# Observations on all variables for 1982 and 1988 only}
\NormalTok{Fatalities_1982_}\DecValTok{1988}\NormalTok{ <-}\StringTok{ }\NormalTok{Fatalities[}\KeywordTok{with}\NormalTok{(Fatalities, year }\OperatorTok{==}\StringTok{ }\DecValTok{1982} \OperatorTok{|}\StringTok{ }\NormalTok{year }\OperatorTok{==}\StringTok{ }\DecValTok{1988}\NormalTok{), ]}
\end{Highlighting}
\end{Shaded}

Next, we estimate all seven models using \texttt{plm()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fatalities_mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }\DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod2 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state, }\DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod3 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year,}
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{,}\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{, }
                       \DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod4 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{drinkagec }
                       \OperatorTok{+}\StringTok{ }\NormalTok{punish }\OperatorTok{+}\StringTok{ }\NormalTok{miles }\OperatorTok{+}\StringTok{ }\NormalTok{unemp }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{,}\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
                       \DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod5 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{drinkagec }
                       \OperatorTok{+}\StringTok{ }\NormalTok{punish }\OperatorTok{+}\StringTok{ }\NormalTok{miles,}
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{,}\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
                       \DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod6 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{drinkage }
                       \OperatorTok{+}\StringTok{ }\NormalTok{punish }\OperatorTok{+}\StringTok{ }\NormalTok{miles }\OperatorTok{+}\StringTok{ }\NormalTok{unemp }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{,}\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
                       \DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod7 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{drinkagec }
                       \OperatorTok{+}\StringTok{ }\NormalTok{punish }\OperatorTok{+}\StringTok{ }\NormalTok{miles }\OperatorTok{+}\StringTok{ }\NormalTok{unemp }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{,}\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
                       \DataTypeTok{data =}\NormalTok{ Fatalities_1982_}\DecValTok{1988}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Yet again we may use \texttt{stargazer()} to generate a comprehensive
tabular presentation of the results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stargazer)}
\KeywordTok{library}\NormalTok{(sandwich)}

\NormalTok{rob_se <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod1, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod2, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod3, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod4, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod5, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod6, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod7, }\DataTypeTok{type=}\StringTok{"HC1"}\NormalTok{)))}
\NormalTok{)}

\KeywordTok{stargazer}\NormalTok{(fatalities_mod1, fatalities_mod2, fatalities_mod3, fatalities_mod4, fatalities_mod5, fatalities_mod6, fatalities_mod7, }
          \DataTypeTok{digits =} \DecValTok{3}\NormalTok{,}
          \DataTypeTok{type =} \StringTok{"latex"}\NormalTok{, }
          \DataTypeTok{se =}\NormalTok{ rob_se,}
          \DataTypeTok{model.numbers =} \OtherTok{FALSE}\NormalTok{,}
          \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"(1)"}\NormalTok{, }\StringTok{"(2)"}\NormalTok{, }\StringTok{"(3)"}\NormalTok{, }\StringTok{"(4)"}\NormalTok{, }\StringTok{"(5)"}\NormalTok{, }\StringTok{"(6)"}\NormalTok{, }\StringTok{"(7)"}\NormalTok{)}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

Dependent variable:

fatal\_rate

OLS

panel

linear

(1)

(2)

(3)

(4)

(5)

(6)

(7)

beertax

0.365***

-0.656**

-0.640*

-0.445

-0.690**

-0.456

-0.926***

(0.053)

(0.289)

(0.350)

(0.291)

(0.345)

(0.301)

(0.337)

drinkagec{[}18,19)

0.028

-0.010

0.037

(0.068)

(0.081)

(0.101)

drinkagec{[}19,20)

-0.018

-0.076

-0.065

(0.049)

(0.066)

(0.097)

drinkagec{[}20,21)

0.032

-0.100*

-0.113

(0.050)

(0.055)

(0.123)

drinkage

-0.002

(0.021)

punishyes

0.038

0.085

0.039

0.089

(0.101)

(0.109)

(0.101)

(0.161)

miles

0.00001

0.00002*

0.00001

0.0001***

(0.00001)

(0.00001)

(0.00001)

(0.00005)

unemp

-0.063***

-0.063***

-0.091***

(0.013)

(0.013)

(0.021)

log(income)

1.816***

1.786***

0.996

(0.624)

(0.631)

(0.666)

Constant

1.853***

(0.047)

Observations

336

336

336

335

335

335

95

R2

0.093

0.041

0.036

0.360

0.066

0.357

0.659

Adjusted R2

0.091

-0.120

-0.149

0.217

-0.134

0.219

0.157

Residual Std. Error

0.544 (df = 334)

F Statistic

34.394*** (df = 1; 334)

12.190*** (df = 1; 287)

10.513*** (df = 1; 281)

19.194*** (df = 8; 273)

3.252*** (df = 6; 275)

25.423*** (df = 6; 275)

9.194*** (df = 8; 38)

Note:

\emph{p\textless{}0.1; \textbf{p\textless{}0.05; }}p\textless{}0.01

While coloumns (2) and (3) recap the results \eqref{eq:efemod} and
@red(eq:cbnfemod), coloumn (1) presents an estimate of the coefficient
of interest in the naive OLS regression of the fatality rate on beer tax
without any fixed effects. We obtain a \emph{positive} estimate for the
coefficient on beer tax that is likely to be heavily biased. Notice that
the model fit is rather bad (\(\overline{R^2} = 0.091\)), too. The sign
of the estimate changes as we extend the model by both entity and time
fixed effects in models (2) and (3). Furthermore \(\overline{R^2}\)
increases substantially as fixed effects are included in the model
equation. Nonetheless, as discussed before, the magnitudes of both
estimates are by far too high.

Thus, model specifications (4) to (7) include covariates that shall
capture the effect of overall state economic conditions as well as the
legal framework. These covariates are

\begin{itemize}
\tightlist
\item
  \texttt{unemp}: a numeric variable stating the state specific
  unemployment rate.
\item
  \texttt{log(income)}: the logarithm of real per capita income (in
  prices of 1988).
\item
  \texttt{miles}: average miles per driver.
\item
  \texttt{drinkage}: state specifiy minimum legal drinking age.
\item
  \texttt{drinkagc}: a discretized version of \texttt{drinkage} that
  classifies states into four categories of minimal drinking age:
  \(18\), \(19\), \(20\), \(21\) and older. R denotes this as
  \texttt{{[}18,19)}, \texttt{{[}19,20)}, \texttt{{[}20,21)} and
  \texttt{{[}21,22{]}}. These categories are included as dummy
  regressors wherby \texttt{{[}21,22{]}} is chosen to be the reference
  category.
\item
  \texttt{punish}: a dummy variable with levels \texttt{yes} and
  \texttt{no} that measures if drunk driving is severely punished by
  mandatory jail time or mandatory community service (first conviction).
\end{itemize}

Considering (4) as the base specification, we observe four interesting
results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Including the covariates does not lead to a major reduction of the
  estimated effect of the beer tax. Plus, the coefficient is not
  significantly different from zero at the level of \(5\%\) as the
  estimate is rather imprecise.
\item
  The minimum legal drinking age \emph{does not} have an effect on
  traffic fatalities: none of the three dummy variables is significantly
  different from zero at any level of significance common in practice.
  Moreover, an \(F\)-Test of the joint hypothesis that all three
  coefficients are zero does not reject.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test if legal drinking age has no explanatory power}
\KeywordTok{linearHypothesis}\NormalTok{(fatalities_mod4,}
                 \DataTypeTok{test =} \StringTok{"F"}\NormalTok{,}
                 \KeywordTok{c}\NormalTok{(}\StringTok{"drinkagec[18,19)=0"}\NormalTok{,}\StringTok{"drinkagec[19,20)=0"}\NormalTok{,}\StringTok{"drinkagec[20,21)"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatalities_mod4, }\DataTypeTok{type =} \StringTok{"HC2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## drinkagec[18,19) = 0
## drinkagec[19,20) = 0
## drinkagec[20,21) = 0
## 
## Model 1: restricted model
## Model 2: fatal_rate ~ beertax + state + year + drinkagec + punish + miles + 
##     unemp + log(income)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(>F)
## 1    276                 
## 2    273  3 0.3514 0.7882
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  There is no evidence that punishment for first offenders has a
  deterring effects on drunk driving: the corresponding coefficient is
  not significant at the \(10\%\) level.
\item
  The economic variables have explanatory power in explaning traffic
  fatalities. We can check that the employment rate and per capita
  income are jointly significant at the level of \(0.1\%\).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test if economic indicators have no explanatory power}
\KeywordTok{linearHypothesis}\NormalTok{(fatalities_mod4, }
                 \DataTypeTok{test =} \StringTok{"F"}\NormalTok{,}
                 \KeywordTok{c}\NormalTok{(}\StringTok{"log(income)"}\NormalTok{, }\StringTok{"unemp"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatalities_mod4, }\DataTypeTok{type =} \StringTok{"HC2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## log(income) = 0
## unemp = 0
## 
## Model 1: restricted model
## Model 2: fatal_rate ~ beertax + state + year + drinkagec + punish + miles + 
##     unemp + log(income)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(>F)    
## 1    275                        
## 2    273  2 30.482 1.125e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Model (5) omitts economic factors. The regression result supports the
presumption that economic indicators should remain part of the model as
the coefficient on beer tax is sensitive to inclusion of the latter.

Results for model (6) are in favour of the presumption that legal
drinking age has little explanatory power and that the coefficient of
interest is not senstive to changes in the functional form of the
relation between drinking age and traffic fatalities.

Estimation of spefcification (7) reveals that reducing the amount of
available information inflates standard errors but does not lead to
drastic changes in coefficient estimates.

\subsubsection*{Summary}\label{summary-2}
\addcontentsline{toc}{subsubsection}{Summary}

We have not found evidence that menacing severe punishments and
increasing minimum drinking ages are appropriate measures for reducing
traffic fatalities due to drunk driving. Nonetheless we conclude that
there is some effect of alcohol taxes on traffic fatalities which,
however, is estimated imprecisely and cannot be interpreted as the
causal effect of interest as there seems to be omitted variable bias.
This bias persists even though we used a panel approach that controls
for entity specific and time fixed effects unobservables. A possible
source for this bias are omitted variables that change both across
entities and over time.

A powerful method that can be used if common panel regression approaches
fail is instrumental variables regression. We will return to this issue
in Chapter 12.

\chapter{Regression with a Binary Dependent
Variable}\label{regression-with-a-binary-dependent-variable}

In this chapter, we discuss a special case of regression models that aim
to explain a limited dependent variable --- a dependent variable with a
limit range --- by multiple regressors.

In particulat we consider models where the dependent variable is binary.
We will see that in such models, the regression function can be
interpreted as a conditional probability function.

We review the following concepts:

\begin{itemize}
\tightlist
\item
  The linear probability model
\item
  Probit models
\item
  Logit model
\item
  Maximum likelihood estimation of nonlinear regression models
\end{itemize}

Of course, we will also see how to estimate abovementioned models using
\texttt{R} and discuss an application where we explore the question
whether there is racial discrimination in the US mortgage market.

\section{Binary Dependent Variables and the Linear Probability
Model}\label{binary-dependent-variables-and-the-linear-probability-model}

Key Concept 11.1

The Linear Probabilty Model

The linear regression model

\[Y_i = \beta_0 + \beta_1 + X_{1i} + \beta_2 X_{2i} + \dots + \beta_k X_{ki} + u_i\]
with a binary depnendent variable \(Y_i\) is called the linear
probability model as
\[E(Y\vert X_1,X_2,\dots,X_k) = P(Y=1\vert X_1, X_2,\dots, X_3)\] such
that
\[ P(Y = 1 \vert X_1, X_2, \dots, X_k) = \beta_0 + \beta_1 + X_{1i} + \beta_2 X_{2i} + \dots + \beta_k X_{ki}.\]

Thus, the coefficient \(\beta_j\) can be interpreted as the change in
the probability that \(Y=1\), holding constant the other \(k-1\)
regressors. Just as in common multiple regression, the \(\beta_j\) can
be estimated using OLS and the robust standard error formulas can be
used for hypothesis testing and computation of confidence intervals.

In most linear probability models, \(R^2\) has no meaningful
interpretation since the regression line can never fit the data
perfectly if the depdent variable is binary and the regressors are
continuous. This can be seen in the application below.

We emphasize that it is \emph{essential} to use robust standard errors
since the \(u_i\) in a linear probability model are always
heteroskedastic.

Following the book, we start by loading the dataset \texttt{HMDA} which
provides data that relate to mortgage applications filed in Boston in
the year of 1990.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load AER package and attach data}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(HMDA)}
\end{Highlighting}
\end{Shaded}

We continue by inspecting the first few observations and compute summary
statistics afterwards.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Inspect data}
\KeywordTok{head}\NormalTok{(HMDA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   deny pirat hirat     lvrat chist mhist phist unemp selfemp insurance
## 1   no 0.221 0.221 0.8000000     5     2    no   3.9      no        no
## 2   no 0.265 0.265 0.9218750     2     2    no   3.2      no        no
## 3   no 0.372 0.248 0.9203980     1     2    no   3.2      no        no
## 4   no 0.320 0.250 0.8604651     1     2    no   4.3      no        no
## 5   no 0.360 0.350 0.6000000     1     1    no   3.2      no        no
## 6   no 0.240 0.170 0.5105263     1     1    no   3.9      no        no
##   condomin afam single hschool
## 1       no   no     no     yes
## 2       no   no    yes     yes
## 3       no   no     no     yes
## 4       no   no     no     yes
## 5       no   no     no     yes
## 6       no   no     no     yes
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(HMDA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   deny          pirat            hirat            lvrat        chist   
##  no :2095   Min.   :0.0000   Min.   :0.0000   Min.   :0.0200   1:1353  
##  yes: 285   1st Qu.:0.2800   1st Qu.:0.2140   1st Qu.:0.6527   2: 441  
##             Median :0.3300   Median :0.2600   Median :0.7795   3: 126  
##             Mean   :0.3308   Mean   :0.2553   Mean   :0.7378   4:  77  
##             3rd Qu.:0.3700   3rd Qu.:0.2988   3rd Qu.:0.8685   5: 182  
##             Max.   :3.0000   Max.   :3.0000   Max.   :1.9500   6: 201  
##  mhist    phist          unemp        selfemp    insurance  condomin  
##  1: 747   no :2205   Min.   : 1.800   no :2103   no :2332   no :1694  
##  2:1571   yes: 175   1st Qu.: 3.100   yes: 277   yes:  48   yes: 686  
##  3:  41              Median : 3.200                                   
##  4:  21              Mean   : 3.774                                   
##                      3rd Qu.: 3.900                                   
##                      Max.   :10.600                                   
##   afam      single     hschool   
##  no :2041   no :1444   no :  39  
##  yes: 339   yes: 936   yes:2341  
##                                  
##                                  
##                                  
## 
\end{verbatim}

The variable we are interesting in modelling is \texttt{deny}, an
indicator for whether an applicants mortgage application has been
accepted (\texttt{deny\ =\ no}) or denied (\texttt{deny\ =\ yes}). A
regressors that can be presumed to have power in explaining whether a
mortgage application has been denied or accepted is \texttt{pirat}, the
size of the anticipated total monthly loan payments relative to the the
applicant's income. It is straightforward to translate this into the
simple regression approach

\begin{align}
  deny = \beta_0 + \beta_1 P/I\ ratio + u. \label{eq:denymod1}
\end{align}

We can estimate this model just as any other linear regression model
using \texttt{lm()}. Before we do, \texttt{deny} must be converted to a
numeric variable using \texttt{as.numeric()}. Note that
\texttt{as.numeric(HMDA\$deny)} will turn \texttt{deny\ =\ no} into
\texttt{deny\ =\ 1} and \texttt{deny\ =\ yes} into \texttt{deny\ =\ 2},
so using \texttt{as.numeric(HMDA\$deny)-1} we obtain levels \texttt{0}
and \texttt{1}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert 'deny' to numeric}
\NormalTok{HMDA}\OperatorTok{$}\NormalTok{deny <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{deny) }\OperatorTok{-}\StringTok{ }\DecValTok{1}

\CommentTok{# estimate a simple linear probabilty model}
\NormalTok{denymod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{pirat, }\DataTypeTok{data =}\NormalTok{ HMDA)}
\NormalTok{denymod1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = deny ~ pirat, data = HMDA)
## 
## Coefficients:
## (Intercept)        pirat  
##    -0.07991      0.60353
\end{verbatim}

We plot the data and the regression line to reproduce Figure 11.1 of the
book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ HMDA}\OperatorTok{$}\NormalTok{pirat, }
\NormalTok{     HMDA}\OperatorTok{$}\NormalTok{deny,}
     \DataTypeTok{main =} \StringTok{"Scatterplot Mortgage Application Denial and the Payment-to-Income Ratio"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"P/I ratio"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Deny"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.4}\NormalTok{,}\FloatTok{1.4}\NormalTok{),}
     \DataTypeTok{cex.main =} \FloatTok{0.8}
\NormalTok{)}

\CommentTok{# add horizontal dashed lines and text}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{, }\StringTok{"Mortgage denied"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\OperatorTok{-}\FloatTok{0.1}\NormalTok{, }\DataTypeTok{cex=} \FloatTok{0.8}\NormalTok{, }\StringTok{"Mortgage approved"}\NormalTok{)}

\CommentTok{# add estimated regression line}
\KeywordTok{abline}\NormalTok{(denymod1, }
       \DataTypeTok{lwd=}\FloatTok{1.8}\NormalTok{, }
       \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-239-1.pdf}

We observe that, according to the estimated model equation, a
\(P/I ratio\) of \(1\) is associated with an expected probability of
mortgage application denial of roughly \(50\%\). The model indicates
that there is a postive relation between \(P/I \ ratio\) and the
probabilty of a denied mortgage application i.e.~individuals with a high
ratio of loan payments to income are more likely to be rejected.

We may use \texttt{coeftest} to obtain robust standard errors for both
coefficient estimators.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# model coefficient summary}
\KeywordTok{coeftest}\NormalTok{(denymod1, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(denymod1, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) -0.079910   0.031967 -2.4998   0.01249 *  
## pirat        0.603535   0.098483  6.1283 1.036e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimated regression line is
\[\widehat{deny} = \underset{(0.032)}{-0.080} + \underset{(0.098)}{0.604} P/I \ ratio.\]
The true coefficient on \(P/I \ ratio\) is statistically different from
\(0\) at the \(1\%\) level. Its estimate can be interpreted as follows:
a \(1\%\) increase in \(P/I \ ratio\) leads to an increase in the
probability of a loan denial by
\(0.604 * 0.01 = 0.00604 \approx 0.6\%\).

Following the book we augment the approach \eqref{eq:denymod1} by an
additional regressor \(black\) which equals \(1\) if the applicant is an
African American and equals \(0\) if the applicant is white. Such a
specification is the baseline for investigation of the question whether
there is racial discrimination in the mortgage market: if being black
has a significant (positive) influence on the probability of a loan
denial when we control for factors that allow for objective assessment
of an applicants credit worthyness, this is an indicator for
discrimination.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# rename variable 'afam'}
\KeywordTok{colnames}\NormalTok{(HMDA)[}\KeywordTok{colnames}\NormalTok{(HMDA) }\OperatorTok{==}\StringTok{ "afam"}\NormalTok{] <-}\StringTok{ "black"}

\CommentTok{# estimate the model}
\NormalTok{denymod2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{pirat }\OperatorTok{+}\StringTok{ }\NormalTok{black, }\DataTypeTok{data =}\NormalTok{ HMDA)}
\KeywordTok{coeftest}\NormalTok{(denymod2, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(denymod2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) -0.090514   0.033430 -2.7076  0.006826 ** 
## pirat        0.559195   0.103671  5.3939 7.575e-08 ***
## blackyes     0.177428   0.025055  7.0815 1.871e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

So the estimated regression function is

\begin{align}
  \widehat{deny} =& \, \underset{(0.029)}{-0.091} + \underset{(0.089)}{0.559} P/I \ ratio + \underset{(0.025)}{0.177} afam. \label{eq:denymod2}
\end{align}

The coefficient on \(black\) is positive and significantly different
from zero at the \(0.01\%\) level. The interpretation is that, holding
constant the \(P/I \ ratio\), being black increases the probability of a
mortgage application denial by about \(17.7\%\). This suggests racial
discrimination. However, the estimation might suffer from omitted
variable bias so this could be a premature conclusion.

\section{Probit and Logit Regression}\label{probit-and-logit-regression}

The linear probability model has a major flaw: it assumes the
conditional probability function to be linear. This does not allow the
probability of observing \(Y=1\) conditional on some regressor to lie
between \(0\) and \(1\). We can easily see this in our reproduction of
Figure 11.1 of the book: for \(P/I \ ratio \geq 1.75\), model
\eqref{eq:denymod1} predicts the probability of a mortgage application
denial to be bigger than \(1\). For applications with \(P/I \ ratio\)
close to \(0\), the predicted probability of denial is even negative so
the model has no meaningful interpretation here.

This circumstance demands for approaches that use a \emph{nonlinear}
function to model the conditional probability function of a binary
dependent variable: probit and logit regression models.

\subsection*{Probit Regression}\label{probit-regression}
\addcontentsline{toc}{subsection}{Probit Regression}

In probit regression, the cumulative standard normal distribution
function \(\Phi\) is used to model the regression function when the
dependent variable is binary:

\begin{align}
  E(Y\vert X) = P(Y=1\vert X) = \Phi(\beta_0 + \beta_1 X) \label{eq:probitmodel}
\end{align}

\(\beta_0 + \beta_1 X\) in \eqref{eq:probitmodel} plays the role of a
quantile \(z\). Remember that
\[\Phi(z) = P(Z \leq z) \ , \ Z \sim \mathcal{N}(0,1)\] such that the
probit coefficient \(\beta_1\) in \eqref{eq:probitmodel} is the change in
\(z\) associated with a one unit change in \(X\). Note that, although
the effect on \(z\) of a change in \(X\) is linear, the link between
\(z\) and the dependent variable \(Y\) is nonlinear since \(\Phi\) is a
nonlinear function.

Since in a probit models the dependent variable is a nonlinear function
of the regressors, the coefficient on \(X\) has no easy interpretation.
According to Key Concept 8.1, the expected change in the probability
that \(Y=1\) due to a change in \(P/I \ ratio\) can be computed in the
following manner:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the predicted probability that \(Y=1\) for the original value
  of \(X\).
\item
  Compute the predicted probability that \(Y=1\) for \(X + \Delta X\).
\item
  Compute the difference between both predicted probabilities.
\end{enumerate}

Of course we can generalize \eqref{eq:probitmodel} to a probit regression
with multiple regressors to mitigate the risk of facing omitted variable
bias. The core knowledge on probit regression is summarized in Key
Concept 11.2

Key Concept 11.2

Probit Model, Predicted Probabilities and Estimated Effects

\[P(Y = 1 \vert X_1, X_2, \dots ,X_k) = \Phi(\beta_0 + \beta_1 + X_1 + \beta_2 X_2 + \dots + \beta_k X_k)\]
is the population probit model with multiple regressors where \(Y\) is a
binary varible, \(X_1, X_2, \dots, X_k\) are regressors and \(\Phi\) is
the cumulative standard normal distribution function.

The predicted probability that \(Y=1\) given \(X_1, X_2, \dots, X_k\)
can be calculated in two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute
  \(z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k\)
\item
  Look up \(\Phi(z)\) using a normal distribution table or by calling
  \texttt{pnorm()}.
\end{enumerate}

\(\beta_j\) is the effect on \(z\) of a one unit change in regressor
\(X_j\), holding constant all other \(k-1\) regressors.

The effect on the predicted probability of a change in a regressor can
be computed as shown in Key Concept 8.1.

In R, probit models can be estimated using the function \texttt{glm()}
from package \texttt{stats}. Using the argument \texttt{family} we
specify that we want to use a probit link function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the simple probit model}
\NormalTok{denyprobit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{pirat, }
                \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"probit"}\NormalTok{), }
                \DataTypeTok{data =}\NormalTok{ HMDA)}

\KeywordTok{coeftest}\NormalTok{(denyprobit, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(denyprobit, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## z test of coefficients:
## 
##             Estimate Std. Error  z value  Pr(>|z|)    
## (Intercept) -2.19415    0.18901 -11.6087 < 2.2e-16 ***
## pirat        2.96787    0.53698   5.5269 3.259e-08 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimated model equation is

\begin{align}
  \widehat{P(deny\vert P/I \ ratio}) = \Phi(\underset{(0.19)}{-2.19} + \underset{(0.54)}{2.97} P/I \ ratio). \label{eq:denyprobit}
\end{align}

Just as in the linear probability model we find that the relation
between the probability of denial and the payments-to-income ratio is
positive and that corresponding coefficient is highly significant.

The following code chunk reproduces Figure 11.2 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ HMDA}\OperatorTok{$}\NormalTok{pirat, }
\NormalTok{     HMDA}\OperatorTok{$}\NormalTok{deny,}
     \DataTypeTok{main =} \StringTok{"Probit Model of the Probability of Denial, Given P/I Ratio"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"P/I ratio"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Deny"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.4}\NormalTok{,}\FloatTok{1.4}\NormalTok{),}
     \DataTypeTok{cex.main =} \FloatTok{0.85}
\NormalTok{)}

\CommentTok{# add horizontal dashed lines and text}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{, }\StringTok{"Mortgage denied"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\OperatorTok{-}\FloatTok{0.1}\NormalTok{, }\DataTypeTok{cex=} \FloatTok{0.8}\NormalTok{, }\StringTok{"Mortgage approved"}\NormalTok{)}

\CommentTok{# add estimated regression line}
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(denyprobit, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{pirat =}\NormalTok{ x), }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}

\KeywordTok{lines}\NormalTok{(x, y, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{, }\DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-243-1.pdf}

Notice that the estimated regression function has a stretched ``S''
shape which typical for the c.d.f. of a of continuous random variable
with symmetric p.d.f. like a normal distributed random variable. The
function is clearly nonlinear and flattens out for large and small
values of \(P/I \ ratio\). More important, it ensures that predicted
conditional probabilities of a denial lie between \(0\) and \(1\).

We may use \texttt{predict()} to compute the predicted change in the
denial probability when \(P/I \ ratio\) is increased from \(0.3\) to
\(0.4\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 1. compute predictions for P/I ratio = 0.3,0.4}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(denyprobit, }
                       \DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\StringTok{"pirat"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{)),}
                       \DataTypeTok{type =} \StringTok{"response"}
\NormalTok{                       )}

\CommentTok{# 2. Compute difference in probabilities}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          2 
## 0.06081433
\end{verbatim}

We find that an increase in the payment-to-income ratio from \(0.3\) to
\(0.4\) is predicted to increase the probability of denial by
approximately \(6.2\) percentage points.

We continue by using a probit model to estimate the effect of race on
the probability of a mortgage application denial.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{denyprobit2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{pirat }\OperatorTok{+}\StringTok{ }\NormalTok{black, }
                \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"probit"}\NormalTok{), }
                \DataTypeTok{data =}\NormalTok{ HMDA)}

\KeywordTok{coeftest}\NormalTok{(denyprobit2, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(denyprobit2, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## z test of coefficients:
## 
##              Estimate Std. Error  z value  Pr(>|z|)    
## (Intercept) -2.258787   0.176608 -12.7898 < 2.2e-16 ***
## pirat        2.741779   0.497673   5.5092 3.605e-08 ***
## blackyes     0.708155   0.083091   8.5227 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimated model equation is

\begin{align}
  \widehat{P(deny\vert P/I \ ratio, black)} = \Phi (\underset{(0.18)}{-2.26} + \underset{(0.50)}{2.74} P/I \ ratio + \underset{(0.08)}{0.71} black). \label{eq:denyprobit2} 
\end{align}

While all coefficients are highly significant, both the estimated
coefficients on the payments-to-income ratio and the indicator for
African American descent are positive. Again, the coefficients are
difficult to interpret but they indicate that first, being black
applicants have a higher higher probability of denial than white
applicants, holding constant the payments-to-income ratio and second,
applicants with a high payments-to-income ratio face a higher risk of
being rejected.

How big is the predicted difference in denial probility between two
hypothetical applicants with the same payments-to-income ratio? As
before, we may use \texttt{predict()} to compute this difference.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 1. compute predictions for P/I ratio = 0.3,0.4}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(denyprobit2, }
                       \DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\StringTok{"black"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{), }\StringTok{"pirat"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.3}\NormalTok{)),}
                       \DataTypeTok{type =} \StringTok{"response"}
\NormalTok{                       )}

\CommentTok{# 2. Compute difference in probabilities}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         2 
## 0.1578117
\end{verbatim}

In this case, the difference in denial probabilities is \(15.8\%\).

\subsection*{Logit Regression}\label{logit-regression}
\addcontentsline{toc}{subsection}{Logit Regression}

Key Concept 11.3

Logit Regression

The logit regression population function is

\begin{align*}
  P(Y=1\vert X_1, X_2, \dots, X_k) =& \, F(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k) \\
  =& \, \frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k)}}.
\end{align*}

The idea is similar do probit regression except that a different c.d.f.
is used: \[F(x) = \frac{1}{1+e^{-x}}\] is the c.d.f. of a standard
logistically distributed random variable.

Key Concept 11.3 summarizes the main differences between logit and
probit regression. As for probit regression, there is no simple
interpretation of coefficients and it is best to consider predicted
probabilitites or differences in predicted probabilities. Here again,
\(t\)-statitics and confidence intervals based on large sample normal
approximation can be computed as usual.

It is fairly easy to estimate a logit regression model using R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{denylogit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{pirat, }
                \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{), }
                \DataTypeTok{data =}\NormalTok{ HMDA)}

\KeywordTok{coeftest}\NormalTok{(denylogit, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(denylogit, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## z test of coefficients:
## 
##             Estimate Std. Error  z value  Pr(>|z|)    
## (Intercept) -4.02843    0.35898 -11.2218 < 2.2e-16 ***
## pirat        5.88450    1.00015   5.8836 4.014e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The subsequent code chunk reproduces figure 11.3 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ HMDA}\OperatorTok{$}\NormalTok{pirat, }
\NormalTok{     HMDA}\OperatorTok{$}\NormalTok{deny,}
     \DataTypeTok{main =} \StringTok{"Probit and Logit Models Model of the Probability of Denial, Given P/I Ratio"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"P/I ratio"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Deny"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.4}\NormalTok{, }\FloatTok{1.4}\NormalTok{),}
     \DataTypeTok{cex.main =} \FloatTok{0.9}
\NormalTok{)}

\CommentTok{# add horizontal dashed lines and text}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{, }\StringTok{"Mortgage denied"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\OperatorTok{-}\FloatTok{0.1}\NormalTok{, }\DataTypeTok{cex=} \FloatTok{0.8}\NormalTok{, }\StringTok{"Mortgage approved"}\NormalTok{)}

\CommentTok{# add estimated regression line of probit and logit models}
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\NormalTok{y_probit <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(denyprobit, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{pirat =}\NormalTok{ x), }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{y_logit <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(denylogit, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{pirat =}\NormalTok{ x), }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}

\KeywordTok{lines}\NormalTok{(x, y_probit, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{, }\DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, y_logit, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{horiz =} \OtherTok{TRUE}\NormalTok{,}
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Probit"}\NormalTok{, }\StringTok{"Logit"}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"steelblue"}\NormalTok{, }\StringTok{"black"}\NormalTok{), }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-248-1} \end{center}

Notice that both models produce very similar estimates of the
probability that a mortgage application will be denied depending on the
applicants payment-to-income ratio.

Following the book we expand the simple logit model of mortage denial
with the additional regressor \(black\) and estimate the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{denylogit2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{pirat }\OperatorTok{+}\StringTok{ }\NormalTok{black, }
                \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{), }
                \DataTypeTok{data =}\NormalTok{ HMDA)}

\KeywordTok{coeftest}\NormalTok{(denylogit2, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(denylogit2, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## z test of coefficients:
## 
##             Estimate Std. Error  z value  Pr(>|z|)    
## (Intercept) -4.12556    0.34597 -11.9245 < 2.2e-16 ***
## pirat        5.37036    0.96376   5.5723 2.514e-08 ***
## blackyes     1.27278    0.14616   8.7081 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We obtain

\begin{align}
  \widehat{P(deny=1 \vert P/I ratio, black)} = F(\underset{(0.35)}{-4.13} + \underset{(0.96)}{5.37} P/I \ ratio + \underset{(0.15)}{1.27} black). \label{eq:denylogit2}
\end{align}

As for the probit model \eqref{eq:denyprobit2} all model coefficients are
highly significant and we obtain postive estimate for the coefficients
on \(P/I \ ratio\) and \(black\). For comparison purposes we compute the
predicted probability of denial for two hypothetical applicants that
differ in race and have a \(P/I \ ratio\) of \(0.3\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 1. compute predictions for P/I ratio = 0.3}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(denylogit2, }
                       \DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\StringTok{"black"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{), }\StringTok{"pirat"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.3}\NormalTok{)),}
                       \DataTypeTok{type =} \StringTok{"response"}
\NormalTok{                      )}

\CommentTok{# 2. Compute difference in probabilities}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         2 
## 0.1492945
\end{verbatim}

We find that the white applicant faces a denial probability of only
\(7.5\%\) percent while the African American is rejected with a
probability of \(22.4\%\) with makes a difference of \(14.9\%\).

\subsubsection*{Comparison of Models}\label{comparison-of-models}
\addcontentsline{toc}{subsubsection}{Comparison of Models}

The linear probability model, the probit model and the logit model
deliver only approximations to the unknown population regression
function \(E(Y\vert X)\). It is not unabigious to decide which model one
should use in practice. The linear probability model has the clear
drawbacks of not beeing able to capture the nonlinear nature of the
population regression function and predicting probabilities outside the
interval of \([0,1]\) for extreme regressor values. Probit and logit
models are hard to interpret but can capture nonlinearities. Both models
produce predictions of probabilities that lie inside \([0,1]\).
Predictions of probit and logit models are often close to each other.
The book suggests to use the method that is easiest to use in the
statistical software of choice. As we have seen, it is equally easy to
estimate probit and logit model using R. We can therefore give no
general guide which method to use.

\section{Estimation and Inference in the Logit and Probit
Models}\label{estimation-and-inference-in-the-logit-and-probit-models}

So far nothing has been said about \emph{how} logit and probit models
are estimated by statistical software. The reason why this is
interesting is that both models are \emph{nonlinear in the parameters}
and thus cannot be estimated using OLS. Instead one uses a concept
called \emph{Maximum Liklihood Estimation}. Another approach is
nonlinear least squares (NLS) estimation.

\subsubsection*{Nonlinear Least Squares}\label{nonlinear-least-squares}
\addcontentsline{toc}{subsubsection}{Nonlinear Least Squares}

Consider the multiple regression probit model

\begin{align}
  E(Y\vert X_1, \dots, X_k) = P(Y=1\vert X_1, \dots, X_k) = \Phi(\beta_0 + \beta_1 X_1 + \dots + \beta_k X_k). \label{eq:multprobit}
\end{align}

Similarly to OLS, NLS estimates the parameters
\(\beta_0,\beta_1,\dots,\beta_k\) by minimizing the sum of squared
mistakes
\[\sum_{i=1}^n\left[ Y_i - \Phi(b_0 + b_1 X_{1i} + \dots + b_k X_{ki}) \right]^2.\]
NLS estimation is a consistent approach that produces estimates which
are normally distributed in large samples. In R there are functions like
\texttt{nls()} from package \texttt{stats} that provide algorithms for
solving nonlinear least squares problems. However, NLS is inefficient,
meaning that there are estimation techniques that have a smaller
variance which is why we will not dwell any further on this topic.

\subsubsection*{Maximum Likelihood
Estimation}\label{maximum-likelihood-estimation}
\addcontentsline{toc}{subsubsection}{Maximum Likelihood Estimation}

In maximum likelihood estimation (MLE) we seek to estimate unknown
parameters choosing them such that the likelihood of drawing the sample
observed is maximized. This probability is given by the likelihood
function, the joint probability distribution of the data treated as a
function of the unknown parameters. Put differently maximum likelihood
of unkown parameters are values that are the most likely one to have
produced the data observed. MLE is more efficient than NLS.

As the MLE is normally distributed in large samples, statistical
inference for coefficients in nonlinear models like logit and probit
regression can be made using the same tools that are used for linear
regression models: we can compute \(t\)-statistics and confidence
intervals.

Many software packages use an MLE algorithm for estimation of nonlinear
models. The function \texttt{glm()} uses an algorithm named
\emph{Iteratively Reweighted Least Squares}.

\subsubsection*{Measures of Fit}\label{measures-of-fit-1}
\addcontentsline{toc}{subsubsection}{Measures of Fit}

It is important to be aware that the usual \(R^2\) and
\(\overline{R^2}\) are \textbf{invalid} for nonlinear regression models.
The reason for this is simple: both measure are derived under the
assumption that the relation between the dependent and the explanatory
variable(s) is linear. This obviously does not hold for probit and logit
models such that \(R^2\) does not fall between \(0\) and \(1\) and there
is no meaningful interpretation. However, statistical softare often
reports these measures anyhow.

For example, consider the summary of the model object
\texttt{denyprobit} produced by \texttt{summary()} for
\eqref{eq:denyprobit}, the probit regression of \(deny\) on \(P/I \ rat\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(denyprobit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = deny ~ pirat, family = binomial(link = "probit"), 
##     data = HMDA)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4140  -0.5281  -0.4750  -0.3900   2.8159  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  -2.1941     0.1378 -15.927  < 2e-16 ***
## pirat         2.9679     0.3858   7.694 1.43e-14 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1744.2  on 2379  degrees of freedom
## Residual deviance: 1663.6  on 2378  degrees of freedom
## AIC: 1667.6
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

Besides reporting wrong standard errors (remember that it is inevitable
to use heteroskedasticity robust standard errors for probit and logit
models), the output also reports \(R^2\) and \(\overline{R^2}\) although
neither of these are valid for this model.

The take away message is that one should be aware which measures are
reported by functions like \texttt{summary()} and whether these measures
are adequate for the model at hand.

There are many measures of fit for nonlinear regression models and there
is no consensus which one should be reported. The situation is even more
complicated because not there is no measure of fit that generally
applicable. For models with a binary response variable like \(deny\) one
could use the following rule: \newline If \(Y_i = 1\) and
\(\widehat{P(Y_i|X_{i1}, \dots, X_{ik})} > 0.5\) or if \(Y_i = 0\) and
\(\widehat{P(Y_i|X_{i1}, \dots, X_{ik})} < 0.5\), consider the \(Y_i\)
as correctly predicted. Otherwise \(Y_i\) is said to be incorrectly
predicted. The measure of fit is the share of correctly predicted
observations. The downside of such an approach is that it does not yield
information on the quality of the prediction.

An alternative are so called pseudo-\(R^2\) measures. In order to
measure quality of fit, these measures compare the value of the
maximized (log-)likelihood of the model with all regressors (the
\emph{full model}) to the likelihood of a model with no regressors
(\emph{null model}, regression on a constant).

Consider a probit regression. The \(\text{pseudo-}R^2\) is given by
\[\text{pseudo-}R^2 = 1 - \frac{\ln(f^{max}_{full})}{\ln(f^{max}_{null})}\]
where \(f^{max}_j \in [0,1]\) denotes the maximized likelihood for model
\(j\).

The reasoning behind this is that the maximized likelihood increases as
additional regressors are added to the model, similarly to thr decrease
in \(SSR\) when regressors are added in a linear regression model. If
the full model has a similar maximized likelihood than the null model,
the full model does not really improve upon on a model that uses only
the information in the dependent variable and
\(\text{pseudo-}R^2 \approx 0\). If the full model fits the data very
good, the maximized likelihood shoud be close \(1\) such that
\(\ln(f^{max}_{full}) \approx 0\) and \(\text{pseudo-}R^2 \approx 1\).
See Appendix 11.2 of the book for more on MLE and pseudo-\(R^2\)
measures.

\texttt{summary()} does not report \(\text{pseudo-}R^2\) for models
estimated by \texttt{glm()} but we can use the entries \emph{residual
deviance} (\texttt{deviance}) and \emph{null deviance}
(\texttt{null.deviance}). These are computed as
\[\text{deviance} = -2 \times \left[\ln(f^{max}_{saturated}) - \ln(f^{max}_{full}) \right]\]
and

\[\text{null deviance} = -2 \times \left[\ln(f^{max}_{saturated}) - \ln(f^{max}_{null}) \right]\]
where \(f^{max}_{saturated}\) is the maximized likelihood for a model
that assumes each observations has its own parameter (\(n+1\) parameters
to be estimated which leads to a perfect fit). For models with binrary
dependent variables, it holds that
\[\text{pseudo-}R^2 = 1 - \frac{\text{deviance}}{\text{null deviance}} = 1- \frac{\ln(f^{max}_{full})}{\ln(f^{max}_{null})} \]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pseudoR2 <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{(denyprobit2}\OperatorTok{$}\NormalTok{deviance) }\OperatorTok{/}\StringTok{ }\NormalTok{(denyprobit2}\OperatorTok{$}\NormalTok{null.deviance)}
\NormalTok{pseudoR2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08594259
\end{verbatim}

Another way to obtain the \(\text{pseudo-}R^2\) is to estimate the null
model using \texttt{glm()} and extract maximized log-likelihoods for
both the null and the full model using the function \texttt{logLik()}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute null model}
\NormalTok{denyprobit_null <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ deny }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"probit"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ HMDA)}

\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{logLik}\NormalTok{(denyprobit2)[}\DecValTok{1}\NormalTok{]}\OperatorTok{/}\KeywordTok{logLik}\NormalTok{(denyprobit_null)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08594259
\end{verbatim}

\section{Application to the Boston HMDA
Data}\label{application-to-the-boston-hmda-data}

Models \eqref{eq:denyprobit2} and \eqref{eq:denylogit2} indicate that denial
rates are higher for African American applicants holding constant
payment-to-income ratio. Both results could be wrong due to omitted
variable bias. In order to obtain a more trustworthy estimate of the
effect of being black on the probability of a mortgage application
denial we estimate a linear probability model as well as several logit
and probit models that control for financial variables and additional
applicant characteristics which are likely to determine the probability
of denial and differ between black and white applicants.

Sample averages as shown in table 11.1 of the book can be easily
reproduced using the functions \texttt{mean()} (as usual for numeric
variables) and \texttt{prop.table()} (for factor variables).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Mean P/I ratio}
\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{pirat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3308136
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# inhouse expense-to-total-income ratio}
\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{hirat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2553461
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# loan-to-value ratio}
\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{lvrat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7377759
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# consumer credit score}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{chist))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.116387
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# mortgage credit score}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{mhist))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.721008
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# public bad credit record}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{phist)}\OperatorTok{-}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.07352941
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# denied mortgage insurance}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{insurance))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##         no        yes 
## 0.97983193 0.02016807
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# self-employed}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{selfemp))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##        no       yes 
## 0.8836134 0.1163866
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# single}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{single))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##        no       yes 
## 0.6067227 0.3932773
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# high school dimploma}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{hschool))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##         no        yes 
## 0.01638655 0.98361345
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# unemployment rate}
\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{unemp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.774496
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# condominium}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{condomin))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##        no       yes 
## 0.7117647 0.2882353
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# black}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{black))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##       no      yes 
## 0.857563 0.142437
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# deny}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{deny))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##         0         1 
## 0.8802521 0.1197479
\end{verbatim}

See Chapter 11.4 of the book or use the R's help function for more info
on variables contained in the \texttt{HMDA} dataset.

We transform loan-to-value ratio (\texttt{lvrat}) into a factor variable
where

\begin{align*}
  lvrat = 
  \begin{cases}
    \text{low} & \text{if} \ \ lvrat < 0.8, \\
    \text{medium} & \text{if} \ \ 0.8 \leq lvrat \leq 0.95, \\
    \text{high} & \text{if} \ \ lvrat > 0.95.
  \end{cases}
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define low, medium and high loan-to-value ratio}
\NormalTok{HMDA}\OperatorTok{$}\NormalTok{lvrat <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}
  \KeywordTok{ifelse}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{lvrat }\OperatorTok{<}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\StringTok{"low"}\NormalTok{,}
  \KeywordTok{ifelse}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{lvrat }\OperatorTok{>=}\StringTok{ }\FloatTok{0.8} \OperatorTok{&}\StringTok{ }\NormalTok{HMDA}\OperatorTok{$}\NormalTok{lvrat }\OperatorTok{<=}\StringTok{ }\FloatTok{0.95}\NormalTok{, }\StringTok{"medium"}\NormalTok{, }\StringTok{"high"}\NormalTok{)),}
  \DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"low"}\NormalTok{, }\StringTok{"medium"}\NormalTok{, }\StringTok{"high"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{# convert credit scores to numeric}
\NormalTok{HMDA}\OperatorTok{$}\NormalTok{mhist <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{mhist)}
\NormalTok{HMDA}\OperatorTok{$}\NormalTok{chist <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{chist)}
\end{Highlighting}
\end{Shaded}

In the next step we reproduce the estimation results presented in table
11.2 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate all 6 models for the denial probability}
\NormalTok{lpm_HMDA <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{black }\OperatorTok{+}\StringTok{ }\NormalTok{pirat }\OperatorTok{+}\StringTok{ }\NormalTok{hirat }\OperatorTok{+}\StringTok{ }\NormalTok{lvrat }\OperatorTok{+}\StringTok{ }\NormalTok{chist }\OperatorTok{+}\StringTok{ }\NormalTok{mhist }\OperatorTok{+}\StringTok{ }\NormalTok{phist }
               \OperatorTok{+}\StringTok{ }\NormalTok{insurance }\OperatorTok{+}\StringTok{ }\NormalTok{selfemp, }\DataTypeTok{data =}\NormalTok{ HMDA)}

\NormalTok{logit_HMDA <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{black }\OperatorTok{+}\StringTok{ }\NormalTok{pirat }\OperatorTok{+}\StringTok{ }\NormalTok{hirat }\OperatorTok{+}\StringTok{ }\NormalTok{lvrat }\OperatorTok{+}\StringTok{ }\NormalTok{chist }\OperatorTok{+}\StringTok{ }\NormalTok{mhist }\OperatorTok{+}\StringTok{ }\NormalTok{phist }
                  \OperatorTok{+}\StringTok{ }\NormalTok{insurance }\OperatorTok{+}\StringTok{ }\NormalTok{selfemp, }
                  \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ HMDA)}

\NormalTok{probit_HMDA_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{black }\OperatorTok{+}\StringTok{ }\NormalTok{pirat }\OperatorTok{+}\StringTok{ }\NormalTok{hirat }\OperatorTok{+}\StringTok{ }\NormalTok{lvrat }\OperatorTok{+}\StringTok{ }\NormalTok{chist }\OperatorTok{+}\StringTok{ }\NormalTok{mhist }\OperatorTok{+}\StringTok{ }\NormalTok{phist }
                     \OperatorTok{+}\StringTok{ }\NormalTok{insurance }\OperatorTok{+}\StringTok{ }\NormalTok{selfemp, }
                     \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"probit"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ HMDA)}

\NormalTok{probit_HMDA_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{black }\OperatorTok{+}\StringTok{ }\NormalTok{pirat }\OperatorTok{+}\StringTok{ }\NormalTok{hirat }\OperatorTok{+}\StringTok{ }\NormalTok{lvrat }\OperatorTok{+}\StringTok{ }\NormalTok{chist }\OperatorTok{+}\StringTok{ }\NormalTok{mhist }\OperatorTok{+}\StringTok{ }\NormalTok{phist }
                     \OperatorTok{+}\StringTok{ }\NormalTok{insurance }\OperatorTok{+}\StringTok{ }\NormalTok{selfemp }\OperatorTok{+}\StringTok{ }\NormalTok{single }\OperatorTok{+}\StringTok{ }\NormalTok{hschool }\OperatorTok{+}\StringTok{ }\NormalTok{unemp, }
                     \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"probit"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ HMDA)}

\NormalTok{probit_HMDA_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{black }\OperatorTok{+}\StringTok{ }\NormalTok{pirat }\OperatorTok{+}\StringTok{ }\NormalTok{hirat }\OperatorTok{+}\StringTok{ }\NormalTok{lvrat }\OperatorTok{+}\StringTok{ }\NormalTok{chist }\OperatorTok{+}\StringTok{ }\NormalTok{mhist }
                     \OperatorTok{+}\StringTok{ }\NormalTok{phist }\OperatorTok{+}\StringTok{ }\NormalTok{insurance }\OperatorTok{+}\StringTok{ }\NormalTok{selfemp }\OperatorTok{+}\StringTok{ }\NormalTok{single }\OperatorTok{+}\StringTok{ }\NormalTok{hschool }\OperatorTok{+}\StringTok{ }\NormalTok{unemp }\OperatorTok{+}\StringTok{ }\NormalTok{condomin }
                     \OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(mhist}\OperatorTok{==}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(mhist}\OperatorTok{==}\DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(chist}\OperatorTok{==}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(chist}\OperatorTok{==}\DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(chist}\OperatorTok{==}\DecValTok{5}\NormalTok{) }
                     \OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(chist}\OperatorTok{==}\DecValTok{6}\NormalTok{), }
                     \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"probit"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ HMDA)}

\NormalTok{probit_HMDA_}\DecValTok{4}\NormalTok{ <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(deny }\OperatorTok{~}\StringTok{ }\NormalTok{black }\OperatorTok{*}\StringTok{ }\NormalTok{(pirat }\OperatorTok{+}\StringTok{ }\NormalTok{hirat) }\OperatorTok{+}\StringTok{ }\NormalTok{lvrat }\OperatorTok{+}\StringTok{ }\NormalTok{chist }\OperatorTok{+}\StringTok{ }\NormalTok{mhist }\OperatorTok{+}\StringTok{ }\NormalTok{phist }
                     \OperatorTok{+}\StringTok{ }\NormalTok{insurance }\OperatorTok{+}\StringTok{ }\NormalTok{selfemp }\OperatorTok{+}\StringTok{ }\NormalTok{single }\OperatorTok{+}\StringTok{ }\NormalTok{hschool }\OperatorTok{+}\StringTok{ }\NormalTok{unemp, }
                     \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"probit"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ HMDA)}
\end{Highlighting}
\end{Shaded}

Just as in previous chapters, we use the package \texttt{sandwich} for
computation of heteroskedasticity-robust standard errors of the
coefficient estimators in all models and store these in a a list which
is then supplied as the argument \texttt{se} in \texttt{stargazer()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stargazer)}
\KeywordTok{library}\NormalTok{(sandwich)}

\NormalTok{rob_se <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(lpm_HMDA, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(logit_HMDA, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(probit_HMDA_}\DecValTok{1}\NormalTok{, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(probit_HMDA_}\DecValTok{2}\NormalTok{, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(probit_HMDA_}\DecValTok{3}\NormalTok{, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(probit_HMDA_}\DecValTok{4}\NormalTok{, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{)))}
\NormalTok{)}

\KeywordTok{stargazer}\NormalTok{(lpm_HMDA, logit_HMDA, probit_HMDA_}\DecValTok{1}\NormalTok{, probit_HMDA_}\DecValTok{2}\NormalTok{, probit_HMDA_}\DecValTok{3}\NormalTok{, probit_HMDA_}\DecValTok{4}\NormalTok{, }
          \DataTypeTok{digits =} \DecValTok{3}\NormalTok{,}
          \DataTypeTok{type =} \StringTok{"latex"}\NormalTok{, }
          \DataTypeTok{se =}\NormalTok{ rob_se,}
          \DataTypeTok{model.numbers =} \OtherTok{FALSE}\NormalTok{,}
          \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"(1)"}\NormalTok{, }\StringTok{"(2)"}\NormalTok{, }\StringTok{"(3)"}\NormalTok{, }\StringTok{"(4)"}\NormalTok{, }\StringTok{"(5)"}\NormalTok{, }\StringTok{"(6)"}\NormalTok{)}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

Dependent variable:

deny

OLS

logistic

probit

(1)

(2)

(3)

(4)

(5)

(6)

blackyes

0.084***

0.688***

0.389***

0.371***

0.363***

0.246

(0.023)

(0.183)

(0.099)

(0.100)

(0.101)

(0.479)

pirat

0.449***

4.764***

2.442***

2.464***

2.622***

2.572***

(0.114)

(1.332)

(0.673)

(0.654)

(0.665)

(0.728)

hirat

-0.048

-0.109

-0.185

-0.302

-0.502

-0.538

(0.110)

(1.298)

(0.689)

(0.689)

(0.715)

(0.755)

lvratmedium

0.031**

0.464***

0.214***

0.216***

0.215**

0.216***

(0.013)

(0.160)

(0.082)

(0.082)

(0.084)

(0.083)

lvrathigh

0.189***

1.495***

0.791***

0.795***

0.836***

0.788***

(0.050)

(0.325)

(0.183)

(0.184)

(0.185)

(0.185)

chist

0.031***

0.290***

0.155***

0.158***

0.344***

0.158***

(0.005)

(0.039)

(0.021)

(0.021)

(0.108)

(0.021)

mhist

0.021*

0.279**

0.148**

0.110

0.162

0.111

(0.011)

(0.138)

(0.073)

(0.076)

(0.104)

(0.077)

phistyes

0.197***

1.226***

0.697***

0.702***

0.717***

0.705***

(0.035)

(0.203)

(0.114)

(0.115)

(0.116)

(0.115)

insuranceyes

0.702***

4.548***

2.557***

2.585***

2.589***

2.590***

(0.045)

(0.576)

(0.305)

(0.299)

(0.306)

(0.299)

selfempyes

0.060***

0.666***

0.359***

0.346***

0.342***

0.348***

(0.021)

(0.214)

(0.113)

(0.116)

(0.116)

(0.116)

singleyes

0.229***

0.230***

0.226***

(0.080)

(0.086)

(0.081)

hschoolyes

-0.613***

-0.604**

-0.620***

(0.229)

(0.237)

(0.229)

unemp

0.030*

0.028

0.030

(0.018)

(0.018)

(0.018)

condominyes

-0.055

(0.096)

I(mhist == 3)

-0.107

(0.301)

I(mhist == 4)

-0.383

(0.427)

I(chist == 3)

-0.226

(0.248)

I(chist == 4)

-0.251

(0.338)

I(chist == 5)

-0.789*

(0.412)

I(chist == 6)

-0.905*

(0.515)

blackyes:pirat

-0.579

(1.550)

blackyes:hirat

1.232

(1.709)

Constant

-0.183***

-5.707***

-3.041***

-2.575***

-2.896***

-2.543***

(0.028)

(0.484)

(0.250)

(0.350)

(0.404)

(0.370)

Observations

2,380

2,380

2,380

2,380

2,380

2,380

R2

0.266

Adjusted R2

0.263

Log Likelihood

-635.637

-636.847

-628.614

-625.064

-628.332

Akaike Inf. Crit.

1,293.273

1,295.694

1,285.227

1,292.129

1,288.664

Residual Std. Error

0.279 (df = 2369)

F Statistic

85.974*** (df = 10; 2369)

Note:

\emph{p\textless{}0.1; \textbf{p\textless{}0.05; }}p\textless{}0.01

Models (1), (2) and (3) are base specifications that include several
financial control variables. They differ only in the way they model the
denial probability. (1) is linear probability model, (2) is a logistic
regression and (3) uses the probit approach.

Since model (1) is a linear model, the coefficients have direct
interpretation. For example, an increase in the consumer credit score by
\(1\) unit is estimated to increase the probability of a loan denial by
about \(0.031\) percentage points. We also find that having a high
loan-to-value ratio is predicted to be not conductive for credit
approval: the coefficient for a loan-to-value ratio higher than \(0.95\)
is \(0.189\) so clients with this property ceteris paribus are estimated
to face an almost \(19\%\) larger risk of denial than those with a low
loan-to-value ratio. For the linear probability model, the coefficient
on black is estimated to be \(0.084\) which indicates the denial
probability for African Americans is \(8.4\%\) larger than for white
applicants with the same characteristics except for race. Apart from
housing expense-to-income ratio and the mortgage credit score, all
coefficients are significant.

Models (2) and (3) provide similar evidence that there is racial
discrimination in the US mortgage market. All coefficients except for
housing expense-to-income ratio (which is not significantly different
from zero) are signiicant at \(1\%\) level. As discussed above, the
nonlinearity makes interpretation of coefficient estimates more
difficult than for model (1). In order to make a statement about the
effect of beeing black, we need to compute the estimated denial
probability for two individuals that differ only in race. For the
comparison we consider two individuals that share mean values for all
numeric regressors. For all qualitative variables we assign the property
that is the most representative for the data at hand. As an example,
consider self-employment: we have seen that about \(88\%\) of all
individuals in the sample are not self-employed such that we set
\texttt{selfemp\ =\ no}. Using this approach, the estimate for the
effect on the denial probability of being African American obtained when
using the logit model (2) is about \(4\%\). The next code chunk shows
how to apply this approach for models (1) to (7) using \texttt{R}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# regressor values for average person, black = "yes"}
\NormalTok{new <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \StringTok{"pirat"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{pirat),}
  \StringTok{"hirat"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{hirat),}
  \StringTok{"lvrat"}\NormalTok{ =}\StringTok{ "low"}\NormalTok{,}
  \StringTok{"chist"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{chist),}
  \StringTok{"mhist"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{mhist),}
  \StringTok{"phist"}\NormalTok{ =}\StringTok{ "no"}\NormalTok{,}
  \StringTok{"insurance"}\NormalTok{ =}\StringTok{ "no"}\NormalTok{,}
  \StringTok{"selfemp"}\NormalTok{ =}\StringTok{ "no"}\NormalTok{,}
  \StringTok{"black"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\NormalTok{),}
  \StringTok{"single"}\NormalTok{ =}\StringTok{ "no"}\NormalTok{,}
  \StringTok{"hschool"}\NormalTok{ =}\StringTok{ "yes"}\NormalTok{,}
  \StringTok{"unemp"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(HMDA}\OperatorTok{$}\NormalTok{unemp),}
  \StringTok{"condomin"}\NormalTok{ =}\StringTok{ "no"}
\NormalTok{)}

\CommentTok{# differnce predicted by the LPM}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lpm_HMDA, }\DataTypeTok{newdata =}\NormalTok{ new)}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          2 
## 0.08369674
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# differnce predicted by the logit model}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(logit_HMDA, }\DataTypeTok{newdata =}\NormalTok{ new, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          2 
## 0.04042135
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# difference predicted by probit model (3)}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(probit_HMDA_}\DecValTok{1}\NormalTok{, }\DataTypeTok{newdata =}\NormalTok{ new, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          2 
## 0.05049716
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# difference predicted by probit model (4)}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(probit_HMDA_}\DecValTok{2}\NormalTok{, }\DataTypeTok{newdata =}\NormalTok{ new, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          2 
## 0.03978918
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# difference predicted by probit model (5)}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(probit_HMDA_}\DecValTok{3}\NormalTok{, }\DataTypeTok{newdata =}\NormalTok{ new, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          2 
## 0.04972468
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# difference predicted by probit model (6)}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(probit_HMDA_}\DecValTok{4}\NormalTok{, }\DataTypeTok{newdata =}\NormalTok{ new, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\KeywordTok{diff}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          2 
## 0.03955893
\end{verbatim}

The estimates of the impact on the denial probability of being black are
similar for models (2) and (3). In particular, it is interesting that
the magnitude of the estimated effects is much smaller than for probit
and logit models that do not control for financial characteristics (see
section 11.2). This indicates that these simple models produce biased
estimates due to the omitted variables.

Regressions (4) to (6) use regression specifications that include
different sets of applicant characteristics and credit rating indicator
variables as well as interactions. However, most of the corresponding
coefficients are not significant and the estimates of the coefficient on
\texttt{black} obtained for these models as well as the estimated
difference in denial probabilities do not deviate much from those
obtained for the similar specifications (2) and (3).

An intersting question related to racial discrimination can be
investigated using the probit model (6) where the interactions
\texttt{blackyes:pirat} and \texttt{blackyes:hirat} are added to the the
specification of model (4). If the coefficient on
\texttt{blackyes:pirat} was different from zero, the effect of the
payment-to-income ratio on the denial probability would be different for
black and white applicants. Similarly, a non-zero coefficient on
\texttt{blackyes:hirat} would indicate that loan officers weight the
risk of bankrupt associated with a high loan-to-value ratio differently
for black and white mortgage applicants. We can test whether these
coefficients are jointly significant at the \(5\%\) level using an
\(F\)-Test.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{linearHypothesis}\NormalTok{(probit_HMDA_}\DecValTok{4}\NormalTok{,}
                 \DataTypeTok{test =} \StringTok{"F"}\NormalTok{,}
                 \KeywordTok{c}\NormalTok{(}\StringTok{"blackyes:pirat=0"}\NormalTok{,}\StringTok{"blackyes:hirat=0"}\NormalTok{),}
                 \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(probit_HMDA_}\DecValTok{4}\NormalTok{, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## blackyes:pirat = 0
## blackyes:hirat = 0
## 
## Model 1: restricted model
## Model 2: deny ~ black * (pirat + hirat) + lvrat + chist + mhist + phist + 
##     insurance + selfemp + single + hschool + unemp
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(>F)
## 1   2366                 
## 2   2364  2 0.2616 0.7698
\end{verbatim}

Since \(p\text{-value} \approx 0.77\) for this test, the null cannot be
rejected. Nonetheless we cannot reject the hypothesis that there is no
racial discrimination at all since the corresponding \(F\)-test has a
\(p\text{-value}\) of about \(0.002\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{linearHypothesis}\NormalTok{(probit_HMDA_}\DecValTok{4}\NormalTok{,}
                 \DataTypeTok{test =} \StringTok{"F"}\NormalTok{,}
                 \KeywordTok{c}\NormalTok{(}\StringTok{"blackyes=0"}\NormalTok{,}\StringTok{"blackyes:pirat=0"}\NormalTok{,}\StringTok{"blackyes:hirat=0"}\NormalTok{),}
                 \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(probit_HMDA_}\DecValTok{4}\NormalTok{, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## blackyes = 0
## blackyes:pirat = 0
## blackyes:hirat = 0
## 
## Model 1: restricted model
## Model 2: deny ~ black * (pirat + hirat) + lvrat + chist + mhist + phist + 
##     insurance + selfemp + single + hschool + unemp
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F   Pr(>F)   
## 1   2367                      
## 2   2364  3 4.8886 0.002168 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\subsubsection*{Summary}\label{summary-3}
\addcontentsline{toc}{subsubsection}{Summary}

Models (1) to (6) provide evidence that there is an effect of being
African American on the probability of a mortgage application denial: in
all specifications, the effect is estimated to be positive (ranging from
\(4\%\) to \(5\%\) for the fictive individuals considered) and
significantly different from zero at the \(1\%\) level. While the linear
probability model seems to overestimate this effect slightly, it still
can be used as an approximation to an intrinsic nonlinear relationship.

See Chapters 11.4 and 11.5 of the book for a discussion of external and
internal validity of this study and some concluding remarks on
regression models where the dependent variable is binary.

\chapter{Instrumental Variables
Regression}\label{instrumental-variables-regression}

As discussed in Chapter 9, regression models may suffer from problems
like omitted variables, measurement errors and simultaneous causality.
If so, the error term is correlated with the regressor of interest and
there is a bias in estimation of the corresponding coefficients. If data
is available, what we have assumed up till now, omitted varibales can be
included in the regression to mitigate the risk of biased estimation of
the causal effect of interest. However, if omitted factors cannot be
measured, multiple regression cannot solve the problem. The same issue
arises if there is simultaneous causality. When causality runs from
\(X\) to \(Y\) and vice versa, there will be an estimation bias that
cannot be corrected for by multiple regression.

A general technique for obtaining a consistent estimator of the
coefficient of interest is instrumental variables (IV) regression. In
this chapter we focus on a powerful IV regression tool called two stage
least squares (TSLS). The first sections briefly recap the general
mechanics and assumptions of IV regression and show how to apply TSLS
estimation using \texttt{R}. Next, IV regression is used for estimating
the elasticity of demand for cigarettes --- a classical example where
multiple regression fails to do the job because of simultaneous
causality.

\hypertarget{TIVEWASRAASI}{\section{The IV Estimator with a Single
Regressor and a Single Instrument}\label{TIVEWASRAASI}}

Cosider the simple regression model

\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + u_i \ \ , \ \ i=1,\dots,n  \label{eq:srm12}
\end{align}

where the error term \(u_i\) is correlated with the regressor \(X_i\)
(we say that \(X\) is \emph{endogenous}) such that OLS is inconsistent
for the true \(\beta_1\). In the most simple case, IV regression uses a
single instrumental variable \(Z\) to obtain a consistent estimator for
\(\beta_1\).

\(Z\) must satisfy two condictions to be a valid instrument:

\textbf{1. Instrument relevance condition}:

\(X\) and its instrument \(Z\) \emph{must be} correlated:
\(corr(Z_i,X_i) \neq 0\).

\textbf{2. Instrument exogeneity condition}:

The instrument \(Z\) \emph{must not be} correlated with the error term
\(u\): \(corr(Z_i,u_i) = 0\).

\subsubsection*{The Two Stage Least Squares
Estimator}\label{the-two-stage-least-squares-estimator}
\addcontentsline{toc}{subsubsection}{The Two Stage Least Squares
Estimator}

As can be guessed from its name, TSLS has to stages. In the first stage,
the variation in the endogenous regressor \(X\) is decomposed into a
problem-free component that is explained by the instrument \(Z\) and a
problematic component that is correlated with the error \(u_i\). The
second stage uses the problem-free component of the variation in \(X\)
to estimate \(\beta_1\).

The first stage regression model is \[X_i = \pi_0 + \pi_1 Z_i + \nu_i\]
where \(\pi_0 + \pi_1 Z_i\) is the component of \(X_i\) that is
explained by \(Z_i\) while \(\nu_i\) is the component that cannot be
explained by \(Z_i\) and exhibits correlation with \(u_i\).

Using OLS estimates \(\widehat{\pi}_0\) and \(\widehat{\pi}_1\) we
obtain predicted values \(\widehat{X}_i \ \ , \ \ i=1,\dots,n\). If
\(Z\) is a valid instrument, the \(\widehat{X}_i\) are problem-free in
the sense that \(\widehat{X}\) is exogeneous in a regression of \(Y\) on
\(\widehat{X}\) what is done in the second stage regression. The second
stage produces \(\widehat{\beta}_0^{TSLS}\) and
\(\widehat{\beta}_1^{TSLS}\), the TSLS estimates of \(\beta_0\) and
\(\beta_1\).

For the case of a single instrument one can show that the TSLS estimator
of \(\beta_1\) is

\begin{align}
\widehat{\beta}_1^{TSLS} = \frac{s_{ZY}}{s_{ZX}} = \frac{\frac{1}{n-1}\sum_{i=1}^n(Y_i - \overline{Y})(Z_i - \overline{Z})}{\frac{1}{n-1}\sum_{i=1}^n(X_i - \overline{X})(Z_i - \overline{Z})} \label{eq:simpletsls}
\end{align}

which is nothing but the ratio of the sample covariance between \(Z\)
and \(Y\) to the sample covariance between \(Z\) and \(X\).

As shown in Appendix 12.3 of the book, \eqref{eq:simpletsls} is a
consistent estimator for \(\beta_1\) in \eqref{eq:srm12} under the
assumption that \(Z\) is a valid instrument. Furthermore, as for every
other OLS estimator that we have considered so far, the central limit
theorem implies that \(\widehat{\beta}_1^{TSLS}\) can be approximated by
a normal distribution if the sample size is large. This allows us to use
\(t\)-statistics and confidence intervals which are also computed by
\texttt{R} functions relevant to us. A more detailed argument on the
large-sample distribution of the TSLS estimator is sketched out in
Appendix 12.3, too.

\subsubsection*{Application to the Demand For
Cigarettes}\label{application-to-the-demand-for-cigarettes}
\addcontentsline{toc}{subsubsection}{Application to the Demand For
Cigarettes}

The relation between demand and price of commodities is a simple yet
widespread problem in many branches of economics. The field of health
economics is inter alia concernced with the study of how
health-affecting behaviour of individuals is influenced by the
health-care system and regulation policy. The probably most prominent
example in public policy debates is smoking as it is related to numerous
illnesses and negative externalities.

It is plausible that cigarette consumption can be reduced by taxing
cigarettes more heavily. The question is how much taxes must be increase
to reach a noticeable reduction in cigarette consumption, e.g. \(20\)
percentage points. Economists use elasticities to answer this kind of
question. Since the price elasticity for demand of cigarettes is
unknown, it must be estimated. As discussed in the box Who Invented
Instrumental Variables Regression presented in Chapter 12.1 of the book,
an OLS regression of log quantity on log price cannot be used to
estimate the effect of interest since there is simultaneous causality
between demand and supply. Instead, IV regression can be used.

We use the data \texttt{CigarettesSW} which comes with the package
\texttt{AER}. It is a panel data set that contains observations on
cigarette consumption and several economic indicators for all 48
continental US States from 1985 to 1995. Following the book, in what
follows we consider data for the cross section of states in 1995 only.

We start by loading the package, attaching the data set and getting an
overview.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(}\StringTok{"CigarettesSW"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(CigarettesSW)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      state      year         cpi          population      
##  AL     : 2   1985:48   Min.   :1.076   Min.   :  478447  
##  AR     : 2   1995:48   1st Qu.:1.076   1st Qu.: 1622606  
##  AZ     : 2             Median :1.300   Median : 3697472  
##  CA     : 2             Mean   :1.300   Mean   : 5168866  
##  CO     : 2             3rd Qu.:1.524   3rd Qu.: 5901500  
##  CT     : 2             Max.   :1.524   Max.   :31493524  
##  (Other):84                                               
##      packs            income               tax            price       
##  Min.   : 49.27   Min.   :  6887097   Min.   :18.00   Min.   : 84.97  
##  1st Qu.: 92.45   1st Qu.: 25520384   1st Qu.:31.00   1st Qu.:102.71  
##  Median :110.16   Median : 61661644   Median :37.00   Median :137.72  
##  Mean   :109.18   Mean   : 99878736   Mean   :42.68   Mean   :143.45  
##  3rd Qu.:123.52   3rd Qu.:127313964   3rd Qu.:50.88   3rd Qu.:176.15  
##  Max.   :197.99   Max.   :771470144   Max.   :99.00   Max.   :240.85  
##                                                                       
##       taxs       
##  Min.   : 21.27  
##  1st Qu.: 34.77  
##  Median : 41.05  
##  Mean   : 48.33  
##  3rd Qu.: 59.48  
##  Max.   :112.63  
## 
\end{verbatim}

See \texttt{?CigarettesSW} for a detailed description of variables.

We are interested in estimating \(\beta_1\) in

\begin{align}
  \log(Q_i^{cigarettes}) = \beta_0 + \beta_1 \log(P_i^{cigarettes}) + u_i \label{eq:cigstsls}
\end{align}

where \(Q_i^{cigarettes}\) is the number of cigarette packs per capita
sold and \(P_i^{cigarettes}\) is the after tax average real price per
pack of cigarettes in state \(i\).

The instrumental variable we are going to use is \(SalesTax\), the
portion of taxes on cigarettes arising from the general sales tax.
\(SalesTax\) is measured in dollars per pack. The idea is that
\(SalesTax\) is a relevant instrument as it is included in the after tax
average price per pack. Also it is plausible that \(SalesTax\) is
exogeneous since the sales tax does not influence to quantity sold
directly but indirectly through the price.

Before computing the TSLS estimate for \(\beta_1\) we have to do some
transformations in order to obtain deflated cross section data for the
year 1995.

We also compute the sample correlation between the sales tax and price
per pack. Notice that the sample correlation is a consistent estimator
of the pupulation. The estimate of approximately \(0.614\) indicates
that \(SalesTax\) and \(P_i^{cigarettes}\) exhibit moderately positive
correlation which is in accordance with our expectations: higher sales
taxes lead to higher prices. However, a correlation analysis like this
is not sufficient for checking whether the instrument is relevant. We
will later come back to the issue of checking weather an instrument is
relevant and exogenous.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute real per capita prices}
\NormalTok{CigarettesSW}\OperatorTok{$}\NormalTok{rprice <-}\StringTok{ }\KeywordTok{with}\NormalTok{(CigarettesSW, price}\OperatorTok{/}\NormalTok{cpi)}

\CommentTok{# SalesTax}
\NormalTok{CigarettesSW}\OperatorTok{$}\NormalTok{salestax <-}\StringTok{ }\KeywordTok{with}\NormalTok{(CigarettesSW, (taxs }\OperatorTok{-}\StringTok{ }\NormalTok{tax)}\OperatorTok{/}\NormalTok{cpi)}

\CommentTok{# check correlation between sales tax and price}
\KeywordTok{cor}\NormalTok{(CigarettesSW}\OperatorTok{$}\NormalTok{salestax, CigarettesSW}\OperatorTok{$}\NormalTok{price)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6141228
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate a subset for the year 1995}
\NormalTok{c1995 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(CigarettesSW, year }\OperatorTok{==}\StringTok{ "1995"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The first stage regression is
\[\log(P_i^{cigarettes}) = \pi_0 + \pi_1 SalesTax_i + \nu_i.\] We
estimate this model in \texttt{R} using \texttt{lm()}. In the second
stage we run a regression of \(\log(Q_i^{cigarettes})\) on
\(\widehat{\log(P_i^{cigarettes})}\) to obtain
\(\widehat{\beta}_0^{TSLS}\) and \(\widehat{\beta}_1^{TSLS}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the sandwich package for robust standard errors}
\KeywordTok{library}\NormalTok{(sandwich)}

\CommentTok{# Stage 1 regression}
\NormalTok{cig_s1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(rprice) }\OperatorTok{~}\StringTok{ }\NormalTok{salestax, }\DataTypeTok{data =}\NormalTok{ c1995)}
\KeywordTok{coeftest}\NormalTok{(cig_s1, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(cig_s1, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error  t value  Pr(>|t|)    
## (Intercept) 4.6165463  0.0289177 159.6444 < 2.2e-16 ***
## salestax    0.0307289  0.0048354   6.3549 8.489e-08 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

For the first stage regression we obtain
\[\widehat{\log(P_i^{cigarettes})} = \underset{(0.03)}{4.62} + \underset{(0.005)}{0.031} SalesTax_i\]
which also predicts the relation between sales tax price per cigarettes
to be postive. How much of the observed variation in
\(\log(P^{cigarettes})\) is explained by the instrumental variable
\(SalesTax\)? This can be answered by looking at the regression's
\(R^2\) which states that about \(47\%\) of the variation in after tax
prices is explained by the variation of the sales tax across states.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(cig_s1)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4709961
\end{verbatim}

We continue by saving \(\widehat{\log(P_i^{cigarettes})}\), the fitted
values predicted by the first stage regression \texttt{cig\_s1}, to the
variable \texttt{lcigp\_pred}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Save predicted values}
\NormalTok{lcigp_pred <-}\StringTok{ }\NormalTok{cig_s1}\OperatorTok{$}\NormalTok{fitted.values}
\end{Highlighting}
\end{Shaded}

Next, we run the second stage regression which gives the TSLS estimates
we seek.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Stage 2 regression}
\NormalTok{cig_s2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(c1995}\OperatorTok{$}\NormalTok{packs) }\OperatorTok{~}\StringTok{ }\NormalTok{lcigp_pred)}
\KeywordTok{coeftest}\NormalTok{(cig_s2, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(cig_s2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)  9.71988    1.70304  5.7074 7.932e-07 ***
## lcigp_pred  -1.08359    0.35563 -3.0469  0.003822 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Thus estimating the model equation \eqref{eq:cigstsls} using TSLS yields

\begin{align}
  \widehat{\log(Q_i^{cigarettes})} = \underset{(1.70)}{9.72} + \underset{(0.36)}{1.08} \log(P_i^{cigarettes}) \label{eq:ecigstsls}
\end{align}

where we write \(\log(P_i^{cigarettes})\) instead of
\(\widehat{\log(P_i^{cigarettes})}\) for convenience.

The function \texttt{ivreg()} from the package \texttt{AER} carries out
TSLS automatically. It is used similarly as \texttt{lm()}. Instruments
can be added to the usual specification of the regression using a \(|\)
seperating the model equation from the instruments. Thus, for the
regression and hand the correct formula is
\texttt{log(packs)\ \textasciitilde{}\ log(rprice)\ \textbar{}\ salestax}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# TSLS using ivreg()}
\NormalTok{cig_ivreg <-}\StringTok{ }\KeywordTok{ivreg}\NormalTok{(}\KeywordTok{log}\NormalTok{(packs) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(rprice) }\OperatorTok{|}\StringTok{ }\NormalTok{salestax, }\DataTypeTok{data =}\NormalTok{ c1995)}

\KeywordTok{coeftest}\NormalTok{(cig_ivreg, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(cig_ivreg, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)  9.71988    1.52832  6.3598 8.346e-08 ***
## log(rprice) -1.08359    0.31892 -3.3977  0.001411 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We find that the coefficient estimates obtain coincide for both
approaches.

\textbf{Two notes on calculation of TSLS standard errors}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We have demonstrated that running the individual regressions for each
  stage of TSLS using lm() leads to the same coefficient estimates as
  when using ivreg(). However, the standard errors reported for the
  second-stage regression, e.g.~by coeftest() or summary(), are
  \emph{invalid}: they \emph{do not} adjust for using predictions from
  the first-stage regression as regressors in the second-stage
  regression! Fortunately, ivreg() performs the necessary adjustment
  automatically. This is another advantage over manual step-by-step
  estimation.
\item
  Just as in mutliple regression, it is important to compute
  heteroskedasticity-robust standard errors as we have done above using
  vcovHC().
\end{enumerate}

The TSLS estimate for \(\beta_1\) in \eqref{eq:ecigstsls} means that an
increase in cigarette prices by one percent reduces cigarette
consumption by roughly \(1.08\) percentage points which is fairly
elastic. However, we know that this estimate might not be trusworthy
although we used IV estimation: there still might be a bias due to
omitted variables. Thus a multiple IV regression approach is needed.

\hypertarget{TGIVRM}{\section{The General IV Regression
Model}\label{TGIVRM}}

The simple IV regression model is easily extended to a multiple
regression model which we refer to as the general IV regression model.
In this model we distinguish between four types of variables: the
dependent variable, included exogenous variables, included endogenous
variables and instrumental variables. Key Concept 12.1 summarizes the
model and the common terminology. See Chapter 12.2 of the book for a
more comprehensive discussion of the individual components of the
general model.

Key Concept 12.1

The General Instrumental Variables Regression Model and Terminology

\begin{align}
  Y_i = \beta_0 + \beta_1 X_{1i} + \dots + \beta_k X_{ki} + \beta_{k+1} W_{1i} + \beta_{k+r} W_{ri} + u_i, \label{eq:givmodel}
\end{align}

\(i=1,\dots,n\) is the general intrumental variables regression model
where

\begin{itemize}
\item
  \(Y_i\) is the dependent variable
\item
  \(\beta_1,\dots,\beta_k\) are \(k\) unknown regression coefficients
\item
  \(X_{1i},\dots,X_{ki}\) are \(k\) endogenous regressors which are
  potentially correlated with \(u_i\)
\item
  \(W_{1i},\dots,W_{ri}\) are \(r\) exogenous regressors which are
  uncorrelated with \(u_i\)
\item
  \(u_i\) is the error term
\item
  \(Z_{1i},\dots,Z_{mi}\) are \(m\) instrumental variables
\end{itemize}

We say that the coefficients are overidentified if \(m>k\) thats is if
there are more instruments than regressors. If \(m<k\), the coefficients
are underidentified and when \(m=k\) they are exactly identified. For
estimation of the IV regression model exact identification or
overidentification is required.

While computing both stages of TSLS individually is not a big deal in
the simple regression model with a single endogenous regressor
\eqref{eq:srm12}, Key Concept 12.2 clarifies why resorting to TSLS
estimation functions like \texttt{ivreg()} is much more convenient when
the set of potentially endogenous regressors (and instruments) is large.

Estimating regression models with TSLS using multiple instruments by
means of \texttt{ivreg()} is straightforward. There are, however, some
subtleties when it comes to correct specification of the regression
formula that should be mentioned.

Assume that you want to estimate the model
\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + W_1 + u\] where \(X_1\) and
\(X_2\) are endogenous regressors that shall be instrumented by \(Z_1\),
\(Z_2\) and \(Z_3\) and \(W_1\) is an exogenous regressor. The
corresponding data is available in a \texttt{data.frame} with coloumn
names \texttt{y}, \texttt{x1}, \texttt{x1}, \texttt{w1}, \texttt{z1},
\texttt{z2} and \texttt{z3}. It might be tempting to specify the
argument \texttt{formula} in your call of \texttt{ivreg()} as
\texttt{y\ \textasciitilde{}\ x1\ +\ x2\ +\ w1\ \textbar{}\ z1\ +\ z2\ +\ z3}
which is wrong. As explained in the documentation of \texttt{ivreg()}
(see \texttt{?ivreg}), it is necessary to list \emph{all} exogenous
variables as instruments, that is joining them by \texttt{+}'s on the
right of \texttt{\textbar{}}:
\texttt{y\ \textasciitilde{}\ x1\ +\ x2\ +\ w1\ \textbar{}\ w1\ +\ z1\ +\ z2\ +\ z3}
where \texttt{w1} is instrumenting itself.

If there is a large number of exogenous variables it may be convenient
to provide an update formula with a \texttt{.} (this includes all
variables except for the dependent variable) right after the
\texttt{\textbar{}} and to exclude all endogenous variables using
\texttt{-}.

Key Concept 12.2

Two Stage Least Squares

Similarly to the simple IV regression model, the gerneral IV model
\eqref{eq:givmodel} can be estimated using the TSLS estimator which is
computed in two stages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{First-stage regression(s)}: Run an OLS regression for each of
  the endogenous variables (\(X_{1i},\dots,X_{ki}\)) on all instrumental
  variables (\(Z_{1i},\dots,Z_{mi}\)) and all exogenous variables
  (\(W_{1i},\dots,W_{ri}\)) and an intercept. Compute the fitted values
  (\(\widehat{X}_{1i},\dots,\widehat{X}_{ki}\)).
\item
  \textbf{Second-stage regression}: OLS regression of the dependent
  variable on predicted values of all endogenous regressors and all
  exogenous variables and an intercept. This delivers
  \(\widehat{\beta}_{0}^{TSLS},\dots,\widehat{\beta}_{k+r}^{TSLS}\)
  which are called the TSLS estimates.
\end{enumerate}

In the general IV regression model the instrument relevance and
exogeneity assumptions are just as in the simple regression model with a
single endogenous regressor and only one instrument. See Key Concept
12.3 for a recap using terminology of gerneral IV regression.

Key Concept 12.3

Two Conditions for Valid Instruments

For \(Z_{1i},\dots,Z_{mi}\) to be a set of valid instruments, the
following two conditions must be fulfilled:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Instrument Relevance}: If there are \(k\) endogenous
  variables, \(r\) exogenous variables and \(m\geq k\) instruments \(Z\)
  and the \(\widehat{X}_{1i}^*,\dots,\widehat{X}_{ki}^*\) are the
  predicted values from the \(k\) population stage one regressions, it
  \emph{must hold that}
  \[(\widehat{X}_{1i}^*,\dots,\widehat{X}_{ki}^*, W_{1i}, \dots, W_{ri},1)\]
  are not perfectly multicollinear. \(1\) denotes the constant regressor
  which equals \(1\) for all observations.
\end{enumerate}

\emph{Note}: If there is only one endogenous regressor \(X_i\), there
must be at least one non-zero coefficient on the \(Z\)'s and the \(W\)'s
in the population regression for this condition to be valid: If all of
these coefficients are zero, all the \(\widehat{X}^*_i\) are just the
mean of \(X\) such that there is perfect multicollinearity.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Instrument Exogeneity}:
\end{enumerate}

All the \(m\) instruments are uncorrelated with the error term:

\[corr(Z_{1i},u_i) = 0,\dots,corr(Z_{mi},u_i) = 0\]

One can show that if the IV regression assumptions presented in Key
Concept 12.4 hold, the TSLS estimator in the general IV model
\eqref{eq:givmodel} is consistent and normally distributed when the sample
size is large. To get an idea of the proof for this statement, see
Appendix 12.3 of the book for a disussion of the special case with a
single regressor, a single instrument and no exogenous variables. The
reasoning behind this carries over to the general IV model. A more
complicated explanation for the general case can be found in Chapter 18
of the book.

For the purpose of this book it is sufficient to bear in mind that
validity of the assumptions stated in Key Concept 12.4 allows us to
obtain valid statistical inference using \texttt{R} functions used to
compute \(t\)-Tests, \(F\)-Tests and confidence intervals for model
coefficients.

Key Concept 12.4

The IV Regression Assumptions

For the general IV regression model in Key Concept 12.1 we assume that

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(E(u_i\vert W_{1i}, \dots, W_{ri}) = 0\)
\item
  \((X_{1i},\dots,X_{ki},W_{1i},\dots,W_{ri},Z_{1i},\dots,Z_{mi})\) are
  i.i.d. draws from their joint distribution
\item
  All \(X\)'s, \(W\)'s, \(Z\)'s and \(Y\)'s have nonzero finite fourth
  moments i.e.~outliers are unlikely
\item
  The \(Z\)'s are valid instruments (see Key Concept 12.3)
\end{enumerate}

\subsubsection*{Application to the Demand for
Cigarettes}\label{application-to-the-demand-for-cigarettes-1}
\addcontentsline{toc}{subsubsection}{Application to the Demand for
Cigarettes}

The estimated elasticity of demand for cigarettes in \eqref{eq:srm12} is
\(1.08\). Although \eqref{eq:srm12} was estimated using IV regression it
is plausible that this IV estimate is biased: in this model, the TSLS
estimator is inconsistent for the true \(\beta_1\) if the instrument
(the real sales tax per pack) correlates with the error term. This is
likely to be the case since there are economic facors like state income
which impact the demand for cigarettes and correlate with the sales tax.
States with high personal income tend to generate tax revenues more by
income taxes and less by sales taxes. Consequently, state income needs
to be included in the regression model.

\begin{align}
  \log(Q_i^{cigarettes}) = \beta_0 + \beta_1 \log(P_i^{cigarettes}) + \beta_2 \log(Income_i) + u_i \label{eq:mcigstsls1}
\end{align}

Before estimating \eqref{eq:mcigstsls1} using \texttt{ivreg()} we define
\(Income\) as real per capita income \texttt{rincome} and add it to the
data set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add rincome to the data set}
\NormalTok{CigarettesSW}\OperatorTok{$}\NormalTok{rincome <-}\StringTok{ }\KeywordTok{with}\NormalTok{(CigarettesSW, income}\OperatorTok{/}\NormalTok{population}\OperatorTok{/}\NormalTok{cpi)}
\NormalTok{c1995 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(CigarettesSW, year }\OperatorTok{==}\StringTok{ "1995"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the model}
\NormalTok{cig_ivreg2 <-}\StringTok{ }\KeywordTok{ivreg}\NormalTok{(}\KeywordTok{log}\NormalTok{(packs) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(rprice) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(rincome) }\OperatorTok{|}\StringTok{ }\KeywordTok{log}\NormalTok{(rincome) }\OperatorTok{+}\StringTok{ }\NormalTok{salestax,}
  \DataTypeTok{data =}\NormalTok{ c1995)}

\KeywordTok{coeftest}\NormalTok{(cig_ivreg2, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(cig_ivreg2, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)   9.43066    1.25939  7.4883 1.935e-09 ***
## log(rprice)  -1.14338    0.37230 -3.0711  0.003611 ** 
## log(rincome)  0.21452    0.31175  0.6881  0.494917    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We obtain

\begin{align}
  \widehat{\log(Q_i^{cigarettes})} = \underset{(1.26)}{9.42} - \underset{(0.37)}{1.14} \log(P_i^{cigarettes}) + \underset{(0.31)}{0.21} \log(Income_i). \label{eq:emcigstsls2}
\end{align}

Following the book we add the cigarette-specific taxes (\(cigtax_i\)) as
a further instrumental variable and estimate again using TSLS.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add cigtax to the data set}
\NormalTok{CigarettesSW}\OperatorTok{$}\NormalTok{cigtax <-}\StringTok{ }\KeywordTok{with}\NormalTok{(CigarettesSW, tax}\OperatorTok{/}\NormalTok{cpi)}
\NormalTok{c1995 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(CigarettesSW, year }\OperatorTok{==}\StringTok{ "1995"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cig_ivreg3 <-}\StringTok{ }\KeywordTok{ivreg}\NormalTok{(}\KeywordTok{log}\NormalTok{(packs) }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(rprice) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(rincome) }\OperatorTok{|}\StringTok{ }
\StringTok{                      }\KeywordTok{log}\NormalTok{(rincome) }\OperatorTok{+}\StringTok{ }\NormalTok{salestax }\OperatorTok{+}\StringTok{ }\NormalTok{cigtax, }\DataTypeTok{data =}\NormalTok{ c1995)}

\KeywordTok{coeftest}\NormalTok{(cig_ivreg3, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(cig_ivreg3, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)   9.89496    0.95922 10.3157 1.947e-13 ***
## log(rprice)  -1.27742    0.24961 -5.1177 6.211e-06 ***
## log(rincome)  0.28040    0.25389  1.1044    0.2753    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Using the two instrumental variables \(salestax_i\) and \(cigtax_i\) we
have \(m=2\) and \(k=1\) so the coefficient on the endogenous regressor
\(\log(P_i^{cigarettes})\) is \emph{overidentified}. The TSLS estimate
of the regression function \eqref{eq:mcigstsls1} is

\begin{align}
  \widehat{\log(Q_i^{cigarettes})} = \underset{(0.96)}{9.89} - \underset{(0.25)}{1.28} \log(P_i^{cigarettes}) + \underset{(0.25)}{0.28} \log(Income_i). \label{eq:emcigstsls3}
\end{align}

Should we trust the estimates presented in \eqref{eq:emcigstsls2} or
rather rely on \eqref{eq:emcigstsls3}? Comparing both equations we find
that the estimation based on both instruments is more precise since in
\eqref{eq:emcigstsls3} all standard errors reported are smaller than in
\eqref{eq:emcigstsls2}. In fact, the standard error for the estimator of
the demand elasticity is only two thirds of the standard error when the
sales tax is the only instrument used. This is due to more information
being used in estimation \eqref{eq:emcigstsls3}. \emph{If} the instruments
are \emph{valid}, \eqref{eq:emcigstsls3} can be considered more reliable.

Without any inference on validity of both instruments it is not sensible
to make a statement like that. This stresses why checking instrument
validity is essential. Chapter 12.3 briefly disscusses guidelines in
checking instrument validity and presents approaches that allow to test
for instrument relevance and exogeneity under certain conditions. These
are then used in an application to the demand for cigarettes in Chapter
12.4.

\section{Checking Instrument
Validity}\label{checking-instrument-validity}

\subsubsection*{Instrument Relevance}\label{instrument-relevance}
\addcontentsline{toc}{subsubsection}{Instrument Relevance}

Instruments that explain little variation in the endogenous regressor
\(X\) are called \textbf{weak instrument}. Weak instruments provide
little information about the variation in \(X\) that is exploited by IV
regression to estimate the effect of interest: the estimate of the
coefficient on the endogenous regressor is estimated inaccurately.
Moreover, weak instruments cause the distribution of the estimator to
deviate considerably from a normal distribution even in large samples
such that known methods for obtaining inference about the true
coefficient on \(X\) may be invalid. See Chapter 12.3 and Appendix 12.4
of the book for a more detailed argument on the undesirable consequences
of using weak instruments in IV regression.

Key Concept 12.5

A Rule of Thumb for Checking for Weak Instruments

Consider the case of a \emph{single} endogenous regressor \(X\) and
\(m\) exogenous instruments \(Z_1,\dots,Z_m\). If the coefficients on
all instruments in the population first-stage regression of a TSLS
estimation are zero, the instruments do not explain any of the variation
in the \(X\) which clearly violates assumption 1 of Key Concept 12.2.
Though the latter case is unlikely to be encountered in practice, we
should ask ourselves to what extend the assumption of instrument
relevance should be fulfilled.

While this is hard to answer for general IV reggression, in the case of
a \emph{single} endogenous regressor \(X\) one may use the following
rule of thumb:

Compute the \(F\)-statistic which corresponds to the hypothesis that the
coefficients on \(Z_1,\dots,Z_m\) are all zero in the first-stage
regression. If the \(F\)-statistic is less than \(10\) the instruments
are weak such that the TSLS estimate of the coefficient on \(X\) is
biased and no valid statistical inference about its true value can be
made. See also Appendix 12.5 of the book.

The rule of thumb stated in Key Concept 12.5 is easily implemented in R.
One simply has to run the first stage regression using \texttt{lm()} and
subsequently compute the heteroskedasticity-robust \(F\)-statistic by
means of \texttt{linearHypothesis()}. This is part of the application to
the demand for cigarettes discussed in Chapter 12.4.

\subsubsection*{If instruments are weak}\label{if-instruments-are-weak}
\addcontentsline{toc}{subsubsection}{If instruments are weak}

There are two options if instruments are weak:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Discard the weak instruments and/or find stronger instruments. While
  the former is only an option if the unknown coefficients remain
  identified when the weak instruments are discarded, the latter can be
  very difficult and even may require a redesign of the whole study.
\item
  Stick with the weak instruments but use methods that improve upon TSLS
  in this scenario, for example limited information maximum likelihood
  estimation, see Appendix 12.5 of the book.
\end{enumerate}

\subsubsection*{When the Assumption of Instrument Exogeneity is
Violated}\label{when-the-assumption-of-instrument-exogeneity-is-violated}
\addcontentsline{toc}{subsubsection}{When the Assumption of Instrument
Exogeneity is Violated}

If there is correlation between an instrument and the error term IV
regression is not consistent estimator (this is shown in Appendix 12.4
of the book). As econometricians we wish to use the available
information for gaining statistical inference whether an instrument is
exogenous or not. The overidentifying restrictions test (also called the
\(J\)-test) is an approach to test the hypothesis that \emph{additional}
instruments are exogenous under the assumption that the set of
instruments consists of enough valid instruments to identify the
coefficients of interest. For the \(J\)-test to be applicable we there
need to be more instruments than endogenous regressors. The idea is
summarized in Key Concept 12.5.

Key Concept 12.5

\(J\)-Statistic / Overidentifying Restriction Test

Take \(\widehat{u}_i^{TSLS} \ , \ i = 1,\dots,n\), the residuals of the
TSLS estimation of the general IV regression model \eqref{eq:givmodel}.
Run the OLS regression

\begin{align}
  \widehat{u}_i^{TSLS} =& \, \delta_0 + \delta_1 Z_{1i} + \dots + \delta_m Z_{mi} + \delta_{m+1} W_{1i} + \dots + \delta_{m+r} W_{ri} + e_i \label{eq:jstatreg}
\end{align}

and test the hypothesis \[H_0: \delta_1 = 0, \dots, \delta_{m} = 0\]
which states that all instruments are exogenous. This can be done using
the corresponding \(F\)-statistic by computing \[J = m  F.\] This test
is the overidentifying restrictions test and the statistic is called the
\(J\)-statistic with \[J \sim \chi^2_{m-k}\] in large samples and the
assumption of homoskedasticity. The degress of freedom \(m-k\) state the
degree of overidentification since this is the number of instruments
\(m\) minus the number of endogenous regressors \(k\).

It is important to note that the \(J\)-statistic discussed in Key
Concept 12.5 is only \(\chi^2_{m-k}\) distributed when the error term
\(e_i\) in the regression \eqref{eq:jstatreg} is homoskedastic. Discussion
of the heteroskedasticity-robust \(J\)-statistic is beyond the scope of
this chapter the reader is refered to Section 18.7 for a theoretical
argument.

As for the procedure shown in Key Concept 12.5, further interest lies in
applying the \(J\)-test using the function \texttt{linearHypothesis()}
in R which will be shown in context of estimation of the demand
elasticity for cigarettes discussed in the next chapter.

\section{Application to the Demand for
Cigarettes}\label{application-to-the-demand-for-cigarettes-2}

Are the general sales tax and the cigarette-specific tax valid
instruments? If not the TSLS attempt to estimate the demand elasticity
for cigarettes discussed in \protect\hyperlink{TGIVRM}{Chapter 12.2} is
not meaningful and we need to rethink the design of the study. As
discussed in \protect\hyperlink{TIVEWASRAASI}{Chapter 12.1} both
variables are likely do be relevant but whether they are exogenous is a
different question.

In the book it is advocated that cigarette-specific taxes could be
endogenous because there might be state specific historical factors like
economic importance of the tabacco farming and cigarette production
industry that enforces its interest in low cigarette specific taxes.
Since it is plausible that tabacco growing states have higher rates of
smoking than others this would lead to endogeneity of cigarette specific
taxes. If data about the size on the tabacco and cigarette industry
would be available we could solve this potential issue by including the
information in the regression which is not the case.

However, since the role of the influence of the tabacco and cigarette
industry is a factor that can be assumed to differ across states but not
over time we may exploit the panel structure of \texttt{CigarettesSW}
instead: as shown in \protect\hyperlink{PDWTTP}{Chapter 10.2},
regression using data on \emph{changes} between two time periods
eliminates such state specific and time invariant effects. Following the
book we consider changes in variables between 1985 and 1995 that is we
are interested in estimating the \emph{long-run elasticity} of the
demand for cigarettes.

The model to be estimated by TSLS using the general sales tax and the
cigarette-specific sals tax as instruments hence is

\begin{align}
  \log(Q_{i,1995}^{cigarettes}) - \log(Q_{i,1995}^{cigarettes}) =& \, \beta_0 + \beta_1 \left[\log(P_{i,1995}^{cigarettes}) - \log(P_{i,1985}^{cigarettes}) \right] \\ & \, + \beta_2 \left[\log(Income_{i,1995}) - \log(Income_{i,1985})\right] + u_i \label{eq:diffivreg}.
\end{align}

Prior to the estimation using \texttt{ivreg()} we define changes from
1985 to 1995 for the dependent variable, the regressors and both
instruments.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# subset data for year 1985}
\NormalTok{c1985 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(CigarettesSW, year }\OperatorTok{==}\StringTok{ "1985"}\NormalTok{)}

\CommentTok{# define changes in variables}
\NormalTok{packsdiff <-}\StringTok{ }\KeywordTok{log}\NormalTok{(c1995}\OperatorTok{$}\NormalTok{packs) }\OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(c1985}\OperatorTok{$}\NormalTok{packs)}

\NormalTok{pricediff <-}\StringTok{ }\KeywordTok{log}\NormalTok{(c1995}\OperatorTok{$}\NormalTok{price}\OperatorTok{/}\NormalTok{c1995}\OperatorTok{$}\NormalTok{cpi) }\OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(c1985}\OperatorTok{$}\NormalTok{price}\OperatorTok{/}\NormalTok{c1985}\OperatorTok{$}\NormalTok{cpi)}

\NormalTok{incomediff <-}\StringTok{ }\KeywordTok{log}\NormalTok{(c1995}\OperatorTok{$}\NormalTok{income}\OperatorTok{/}\NormalTok{c1995}\OperatorTok{$}\NormalTok{population}\OperatorTok{/}\NormalTok{c1995}\OperatorTok{$}\NormalTok{cpi) }\OperatorTok{-}
\KeywordTok{log}\NormalTok{(c1985}\OperatorTok{$}\NormalTok{income}\OperatorTok{/}\NormalTok{c1985}\OperatorTok{$}\NormalTok{population}\OperatorTok{/}\NormalTok{c1985}\OperatorTok{$}\NormalTok{cpi)}

\NormalTok{salestaxdiff <-}\StringTok{ }\NormalTok{(c1995}\OperatorTok{$}\NormalTok{taxs }\OperatorTok{-}\StringTok{ }\NormalTok{c1995}\OperatorTok{$}\NormalTok{tax)}\OperatorTok{/}\NormalTok{c1995}\OperatorTok{$}\NormalTok{cpi }\OperatorTok{-}\StringTok{ }\NormalTok{(c1985}\OperatorTok{$}\NormalTok{taxs }\OperatorTok{-}\StringTok{ }\NormalTok{c1985}\OperatorTok{$}\NormalTok{tax)}\OperatorTok{/}\NormalTok{c1985}\OperatorTok{$}\NormalTok{cpi}

\NormalTok{cigtaxdiff <-}\StringTok{ }\NormalTok{c1995}\OperatorTok{$}\NormalTok{tax}\OperatorTok{/}\NormalTok{c1995}\OperatorTok{$}\NormalTok{cpi }\OperatorTok{-}\StringTok{ }\NormalTok{c1985}\OperatorTok{$}\NormalTok{tax}\OperatorTok{/}\NormalTok{c1985}\OperatorTok{$}\NormalTok{cpi}
\end{Highlighting}
\end{Shaded}

We perform three estimations of \eqref{eq:diffivreg} using
\texttt{ivreg()}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  2SLS using only the difference in the sales taxes between 1985 and
  1995 as the instrument.
\item
  2SLS using only the dfference in the cigarette-specific sales taxes
  1985 and 1995 as the instrument.
\item
  2SLS using both the difference in the sales taxes 1985 and 1995 and
  the dfference in the cigarette-specific sales taxes 1985 and 1995 as
  instruments.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate models}
\NormalTok{cig_ivreg_diff1 <-}\StringTok{ }\KeywordTok{ivreg}\NormalTok{(packsdiff }\OperatorTok{~}\StringTok{ }\NormalTok{pricediff }\OperatorTok{+}\StringTok{ }\NormalTok{incomediff }\OperatorTok{|}\StringTok{ }\NormalTok{incomediff }\OperatorTok{+}\StringTok{ }\NormalTok{salestaxdiff)}

\NormalTok{cig_ivreg_diff2 <-}\StringTok{ }\KeywordTok{ivreg}\NormalTok{(packsdiff }\OperatorTok{~}\StringTok{ }\NormalTok{pricediff }\OperatorTok{+}\StringTok{ }\NormalTok{incomediff }\OperatorTok{|}\StringTok{ }\NormalTok{incomediff }\OperatorTok{+}\StringTok{ }\NormalTok{cigtaxdiff)}

\NormalTok{cig_ivreg_diff3 <-}\StringTok{ }\KeywordTok{ivreg}\NormalTok{(packsdiff }\OperatorTok{~}\StringTok{ }\NormalTok{pricediff }\OperatorTok{+}\StringTok{ }\NormalTok{incomediff }\OperatorTok{|}\StringTok{ }\NormalTok{incomediff }\OperatorTok{+}\StringTok{ }\NormalTok{salestaxdiff }\OperatorTok{+}\StringTok{ }\NormalTok{cigtaxdiff)}
\end{Highlighting}
\end{Shaded}

As usual we use \texttt{coeftest()} in conjunction with
\texttt{vcovHC()} to obtain robust coefficient summaries for all three
models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# robust coefficient summary 1}
\KeywordTok{coeftest}\NormalTok{(cig_ivreg_diff1, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(cig_ivreg_diff1, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) -0.117962   0.068217 -1.7292   0.09062 .  
## pricediff   -0.938014   0.207502 -4.5205 4.454e-05 ***
## incomediff   0.525970   0.339494  1.5493   0.12832    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# robust coefficient summary 2}
\KeywordTok{coeftest}\NormalTok{(cig_ivreg_diff2, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(cig_ivreg_diff2, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) -0.017049   0.067217 -0.2536    0.8009    
## pricediff   -1.342515   0.228661 -5.8712 4.848e-07 ***
## incomediff   0.428146   0.298718  1.4333    0.1587    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# robust coefficient summary 3}
\KeywordTok{coeftest}\NormalTok{(cig_ivreg_diff3, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(cig_ivreg_diff3, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) -0.052003   0.062488 -0.8322    0.4097    
## pricediff   -1.202403   0.196943 -6.1053 2.178e-07 ***
## incomediff   0.462030   0.309341  1.4936    0.1423    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We proceed by generating a tabulated summary of the estimation results
using \texttt{stargazer()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# gather robust standard errors in a list}
\NormalTok{rob_se <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(cig_ivreg_diff1, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(cig_ivreg_diff2, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(cig_ivreg_diff3, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{)))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

2SLS Estimates of the Long-Term Elasticity of the Demand for Cigarettes
using Panel Data

Dependent variable: 1985-1995 difference in log per pack price

IV: salestax

IV: cigtax

IVs: salestax, cigtax

(1)

(2)

(3)

pricediff

-0.94***

-1.34***

-1.20***

(0.21)

(0.23)

(0.20)

incomediff

0.53

0.43

0.46

(0.34)

(0.30)

(0.31)

Constant

-0.12*

-0.02

-0.05

(0.07)

(0.07)

(0.06)

Observations

48

48

48

R2

0.55

0.52

0.55

Adjusted R2

0.53

0.50

0.53

Residual Std. Error (df = 45)

0.09

0.09

0.09

We observe negative estimates of the coefficient on \texttt{pricediff}
that are quite different in magnitude. Which one should we trust most?
This hinges only on the validity of instruments used. To assess this we
compute \(F\)-statistics for the first-stage regression of all three
models to check instrument relevance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# First-stage regressions}
\NormalTok{mod_relevance1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(pricediff }\OperatorTok{~}\StringTok{ }\NormalTok{salestaxdiff }\OperatorTok{+}\StringTok{ }\NormalTok{incomediff)}
\NormalTok{mod_relevance2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(pricediff }\OperatorTok{~}\StringTok{ }\NormalTok{cigtaxdiff }\OperatorTok{+}\StringTok{ }\NormalTok{incomediff)}
\NormalTok{mod_relevance3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(pricediff }\OperatorTok{~}\StringTok{ }\NormalTok{incomediff }\OperatorTok{+}\StringTok{ }\NormalTok{salestaxdiff }\OperatorTok{+}\StringTok{ }\NormalTok{cigtaxdiff)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check instrument relevance for model 1}
\KeywordTok{linearHypothesis}\NormalTok{(mod_relevance1, }
                 \StringTok{"salestaxdiff = 0"}\NormalTok{, }
                 \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(mod_relevance1, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## salestaxdiff = 0
## 
## Model 1: restricted model
## Model 2: pricediff ~ salestaxdiff + incomediff
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(>F)    
## 1     46                        
## 2     45  1 33.674 6.119e-07 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check instrument relevance for model 2}
\KeywordTok{linearHypothesis}\NormalTok{(mod_relevance2, }
                 \StringTok{"cigtaxdiff = 0"}\NormalTok{, }
                 \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(mod_relevance2, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## cigtaxdiff = 0
## 
## Model 1: restricted model
## Model 2: pricediff ~ cigtaxdiff + incomediff
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(>F)    
## 1     46                        
## 2     45  1 107.18 1.735e-13 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check instrument relevance for model 3}
\KeywordTok{linearHypothesis}\NormalTok{(mod_relevance3, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"salestaxdiff = 0"}\NormalTok{, }\StringTok{"cigtaxdiff = 0"}\NormalTok{), }
                 \DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(mod_relevance3, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## salestaxdiff = 0
## cigtaxdiff = 0
## 
## Model 1: restricted model
## Model 2: pricediff ~ incomediff + salestaxdiff + cigtaxdiff
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(>F)    
## 1     46                        
## 2     44  2 88.616 3.709e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We also conduct the overidentifying restrictions test for model three
which is the only model where the coefficient on the difference in log
prices is overidentified (\(m=2\), \(k=1\)) such that the
\(J\)-statistic can be computed. To do this we take the residuals stored
in \texttt{cig\_ivreg\_diff3} and regress them on both instruments and
the presumably exogenous regressor \texttt{incomediff}. Yet again use
\texttt{linearHypothesis()} to test jointly whether the coefficients on
both instruments are zero which is necessary for the exogeneity
assumption to be fulfilled. Note that with \texttt{test\ =\ "Chisq"} we
obtain a chi-squared distributed test statistic instead of a
\(F\)-statistic.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute J-statistic}

\NormalTok{cig_iv_OR <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{residuals}\NormalTok{(cig_ivreg_diff3) }\OperatorTok{~}\StringTok{ }\NormalTok{incomediff }\OperatorTok{+}\StringTok{ }\NormalTok{salestaxdiff }\OperatorTok{+}\StringTok{ }\NormalTok{cigtaxdiff)}

\NormalTok{cig_OR_test <-}\StringTok{ }\KeywordTok{linearHypothesis}\NormalTok{(cig_iv_OR, }
                               \KeywordTok{c}\NormalTok{(}\StringTok{"salestaxdiff = 0"}\NormalTok{, }\StringTok{"cigtaxdiff = 0"}\NormalTok{), }
                               \DataTypeTok{test =} \StringTok{"Chisq"}\NormalTok{)}
\NormalTok{cig_OR_test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## salestaxdiff = 0
## cigtaxdiff = 0
## 
## Model 1: restricted model
## Model 2: residuals(cig_ivreg_diff3) ~ incomediff + salestaxdiff + cigtaxdiff
## 
##   Res.Df     RSS Df Sum of Sq Chisq Pr(>Chisq)  
## 1     46 0.37472                                
## 2     44 0.33695  2  0.037769 4.932    0.08492 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\textbf{Caution}: In this case the \(p\)-Value reported by
\texttt{linearHypothesis()} is wrong because the degrees of freedom are
set to \(2\). This differs from the degree of overidetification
(\(m-k=2-1=1\)) so the \(J\)-statistic is \(\chi^2_1\) distributed
instead of following a \(\chi^2_2\) distribution as assumed defaultly
\texttt{linearHypothesis()}. We may compute the correct \(p\)-Value
using \texttt{pchisq()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute correct p-value for J-statistic}
\KeywordTok{pchisq}\NormalTok{(cig_OR_test[}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{], }\DataTypeTok{df =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lower.tail =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.02636406
\end{verbatim}

Since this value is smaller than \(0.05\) we reject the hypothesis that
both instruments are exogenous at the level of \(5\%\). This means one
the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The sales tax is an invalid instrument.
\item
  The cigarettes-specific sales tax is an invalid instrument.
\item
  Both instruments are invaild.
\end{enumerate}

In the book it is argued that the assumption of instrument exogeneity is
more likely to hold for the gerneral sales tax (see Chapter 12.4 of the
book) such that the IV estimate of the long-run elasticity of demand for
cigarettes we consider the most trustworthy is \(-0.94\), the estimate
obtained using 2SLS with only the general sales tax as an instrument.

The interpretation of this estimate is that over a 10-year period, an
increase in the average price per package by one percent is expected to
decrease consumption by about \(0.94\) percentage points. This suggests
that, in the long run, price increases can reduce cigarette consumption
considerably.

\section{Where Do Valid Instruments Come
From?}\label{where-do-valid-instruments-come-from}

Chapter 12.5 of the books presents a comprehensive discussion of
approaches to find valid instruments in practice by the example of three
research questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Does putting criminals in jail reduce crime?
\item
  Does cutting class sizes invrease test scores?
\item
  Does aggressive treatment of heart attacks prolong lives?
\end{enumerate}

This section is not directly related to applications in R which is why
we do not discuss the contents here. We encourage you to work through
this on your own.

\subsubsection*{Summary}\label{summary-4}
\addcontentsline{toc}{subsubsection}{Summary}

\texttt{ivreg()} from the package AER provides convenient
functionalities to estimate IV regression models in R. It is an
implementation of the 2SLS estimation approach.

Besides treating IV estimation, we have also discussed how to test for
weak instruments and how to conduct an overidentifying restrictions test
when there are more instrumental variables than endogenous regressors
using R.

In an empirical application we have shown how \texttt{ivreg()} can be
used to estimate the long-run elasticity of demand for cigarettes based
on \texttt{CigarettesSW}, a panel data set on cigarette consumption and
economic indicators for all 48 continental US states for time periods
1985 and 1995. Different sets of instruments were used in the process
and it has been argued why using the general sales tax as the only
instrument is the prefered choice here. The estimate of the demand
elasticity deemed the most trustworthy is \(-0.94\). This estimate
suggests that there is a remarkable negative long-run effect on
cigarette consumption of increasing the average price per pack.

\chapter{Experiments and
Quasi-Experiments}\label{experiments-and-quasi-experiments}

This chapter discusses statistical tools that are commonly applied in
program evaluation which is a field of study where resarchers are
interested in measuring the causal effects of programs, policies or
other interventions / treatments. An optimal research design for this
purpose is what statisticains call an ideal randomized controlled
experiment. The basic idea is to randomly assign subjects in two
different groups, one that receives the treatment (treatment group) and
one that does not (control group) and to compare outcomes for both
groups in order to get an estimate of the average treatment effect.

Note that \emph{experimental} data is fundamentally different from
\emph{observational} data. For example, one might be interested in
measuring how much the performance of students in a standardized test
differs between two classes where one of which has a student-teacher
ratio which is considered regular and the other one has fewer students
using a randomized controlled experiment. The data produced by such an
experiment would be different from e.g.~the observed cross-section data
on the students' performance used throughout Chapters 4 to 8 where class
sizes are \emph{not randomly assigned} to students but are the results
of an economic dicision where educational objectives and budgetary
aspects where balanced.

For economists, randomized controlled experiments are often difficult to
implement or even infeasable. For example, due to ethic, moral and legal
reasons it is practically impossible for a business owner to estimate
the causal effect the productivity of workers of setting them under
psychological stress using an experiment where workers are randomly
assigned either to the treament group that is pressed under time or to
the control group where work is under regular conditions, at best
without knowlegde of beeing in an experiment.\footnote{See the box
  \emph{The Hawthorne Effect} on page 528 of the book.}

However sometimes it is the case that external cirumstances which appear
random produce what is called a \emph{quasi-experiment} or \emph{natural
experiment}. This ``as if'' randomness allows for estimation of causal
effects that are of interest for economist using tools which are very
similar to those valid for ideal randomized controlled experiments.
These tools draw heavily on the theory of multiple regression and also
on IV regression (see chapter 12). We will review the core aspects of
these methods and demonstrate how to apply them in R using the STAR data
set.\footnote{See the
  \href{https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/10766}{description}
  of the data set.}

\section{Potential Outcomes, Causal Effects and Idealized
Experiments}\label{potential-outcomes-causal-effects-and-idealized-experiments}

In the following we will briefly recap the idea of the average causal
effect and how it can be estimated using the \emph{differences
estimator}. We advise You to read through Chapter 13.1 of the book for a
better understanding.

\subsubsection*{Potential Outcomes and the average causal
Effect}\label{potential-outcomes-and-the-average-causal-effect}
\addcontentsline{toc}{subsubsection}{Potential Outcomes and the average
causal Effect}

\textbf{A potential} outcome is the outcome for an individual under a
potential treatment. For this individual, we say that the causal effect
of the treatment is the difference in the potential outcome if the
indiviual receives the treatment and the potential outcome if she does
not. Since this causal effect may be different for different individuals
and it is not possible to measure the causal effect for a single
individual, one is interested in studying the \textbf{average causal
effect} of the treatment, hence also called the \textbf{average
treatment effect.}

In a ideal randomized controlled experiment the following conditions are
fulfilled:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The subjects are selected at random from the population
\item
  The subjects are randomly assigned to treatment and control group
\end{enumerate}

Condition 1. guarantees that the subjects' potential outcomes are drawn
randomly from the same distribution such that the expected value of the
causal effect in the sample is equal to the average causual effect in
the distribution. Condition 2. ensures that receipt of treamtment is
independent from the subjects potential outcomes. If both conditions are
fulfilled, the expected causual effect is the expected outcome in the
treatment group minus the expected outcome in the control group. Using
conditional expecations we have
\[\text{Average causal effect} =  E(Y_i\vert X_i=1) -  E(Y_i\vert X_i=0)\]
where \(X_i\) is a binary treatment indicator.

The average causal effect can be estimated using the \emph{differences
estimator}, which is nothing but the OLS estimator in the simple
regression model

\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + u_i \ \ , \ \ i=1,\dots,n \label{eq:diffest}
\end{align}

where random assignment ensures \(E(u_i\vert X_i) = 0\).

The OLS estimator in the regression model

\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_{1i} + \dots + \beta_{1+r} W_{ri} + u_i \ \ , \ \ i=1,\dots,n \label{eq:diffestwar}
\end{align}

with additional regressors \(W_1,\dots,W_r\) is called the
\emph{differences estimator with additional regressors}. It is assumed
that conditional mean independence holds, i.e.
\[E(u_i\vert X_i , W_i) = E(u_i\vert W_i) = 0.\] The \emph{differences
estimator with additional regressors} is more efficient than the
\emph{differences estimator} if the additional regressors explain some
of the variation in \(Y_i\).

\section{Threats to Validity of
Experiments}\label{threats-to-validity-of-experiments}

The concepts of internal and external validity discussed in Key Concept
9.1 are also applicable for studies based on experimental and
quasi-experimental data. Chapter 13.2 of the book provides a sound
explanation of the particular threats to internal and external validity
of experiments including examples. We limit ourselves to a short
repetition of the threats listed. Consult the book for a more detailed
explanation.

\subsubsection*{Threats to Internal
Validity}\label{threats-to-internal-validity-1}
\addcontentsline{toc}{subsubsection}{Threats to Internal Validity}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Failure to randomize}

  If the subjects are not randomly assigned to the treatment group, then
  the outcomes will be contaminated with the effect of the subjects'
  individuals characteristics or preferences and it is not possible to
  obtain an unbiased estimate of the treatment effect. One can test for
  nonrandom assignment using a significance test (F-Test) on the
  coefficient in the regression model
  \[X_i = \beta_0 + \beta_1 W_{1i} + \dots +\beta_2 W_{ri} + u_i \ \ , \ \ i=1,\dots,n.\]
\item
  \textbf{Failure to follow the treatment protocol}

  If subjects do not follow the treatment protocol, i.e.~some subjects
  in the treatment group manage to avoid receiving the treatment and/or
  some subjects in the control group manage to receive the treatment
  (\emph{partial compliance}), there is correlation between \(X_i\) und
  \(u_i\) such that the OLS estimator of the average treatment effect
  will be biased. If there are data on \emph{both} treatment actually
  recieved (\(X_i\)) and initial random assignment (\(Z_i\)), IV
  regression of the models \eqref{eq:diffest} and \eqref{eq:diffestwar} is a
  remedy.
\item
  \textbf{Attrition}

  Attrition results in a nonrandomly selected sample. If subjects
  systematically drop out of the study after beeing assigned to the
  control or the treatment group, where systematic means that the reason
  of the drop out is related to the treatment, there will be correlation
  between \(X_i\) and \(u_i\) and hence bias in the OLS estimator of the
  treatment effect.
\item
  \textbf{Experimental effects}

  If human subjects in treatment group and/or control group know that
  they are in an experiment, they might adapt their behaviour in a way
  that prevents unbiased estimation of the treatment effect.
\item
  \textbf{Small sample sizes}

  As we know from the theory of linear regression, small sample sizes
  lead to imprecise estimation of coefficients in regression models and
  thus imply imprecise estimation of the causal effect. Furthermore,
  confidence intervals and hypothesis test may produce wrong inference
  when the sample size is small.
\end{enumerate}

\subsubsection*{Threats to External
Validity}\label{threats-to-external-validity-1}
\addcontentsline{toc}{subsubsection}{Threats to External Validity}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Nonrepresentative sample}

  If the population studied and the population of interest are not
  sufficiently similar, there is no justification in generalizing the
  results.
\item
  \textbf{Nonrepresentative program or policy}

  If the program or policy differs for the population studied differs
  considerably from the program (to be) applied to population(s)
  interest, the results cannot be generalized. For example, a
  small-scale programm with low funding might have different effects
  than the widely available scaled-up programm that is actually
  implemented. There are other factors like duration and the extend of
  monitoring that should be considered here.
\item
  \textbf{General equilibrium effects}

  If market and/or environmental conditions cannot be kept constant when
  an internally valid program is implemented broadly, the external
  validity may be doubted.
\end{enumerate}

\section{Experimental Estimates of the Effect of Class Size
Reductions}\label{experimental-estimates-of-the-effect-of-class-size-reductions}

\subsection*{Experimental Design and the Data
Set}\label{experimental-design-and-the-data-set}
\addcontentsline{toc}{subsection}{Experimental Design and the Data Set}

The Project \emph{Sudent-Teacher Achievement Ratio} (\emph{STAR}) was a
large randomized controlled experiment with the aim of ascerting whether
a class size reduction is effective in improving education. It has been
conducted in 80 Tennesse elementary schools over a period of four years
during the 1980s by the state Derpartment of Education.

In the first year, about 6400 students were randomly assigned into one
of three interventions: small class (13 to 17 students per teacher),
regular class (22 to 25 students per teacher), and regular-with-aide
class (22 to 25 students with a full-time teacher's aide). Classroom
teachers were also randomly assigned to the classes they would teach.
The interventions were initiated as the students entered school in
kindergarten and continued through third grade. Thus, we have the
following structure of control and treatment groups across grades:

\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:starstructure} Control and treatment groups in the STAR
experiment}\tabularnewline
\toprule
\begin{minipage}[b]{0.14\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
K\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
1\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
2\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
3\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.14\columnwidth}\raggedright\strut
\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
K\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
1\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
2\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\raggedright\strut
3\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
\textbf{Treatment 1}\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Small class\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Small class\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Small class\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Small class\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
\textbf{Treatment 2}\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Regular class + aide\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Regular class + aide\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Regular class + aide\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Regular class + aide\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
\textbf{Control}\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Regular class\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Regular class\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Regular class\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\raggedright\strut
Regular class\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Each year, the students learning progress was assessed using the sum of
the points scored on the math and reading portion of a standardized test
(the
\href{https://en.wikipedia.org/wiki/Stanford_Achievement_Test_Series}{Stanford
Achievement Test}).

The \emph{STAR} data set is part of the package AER.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load the package AER and the STAR data set}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(STAR)}
\end{Highlighting}
\end{Shaded}

Using \texttt{head(STAR)} we find that there is a variety of factor
variables that describe student and teacher characteristics as well as
various school indicators, all of which seperately recorded for the four
different grades. The data is in \emph{wide format}, that is each
variable has its own coloumn and for each student, the rows contain
observations on these variables. Using \texttt{dim(STAR)} we find that
there are a total of 11598 observations on 47 variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get an overview}
\KeywordTok{head}\NormalTok{(STAR, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      gender ethnicity   birth stark star1 star2   star3 readk read1 read2
## 1122 female      afam 1979 Q3  <NA>  <NA>  <NA> regular    NA    NA    NA
## 1137 female      cauc 1980 Q1 small small small   small   447   507   568
##      read3 mathk math1 math2 math3   lunchk lunch1   lunch2 lunch3 schoolk
## 1122   580    NA    NA    NA   564     <NA>   <NA>     <NA>   free    <NA>
## 1137   587   473   538   579   593 non-free   free non-free   free   rural
##      school1 school2  school3  degreek  degree1  degree2  degree3 ladderk
## 1122    <NA>    <NA> suburban     <NA>     <NA>     <NA> bachelor    <NA>
## 1137   rural   rural    rural bachelor bachelor bachelor bachelor  level1
##      ladder1    ladder2    ladder3 experiencek experience1 experience2
## 1122    <NA>       <NA>     level1          NA          NA          NA
## 1137  level1 apprentice apprentice           7           7           3
##      experience3 tethnicityk tethnicity1 tethnicity2 tethnicity3 systemk
## 1122          30        <NA>        <NA>        <NA>        cauc    <NA>
## 1137           1        cauc        cauc        cauc        cauc      30
##      system1 system2 system3 schoolidk schoolid1 schoolid2 schoolid3
## 1122    <NA>    <NA>      22      <NA>      <NA>      <NA>        54
## 1137      30      30      30        63        63        63        63
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(STAR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11598    47
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get variable names}
\KeywordTok{names}\NormalTok{(STAR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "gender"      "ethnicity"   "birth"       "stark"       "star1"      
##  [6] "star2"       "star3"       "readk"       "read1"       "read2"      
## [11] "read3"       "mathk"       "math1"       "math2"       "math3"      
## [16] "lunchk"      "lunch1"      "lunch2"      "lunch3"      "schoolk"    
## [21] "school1"     "school2"     "school3"     "degreek"     "degree1"    
## [26] "degree2"     "degree3"     "ladderk"     "ladder1"     "ladder2"    
## [31] "ladder3"     "experiencek" "experience1" "experience2" "experience3"
## [36] "tethnicityk" "tethnicity1" "tethnicity2" "tethnicity3" "systemk"    
## [41] "system1"     "system2"     "system3"     "schoolidk"   "schoolid1"  
## [46] "schoolid2"   "schoolid3"
\end{verbatim}

Notice that a majority of the variable names contain a suffix
(\texttt{k}, \texttt{1}, \texttt{2} or \texttt{3}) stating the grade
which the respective variable is refering to. This facilitates
regression analysis because it allows to adjust the \texttt{formula}
argument in \texttt{lm()} for each grade by simply changing the
variables' suffixes accordingly.

Inspecting the outcome produced by \texttt{head()} we see that some
values recorded are \texttt{NA} and
\texttt{\textless{}NA\textgreater{}}, i.e.~there is no data of this
variable for the student under consideration. This lies in the nature of
the data: for example, take the first observation \texttt{STAR{[}1,{]}}.
We find that the student entered the experiment in third grade in a
regular class which is why the class size is recorded in \texttt{star3}
and other class type indicator variables are
\texttt{\textless{}NA\textgreater{}}. For the same reason there are no
recordings of her math and reading score but for third grade. It is
straightforward to get only the
non-\texttt{NA}/\texttt{\textless{}NA\textgreater{}} recordings for her:
we simply drop the \texttt{NA}s using \texttt{is.na()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# drop NA recordings for the first observation}
\NormalTok{STAR[}\DecValTok{1}\NormalTok{,}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(STAR[}\DecValTok{1}\NormalTok{,])]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      gender ethnicity   birth   star3 read3 math3 lunch3  school3  degree3
## 1122 female      afam 1979 Q3 regular   580   564   free suburban bachelor
##      ladder3 experience3 tethnicity3 system3 schoolid3
## 1122  level1          30        cauc      22        54
\end{verbatim}

In general it is not necessary to perform such action when doing
regression analysis because \texttt{lm()} excludes incomplete cases,
i.e.~rows with missing data, by default. You should be aware that
missing data may imply a small sample size and thus may lead to
imprecise estimation and wrong inference which is, however, not not an
issue for the study at hand since, as we will see below, sample sizes
lie beyond 5000 observations for each regression conducted.

\subsection*{Analysis of the STAR Data}\label{analysis-of-the-star-data}
\addcontentsline{toc}{subsection}{Analysis of the STAR Data}

As can be seen from \ref{tab:starstructure} there are two treatment
groups in each grade, small classes with only 13 to 17 students and
regular classes with 22 to 25 students and a teaching aide. Thus, two
binary variables, each beeing an indicator for the respective treatment
group, are introduced for the differences estimator to capture the
treatment effect for both treatment groups seperately. This yields the
population regression model

\begin{align}
  Y_i = \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + u_i, \label{eq:starpopreg}
\end{align}

with test score \(Y_i\), the small class indicator \(SmallClass_i\) and
\(RegAide_i\), the indicator for a regular class with aide.

We can easily reproduce the results presented in table 13.1 of the book
by performing the regression \eqref{eq:starpopreg} for each grade
seperately. Note that for each student, the dependent variable is simply
the sum of the points scored in the math and reading portion of the test
which is why we make use of the function \texttt{I()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute differences Estimates for each grades}
\NormalTok{fmk <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{(readk }\OperatorTok{+}\StringTok{ }\NormalTok{mathk) }\OperatorTok{~}\StringTok{ }\NormalTok{stark, }\DataTypeTok{data =}\NormalTok{ STAR)}
\NormalTok{fm1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{(read1 }\OperatorTok{+}\StringTok{ }\NormalTok{math1) }\OperatorTok{~}\StringTok{ }\NormalTok{star1, }\DataTypeTok{data =}\NormalTok{ STAR)}
\NormalTok{fm2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{(read2 }\OperatorTok{+}\StringTok{ }\NormalTok{math2) }\OperatorTok{~}\StringTok{ }\NormalTok{star2, }\DataTypeTok{data =}\NormalTok{ STAR)}
\NormalTok{fm3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{(read3 }\OperatorTok{+}\StringTok{ }\NormalTok{math3) }\OperatorTok{~}\StringTok{ }\NormalTok{star3, }\DataTypeTok{data =}\NormalTok{ STAR)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Obtain coefficient matrix using robust standard errors}
\KeywordTok{coeftest}\NormalTok{(fmk, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fmk, }\DataTypeTok{type=} \StringTok{"HC0"}\NormalTok{))}
\KeywordTok{coeftest}\NormalTok{(fm1, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fm1, }\DataTypeTok{type=} \StringTok{"HC0"}\NormalTok{))}
\KeywordTok{coeftest}\NormalTok{(fm2, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fm2, }\DataTypeTok{type=} \StringTok{"HC0"}\NormalTok{))}
\KeywordTok{coeftest}\NormalTok{(fm3, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fm3, }\DataTypeTok{type=} \StringTok{"HC0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                    Estimate Std. Error  t value  Pr(>|t|)    
## (Intercept)       918.04289    1.63297 562.1930 < 2.2e-16 ***
## starksmall         13.89899    2.45346   5.6651 1.541e-08 ***
## starkregular+aide   0.31394    2.27039   0.1383      0.89    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                    Estimate Std. Error  t value  Pr(>|t|)    
## (Intercept)       1039.3926     1.7842 582.5691 < 2.2e-16 ***
## star1small          29.7808     2.8305  10.5215 < 2.2e-16 ***
## star1regular+aide   11.9587     2.6514   4.5104 6.588e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                    Estimate Std. Error  t value  Pr(>|t|)    
## (Intercept)       1157.8066     1.8146 638.0403 < 2.2e-16 ***
## star2small          19.3944     2.7110   7.1540 9.428e-13 ***
## star2regular+aide    3.4791     2.5441   1.3675    0.1715    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                     Estimate Std. Error  t value  Pr(>|t|)    
## (Intercept)       1228.50636    1.67959 731.4322 < 2.2e-16 ***
## star3small          15.58660    2.39544   6.5068 8.303e-11 ***
## star3regular+aide   -0.29094    2.27214  -0.1280    0.8981    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We may gather the results and present them in a LaTeX or HTML table
using \texttt{stargazer()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute robust standard errors for each model and gather them in a list}
\NormalTok{rob_se_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fmk, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fm1, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fm2, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fm2, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{)))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stargazer)}

\KeywordTok{stargazer}\NormalTok{(fmk,fm1,fm2,fm3,}
  \DataTypeTok{title =} \StringTok{"Project STAR: Differences Estimates"}\NormalTok{,}
  \DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }
  \DataTypeTok{type =} \StringTok{"latex"}\NormalTok{,}
  \DataTypeTok{model.numbers =}\NormalTok{ F,}
  \DataTypeTok{omit.table.layout =} \StringTok{"n"}\NormalTok{,}
  \DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }
  \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"K"}\NormalTok{, }\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"3"}\NormalTok{),}
  \DataTypeTok{dep.var.caption  =} \StringTok{"Grade"}\NormalTok{,}
  \DataTypeTok{dep.var.labels.include =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{se =}\NormalTok{ rob_se_}\DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Table 13.2: Project STAR - Differences Estimates

Grade

K

1

2

3

starksmall

13.90***

(2.45)

starkregular+aide

0.31

(2.27)

star1small

29.78***

(2.83)

star1regular+aide

11.96***

(2.65)

star2small

19.39***

(2.71)

star2regular+aide

3.48

(2.54)

star3small

15.59

star3regular+aide

-0.29

Constant

918.04***

1,039.39***

1,157.81***

1,228.51***

(1.63)

(1.78)

(1.81)

(1.81)

Observations

5,786

6,379

6,049

5,967

R2

0.01

0.02

0.01

0.01

Adjusted R2

0.01

0.02

0.01

0.01

Residual Std. Error

73.49 (df = 5783)

90.50 (df = 6376)

83.69 (df = 6046)

72.91 (df = 5964)

F Statistic

21.26*** (df = 2; 5783)

56.34*** (df = 2; 6376)

28.71*** (df = 2; 6046)

30.25*** (df = 2; 5964)

The estimates suggest that the class size reduction does improve
performance of the students. Except for grade 1, the estimates of the
coefficient on \(SmallClass\) are roughly of the same magnitude (the
estimates lie between 13.90 and 19.39 points) and statistically
significant at the level of \(1\%\). Furthermore we see that adding a
teaching aide to a regular class has little, possibly zero, effect on
the performance of the students.

Following the book, we continue by augmenting the population regression
\eqref{eq:starpopreg} by different sets of additional regressors for two
reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If the additional regressors explain some of the observed variation in
  the dependent variable we obtain more efficient estimates of the
  coefficients of interest.
\item
  If the treatment is not received at random due to failures to follows
  the treatment protocol (see section 13.3 of the book) the estimates
  obtained using the model specification \eqref{eq:starpopreg} may be
  biased. Adding additional regressors may solve this problem.
\end{enumerate}

In particular, we consider the following student and teacher
characteristics

\begin{itemize}
\tightlist
\item
  \(experience\) --- Teacher's years of experience
\item
  \(boy\) --- Student is a boy (dummy)
\item
  \(lunch\) --- Free lunch eligibility (dummy)
\item
  \(black\) --- Student is African-American (dummy)
\item
  \(race\) --- Student's race is other than black or white (dummy)
\item
  \(\text{schoolid}\) --- School indicator variables
\end{itemize}

in four different population regression specifications:

\begin{align}
  Y_i =& \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + u_i, \label{eq:augstarpopreg1} \\
  Y_i =& \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + \beta_3 experience_i + u_i, \label{eq:augstarpopreg2} \\
  Y_i =& \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + \beta_3 experience_i + \text{school indicators} + u_i \label{eq:augstarpopreg3} \\
  Y_i =& \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + u_i + \beta_3 experience_i + \beta_4 boy + \beta_5 lunch \label{eq:augstarpopreg4} \\ &+ \beta_6 black + \beta_7 race + \text{schoolid} + u_i 
\end{align}

Beforehand to estimation, we apply some subsetting and some data
wrangling. We do this using functions from the packages \texttt{dplyr}
and \texttt{tidyr} which are both part of \emph{tidyverse}, a collection
of R packages designed for data science and handling big data
sets.\footnote{See the \href{https://www.tidyverse.org/}{official site}
  for more on \emph{tidyverse} packages.} The functions
\texttt{\%\textgreater{}\%}, \texttt{transmute()} and \texttt{mutate()}
are sufficient for us here. \texttt{\%\textgreater{}\%} allows to chain
function calls together. \texttt{transmute()} allows to subset the data
set by naming the variables to be kept and \texttt{mutate()} is
convenient for adding new variables based existing ones while preserving
the latter.

From looking at the regression models \eqref{eq:augstarpopreg1} to
\eqref{eq:augstarpopreg4} we find that variables obviously needed to
perform the analysis are \texttt{gender}, \texttt{ethnicity},
\texttt{stark}, \texttt{readk}, \texttt{mathk}, \texttt{lunchk},
\texttt{experiencek} and \texttt{schoolidk}. After dropping the
remaining variables using \texttt{transmute()} we use \texttt{mutate()}
to add three additional binary variables which are derivatives of
existing ones: \texttt{black}, \texttt{race} and \texttt{boy}. They are
generated using logical statements within the function
\texttt{ifelse()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages 'dplyr' and 'tidyr' for data wrangling functionalities}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(tidyr)}

\CommentTok{# generate subset with kindergarten data}
\NormalTok{STARK <-}\StringTok{ }\NormalTok{STAR }\OperatorTok{%>%}\StringTok{ }
\StringTok{      }\KeywordTok{transmute}\NormalTok{(}
\NormalTok{                gender,}
\NormalTok{                ethnicity,}
\NormalTok{                stark,}
\NormalTok{                readk,}
\NormalTok{                mathk,}
\NormalTok{                lunchk,}
\NormalTok{                experiencek,}
\NormalTok{                schoolidk}
\NormalTok{                ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{      }\KeywordTok{mutate}\NormalTok{(}
             \DataTypeTok{black =} \KeywordTok{ifelse}\NormalTok{(ethnicity }\OperatorTok{==}\StringTok{ "afam"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
             \DataTypeTok{race =} \KeywordTok{ifelse}\NormalTok{(ethnicity }\OperatorTok{==}\StringTok{ "afam"} \OperatorTok{|}\StringTok{ }\NormalTok{ethnicity }\OperatorTok{==}\StringTok{ "cauc"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
             \DataTypeTok{boy =} \KeywordTok{ifelse}\NormalTok{(gender }\OperatorTok{==}\StringTok{ "male"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate models }
\NormalTok{gradeK1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{(mathk }\OperatorTok{+}\StringTok{ }\NormalTok{readk) }\OperatorTok{~}\StringTok{ }\NormalTok{stark }\OperatorTok{+}\StringTok{ }\NormalTok{experiencek, }\DataTypeTok{data =}\NormalTok{ STARK)}
\NormalTok{gradeK2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{(mathk }\OperatorTok{+}\StringTok{ }\NormalTok{readk) }\OperatorTok{~}\StringTok{ }\NormalTok{stark }\OperatorTok{+}\StringTok{ }\NormalTok{experiencek }\OperatorTok{+}\StringTok{ }\NormalTok{schoolidk, }\DataTypeTok{data =}\NormalTok{ STARK)}
\NormalTok{gradeK3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{(mathk }\OperatorTok{+}\StringTok{ }\NormalTok{readk) }\OperatorTok{~}\StringTok{ }\NormalTok{stark }\OperatorTok{+}\StringTok{ }\NormalTok{experiencek }\OperatorTok{+}\StringTok{ }\NormalTok{boy }\OperatorTok{+}\StringTok{ }\NormalTok{lunchk }\OperatorTok{+}\StringTok{ }\NormalTok{black }\OperatorTok{+}\StringTok{ }\NormalTok{race }\OperatorTok{+}\StringTok{ }\NormalTok{schoolidk, }\DataTypeTok{data =}\NormalTok{ STARK)}
\end{Highlighting}
\end{Shaded}

Since the models \texttt{gradeK2} and \texttt{gradeK3} include a
constant term, there are a total of 79 school indicator dummies. For
brevity, we exclude their estimated coefficients in the output produced
by \texttt{coeftest()} by subsetting the matrices.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# obtain robust inference on the significance of coefficients}
\KeywordTok{coeftest}\NormalTok{(gradeK1, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(gradeK1, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                    Estimate Std. Error  t value  Pr(>|t|)    
## (Intercept)       904.72124    2.22157 407.2433 < 2.2e-16 ***
## starksmall         14.00613    2.44620   5.7257 1.082e-08 ***
## starkregular+aide  -0.60058    2.25352  -0.2665    0.7899    
## experiencek         1.46903    0.16923   8.6808 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coeftest}\NormalTok{(gradeK2, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(gradeK2, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      Estimate Std. Error     t value     Pr(>|t|)
## (Intercept)       925.6748750  7.5981111 121.8296050 0.000000e+00
## starksmall         15.9330822  2.2251817   7.1603511 9.067413e-13
## starkregular+aide   1.2151960  2.0208171   0.6013389 5.476382e-01
## experiencek         0.7431059  0.1685504   4.4088046 1.058577e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coeftest}\NormalTok{(gradeK3, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(gradeK3, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))[}\DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      Estimate Std. Error     t value     Pr(>|t|)
## (Intercept)       937.6831330 14.2647435  65.7343143 0.000000e+00
## starksmall         15.8900507  2.1389984   7.4287344 1.259182e-13
## starkregular+aide   1.7869378  1.9467305   0.9179174 3.587012e-01
## experiencek         0.6627251  0.1646838   4.0242278 5.790802e-05
## boy               -12.0905123  1.6600732  -7.2831199 3.707612e-13
## lunchkfree        -34.7033021  1.9721158 -17.5969896 1.586414e-67
## black             -25.4305130  3.4724200  -7.3235707 2.752262e-13
\end{verbatim}

We use \texttt{stargazer()} to gather all relevant information in a
structured table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute robust standard errors for each model and gather them in a list}
\NormalTok{rob_se_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fmk, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(gradeK1, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(gradeK2, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(gradeK3, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{)))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{stargazer}\NormalTok{(fmk,fm1,fm2,fm3,}
  \DataTypeTok{title =} \StringTok{"Table 13.3: Project STAR - Differences Estimates with Additional Regressors for Kindergarten"}\NormalTok{,}
  \DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }
  \DataTypeTok{type =} \StringTok{"latex"}\NormalTok{,}
  \DataTypeTok{model.numbers =}\NormalTok{ F,}
  \DataTypeTok{omit.table.layout =} \StringTok{"n"}\NormalTok{,}
  \DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }
  \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"(1)"}\NormalTok{, }\StringTok{"(2)"}\NormalTok{, }\StringTok{"(3)"}\NormalTok{, }\StringTok{"(4)"}\NormalTok{),}
  \DataTypeTok{dep.var.caption  =} \StringTok{"Test Score in Kindergarten"}\NormalTok{,}
  \DataTypeTok{dep.var.labels.include =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{se =}\NormalTok{ rob_se_}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Table 13.3: Project STAR - Differences Estimates with Additional
Regressors for Kindergarten

Test Score in Kindergarten

(1)

(2)

(3)

(4)

starksmall

13.90***

14.01***

15.93***

15.89***

(2.45)

(2.45)

(2.23)

(2.14)

starkregular+aide

0.31

-0.60

1.22

1.79

(2.27)

(2.25)

(2.02)

(1.95)

experiencek

1.47***

0.74***

0.66***

(0.17)

(0.17)

(0.16)

boy

-12.09***

(1.66)

lunchkfree

-34.70***

(1.97)

black

-25.43***

(3.47)

race

8.50

(12.43)

Constant

918.04***

904.72***

925.67***

937.68***

(1.63)

(2.22)

(7.60)

(14.26)

School indicators?

no

no

yes

yes

Observations

5,786

5,766

5,766

5,748

R2

0.01

0.02

0.23

0.29

Adjusted R2

0.01

0.02

0.22

0.28

Residual Std. Error

73.49 (df = 5783)

73.08 (df = 5762)

65.08 (df = 5684)

62.66 (df = 5662)

F Statistic

21.26*** (df = 2; 5783)

39.86*** (df = 3; 5762)

21.41*** (df = 81; 5684)

27.36*** (df = 85; 5662)

While the results in column (1) of table Table 13.3 are just a
repetition of the outcomes obtained for \eqref{eq:starpopreg} before,
columns (2) to (4) reveal that adding student characteristics and school
fixed effects does not lead to significantly different estimates of the
treatment effects. This result makes it more plausible that the
estimates of the effects obainted using model \eqref{eq:starpopreg} do not
suffer from failure of random assignment. Note that there is some
decrease in the standard errors and some increase in \(\overline{R}^2\),
implying that the estimates are more precise.

Because teachers where randomly assigned to classes, inclusion of school
fixed effect allows us to estimate the effect of a teacher's experience
on test scores of students in kindergarten. For regression (3) we find
that the average effect of 10 years experience on test scores is
predicted to be \(10*0.74=7.4\) points. It is important to be aware of
the fact that the other estimates on student characteristics used in
regression (4) \emph{do not} have causal interpretation due to nonrandom
assignment.\footnote{See Chapter 13.3 of the book for a detailed
  discussion.}

To answer the question whether the estimated effect presented in table
13.2 are large or small in a practical sense, we translate the predicted
changes in test scores to units of standard deviation in order to allow
for a comparison (see Section 9.4).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute sample standard deviations of test scores}
\NormalTok{SSD <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
        \StringTok{"K"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(STAR}\OperatorTok{$}\NormalTok{readk }\OperatorTok{+}\StringTok{ }\NormalTok{STAR}\OperatorTok{$}\NormalTok{mathk)),}
        \StringTok{"1"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(STAR}\OperatorTok{$}\NormalTok{read1 }\OperatorTok{+}\StringTok{ }\NormalTok{STAR}\OperatorTok{$}\NormalTok{math1)),}
        \StringTok{"2"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(STAR}\OperatorTok{$}\NormalTok{read2 }\OperatorTok{+}\StringTok{ }\NormalTok{STAR}\OperatorTok{$}\NormalTok{math2)),}
        \StringTok{"3"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(STAR}\OperatorTok{$}\NormalTok{read3 }\OperatorTok{+}\StringTok{ }\NormalTok{STAR}\OperatorTok{$}\NormalTok{math3))}
\NormalTok{)}

\CommentTok{# translate effects of small classes to standard deviations}
\NormalTok{Small <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
           \StringTok{"K"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fmk)[}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{1}\NormalTok{]),}
           \StringTok{"1"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fm1)[}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{2}\NormalTok{]),}
           \StringTok{"2"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fm2)[}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{3}\NormalTok{]),}
           \StringTok{"3"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fm3)[}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{4}\NormalTok{])}
\NormalTok{)}

\CommentTok{# adjust standard errors}
\NormalTok{SmallSE <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
             \StringTok{"K"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(rob_se_}\DecValTok{1}\NormalTok{[[}\DecValTok{1}\NormalTok{]][}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{1}\NormalTok{]),}
             \StringTok{"1"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(rob_se_}\DecValTok{1}\NormalTok{[[}\DecValTok{2}\NormalTok{]][}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{2}\NormalTok{]),}
             \StringTok{"2"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(rob_se_}\DecValTok{1}\NormalTok{[[}\DecValTok{3}\NormalTok{]][}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{3}\NormalTok{]),}
             \StringTok{"3"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(rob_se_}\DecValTok{1}\NormalTok{[[}\DecValTok{4}\NormalTok{]][}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{4}\NormalTok{])   }
\NormalTok{)}

\CommentTok{# translate effects of regular classes with aide to standard deviations}
\NormalTok{RegAide<-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
            \StringTok{"K"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fmk)[}\DecValTok{3}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{1}\NormalTok{]),}
            \StringTok{"1"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fm1)[}\DecValTok{3}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{2}\NormalTok{]),}
            \StringTok{"2"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fm2)[}\DecValTok{3}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{3}\NormalTok{]),}
            \StringTok{"3"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{coef}\NormalTok{(fm3)[}\DecValTok{3}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{4}\NormalTok{])}
\NormalTok{)}

\CommentTok{# adjust standard errors}
\NormalTok{RegAideSE <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
               \StringTok{"K"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(rob_se_}\DecValTok{1}\NormalTok{[[}\DecValTok{1}\NormalTok{]][}\DecValTok{3}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{1}\NormalTok{]),}
               \StringTok{"1"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(rob_se_}\DecValTok{1}\NormalTok{[[}\DecValTok{2}\NormalTok{]][}\DecValTok{3}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{2}\NormalTok{]),}
               \StringTok{"2"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(rob_se_}\DecValTok{1}\NormalTok{[[}\DecValTok{3}\NormalTok{]][}\DecValTok{3}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{3}\NormalTok{]),}
               \StringTok{"3"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(rob_se_}\DecValTok{1}\NormalTok{[[}\DecValTok{4}\NormalTok{]][}\DecValTok{3}\NormalTok{]}\OperatorTok{/}\NormalTok{SSD[}\DecValTok{4}\NormalTok{])   }
\NormalTok{)}

\CommentTok{# Gather the results in a data.frame and round}
\NormalTok{df <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}
  \KeywordTok{round}\NormalTok{(}
    \KeywordTok{data.frame}\NormalTok{(}
\NormalTok{      Small, SmallSE,RegAide, RegAideSE, SSD}
\NormalTok{      ),}
    \DataTypeTok{digits =}  \DecValTok{2}
\NormalTok{    )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

It is fairly easy to turn the data.frame \texttt{df} into a LaTeX table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate simply LaTeX table using stargazer}
\KeywordTok{stargazer}\NormalTok{(df,}
          \DataTypeTok{title =} \StringTok{"Table 13.4: Estimated Class Size Effects (in Units of Standard Deviations)"}\NormalTok{,}
          \DataTypeTok{type =} \StringTok{"html"}\NormalTok{, }
          \DataTypeTok{summary =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Table 13.4: Estimated Class Size Effects (in Units of Standard
Deviations)

K

1

2

3

Small

0.190

0.330

0.230

0.210

SmallSE

0.030

0.030

0.030

0.040

RegAide

0

0.130

0.040

0

RegAideSE

0.030

0.030

0.030

0.030

SSD

73.750

91.280

84.080

73.270

On the one hand, we find that the estimated effects of a small classes
is largest for grade 1. As pointed out in the book, this is probably
because student in the control group for grade 1 did poorly on the test
for some unknown reason or simply due to random variation. On the other
hand, the difference between the estimated effect of beeing in a small
class and beeing in a regular classes with an aide is roughly 0.2
standard deviations for all grades. This leads to the conclusion that
the effect of beeing in a regular sized class with an aide is zero and
the effect of beeing in a small class is roughly the same for all
grades.

The remainder of Chapter 13.3 in the book discusses to what extent these
experimental estimates are comparable with observational estimates
obtained using data on school districts in California and Massachusetts
in Chapter 9. It turns out that the estimates are indeed very similar.
Please refer to the aforementioned section in the book for the a
discussion.

\section{Quasi Experiments}\label{quasi-experiments}

In quasi-experiments, ``as if'' randomness is exploited to use methods
similar to those that have been discussed in the previous Chapter. There
are two types of quasi-experiments:\footnote{See chapter 13.4 of the
  book for some example studies that are based on quasi-experiments.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Random variations in individual circumstances allows to view the
  treatment ``as if'' it was randomly determined.
\item
  The treatment is only partially determined by ``as if'' random
  variation.
\end{enumerate}

The former allows to estimate the effect using either model
\eqref{eq:diffestwar}, i.e.~the \emph{difference estimator with additional
regressors}, or, if there is doubt that the ``as if'' randomness does
not entirely ensure that there are no systematic differences between
control and treatment group, using the \emph{differences-in-differences}
(DID) estimator. In the latter case, an IV approach for estimation of a
model like \eqref{eq:diffestwar} which uses the source of ``as if''
randomness in treatment assignment as the instrument may be applied.

Some more advanced techniques that are helpful in settings where the
treatment assignment is (partially) determined by a threshold in a
so-called running variable are \emph{sharp regression discontinuity
design} (RDD) and \emph{fuzzy regression discontinuity design} (FRDD).

We will briefly review these techniques and, since the book does not
provide any empirical examples in this section, we will use our own
simulated data in a minimal example to discuss how DiD, RDD and FRDD can
be applied in R.

\subsection*{The Differences-in-Differences
Estimator}\label{the-differences-in-differences-estimator}
\addcontentsline{toc}{subsection}{The Differences-in-Differences
Estimator}

In quasi-experiments is often the case that the source of ``as if''
randomness in treatment assignment cannot entirely prevent systematic
differences between control and treatment group. This problem was
encountered by Card and Krüger (1994) who used geography as the ``as
if'' random treatment assignment to study the effect on employment of
workers in the fast-food restaurants caused by an increase in the state
minimum wage in New Jersey in the year of 1992. Their idea was to use
the fact that the increase in minimum wage applied to employees in New
Jersey (treatment group) but not to those living in the neighbouring
Pennsylvania (control group).

It is quite conceivable that such a wage hike is not correlated with
other determinants of employment. However, there still might by some
state-specific differences an thus differences between control and
treatment group. This would render the \emph{differences estimator}
biased and inconsistent. Card and Krüger (1994) solved this by using a
DID estimator: they collected data in February 1992 (before the
treatment) and November 1992 (after the treatment) for the same
restaurants and estimated the effect of the wage hike by analyzing
differences in the differences in employment for New Jersey and
Pensilvania before and after the increase.\footnote{See also the box
  \emph{What is the Effect on Employment of the Minimum Wage?} in
  Chapter 13.4 of the book.} The DID estimator is

\begin{align}
  \widehat{\beta}_1^{\text{diffs-in-diffs}} =& \, (\overline{Y}^{\text{treatment,after}} - \overline{Y}^{\text{treatment,before}}) - (\overline{Y}^{\text{control,after}} - \overline{Y}^{\text{control,before}}) \\
  =& \Delta \overline{Y}^{\text{treatment}} - \Delta \overline{Y}^{\text{control}} \label{eq:DID}
\end{align}

with

\begin{itemize}
\item
  \(\overline{Y}^{\text{treatment,before}}\) - the sample average in the
  treatment group, before the treatment,
\item
  \(\overline{Y}^{\text{treatment,after}}\) - the sample average in the
  treatment group, after the treatment,
\item
  \(\overline{Y}^{\text{treatment,before}}\) - the sample average in the
  control group, before the treatment,
\item
  \(\overline{Y}^{\text{treatment,after}}\) - the sample average in the
  control group, after the treatment.
\end{itemize}

We may use R to reproduce figure 13.1 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# initialize plot and add control group}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{), }
     \DataTypeTok{type =} \StringTok{"p"}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{12}\NormalTok{),}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.3}\NormalTok{,}\FloatTok{1.3}\NormalTok{),}
     \DataTypeTok{main =} \StringTok{"The Differences-in-Differences Estimator"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Period"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,}
     \DataTypeTok{xaxt =} \StringTok{"n"}\NormalTok{,}
     \DataTypeTok{yaxt =} \StringTok{"n"}
\NormalTok{)}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"before"}\NormalTok{,}\StringTok{"after"}\NormalTok{))}
\KeywordTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{13}\NormalTok{))}

\CommentTok{# add treatment group}
\KeywordTok{points}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{11}\NormalTok{), }
       \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{,}
       \DataTypeTok{pch =} \DecValTok{20}
\NormalTok{)}

\CommentTok{# add line segments}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{11}\NormalTok{), }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{), }\DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{9}\NormalTok{), }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{,}\DecValTok{11}\NormalTok{), }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add annotations}
\KeywordTok{text}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\KeywordTok{expression}\NormalTok{(}\KeywordTok{hat}\NormalTok{(beta)[}\DecValTok{1}\NormalTok{]}\OperatorTok{^}\NormalTok{\{DID\}), }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{pos =} \DecValTok{4}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{5.5}\NormalTok{, }\StringTok{"s. mean control"}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{ , }\DataTypeTok{pos =} \DecValTok{4}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{6.8}\NormalTok{, }\StringTok{"s. mean treatment"}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{ , }\DataTypeTok{pos =} \DecValTok{4}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{7.9}\NormalTok{, }\StringTok{"s. mean control"}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{ , }\DataTypeTok{pos =} \DecValTok{4}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{11.1}\NormalTok{, }\StringTok{"s. mean treatment"}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{ , }\DataTypeTok{pos =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-303-1} \end{center}

The DID estimator \eqref{eq:DID} can als be written in regression
notation: \(\widehat{\beta}_1^{\text{DID}}\) is the OLS estimator of
\(\beta_1\) in the model

\begin{align}
  \Delta Y_i = \beta_0 + \beta_1 X_i + u_i \label{eq:did}
\end{align}

where \(\Delta Y_i\) denotes the difference in pre- and post-treatment
outcomes of individual \(i\) and \(X_i\) is the treatment indicator.

Adding additional regressors that measure pre-treatment characteristics
to \eqref{eq:did} we obtain

\begin{align}
  \Delta Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_{1i} + \dots + \beta_{1+r} W_{ri} + u_i, \label{eq:didwar}
\end{align}

the \emph{difference-in-differences estimator} with additional
regressors. The additional regressors may lead to a more precise
estimate of \(\beta_1\).

We will keep things simple and focus on estimation of the treatment
effect using DID in the most simplest case, that is a control and a
treatment group observed for two time periods --- one before and one
after the treatment. In particular, we will see that there are three
different ways to proceed.

First, we simulate pre- and post-treatment data using R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sample size}
\NormalTok{n <-}\StringTok{ }\DecValTok{200}

\CommentTok{# treatment effect}
\NormalTok{TEffect <-}\StringTok{ }\DecValTok{4}

\CommentTok{# treatment dummy}
\NormalTok{TDummy <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n}\OperatorTok{/}\DecValTok{2}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n}\OperatorTok{/}\DecValTok{2}\NormalTok{))}

\CommentTok{# simulate pre- and post-treatment values of the dep. variable}
\NormalTok{y_pre <-}\StringTok{ }\DecValTok{7} \OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{y_pre[}\DecValTok{1}\OperatorTok{:}\NormalTok{n}\OperatorTok{/}\DecValTok{2}\NormalTok{] <-}\StringTok{ }\NormalTok{y_pre[}\DecValTok{1}\OperatorTok{:}\NormalTok{n}\OperatorTok{/}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{y_post <-}\StringTok{ }\DecValTok{7} \OperatorTok{+}\StringTok{ }\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{TEffect }\OperatorTok{*}\StringTok{ }\NormalTok{TDummy }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{y_post[}\DecValTok{1}\OperatorTok{:}\NormalTok{n}\OperatorTok{/}\DecValTok{2}\NormalTok{] <-}\StringTok{ }\NormalTok{y_post[}\DecValTok{1}\OperatorTok{:}\NormalTok{n}\OperatorTok{/}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1} 
\end{Highlighting}
\end{Shaded}

We continue by plotting the data. The function \texttt{jitter()} is used
to add some artificial dispersion in the horizontal component of the
points so that there is less overplotting. \texttt{alpha()} from the
package \texttt{scales} allows to adjust the opacity of colors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(scales)}

\NormalTok{pre <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{length}\NormalTok{(y_pre[TDummy}\OperatorTok{==}\DecValTok{0}\NormalTok{]))}
\NormalTok{post <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{length}\NormalTok{(y_pre[TDummy}\OperatorTok{==}\DecValTok{0}\NormalTok{]))}

\CommentTok{# plot control group in t=1}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{jitter}\NormalTok{(pre, }\FloatTok{0.6}\NormalTok{), }
\NormalTok{     y_pre[TDummy}\OperatorTok{==}\DecValTok{0}\NormalTok{], }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{16}\NormalTok{), }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
     \DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Period"}\NormalTok{,}
     \DataTypeTok{xaxt =} \StringTok{"n"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Artificial Data for DID Estimation"}
\NormalTok{     )}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"before"}\NormalTok{,}\StringTok{"after"}\NormalTok{))}

\CommentTok{# add treatment group in t=1}
\KeywordTok{points}\NormalTok{(}\KeywordTok{jitter}\NormalTok{(pre, }\FloatTok{0.6}\NormalTok{), }
\NormalTok{       y_pre[TDummy}\OperatorTok{==}\DecValTok{1}\NormalTok{], }
       \DataTypeTok{col =} \KeywordTok{alpha}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }
       \DataTypeTok{pch =} \DecValTok{20}
\NormalTok{       )}

\CommentTok{# add control group in t=2}
\KeywordTok{points}\NormalTok{(}\KeywordTok{jitter}\NormalTok{(post, }\FloatTok{0.6}\NormalTok{),}
\NormalTok{       y_post[TDummy}\OperatorTok{==}\DecValTok{0}\NormalTok{], }
       \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
       \DataTypeTok{pch =} \DecValTok{20}
\NormalTok{       )}

\CommentTok{# add treatment group in t=2}
\KeywordTok{points}\NormalTok{(}\KeywordTok{jitter}\NormalTok{(post, }\FloatTok{0.6}\NormalTok{), }
\NormalTok{       y_post[TDummy}\OperatorTok{==}\DecValTok{1}\NormalTok{], }
       \DataTypeTok{col =} \KeywordTok{alpha}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
       \DataTypeTok{pch =} \DecValTok{20}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-305-1} \end{center}

Notice that both observations from control and treatment group have a
higher sample mean after the treatment but that the increase is stronger
for the treatment group. Using DID we may estimate how much of that
difference in the difference between groups is due to the treatment.

It is straightforward to compute the DID estimate in the fashion of
\eqref{eq:DID}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute the DID estimator for the treatment effect 'by hand'}
\KeywordTok{mean}\NormalTok{(y_post[TDummy}\OperatorTok{==}\DecValTok{1}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(y_pre[TDummy}\OperatorTok{==}\DecValTok{1}\NormalTok{]) }\OperatorTok{-}\StringTok{ }
\NormalTok{(}\KeywordTok{mean}\NormalTok{(y_post[TDummy}\OperatorTok{==}\DecValTok{0}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(y_pre[TDummy}\OperatorTok{==}\DecValTok{0}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.960268
\end{verbatim}

Notice that the estimate is close to \(4\), the value chosen as the
treatment effect \texttt{TEffect} above. Since \eqref{eq:did} is a simple
linear model, we may perform OLS estimation of this regression
specification using \texttt{lm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute the DiD estimator using a linear model}
\KeywordTok{lm}\NormalTok{(}\KeywordTok{I}\NormalTok{(y_post }\OperatorTok{-}\StringTok{ }\NormalTok{y_pre) }\OperatorTok{~}\StringTok{ }\NormalTok{TDummy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = I(y_post - y_pre) ~ TDummy)
## 
## Coefficients:
## (Intercept)       TDummy  
##       2.104        3.960
\end{verbatim}

We find that the estimates conincide. Furthermore, one can show that the
DID estimate obained by estimating specification \eqref{eq:did} OLS is the
same as the OLS estimate of \(\beta_{TE}\) in

\begin{align}
  Y_i =& \beta_0 + \beta_1 D_i + \beta_2 Period_i + \beta_{TE} (Period_i \times D_i) \label{eq:DIDint}
\end{align}

where \(D_i\) is the binary treatment indicator, \(Period_i\) is a
binary indicator for the after treatment period and the
\(Period_i \times D_i\) is the interaction of both.

As for \eqref{eq:did}, estimation of \eqref{eq:DIDint} using R is
straightforward. See Chapter 8 for a discussion of interaction terms in
regression models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prepare Data for DID regression using interaction term }
\NormalTok{d <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"Y"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(y_pre,y_post),}
                \StringTok{"Treatment"}\NormalTok{ =}\StringTok{ }\NormalTok{TDummy, }
                \StringTok{"Period"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"1"}\NormalTok{, n), }\KeywordTok{rep}\NormalTok{(}\StringTok{"2"}\NormalTok{, n))}
\NormalTok{                )}

\CommentTok{# estimate the model}
\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment }\OperatorTok{*}\StringTok{ }\NormalTok{Period, }\DataTypeTok{data =}\NormalTok{ d)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ Treatment * Period, data = d)
## 
## Coefficients:
##       (Intercept)          Treatment            Period2  
##             5.858              1.197              2.104  
## Treatment:Period2  
##             3.960
\end{verbatim}

As expected, the estimate of the coefficient on the interaction of the
treatment dummy and the time dummy coincides with the estimates obtained
using the approach \eqref{eq:DID} and OLS estimation of \eqref{eq:did}.

\subsection*{Regression Discontinuity
Estimators}\label{regression-discontinuity-estimators}
\addcontentsline{toc}{subsection}{Regression Discontinuity Estimators}

Consider the model

\begin{align}
  Y_i =& \beta_0 + \beta_1 X_i + \beta_2 W_i + u_i \label{eq:SRDDsetting}
\end{align}

and let

\begin{align*}
X_i =& 
  \begin{cases}
    1, & W_i \geq c \\
    0, & W_i < c
  \end{cases}
\end{align*}

so that the receipt of treatment, \(X_i\), is determined by some
threshold \(c\) of a continuous variable \(W_i\), the so called running
variable. The idea of \emph{regression discontinuity design} is to use
observations with a \(W_i\) close to \(c\) for estimation of
\(\beta_1\), the average treatment effect for individuals with
\(W_i = c\) which is assumed to be a good approximation to the treatment
effect in the population. \eqref{eq:SRDDsetting} is called a \emph{sharp
regression discontinuity design} because treatment assignment is
deterministic and discontinuous at the cutoff: all observations with
\(W_i < c\) do not receive treatment and all observations where
\(W_i \geq c\) are treated.

The subsequent code chunks show how to estimate a linear SRDD using R
and how to produce plots in the way of Figure 13.2 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate sample data}
\NormalTok{W <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\DecValTok{3} \OperatorTok{+}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{W }\OperatorTok{+}\StringTok{ }\DecValTok{10} \OperatorTok{*}\StringTok{ }\NormalTok{(W}\OperatorTok{>=}\DecValTok{0}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the package 'rddtools'}
\KeywordTok{library}\NormalTok{(rddtools)}

\CommentTok{# construct rdd_data }
\NormalTok{data <-}\StringTok{ }\KeywordTok{rdd_data}\NormalTok{(y, W, }\DataTypeTok{cutpoint =} \DecValTok{0}\NormalTok{)}

\CommentTok{# plot the sample data}
\KeywordTok{plot}\NormalTok{(data,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{cex =} \FloatTok{0.35}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"W"}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"Y"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-310-1} \end{center}

The argument \texttt{nbins} sets the number of bins the running variable
is devided in. The dots represent bin averages of the outcome variable.

We may use the function \texttt{rdd\_reg\_lm()} to estimate the
treatment effect using model \eqref{eq:SRDDsetting} for the artificial
data generated above. By choosing \texttt{slope\ =\ "same"} we restrict
the slopes of the estimated regression function to be the same on both
sides of the jump.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the sharp RDD model}
\NormalTok{rdd_mod <-}\StringTok{ }\KeywordTok{rdd_reg_lm}\NormalTok{(}\DataTypeTok{rdd_object =}\NormalTok{ data, }
                      \DataTypeTok{slope =} \StringTok{"same"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(rdd_mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ ., data = dat_step1, weights = weights)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2361 -0.6779 -0.0039  0.7113  3.0096 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  2.93889    0.07082   41.50   <2e-16 ***
## D           10.12692    0.12631   80.18   <2e-16 ***
## x            1.88249    0.11074   17.00   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.019 on 997 degrees of freedom
## Multiple R-squared:  0.972,  Adjusted R-squared:  0.972 
## F-statistic: 1.732e+04 on 2 and 997 DF,  p-value: < 2.2e-16
\end{verbatim}

The coefficient estimate of interest is labeled D. Note that the
estimate is very close to the treatment effect chosen in the DGP above.

It is easy to visualize the result: simply call \texttt{plot()} on the
estimated model object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rdd_mod,}
     \DataTypeTok{cex =} \FloatTok{0.35}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"W"}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-312-1} \end{center}

Note that, as above, the dots represent averages of binned observations.

So far we have assumed that crossing of the threshold determines receipt
of treatment so that the jump of the population regression functions at
the threshold can be regarded as the causal effect of the treatment.

When crossing of the threshold \(c\) is not the only cause for receipt
of the treatment and instead other, treatment is not a deterministic
function of \(W_i\). Instead, it is useful to think of the \(c\) as a
threshold where the \emph{probability of receiving} the treatment jumps.

This jump may be due to unobservable variables that have impact on the
\emph{probability} of being treated. Thus, \(X_i\) in
\eqref{eq:SRDDsetting} will be correlated with the error \(u_i\) and it
becomes more difficult to consistently estimate the treatment effect. In
this setting, using a \emph{fuzzy regression discontinuity design} which
is based an IV approach may be a remedy: take the binary variable
\(Z_i\) as an indicator for crossing of the threshold,

\begin{align*}
  Z_i = \begin{cases}
    1, & W_i \geq c \\
    0, & W_i < c,
  \end{cases}
\end{align*}

and that \(Z_i\) relates to \(Y_i\) only through the treatment indicator
\(X_i\). Then \(Z_i\) and \(u_i\) are uncorrelated but \(Z_i\)
influences receipt of treatment so it is correlated with \(X_i\). Thus,
\(Z_i\) is a valid instrument for \(X_i\) and \eqref{eq:SRDDsetting} can
be estimated using TSLS.

The following code chunk generates sample data where observations with a
value of the running variable \(W_i\) below the cutoff \(c=0\) do not
receive treatment and observations with \(W_i \geq 0\) do receive
treatment with a probability of \(80\%\) so that treatment status is
only partially determined by the running variable and the cutoff.
Treatment leads to an increase in \(Y\) by \(2\) units. Observations
with \(W_i \geq 0\) that do not receive treatment are called
\emph{no-shows}: think of an individual that was assigned to receive the
treatment but somehow managed to avoid it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}

\CommentTok{# generate sample data}
\NormalTok{mu <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)}
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{d <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{2000}\NormalTok{, mu, sigma))}
\KeywordTok{colnames}\NormalTok{(d) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"W"}\NormalTok{, }\StringTok{"Y"}\NormalTok{)}

\CommentTok{# Introduce fuzziness}
\NormalTok{d}\OperatorTok{$}\NormalTok{treatProb <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(d}\OperatorTok{$}\NormalTok{W }\OperatorTok{<}\StringTok{ }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{0.8}\NormalTok{)}

\NormalTok{fuzz <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(d}\OperatorTok{$}\NormalTok{treatProb, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{prob =}\NormalTok{ x))}

\CommentTok{# Treatment effect}
\NormalTok{d}\OperatorTok{$}\NormalTok{Y <-}\StringTok{ }\NormalTok{d}\OperatorTok{$}\NormalTok{Y }\OperatorTok{+}\StringTok{ }\NormalTok{fuzz }\OperatorTok{*}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

We plot all observations and use blue color to mark individuals that did
not receive the treatment and use red for those that received the
treatment.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Colored plot of tretment and control group}
\KeywordTok{plot}\NormalTok{(d}\OperatorTok{$}\NormalTok{W, d}\OperatorTok{$}\NormalTok{Y,}
     \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"steelblue"}\NormalTok{,}\StringTok{"darkred"}\NormalTok{)[}\KeywordTok{factor}\NormalTok{(fuzz)], }
     \DataTypeTok{pch=} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{cex =} \FloatTok{0.5}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{,}\DecValTok{5}\NormalTok{),}
     \DataTypeTok{xlab =} \StringTok{"W"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Y"}
\NormalTok{)}

\CommentTok{# add dashed vertical line at cutoff}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-314-1} \end{center}

Obviously, receipt of treatment is no longer a deterministic function of
the running variable \(W\). Some observations with \(W\geq0\) \emph{did
not} receive the treatment. We may estimate a FRDD by additionally
setting \texttt{treatProb} as the assignment variable \texttt{z} in
\texttt{rdd\_data()}. Then \texttt{rdd\_reg\_lm()} applies the folling
TSLS procedure: treatment is predicted using \(W_i\) and the cutoff
dummy \(Z_i\), the instrumental variable, in the first stage regression.
The fitted values from the first stage regression are used to obtain an
unbiased estimate of the treatment effect using the second stage where
the outcome \(Y\) is regressed on the fitted values and the running
variable \(W_i\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate the Fuzzy RDD}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rdd_data}\NormalTok{(d}\OperatorTok{$}\NormalTok{Y, d}\OperatorTok{$}\NormalTok{W, }
                 \DataTypeTok{cutpoint =} \DecValTok{0}\NormalTok{, }
                 \DataTypeTok{z =}\NormalTok{ d}\OperatorTok{$}\NormalTok{treatProb)}

\NormalTok{frdd_mod <-}\StringTok{ }\KeywordTok{rdd_reg_lm}\NormalTok{(}\DataTypeTok{rdd_object =}\NormalTok{ data, }
                       \DataTypeTok{slope =} \StringTok{"same"}\NormalTok{)}
\NormalTok{frdd_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ### RDD regression: parametric ###
##  Polynomial order:  1 
##  Slopes:  same 
##  Number of obs: 2000 (left: 999, right: 1001)
## 
##  Coefficient:
##   Estimate Std. Error t value  Pr(>|t|)    
## D 1.981297   0.084696  23.393 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimate is close to \(2\), the population treatment effect. We may
call \texttt{plot()} on the model object to obtain a graph consisting of
binned data and the estimated regression function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(frdd_mod, }
     \DataTypeTok{cex =} \FloatTok{0.5}\NormalTok{, }
     \DataTypeTok{lwd =} \FloatTok{0.4}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{,}\DecValTok{5}\NormalTok{),}
     \DataTypeTok{xlab =} \StringTok{"W"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Y"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-316-1} \end{center}

What if we would use a SRDD instead, therby ignoring the fact that
treatment is not perfectly determined by the cutoff in \(W\)?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate Sharp RDD}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rdd_data}\NormalTok{(d}\OperatorTok{$}\NormalTok{Y, d}\OperatorTok{$}\NormalTok{W, }
                 \DataTypeTok{cutpoint =} \DecValTok{0}
\NormalTok{)}
\NormalTok{srdd_mod <-}\StringTok{ }\KeywordTok{rdd_reg_lm}\NormalTok{(}\DataTypeTok{rdd_object =}\NormalTok{ data, }
                       \DataTypeTok{slope =} \StringTok{"same"}\NormalTok{)}
\NormalTok{srdd_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ### RDD regression: parametric ###
##  Polynomial order:  1 
##  Slopes:  same 
##  Number of obs: 2000 (left: 999, right: 1001)
## 
##  Coefficient:
##   Estimate Std. Error t value  Pr(>|t|)    
## D 1.585038   0.067756  23.393 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimate obtained using a SRDD shows a substantial downward bias. In
fact this procedure is inconsistent for the true causal effect so
enlarging the sample does not alleviate the bias.

Hereinafter the book continues with a discussion of potential problems
with quasi-experiments. As for all empirical studies, these potential
problems relate to internal and external validity. This part is followed
by a thechnical discussion of treatment effect estimation when the
causal effect of treatment is heterogeneous in the population. We
encourage you to work on these sections on your own.

\subsubsection*{Summary}\label{summary-5}
\addcontentsline{toc}{subsubsection}{Summary}

In this chapter we have introduced the concept of causal effect in
randomized controlled experiments and quasi-experiments where variations
in circumstances or accidents of nature are treated as sources of ``as
if'' random assignment to treatment. We have also discussed methods that
allow for unbiased and consistent estimation of these effects in both
settings. This included the \emph{differences estimator}, the
\emph{differences-in-differences estimator} as well as \emph{sharp} and
\emph{fuzzy regression discontinuity design} estimators. It was shown
how to apply these estimation techniques in R.

In an empirical application we have shown how to replicate the results
of the analysis of the STAR data presented in Chapter 13.3 of the book
using R. This study uses a randomized controlled experiment to assess
whether smaller classes improve students' performance on standardized
tests. We have stressed that, being related to a randomized controlled
experiment, the data produced by this study are fundamentally different
than those used in the cross-section studies Chapters 4 to 8 are
concerned with and thus motivated usage of a \emph{differences
estimator}.

Chapter 13.4 demonstrated how estimates of treatment effects can be
obtained when the design of the study is a quasi-experiment that allows
for \emph{differences-in-differences} or \emph{regression discontinuity
design} estimators. In particular, we have introduced core functions of
the package \texttt{rddtools} that are convenient for estimation as well
as graphical analysis of the research design before and after estimating
a discontinuity design.

\chapter{Introduction to Time Series Regression and
Forecasting}\label{introduction-to-time-series-regression-and-forecasting}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dynlm)}
\KeywordTok{library}\NormalTok{(stargazer)}
\KeywordTok{library}\NormalTok{(scales)}
\KeywordTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

Time series data is data that is collected for a single entitity over
time. This is fundamentally different from cross-section data which is
data on \emph{multiple} entities at the same point in time. Time series
data allows estimation of the effect on \(Y\) of a change in \(X\)
\emph{over time}. This is what econometricians call a \emph{dynamic
causal effect}. Let us go back to the application to cigarette
consumption of Chapter 12 where we where interest in estimating the
effect on cigarette demand of a price increase caused by a raise of the
general sales tax. One might use time series data to assess the effect
of causal effect of a tax hike on smoking both, initially and in
subsequent periods.

Another application of time series data is forecasting. For example,
weather services use time series models to predict tomorrow's average
temperatur using todays average termperature and average termperatures
of the past, or, to motivate an economic example, central banks are
interested in forecasting next month's unemployment rates.

The remainder of the book deals with the econometric techniques for the
analysis of time series data and application of the latter to problems
of forecasting and estimation of dynamic causal effects. This section
covers the basic concepts presented in Chapter 14 of the book, explains
how to visualize time series data and demonstrates how to estimate
simple autoregressive models where the regressors are past values of the
dependent variable or other variables. In this context we will also
discuss the concept of stationarity, an important property which has
far-reaching consequences since it determines whether the past of a
series has any power in explaining the series' future.

Most empirical applications in this chapter are concerned with
forecasting and use data on U.S. macroeconomic indicators or financial
time series like Gross Domestic Product (GDP), the unemployment rate or
excess stock returns.

\section{Using Regression Models for
Forecasting}\label{using-regression-models-for-forecasting}

What is the difference between estimating models for assessment of
causal effects and forecasting? Consider again the simple example of
estimating the casual effect on test scores of the student-teacher ratio
from Chapter 4.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(CASchools)   }
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{STR <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{students}\OperatorTok{/}\NormalTok{CASchools}\OperatorTok{$}\NormalTok{teachers       }
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{score <-}\StringTok{ }\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{read }\OperatorTok{+}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{math)}\OperatorTok{/}\DecValTok{2}

\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\NormalTok{mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28
\end{verbatim}

As has been stressed in Chapter 6, the estimate of the coefficient on
the student-teacher ratio does not have causal interpretation due to
omitted variable bias. However, in terms deciding which schooling to
send her child to, it might nevertheless be appealig for a parent to use
\texttt{mod} for forecasting test scores in schooling districts where no
public data about on scores are available.

As an example, assume that the average class in a district has \(25\)
students. There is no such thing like a perfect forecast but the
following one-liner might be helpful for the parent to decide.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(mod, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\StringTok{"STR"}\NormalTok{ =}\StringTok{ }\DecValTok{25}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1 
## 641.9377
\end{verbatim}

In a time series context, the parent could use data on present and past
years test scores to predict to forecast next years test scores --- a
typical application for an autoregressive model.

\section{Time Series Data and Serial
Correlation}\label{time-series-data-and-serial-correlation}

GDP measures the productivity of an economy. It is commonly defined as
the value of goods services produced over a given time period. The data
set us\_macro\_quarterly.xlsx is provided by the authors and can be
downloaded
\href{http://wps.aw.com/aw_stock_ie_3/178/45691/11696965.cw/index.html}{here}.
It provides data on quarterly data on US real (i.e.~inflation adjusted)
GDP from years 1947 to 2004.

As before, a good point to start with in pre-estimation is plotting the
data. The package \texttt{quantmod} provides some very convenient
functions for plotting and computing with time series data. We also load
the package \texttt{readxl} to read the data into R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# attach the packages}
\KeywordTok{library}\NormalTok{(quantmod)}
\end{Highlighting}
\end{Shaded}

We begin by importing the data set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load US macroeconomic data}
\NormalTok{USMacroSWQ <-}\StringTok{ }\KeywordTok{read_xlsx}\NormalTok{(}\StringTok{"Data/us_macro_quarterly.xlsx"}\NormalTok{,}
                         \DataTypeTok{sheet =} \DecValTok{1}\NormalTok{,}
                         \DataTypeTok{col_types =} \KeywordTok{c}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\StringTok{"numeric"}\NormalTok{, }\DecValTok{9}\NormalTok{))}
\NormalTok{                        )}

\CommentTok{# format date column}
\NormalTok{USMacroSWQ}\OperatorTok{$}\NormalTok{X__}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{as.yearqtr}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{X__}\DecValTok{1}\NormalTok{, }\DataTypeTok{format =} \StringTok{"%Y:0%q"}\NormalTok{)}

\CommentTok{# adjust column names}
\KeywordTok{colnames}\NormalTok{(USMacroSWQ) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Date"}\NormalTok{, }\StringTok{"GDPC96"}\NormalTok{, }\StringTok{"JAPAN_IP"}\NormalTok{, }\StringTok{"PCECTPI"}\NormalTok{, }\StringTok{"GS10"}\NormalTok{, }\StringTok{"GS1"}\NormalTok{, }\StringTok{"TB3MS"}\NormalTok{, }\StringTok{"UNRATE"}\NormalTok{, }\StringTok{"EXUSUK"}\NormalTok{, }\StringTok{"CPIAUCSL"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When dealing with time series data in R it is preferable to work with
time-series objects that keep track of the frequency of the data and are
extensible. In what follows we will use xts objects, see \texttt{?xts}.
Since the data In \texttt{USMacroSWQ} are in quarterly frequence we
convert the first column to \texttt{yearqtr} format before generating
the xts object \texttt{GDP}.

The function \texttt{Delt()} from the package \texttt{Quantmod} computes
growth rates. Note that, since the data is quarterly, we anualize the
quarterly changes to obtain anual growth rates as
\[Rate_{annual} = (1+Rate_{quarterly})^4-1\].

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# GDP series as xts object}
\NormalTok{GDP <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{GDPC96, USMacroSWQ}\OperatorTok{$}\NormalTok{Date)[}\StringTok{"1960::2013"}\NormalTok{]}

\CommentTok{# GDP growth series as xts object}
\NormalTok{GDPGrowth <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(}\DecValTok{400} \OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(GDP}\OperatorTok{/}\KeywordTok{lag}\NormalTok{(GDP)))}
\end{Highlighting}
\end{Shaded}

We use \texttt{chart.TimeSeries()} from the package
\texttt{PerformanceAnalytics} to generate plots of \texttt{GDP} and
\texttt{GDPGrowth}. The following code chunks reproduce figure 14.1 of
the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# reproduce figure 14.1 (a) of the book}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(GDP)),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Logarithm"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"US Quarterly Real GDP"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-324-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# reproduce figure 14.1 (b) of the book}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(GDPGrowth),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Logarithm"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Growth Rates in US Real GDP"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-325-1} \end{center}

\subsection*{Notation, Lags, Differences, Logarithms and Growth
Rates}\label{notation-lags-differences-logarithms-and-growth-rates}
\addcontentsline{toc}{subsection}{Notation, Lags, Differences,
Logarithms and Growth Rates}

If observations of a variable \(Y\) are recorded over time, we denote
\(Y_t\) the value observed at time \(t\). The period between two
sequential observations \(Y_t\) and \(Y_{t-1}\) is a unit of time:
hours, days, weeks, months, quarters, years and so on. Key Concept 14.1
introduces the essential terminology and notation for time series data
we will use.

Key Concept 14.1

Lags, First Differences, Logarithms and Growth Rates

\begin{itemize}
\item
  Previous values of a time series are called \emph{lags}. The first lag
  of \(Y_t\) is \(Y_{t-1}\). The \(j^th\) lag of \(Y_t\) is \(Y_{t-j}\).
  In R, lags of univariate or multivariate time series are conveniently
  computed by \texttt{lag()}, see \texttt{?lag}
\item
  Sometimes we need to work with a differenced series. The first
  difference of a series is \(\Delta Y_{t} = Y_t - Y_{t-1}\) --- the
  difference between periods \(t\) and \(t-1\). If \texttt{Y} is a time
  series, the first lag is computed as \texttt{Y-lag(Y)}.
\item
  It may be convenient to work with the first difference in logarithms
  of a series. We denote this by
  \(\Delta \log(Y_t) = \log(Y_t) - \log(Y_{t-1})\). For a time series
  \texttt{Y}, this obtained using \texttt{log(Y/lag(Y))}.
\item
  \(100 \Delta Y_t\) is an approximation for the percentage change
  between \(Y_t\) and \(Y_{t-1}\).
\end{itemize}

The definitions made in Key Concept 14.1 are useful because of two
properties that are common to many economic time series:

\begin{itemize}
\item
  Exponential growth: some economic series grow approximately
  exponentially such that taking logarithm of the series makes them
  approximately linear.
\item
  The standard deviation of many economic time series is approximately
  proportional to their level. Therefore, the standard deviation of the
  logarithm of such a series is approximately constant.
\end{itemize}

Furthermore, it is common to report rates of growth in macroeconomic
series which is why \(\log\)-differences are often used.

Table 14.1 of the book present quaterly U.S. GDP, its logarithm, the
anualized growth rate and the first lag of the anualized growth rate
series for the period 2012:Q1 - 2013:Q1. The following simple function
can be used to compute these quantities for time series \texttt{series}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute logarithms, annual growth rates and 1st lag of growth rates}
\NormalTok{quants <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(series) \{}
\NormalTok{  s <-}\StringTok{ }\NormalTok{series}
  \KeywordTok{return}\NormalTok{(}
    \KeywordTok{data.frame}\NormalTok{(}\StringTok{"Lead"}\NormalTok{ =}\StringTok{ }\NormalTok{s,}
               \StringTok{"Logarithm"}\NormalTok{ =}\StringTok{ }\KeywordTok{log}\NormalTok{(s),}
               \StringTok{"AnnualGrowthRate"}\NormalTok{ =}\StringTok{ }\DecValTok{400} \OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(s}\OperatorTok{/}\KeywordTok{lag}\NormalTok{(s)),}
               \StringTok{"1stLagAnnualGrowthRate"}\NormalTok{ =}\StringTok{ }\KeywordTok{lag}\NormalTok{(}\DecValTok{400} \OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(s}\OperatorTok{/}\KeywordTok{lag}\NormalTok{(s)))}
\NormalTok{      )}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Notice that the annual growth rate is computed using the approximation
\[ Annual Growth Y_t = 400 \cdot \left[\log(Y_t) - \log(Y_{t-1})\right] \]
discussed in Key Concept 14.1.

We call \texttt{quants()} on observations for the period 2011 Q3 - 2013
Q1.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quants}\NormalTok{(GDP[}\StringTok{"2011-07::2013-01"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             Lead Logarithm AnnualGrowthRate X1stLagAnnualGrowthRate
## 2011 Q3 15062.14  9.619940               NA                      NA
## 2011 Q4 15242.14  9.631819        4.7518062                      NA
## 2012 Q1 15381.56  9.640925        3.6422231               4.7518062
## 2012 Q2 15427.67  9.643918        1.1972004               3.6422231
## 2012 Q3 15533.99  9.650785        2.7470216               1.1972004
## 2012 Q4 15539.63  9.651149        0.1452808               2.7470216
## 2013 Q1 15583.95  9.653997        1.1392015               0.1452808
\end{verbatim}

\subsubsection*{Autocorrelation}\label{autocorrelation}
\addcontentsline{toc}{subsubsection}{Autocorrelation}

Observations of a time series are typically correlated. This type
correlation is called \emph{autocorrelation} or \emph{serial
correlation}. Key Concept 14.2 summarizes the concepts of population
autocovariance and population autocorrelation and shows how to compute
their sample equivalents.

Key Concept 14.2

Autocorrelation and Autocovariance

The covariance between \(Y_t\) and its \(j^{th}\) lag, \(Y_{t-j}\), is
called the \(j^{th}\) \emph{autocovariance} of the series \(Y_t\). The
\(j^{th}\) \emph{autocorrelation coefficient}, also called the
\emph{serial correlation coefficient}, measures the correlation between
\(Y_t\) and \(Y_{t-j}\).

We thus have

\begin{align*}
  j^{th} \text{autocovariance} =& \, cov(Y_t,Y_{t-j}) \\
  j^{th} \text{autocorrelation} = \rho_j =& \, corr(Y_t,Y_{t-j}) = \frac{cov(Y_t,Y_{t-j)}}{\sqrt{var(Y_t)var{Y_{t-j}}}}.
\end{align*}

Population Autocovariance and population autocorrelation can be
estimated by \(\widehat{cov(Y_t,Y_{t-j})}\), the sample autocovariance,
and \(\widehat{\rho}_j\), the sample autocorrelation.

\begin{align*}
  \widehat{cov(Y_t,Y_{t-j})} =& \, \frac{1}{T} \sum_{t=j+1}^T (Y_t - \overline{Y}_{j+1:T})(Y_{t-j} - \overline{Y}_{1:T-j}) \\
  \widehat{\rho}_j =& \, \frac{\widehat{cov(Y_t,Y_{t-j})}}{\widehat{var(Y_t)}}.
\end{align*}

In R the function \texttt{acf()} from the package stats computes the
sample autocovariance or the sample autocorrelation function.

Using \texttt{acf()} it is straightforward to compute the first four
sample autocorrelations of the series \texttt{GDPGrowth}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(GDPGrowth), }\DataTypeTok{lag.max =} \DecValTok{4}\NormalTok{, }\DataTypeTok{plot =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Autocorrelations of series 'na.omit(GDPGrowth)', by lag
## 
##  0.00  0.25  0.50  0.75  1.00 
## 1.000 0.352 0.273 0.114 0.106
\end{verbatim}

This is evidence that there is mild positive autocorrelation in the
growth of GDP: if GDP grows faster than average in one period, there is
a tendency that it grows faster than average in the following period.

\subsubsection*{Other Examples of Economic Time
Series}\label{other-examples-of-economic-time-series}
\addcontentsline{toc}{subsubsection}{Other Examples of Economic Time
Series}

Figure 14.2 of the book presents four plots. The U.S. unemployment rate,
the U.S. Dollar / British Pound exchange rate, The logarithm of the
Janapese industrial production index as well as daily changes in the
Whilshire 5000 stock price index, a financial time series. The next code
chunck reproduces the plots of the three macroenomic series and adds
percentage changes in the daily values of the New York Stock Exchange
Composite index as a fourth one (the data set, \texttt{NYSESW} comes
with the \texttt{AER} package).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define series as xts objects}
\NormalTok{USUnemp <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{UNRATE, USMacroSWQ}\OperatorTok{$}\NormalTok{Date)[}\StringTok{"1960::2013"}\NormalTok{]}

\NormalTok{DollarPoundFX <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{EXUSUK, USMacroSWQ}\OperatorTok{$}\NormalTok{Date)[}\StringTok{"1960::2013"}\NormalTok{]}
  
\NormalTok{JPIndProd <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(}\KeywordTok{log}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{JAPAN_IP), USMacroSWQ}\OperatorTok{$}\NormalTok{Date)[}\StringTok{"1960::2013"}\NormalTok{]}

\KeywordTok{data}\NormalTok{(}\StringTok{"NYSESW"}\NormalTok{)  }
\NormalTok{NYSESW <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(}\KeywordTok{Delt}\NormalTok{(NYSESW))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(USUnemp),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"US Unemployment Rate"}\NormalTok{,}
     \DataTypeTok{cex.main =} \DecValTok{1}
\NormalTok{)}


\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(DollarPoundFX),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Dollar per pound"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"U.S. Dollar / B. Pound Exchange Rate"}\NormalTok{,}
     \DataTypeTok{cex.main =} \DecValTok{1}
\NormalTok{)}

\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(JPIndProd),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Logarithm"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Japanese Industrial Production"}\NormalTok{,}
     \DataTypeTok{cex.main =} \DecValTok{1}
\NormalTok{)}

\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(NYSESW),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent per Day"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"New York Stock Exchange Composite Index"}\NormalTok{,}
     \DataTypeTok{cex.main =} \DecValTok{1}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-330-1} \end{center}

Note that the series show quite different characteristics. The
unemployment rate increases during recessions and declines in times of
economic recoveries and growth. The Dollar/Pund exchange rates shows a
deterministic patern until the end of the Bretton Woods system. Japans
industrial production exhibits an upward trend and decreasing growth.
Daily changes in the New York Stock Exchange composite index seems to be
random around the zero line. Sample autocorrelations confirm this
conjecture.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(NYSESW), }\DataTypeTok{plot =}\NormalTok{ F, }\DataTypeTok{lag.max =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Autocorrelations of series 'na.omit(NYSESW)', by lag
## 
##      0      1      2      3      4      5      6      7      8      9 
##  1.000  0.040 -0.016 -0.023  0.000 -0.036 -0.027 -0.059  0.013  0.017 
##     10 
##  0.004
\end{verbatim}

The first 10 sample autocorrelation coefficients are very close to zero.
Further evidence can be found by looking at the plot generated by
\texttt{acf()} by default.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acf}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(NYSESW), }\DataTypeTok{main =} \StringTok{"Sample Autocorrelation for NYSESW Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-332-1} \end{center}

The blue dashed lines represent an approximate \(95\%\) confidence
interval if there was no serial correlation. If a sample autocorrelation
lies beyond these bands, it is statistically different from zero. For
most lags we see that the sample autocorrelation does not exceed the
confidence bands and there are only a few that lie marginally beyond the
limits.

Furthermore, note that NYSWSW series shows a pattern which
econometricians call \emph{volatility clustering}: there are periods of
high and periods of low variance. This is common for many financial time
series.

\section{Autoregressions}\label{autoregressions}

Growth forecasts are important for many economic entitites. For example,
the production industry relies forecasts of GDP growth published by the
central bank when deciding on future budgets and production plans.
Autoregressive models are heavily used in economic forecasting. An
autoregressive model relates a time series variable to its past values.
Autoregressive models are heavily used in forecasting. This chapter
discusses the basic ideas of autoregressions models, shows how they are
estimated and discusses an application to forecasting of GDP growth
using R.

\subsubsection{The First-Order Autoregressive
Model}\label{the-first-order-autoregressive-model}

It is intuitive that the immediate past of a time series should have
power to predict its near future. The simplest autoregressive model uses
only the most recent outcome of the time series observed to predict
future value. Such a model is called a first-order autoregressive model
which is often abbriviated AR(1) where the 1 indicates that the order is
one.

\begin{align*}
  Y_t = \beta_0 + \beta_1 Y_{t-1} + u_t
\end{align*}

is the AR(1) population model of a time series \(Y_t\).

For the GDP growth series, an autoregressive model of order one uses
only the information on GDP growth observed in the last quarter to
predict a future growth rate. The first-order autoregression model of
the GDP growth rate can be estimated by computing OLS estimates in the
regression of \(GDPGR_t\) on \(GDPGR_{t-1}\),

\begin{align}
  \widehat{GDPGR}_t = \beta_0 + \beta_1 GDPGR_{t-1} + u_t. \label{eq:GDPGRAR1}
\end{align}

Following the book we use data from 1962 to 2012 to estimate
\eqref{eq:GDPGRAR1}. This is easily done with the function
\texttt{ar.ols()} from package stats.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Subset data}
\NormalTok{GDPGRSub <-}\StringTok{ }\NormalTok{GDPGrowth[}\StringTok{"1962::2012"}\NormalTok{]}

\CommentTok{# estimate the model}
\KeywordTok{ar.ols}\NormalTok{(GDPGRSub, }
       \DataTypeTok{order.max =} \DecValTok{1}\NormalTok{, }
       \DataTypeTok{demean =}\NormalTok{ F, }
       \DataTypeTok{intercept =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## ar.ols(x = GDPGRSub, order.max = 1, demean = F, intercept = T)
## 
## Coefficients:
##      1  
## 0.3384  
## 
## Intercept: 1.995 (0.2993) 
## 
## Order selected 1  sigma^2 estimated as  9.886
\end{verbatim}

We can check that the computations done by \texttt{ar.ols()} are the
same as done by \texttt{lm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# length of data set}
\NormalTok{N <-}\KeywordTok{length}\NormalTok{(GDPGRSub)}

\NormalTok{GDPGR_leads <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(GDPGRSub[}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\NormalTok{GDPGR_lags <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(GDPGRSub[}\OperatorTok{-}\NormalTok{N])}

\CommentTok{# estimate the model}
\NormalTok{armod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(GDPGR_leads }\OperatorTok{~}\StringTok{ }\NormalTok{GDPGR_lags)}
\NormalTok{armod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = GDPGR_leads ~ GDPGR_lags)
## 
## Coefficients:
## (Intercept)   GDPGR_lags  
##      1.9950       0.3384
\end{verbatim}

As usual, we may use \texttt{coeftest()} to obtain a robust summary on
the estimated regression coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# robust summary}
\KeywordTok{coeftest}\NormalTok{(armod, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(armod, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 1.994986   0.349539  5.7075 4.070e-08 ***
## GDPGR_lags  0.338436   0.075812  4.4641 1.339e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Thus the estimated model is

\begin{align}
  \widehat{GDPGR}_t = \underset{(0.350)}{1.995} + \underset{(0.076)}{0.338} GDPGR_{t-1} \label{eq:gdpgrar1}.
\end{align}

Notice that we omit the first observation for \(GDPGR_{1962 \ Q1}\) from
the dependent variable vector since
\(GDPGR_{1962 \ Q1 - 1} = GDPGR_{1961 \ Q4}\), is not included in the
sample. Similarly, the last observation, \(GDPGR_{2012 \ Q4}\), is
excluded from the predictor vector since the data set does not include
\(GDPGR_{2012 \ Q4 + 1} = GDPGR_{2013 \ Q1}\). Put differently, when
estimation the model, one observation is lost because of the time series
structure of the data.

\subsubsection*{Forecasts and Forecast
Errors}\label{forecasts-and-forecast-errors}
\addcontentsline{toc}{subsubsection}{Forecasts and Forecast Errors}

Suppose a random variable \(Y_t\) follows an AR(1) model with an
intercept and that you have an OLS estimate of the model on the basis of
observations for \(T\) periods. Then you may use the AR(1) model to
obtain \(Y_{T+1\vert T}\), a forecast for \(Y_{T+1}\) where

\begin{align*}
  \widehat{Y}_{T+1\vert T} = \widehat{\beta}_0 + \widehat{\beta}_1 Y_T.
\end{align*}

The forecast error is

\begin{align*}
  \text{Forecast error} = Y_{T+1} - \widehat{Y}_{T+1\vert T}.
\end{align*}

\subsubsection*{Forecasts versus predicted
values}\label{forecasts-versus-predicted-values}
\addcontentsline{toc}{subsubsection}{Forecasts versus predicted values}

Note that \emph{forecasted values} of \(Y_t\) are \emph{not} what we
refer to as \emph{OLS predicted values} of \(Y_t\). Also, the forecast
error is \emph{not} an OLS residual. Forecasts and forecast errors are
obtained using \emph{out-of-sample} values while predicted values and
residuals are computed for actually \emph{in-sample} values that were
actually observed and used in estimation of the model.

The root mean squared forecast error (RMSFE) measures the typical size
of the forecast error and is defined as

\begin{align*}
  RMSFE = \sqrt{E\left[\left(Y_{T+1} - \widehat{Y}_{T+1\vert T}\right)^2\right]}.
\end{align*}

\subsubsection*{Application to GDP
Growth}\label{application-to-gdp-growth}
\addcontentsline{toc}{subsubsection}{Application to GDP Growth}

Using \eqref{eq:gdpgrar1}, the estimated AR(1) model of GDP growth, we may
perform the forecast for GDP growth in 2013:Q1 (remember that the model
was estimated using data for periods 1962:Q1 - 2012:Q4 so 2013:Q1 is an
out of sample period). This is done by plugging
\(GDPGR_{2012:Q4} \approx 0.15\) in \eqref{eq:gdpgrar1},

\begin{align*}
  \widehat{GDPGR}_{2013:Q1} = 1.995 + 0.348 \cdot 0.15 = 2.047.
\end{align*}

\texttt{forecast()} from the forecast package has some useful features
for forecasting of time series data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(forecast)}

\CommentTok{# GDP growth rate 2012:Q4}
\NormalTok{new <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"GDPGR_lags"}\NormalTok{ =}\StringTok{ }\NormalTok{GDPGR_leads[N}\OperatorTok{-}\DecValTok{1}\NormalTok{])}

\CommentTok{# predict GDP growth rate 2013:Q1}
\KeywordTok{forecast}\NormalTok{(armod, }\DataTypeTok{newdata =}\NormalTok{ new)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
## 1       2.044155 -2.036225 6.124534 -4.213414 8.301723
\end{verbatim}

Using \texttt{forecast()} we obtain the same point forecast of about
2.0, along with \(80\%\) and \(95\%\) forecast intervals. We conclude
that our AR(1) model forecasts GDP growth to be \(2\%\) in 2013:Q1.

How accurate is this forecast? First, notice that the forecast error is
pretty big: \(GDPGR_{2013:Q1} \approx 1.1\%\) while our forecast is
\(2\%\). Second, by calling \texttt{summary()} on \texttt{armod} we find
that the model explains only little of the variation in the growth rate
of GDP and the \(SER\) is about \(3.16\). Leaving aside forecast
uncertainty due to estimation of \(\beta_0\) and \(\beta_1\), the
\(RMSFE\) must be at least \(3.16\%\) which is pretty inaccuarate.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the forecast error}
\KeywordTok{forecast}\NormalTok{(armod, }\DataTypeTok{newdata =}\NormalTok{ new)}\OperatorTok{$}\NormalTok{mean }\OperatorTok{-}\StringTok{ }\NormalTok{GDPGrowth[}\StringTok{"2013"}\NormalTok{][}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 x
## 2013 Q1 0.9049532
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# R^2}
\KeywordTok{summary}\NormalTok{(armod)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1149576
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# SER}
\KeywordTok{summary}\NormalTok{(armod)}\OperatorTok{$}\NormalTok{sigma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.15979
\end{verbatim}

\subsection*{\texorpdfstring{The \(p^{th}\)-Order Autoregressive
Model}{The p\^{}\{th\}-Order Autoregressive Model}}\label{the-pth-order-autoregressive-model}
\addcontentsline{toc}{subsection}{The \(p^{th}\)-Order Autoregressive
Model}

For forecasting GPD growth in period \(t\), the AR(\(1\)) model
\eqref{eq:gdpgrar1} disregards any information in the past of the series
that is more distant than one period. An AR(\(p\)) model incorporates
the information of \(p\) lags of the series. The idea is explained in
Key Concept 14.3.

Key Concept 14.3

Autoregressions

An AR(\(p\)) model assumes that a time series \(Y_t\) can be represented
by a linear function of \(p\) of its lagged values. We say that

\begin{align*}
  Y_t = \beta_0 + \beta_1 Y_{t-1} + \beta_2 Y_{t-2} + \dots + \beta_p Y_{t-p} + u_t.
\end{align*}

is an autoregressive model of order \(p\) where
\(E(u_t\vert Y_{t-1}, Y_{t-2}, \dots)=0\).

Following the book, we estimate an AR(\(2\)) model of the GDP growth
series from 1962:Q1 to 2012:Q4.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate AR2 model}
\NormalTok{GDPGR_AR2 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(}\KeywordTok{ts}\NormalTok{(GDPGR_leads) }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{ts}\NormalTok{(GDPGR_leads)) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{ts}\NormalTok{(GDPGR_leads), }\DecValTok{2}\NormalTok{))}

\KeywordTok{coeftest}\NormalTok{(GDPGR_AR2, }\DataTypeTok{vcov. =}\NormalTok{ sandwich)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                       Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)           1.631747   0.402023  4.0588 7.096e-05 ***
## L(ts(GDPGR_leads))    0.277787   0.079250  3.5052 0.0005643 ***
## L(ts(GDPGR_leads), 2) 0.179269   0.079951  2.2422 0.0260560 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimation yields

\begin{align}
  \widehat{GDPGR}_t = \underset{(0.40)}{1.63} + \underset{(0.08)}{0.28} GDPGR_{t-1} + \underset{(0.08)}{0.18} GDPGR_{t-1}. \label{eq:GDPGRAR2}
\end{align}

We see that the coefficient on the second lag is significantly different
from zero. Note that the fit improves slightly: \(\overline{R^2}\) grows
from \(0.11\) for the AR(\(1\)) model to about \(0.14\) and the \(SER\)
reduces to \(3.13\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# R^2}
\KeywordTok{summary}\NormalTok{(GDPGR_AR2)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1425484
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# SER}
\KeywordTok{summary}\NormalTok{(GDPGR_AR2)}\OperatorTok{$}\NormalTok{sigma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.132122
\end{verbatim}

We may use the AR(\(2\)) model to obtain a forecast for GDP growth in
2013:Q1 in the same manner as for the AR(1) model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# AR(2) forecast of GDP growth in 2013:Q1 }
\StringTok{"Forecast"}\NormalTok{ <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"2013:Q1"}\NormalTok{ =}\StringTok{ }\KeywordTok{coef}\NormalTok{(GDPGR_AR2) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, GDPGR_leads[N}\OperatorTok{-}\DecValTok{1}\NormalTok{], GDPGR_leads[N}\OperatorTok{-}\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

This leads to a forecast error of roughly \(-1\%\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute AR(2) forecast error }
\NormalTok{GDPGrowth[}\StringTok{"2013"}\NormalTok{][}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{Forecast}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 x
## 2013 Q1 -1.025358
\end{verbatim}

\section{Can You Beat the Market? (Part
I)}\label{can-you-beat-the-market-part-i}

The thoery of efficient capital markets states stock prices embody all
currently available information. If this hypothesis holds, it should not
be possible to estimate a useful model for forecasting future stock
returns using publicly available information on past returns (this is
also refered to as the weak-form efficiency hypothesis): if it was
possible to forecast the market, traders would be able to make
arbitrage, e.g.~by relying on an AR(\(2\)) model, they would use
information that is not already priced-in which would be not consistent
with the theory.

This idea is presented in the Box \emph{Can You Beat the Market? (Part
I)} on page 582 of the book. This section reproduces the estimation
results.

We start by importing monthly data from 1931:1 to 2002:12 on excess
returns of a broad-based index of stock prices, the CRSP value-weighted
index. The data are provided by the authors of the book as an excel
sheet which can be downloaded
\href{http://wps.aw.com/wps/media/objects/11422/11696965/data3eu/Stock_Returns_1931_2002.xlsx}{here}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in data on stock returns}
\NormalTok{SReturns <-}\StringTok{ }\KeywordTok{read_xlsx}\NormalTok{(}\StringTok{"Data/Stock_Returns_1931_2002.xlsx"}\NormalTok{,}
                      \DataTypeTok{sheet =} \DecValTok{1}\NormalTok{,}
                      \DataTypeTok{col_types =} \StringTok{"numeric"}
\NormalTok{                      )}
\end{Highlighting}
\end{Shaded}

We continue by converting the data to an object of class ts.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert to ts object}
\NormalTok{StockReturns <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(SReturns[, }\DecValTok{3}\OperatorTok{:}\DecValTok{4}\NormalTok{], }
        \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1931}\NormalTok{, }\DecValTok{1}\NormalTok{), }
        \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{, }\DecValTok{12}\NormalTok{), }
        \DataTypeTok{frequency =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, we estimate AR(\(1\)), AR(\(2\)) and AR(\(4\)) models of excess
returns from 1960:1 to 2002:12.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# AR(1)}
\NormalTok{SR_AR1 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ExReturn }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn), }
      \DataTypeTok{data =}\NormalTok{ StockReturns, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{,}\DecValTok{12}\NormalTok{))}

\CommentTok{# AR(2)}
\NormalTok{SR_AR2 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ExReturn }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn, }\DecValTok{2}\NormalTok{), }
      \DataTypeTok{data =}\NormalTok{ StockReturns, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{,}\DecValTok{12}\NormalTok{))}

\CommentTok{# AR(4)}
\NormalTok{SR_AR4 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ExReturn }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn, }\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn, }\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn, }\DecValTok{4}\NormalTok{), }
      \DataTypeTok{data =}\NormalTok{ StockReturns, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{,}\DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

After computing robust standard errors, we gather the results in a
stargazer table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rob_se <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{sandwich}\NormalTok{(SR_AR1))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{sandwich}\NormalTok{(SR_AR2))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{sandwich}\NormalTok{(SR_AR4)))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{stargazer}\NormalTok{(SR_AR1, SR_AR2, SR_AR4,}
  \DataTypeTok{title =} \StringTok{"Autoregressive Models of Monthly Excess Stock Returns"}\NormalTok{,}
  \DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }
  \DataTypeTok{model.numbers =}\NormalTok{ F,}
  \DataTypeTok{omit.table.layout =} \StringTok{"n"}\NormalTok{,}
  \DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }
  \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"AR(1)"}\NormalTok{, }\StringTok{"AR(2)"}\NormalTok{, }\StringTok{"AR(4)"}\NormalTok{),}
  \DataTypeTok{dep.var.caption  =} \StringTok{"Excess returns on the CSRP value-weighted index"}\NormalTok{,}
  \DataTypeTok{dep.var.labels.include =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{covariate.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"$excess return_\{t-1\}$"}\NormalTok{, }\StringTok{"$excess return_\{t-2\}$"}\NormalTok{, }
                       \StringTok{"$excess return_\{t-3\}$"}\NormalTok{, }\StringTok{"$excess return_\{t-4\}$"}\NormalTok{, }
                       \StringTok{"Intercept"}\NormalTok{),}
  \DataTypeTok{se =}\NormalTok{ rob_se,}
  \DataTypeTok{omit.stat =} \KeywordTok{c}\NormalTok{(}\StringTok{"rsq"}\NormalTok{)}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

Autoregressive Models of Monthly Excess Stock Returns

Excess returns on the CSRP value-weighted index

AR(1)

AR(2)

AR(4)

excess returnt-1

0.05

0.05

0.05

(0.05)

(0.05)

(0.05)

excess returnt-2

-0.05

-0.05

(0.05)

(0.05)

excess returnt-3

0.01

(0.05)

excess returnt-4

-0.02

(0.05)

Intercept

0.31

0.33*

0.33

(0.20)

(0.20)

(0.20)

Observations

516

516

516

Adjusted R2

0.001

0.001

-0.002

Residual Std. Error

4.33 (df = 514)

4.33 (df = 513)

4.34 (df = 511)

F Statistic

1.31 (df = 1; 514)

1.37 (df = 2; 513)

0.72 (df = 4; 511)

The results presented above are consistent with the hypothesis of
efficient financial markets: there are no statistically significant
coefficients in any of estimated models and the the hypotheses that the
respective set lags has no power in explaining today's returns cannot be
rejected. Notice also that \(\overline{R^2}\) is almost zero in all
models and even negative for the AR(\(4\)) model. This suggests that
none of the models is useful for forecasting stock returns.

\section{Additional Predictors and The ADL
Model}\label{additional-predictors-and-the-adl-model}

Instead of only using the dependent variable's lags as predictors, an
autoregressive distributed lag (ADL) model uses also other lags of other
variables for forecasting. The general ADL model is summarized in Key
Concept 14.4

Key Concept 14.4

The Autoregressive Distributed Lag Model

An ADL(\(p\),\(q\)) model assumes that a time series \(Y_t\) can be
represented by a linear function of \(p\) of its lagged values and \(q\)
lags of \(X_t\), another time series. We say that

\begin{align*}
  Y_t =& \, \beta_0 + \beta_1 Y_{t-1} + \beta_2 Y_{t-2} + \dots + \beta_p Y_{t-p} \\ 
      +& \, \delta_1 X_{t-1} + \delta_2 X_{t-2} + \dots + \delta_q X_{t-q} X_{t-q} + u_t.
\end{align*}

is an \emph{autoregressive distributed lag model} with \(p\) lags of
\(Y_t\) and \(q\) lags of \(X_t\) where
\(E(u_t\vert Y_{t-1}, Y_{t-2}, \dots, X_{t-1}, X_{t-2}, \dots)=0\).

\subsubsection*{Forecasting GDP Growth Using the Term
Spread}\label{forecasting-gdp-growth-using-the-term-spread}
\addcontentsline{toc}{subsubsection}{Forecasting GDP Growth Using the
Term Spread}

Interest on long-term and short term treasury bonds are closly linked to
the macroeconomic development. While interest rates on both types of
bonds have the same long-run tendencies, they behave quite differently
in the short run. The difference in interest rates of two bonds with
distinct matuarity is called the \emph{term spread}.

The following code chuncks reproduce figure 14.3 of the book which
displays interest rates of 10-year U.S. Treasury bonds and 3 month U.S.
Treasury bills from 1960 to 2012.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 3 months Treasury bills interest rate}
\NormalTok{TB3MS <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{TB3MS, USMacroSWQ}\OperatorTok{$}\NormalTok{Date)[}\StringTok{"1960::2012"}\NormalTok{]}

\CommentTok{# 10 years Treasury bonds interest rate}
\NormalTok{TB10YS <-}\StringTok{ }\KeywordTok{xts}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{GS10, USMacroSWQ}\OperatorTok{$}\NormalTok{Date)[}\StringTok{"1960::2012"}\NormalTok{]}

\CommentTok{# term spread}
\NormalTok{TSpread <-}\StringTok{ }\NormalTok{TB10YS }\OperatorTok{-}\StringTok{ }\NormalTok{TB3MS}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# reproduce figure 14.2 (a) of the book}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{merge}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(TB3MS),}\KeywordTok{as.zoo}\NormalTok{(TB10YS)), }
     \DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{, }
     \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\StringTok{"steelblue"}\NormalTok{),}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent per annum"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Interest Rates"}
\NormalTok{)}

\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"TB3MS"}\NormalTok{,}\StringTok{"TB10YS"}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\StringTok{"steelblue"}\NormalTok{),}
       \DataTypeTok{lwd =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-349-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# reproduce figure 14.2 (b) of the book}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(TSpread), }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent per annum"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Term Spread"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-349-2} \end{center}

Notice that before recessions, the gap between interests on long-term
bonds and short term bills narrows and consequently the term spread
declines drastically towards zero or even falls below zero in times of
economic stress. This information might be used to improve forecasts of
future GDP growth.

We check this by estimating an ADL(\(2\),\(1\)) model and an
ADL(\(2\),\(2\)) model of the GDP growth rate using lags of GDP growth
and lags of the term spread as regressors and use both models for
forecasting the GDP growth in 2013:Q1.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert series to ts objects}
\NormalTok{GDPGrowth_ts <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(GDPGrowth, }
                  \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                  \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{, }\DecValTok{4}\NormalTok{), }
                  \DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}

\NormalTok{TSpread_ts <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(TSpread, }
                \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{), }
                \DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}

\CommentTok{# join both series}
\NormalTok{ADLdata <-}\StringTok{ }\KeywordTok{ts.union}\NormalTok{(GDPGrowth_ts, TSpread_ts)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the ADL(2,1) model of GDP growth}
\NormalTok{GDPGR_ADL21 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(GDPGrowth_ts }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts, }\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread_ts), }
      \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}

\KeywordTok{coeftest}\NormalTok{(GDPGR_ADL21, }\DataTypeTok{vcov. =}\NormalTok{ sandwich)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                    Estimate Std. Error t value Pr(>|t|)   
## (Intercept)        0.954990   0.486976  1.9611 0.051260 . 
## L(GDPGrowth_ts)    0.267729   0.082562  3.2428 0.001387 **
## L(GDPGrowth_ts, 2) 0.192370   0.077683  2.4763 0.014104 * 
## L(TSpread_ts)      0.444047   0.182637  2.4313 0.015925 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimated equation of the ADL(\(2\), \(1\)) model is

\begin{align}
  \widehat{GDPGR}_t = \underset{(0.49)}{0.96} + \underset{(0.08)}{0.26} GDPGR_{t-1} + \underset{(0.08)}{0.19} GDPGR_{t-2} + \underset{(0.18)}{0.44} TSpread_{t-1} \label{eq:gdpgradl21}
\end{align}

Notice that all coefficients are significant at the level of \(5\%\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 2012:Q3 / 2012:Q4 data on GDP growth and term spread}
\NormalTok{t <-}\StringTok{ }\KeywordTok{window}\NormalTok{(ADLdata, }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{3}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}

\CommentTok{# ADL(2,1) GDP growth forecast for 2013:Q1}
\NormalTok{ADL21_forecast <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(GDPGR_ADL21) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, t[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{], t[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], t[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\NormalTok{ADL21_forecast}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]
## [1,] 2.241689
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Forecast error}
\KeywordTok{window}\NormalTok{(GDPGrowth_ts, }\KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\OperatorTok{-}\StringTok{ }\NormalTok{ADL21_forecast}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Qtr1
## 2013 -1.102487
\end{verbatim}

Model \eqref{eq:gdpgradl21} predicts the GDP growth in 2013:Q1 to be
\(2.24\%\) which leads to a forecast error of \(-1.10\%\).

We estimate the ADL(\(2\),\(2\)) specification to see whether adding
additional information on past term spread improves the forecast.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the ADL(2,2) model of GDP growth}
\NormalTok{GDPGR_ADL22 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(GDPGrowth_ts }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts, }\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread_ts) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread_ts, }\DecValTok{2}\NormalTok{),                     }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}

\KeywordTok{coeftest}\NormalTok{(GDPGR_ADL22, }\DataTypeTok{vcov. =}\NormalTok{ sandwich)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                     Estimate Std. Error t value Pr(>|t|)   
## (Intercept)         0.967967   0.472470  2.0487 0.041800 * 
## L(GDPGrowth_ts)     0.243175   0.077836  3.1242 0.002049 **
## L(GDPGrowth_ts, 2)  0.177070   0.077027  2.2988 0.022555 * 
## L(TSpread_ts)      -0.139554   0.422162 -0.3306 0.741317   
## L(TSpread_ts, 2)    0.656347   0.429802  1.5271 0.128326   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

For the ADL(\(2\),\(2\)) model we obtain

\begin{align}
  \widehat{GDPGR}_t =& \, \underset{(0.47)}{0.98} + \underset{(0.08)}{0.24} GDPGR_{t-1} \\
                        +& \, \underset{(0.08)}{0.18} GDPGR_{t-2} + \underset{(0.42)}{-0.14} TSpread_{t-1} + \underset{(0.43)}{0.66} TSpread_{t-2}. \label{eq:gdpgradl22}
\end{align}

The coefficients on both lags of the term spread are not significant at
the \(10\%\) level.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ADL(2,2) GDP growth forecast for 2013:Q1}
\NormalTok{ADL22_forecast <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(GDPGR_ADL22) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, t[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{], t[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], t[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{], t[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\NormalTok{ADL22_forecast}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]
## [1,] 2.274407
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Forecast error}
\KeywordTok{window}\NormalTok{(GDPGrowth_ts, }\KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\OperatorTok{-}\StringTok{ }\NormalTok{ADL22_forecast}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Qtr1
## 2013 -1.135206
\end{verbatim}

The ADL(\(2\),\(2\)) forecast of GDP growth in 2013:Q1 is \(2.27\%\)
which imples a forecast error of \(1.14\%\).

Do the ADL models \eqref{eq:gdpgradl21} and \eqref{eq:gdpgradl22} improve
upon the simple AR(\(2\)) model \eqref{eq:GDPGRAR2} in terms of
forecasting GPD growth in 2013:Q1? The answer is yes: while \(SER\) and
\(\overline{R}^2\) improve only slightly, an \(F\)-test on the term
spread coefficients in \eqref{eq:gdpgradl22} provides evidence that the
model does better in explaining GPD growth than the AR(\(2\)) model as
the hypothesis that both coefficients are zero cannot be rejected at the
significance level of \(5\%\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compare adj. R2}
\KeywordTok{c}\NormalTok{(}
  \StringTok{"Adj.R2 AR(2)"}\NormalTok{ =}\StringTok{ }\KeywordTok{summary}\NormalTok{(GDPGR_AR2)}\OperatorTok{$}\NormalTok{r.squared,}
  \StringTok{"Adj.R2 ADL(2,1)"}\NormalTok{ =}\StringTok{ }\KeywordTok{summary}\NormalTok{(GDPGR_ADL21)}\OperatorTok{$}\NormalTok{r.squared,}
  \StringTok{"Adj.R2 ADL(2,2)"}\NormalTok{ =}\StringTok{ }\KeywordTok{summary}\NormalTok{(GDPGR_ADL22)}\OperatorTok{$}\NormalTok{r.squared}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Adj.R2 AR(2) Adj.R2 ADL(2,1) Adj.R2 ADL(2,2) 
##       0.1425484       0.1743996       0.1855245
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compare SER}
\KeywordTok{c}\NormalTok{(}
  \StringTok{"SER AR(2)"}\NormalTok{ =}\StringTok{ }\KeywordTok{summary}\NormalTok{(GDPGR_AR2)}\OperatorTok{$}\NormalTok{sigma,}
  \StringTok{"SER ADL(2,1)"}\NormalTok{ =}\StringTok{ }\KeywordTok{summary}\NormalTok{(GDPGR_ADL21)}\OperatorTok{$}\NormalTok{sigma,}
  \StringTok{"SER ADL(2,2)"}\NormalTok{ =}\StringTok{ }\KeywordTok{summary}\NormalTok{(GDPGR_ADL22)}\OperatorTok{$}\NormalTok{sigma}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    SER AR(2) SER ADL(2,1) SER ADL(2,2) 
##     3.132122     3.070760     3.057655
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# F-test on coefficients of term spread}
\KeywordTok{linearHypothesis}\NormalTok{(GDPGR_ADL22, }
                 \KeywordTok{c}\NormalTok{(}\StringTok{"L(TSpread_ts)=0"}\NormalTok{,}\StringTok{"L(TSpread_ts, 2)=0"}\NormalTok{),}
                 \DataTypeTok{vcov. =}\NormalTok{ sandwich}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## L(TSpread_ts) = 0
## L(TSpread_ts, 2) = 0
## 
## Model 1: restricted model
## Model 2: GDPGrowth_ts ~ L(GDPGrowth_ts) + L(GDPGrowth_ts, 2) + L(TSpread_ts) + 
##     L(TSpread_ts, 2)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(>F)  
## 1    201                    
## 2    199  2 4.4344 0.01306 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\subsubsection*{Stationarity}\label{stationarity}
\addcontentsline{toc}{subsubsection}{Stationarity}

In general, forecasts an be improved by using mutliple predictors ---
just as in cross-sectional regression. For time series regressions to
yield reliable models, the assumption of \emph{stationarity} must be
fulfilled. Key Concept 14.5 explains what stationarity is.

Key Concept 14.5

Stationarity

We say that a time series \(Y_t\) is stationary if its probability
distribution is time independent, that is the joint distribution
\(Y_{s+1}, Y_{s+2},\dots,Y_{s+T}\) does not change as \(s\) varies,
regardless of \(T\).

Similarly, we say that two time series \(X_t\) and \(Y_t\) are
\emph{jointly stationary} if the joint distribution of
\((X_{s+1},Y_{s+1}, X_{s+2},Y_{s+2} \dots, X_{s+T}Y_{s+T})\) does not
depend on \(s\), regardless of \(T\).

In a probabilistic sense, stationarity means that information about how
a time series evolves in the future is inherent to its past. If this is
not the case, we cannot use the past of a series as a reliable guideline
for its future.

\subsubsection*{Time Series Regression with Multiple
Predictors}\label{time-series-regression-with-multiple-predictors}
\addcontentsline{toc}{subsubsection}{Time Series Regression with
Multiple Predictors}

The concept of stationarity is a key assumption in the general time
series regression model with multiple predictors. Key Concept 14.6
elaborates the model and its assumptions.

Key Concept 14.6

Time Series Regression with Multiple Predictors

The general time series regression model extends the ADL such that
multiple regressors and their lags are included. It uses \(p\) lags of
the dependent variable and \(q_l\) lags of \(l\) additional predictors
where \(l=1,\dots,k\):

\begin{equation}
  \begin{aligned}
  Y_t =& \, \beta_0 + \beta_1 Y_{t-1} + \beta_2 Y_{t-2} + \dots + \beta_{p} Y_{t-p} \\
      +& \, \delta_{11} X_{1,t-1} + \delta_{12} X_{1,t-2} + \dots + \delta_{1q} X_{1,t-q} \\
      +& \, \dots \\
      +& \, \delta_{k1} X_{k,t-1} + \delta_{k2} X_{k,t-2} + \dots + \delta_{kq} X_{k,t-q} \\
      +& \, u_t 
\end{aligned}
\end{equation}

For estimation we make the following assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The error term \(u_t\) has conditional mean zero given all regressors
  and their lags:
  \[E(u_t\vert Y_{t-1}, Y_{t-2}, \dots, X_{1,t-1}, X_{1,t-2} \dots, X_{k,t-1}, X_{k,t-2}, \dots)\]
  This assumption is an extension of the conditional mean zero
  assumption used for AR and ADL models and guarantees that the gernal
  time series regression model stated above gives the best forecast of
  \(Y_t\) given its lags, the additional regressors
  \(X_{1,t},\dots,X_{k,t}\) and their lags.
\item
  The \(i.i.d\) assumption for cross-sectional data is not (entirely)
  meaningful for time series data. We replace it by the following one
  with two parts:

  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \item
    The \((Y_{t}, X_{1,t}, \dots, X_{k,t})\) have a stationary
    distribution (the ``identically distributed'' part of the i.i.d.
    assumption for cross-setional data). If this does not hold,
    forecasts may be biased and infernce can be strongly misleading.
  \item
    \((Y_{t}, X_{1,t}, \dots, X_{k,t})\) and
    \((Y_{t-j}, X_{1,t-j}, \dots, X_{k,t-j})\) become independent as
    \(j\) gets large (the ``idependly'' distributed part of the i.i.d.
    assumption for cross-sectional data). This assumption is also called
    \emph{weak dependence}. It ensures that, in large samples, the WLLN
    and the central limit theorem hold.
  \end{enumerate}
\item
  Large outliers are unlikely:
  \(E(X_{1,t}^4), E(X_{2,t}^4), \dots, E(X_{k,t}^4)\) and \(E(Y_t^4)\)
  have nonzero, finite fourth moments.
\item
  No perfect multicollinearity.
\end{enumerate}

Since many economic time series appear to be nonstationary, assumption 2
of Key Concept 14.6 is a crucial one in applied macroeconomics and
finance which is why statistical test for stationarity / nonstationarity
have been developed. Chapters 14.6 and 14.7 are devoted to this topic.

\subsubsection*{Statistical inference and the Granger causality
test}\label{statistical-inference-and-the-granger-causality-test}
\addcontentsline{toc}{subsubsection}{Statistical inference and the
Granger causality test}

If a \(X\) is a useful predictor for \(Y\), in a regression of \(Y_t\)
on lags of its own and lags of \(X_t\), not all of the coefficients on
the lags on \(X_t\) are zero. This concept is called ``Granger
causality'' and is an interesting hypothesis to test. Key Concept 14.7
summarizes the idea.

Key Concept 14.7

Granger Causality Tests

The Granger causality test (Granger, 1969) is an \(F\)-test of the null
hypothesis that \emph{all} lags of a variable \(X\) included in a time
series regression model do not have predictive power for \(Y_t\). The
Granger causality test does not test whether \(X\) actually
\emph{causes} \(Y\) but whether the included lags have are informative
in terms of predicting \(Y\).

Notice that we have already performed a Granger causality test on the
coefficients of term spread in \eqref{eq:gdpgradl22}, the ADL(\(2\),\(2\))
model of GDP growth and concluded that at least on of the first two lags
of term spread has predictive power for GDP growth.

\subsection*{Forecast Uncertainty and Forecast
Intervals}\label{forecast-uncertainty-and-forecast-intervals}
\addcontentsline{toc}{subsection}{Forecast Uncertainty and Forecast
Intervals}

In general, it is a good practice to report a measure of the uncertainty
inherent the estimation. Uncertainty is particulary of interest when
forecasting a time series. For example, consider a simple ADL\((1,1)\)
model

\begin{align*}
  Y_t = \beta_0 + \beta_1 Y_{t-1} + \delta_1 X_{t-1} + u_t
\end{align*}

where \(u_t\) is a homoskedastic error term. Then, the forecast error is

\begin{align*}
  Y_{T+1} - \widehat{Y}_{T+1\vert T} = u_{T+1} - \left[(\widehat{\beta}_0 - \beta_0) + (\widehat{\beta}_1 - \beta_1) Y_T + (\widehat{\delta_1} - \delta_1) X_t \right]
\end{align*}

The mean squared forecast error (MSFE) and the RMFSE are

\begin{align*}
  MFSE =& \, E\left[(Y_{T+1} - \widehat{Y}_{T+1\vert T})^2 \right] \\
       =& \, \sigma_u^2 + var\left[ (\widehat{\beta}_0 - \beta_0) + (\widehat{\beta}_1 - \beta_1) Y_T + (\widehat{\delta_1} - \delta_1) X_t \right], \\
  RMFSE =& \, \sqrt{\sigma_u^2 + var\left[ (\widehat{\beta}_0 - \beta_0) + (\widehat{\beta}_1 - \beta_1) Y_T + (\widehat{\delta_1} - \delta_1) X_t \right]}.
\end{align*}

A \(95\%\) forecast interval is an interval that covers the true value
of \(Y_{T+1}\) in \(95\%\) of repeated applications. Note that there is
a major difference in computing a confidence interval and a forecast
interval: when computing a confidence interval of a point estimate we
use large sample approximations that are justified by the central limit
theorem and thus are valid for a large range of error term
distributions. For computation of a forecast interval of \(Y_{T+1}\),
however, we must make an additional assumption about the distribution of
\(u_{T+1}\), the error term in period \(T+1\). Assuming that \(u_{T+1}\)
is normally distributed on can construct a \(95\%\) \emph{forecast
interval} for \(Y_{T+1}\) using
\(SE(Y_{T+1} - \widehat{Y}_{T+1\vert T})\), an estimate of the RMSFE:

\begin{align*}
  \widehat{Y}_{T+1\vert T} \pm 1.96 \cdot SE(Y_{T+1} - \widehat{Y}_{T+1\vert T})
\end{align*}

Of course the computation gets more complicated when the error term is
heteroskedastic or if we are interested in computing a forecast interval
for \(T+s, s>1\).

In some applications it is useful to report multiple forecast intervals
for each step in range of subsequents periods, see the Box \emph{The
River of Blood} on p.~592 of the book. Such forecast ranges can be
visualized in a so-called fan chart. We will not replicate the fan chart
presented in Figure 14.2 of book because the underlying model is by far
more compley than the simple AR and ADL models treated here. Instead, in
the example below we use simulated time series data and estimate an
ADL(\(1\),\(1\)) model which is than used for forecasting the subsequent
25 future outcomes of the series. We choose
\texttt{level\ =\ seq(5,99,10)} in the call of \texttt{forecast()} such
that forecast intervals with levels \(5\%,15\%,\dots,95\%\) are computed
for each point forecast of the series.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate time series}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\NormalTok{Z <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.5}\NormalTok{),  }\DataTypeTok{n =} \DecValTok{200}\NormalTok{)}
\NormalTok{X <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\DataTypeTok{ar =} \FloatTok{0.2}\NormalTok{),  }\DataTypeTok{n =} \DecValTok{200}\NormalTok{)}

\NormalTok{Y <-}\StringTok{ }\FloatTok{0.7} \OperatorTok{*}\StringTok{ }\NormalTok{Z }\OperatorTok{+}\StringTok{ }\FloatTok{0.7} \OperatorTok{*}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{# estimate an ADL(1,1) model using arima, see ?arima}
\NormalTok{model <-}\StringTok{ }\KeywordTok{arima}\NormalTok{(Y, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\CommentTok{# compute points forecasts and prediction intervals for next 25 periods}
\NormalTok{fc <-}\StringTok{ }\KeywordTok{forecast}\NormalTok{(model, }\DataTypeTok{h =} \DecValTok{25}\NormalTok{, }\DataTypeTok{level =} \KeywordTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{10}\NormalTok{))}

\CommentTok{# plot fan chart}
\KeywordTok{plot}\NormalTok{(fc, }
     \DataTypeTok{main =} \StringTok{"Forecast Fan Chart for ADL(2,2) Model of Simulated Data"}\NormalTok{, }
     \DataTypeTok{showgap =}\NormalTok{ F, }
     \DataTypeTok{fcol =} \StringTok{"red"}\NormalTok{,}
     \DataTypeTok{flty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-356-1} \end{center}

The dashed red line shows point forecasts of the series for the next 25
periods based on an \(ADL(1,1)\) model and the shaded areas represent
the prediction intervals. The degree of shading indicates the level of
the prediction interval. The darkest of the blue bands displays the
\(5\%\) forecast intervals and the color fades towards grey as the level
of the intervals increases.

\section{Lag Length Selection Using Information
Criteria}\label{lag-length-selection-using-information-criteria}

The selection of lag lengths in AR and ADL models can be partially
governed by economic theory. However, there are statistical methods that
are helpful to determine how many lags should be included as regressors.
In general, to many lags inflate the standard errors of coefficient
estimates and thus imply an increase in the forecast error while
omitting lags that should be included in the model may result in
estimation bias.

The order of an AR model can be determined using two approaches

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{\(F\)-test approach}:

  Estimate an AR(\(p\)) model and test significance of the largest
  lag(s). If the test rejects, drop the respective lag(s) from the
  model. This approach has the tendency to produce models where the
  order is too large: in a significance test we always face the risk of
  rejecting a true null hypothesis!
\item
  \textbf{Relying on an information criterion}:

  The circumvent the issue of producing too large models, one may choose
  the lag order that minimizes one of the following two information
  criteria:

  \begin{itemize}
  \item
    The \emph{Bayes information criterion} (BIC):

    \[BIC(p) = \log\left(\frac{SSR(p)}{T}\right) + (p + 1) \frac{\log(T)}{T}\]
  \item
    The \emph{Akaike information criterion} (AIC):

    \[AIC(p) = \log\left(\frac{SSR(p)}{T}\right) + (p + 1) \frac{2}{T}\]
  \end{itemize}

  Both criteria are estimators of the optimal lag length \(p\). The lag
  order \(\widehat{p}\) that minimizes the respective criterion is
  called the \emph{BIC estimate} or the \emph{AIC estimate} of the
  optimal model order. The basic idea of both criteria is that the
  \(SSR\) decreases as additional lags are added to the model such that
  the first addend decreases wheres the second addend increases as the
  lag order grows. One can show that the the \(BIC\) is a consistent
  estimator of the true lag order while the AIC is not which is due to
  the differing factors in the second addend. Nevertheless, both
  estimators are used in practice where the \(AIC\) is sometimes used as
  an alternative when the \(BIC\) yields a model with too few lags.
\end{enumerate}

\texttt{dynlm()} does not compute informatin criteria by default. We
will therefore write a short function that reports the \(BIC\) (and also
the chosen lag order \(p\) and \(R^2\)) for objects of class
\texttt{dynlm}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute BIC for AR models }
\NormalTok{BIC <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(model) \{}
  
\NormalTok{  ssr <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(model}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{  t <-}\StringTok{ }\KeywordTok{length}\NormalTok{(model}\OperatorTok{$}\NormalTok{residuals)}
\NormalTok{  npar <-}\StringTok{ }\KeywordTok{length}\NormalTok{(model}\OperatorTok{$}\NormalTok{coef)}
  
  \KeywordTok{return}\NormalTok{(}
    \KeywordTok{round}\NormalTok{(}
      \KeywordTok{c}\NormalTok{(}
      \StringTok{"p"}\NormalTok{ =}\StringTok{ }\NormalTok{npar }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{,}
      \StringTok{"BIC"}\NormalTok{ =}\StringTok{ }\KeywordTok{log}\NormalTok{(ssr}\OperatorTok{/}\NormalTok{t) }\OperatorTok{+}\StringTok{ }\NormalTok{npar }\OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(t)}\OperatorTok{/}\NormalTok{t,}
      \StringTok{"R2"}\NormalTok{ =}\StringTok{ }\KeywordTok{summary}\NormalTok{(model)}\OperatorTok{$}\NormalTok{r.squared}
\NormalTok{      ), }\DecValTok{4}
\NormalTok{    )}
\NormalTok{  )}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Table 14.3 of the book presents a breakdown of how the \(BIC\) is
computed for AR(\(p\)) models of GDP growth with order \(p=1,\dots,6\).
The final result can be easily reproduced using \texttt{sapply()} and
the function \texttt{BIC()} defined above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply BIC() to an intercept only model of GDP growth}
\KeywordTok{BIC}\NormalTok{(}
  \KeywordTok{dynlm}\NormalTok{(}\KeywordTok{ts}\NormalTok{(GDPGR_leads) }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      p    BIC     R2 
## 0.0000 2.4394 0.0000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# loop BIC over models of different orders}
\NormalTok{order <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{6}

\NormalTok{BICs <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(order, }
       \ControlFlowTok{function}\NormalTok{(x) }
        \StringTok{"AR"}\NormalTok{ =}\StringTok{ }\KeywordTok{BIC}\NormalTok{(}
          \KeywordTok{dynlm}\NormalTok{(}\KeywordTok{ts}\NormalTok{(GDPGR_leads) }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{ts}\NormalTok{(GDPGR_leads), }\DecValTok{1}\OperatorTok{:}\NormalTok{x))}
\NormalTok{          )}
\NormalTok{       )}

\NormalTok{BICs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]
## p   1.0000 2.0000 3.0000 4.0000 5.0000 6.0000
## BIC 2.3486 2.3475 2.3774 2.4034 2.4188 2.4429
## R2  0.1143 0.1425 0.1434 0.1478 0.1604 0.1591
\end{verbatim}

Note that increasing the lag order increases \(R^2\) becaused \(SSR\)
decreases as additional lags are added to the model but according to the
\(BIC\), we should decide for the AR(\(2\)) model instead of the
AR(\(6\)) model. It helps to decide wether the decrease in \(SSR\) is
enough to justify adding another regressor.

If we would have to compare a bigger set of models, a convenient way to
select the model with the lowest \(BIC\) is using the function
\texttt{which.min()}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select the AR model with the smallest BIC}
\NormalTok{BICs[, }\KeywordTok{which.min}\NormalTok{(BICs[}\DecValTok{2}\NormalTok{, ])]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      p    BIC     R2 
## 2.0000 2.3475 0.1425
\end{verbatim}

The \(BIC\) may also be used to select lag lengths in time series
regression models with multiple predictors. In a model with \(K\)
coefficients, including the intercept, we have

\begin{align*}
    BIC(K) = \log\left(\frac{SSR(K)}{T}\right) + K \frac{\log(T)}{T}.
\end{align*}

Notice that choosing the optimal model according to the \(BIC\) can be
computationally demaning because there may be many different
combinations of lag lengths when there are more multiple predictors.

To motivate an example, we estimate ADL(\(p\),\(q\)) models of GDP
growth where, as above, the additional variable is the term spread
between short-term and long-term bonds. We impose the restriction that
\(p=q_1=\dots=q_k\) so that only \(p_{max}\) models
(\(p=1,\dots,p_{max}\)) need to be estimated. In the example below we
choose \(p_{max} = 12\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# loop BIC over ADL models }
\NormalTok{order <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{12}

\NormalTok{BICs <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(order, }
       \ControlFlowTok{function}\NormalTok{(x) }
         \KeywordTok{BIC}\NormalTok{(}
          \KeywordTok{dynlm}\NormalTok{(GDPGrowth_ts }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts, }\DecValTok{1}\OperatorTok{:}\NormalTok{x) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread_ts, }\DecValTok{1}\OperatorTok{:}\NormalTok{x), }
          \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{          )}
\NormalTok{       )}

\NormalTok{BICs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]   [,2]   [,3]   [,4]    [,5]    [,6]    [,7]    [,8]    [,9]
## p   2.0000 4.0000 6.0000 8.0000 10.0000 12.0000 14.0000 16.0000 18.0000
## BIC 2.3411 2.3408 2.3813 2.4181  2.4568  2.5048  2.5539  2.6029  2.6182
## R2  0.1417 0.1855 0.1950 0.2072  0.2178  0.2211  0.2234  0.2253  0.2581
##       [,10]   [,11]   [,12]
## p   20.0000 22.0000 24.0000
## BIC  2.6646  2.7205  2.7664
## R2   0.2678  0.2702  0.2803
\end{verbatim}

Notice that from the definition of \texttt{BIC()}, for ADL models with
\(p=q\) it follows that p reports the number of estimated coefficients
\emph{excluding} the intercept. Tus the lag order is obtained by
deviding p by 2.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select the ADL model with the smallest BIC}
\NormalTok{BICs[, }\KeywordTok{which.min}\NormalTok{(BICs[}\DecValTok{2}\NormalTok{, ])]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      p    BIC     R2 
## 4.0000 2.3408 0.1855
\end{verbatim}

The \(BIC\) is in favour of the ADL(\(2\),\(2\)) model
\eqref{eq:gdpgradl22} we have estimated before.

\section{Nonstationarity I: Trends}\label{nonstationarity-i-trends}

If a series is nonstationary, conventional hypothesis tests, confidence
intervals and forecasts can be strongly misleading. The assumption of
stationarity is violated if a series exhibits trends or breaks and the
resulting complications in an econometric analysis depend on the
specific type of nonstationarity. This section focuses on time series
that exhibit trends.

A series is said to exhibit a trend if it fluctuates around a persistent
long-term movement. One distinguishes between \emph{deterministic} and
\emph{stochastic} trends.

\begin{itemize}
\item
  We say that a trend is \emph{deterministic} if it is a nonrandom
  function of time.
\item
  A trend is said to be \emph{stochastic} if it is a random function of
  time.
\end{itemize}

A careful look at the figures we have produced in Chapter 14.2 reveals
that many economic time series show a trending behaviour that is
probably best modeled by stochastic trends. This is why the book focuses
on the treatment of stochastic trends.

\subsubsection*{The Random Walk Model of a
Trend}\label{the-random-walk-model-of-a-trend}
\addcontentsline{toc}{subsubsection}{The Random Walk Model of a Trend}

The simplest way to model a time series \(Y_t\) that has stochastic
trend is the \emph{random walk}

\begin{align}
  Y_t = Y_{t-1} + u_t \label{eq:randomwalk}
\end{align}

where the \(u_t\) are i.i.d. errors with
\(E(u_t\vert Y_{t-1}, Y_{t-2}, \dots) = 0\). Note that

\begin{align*}
  E(Y_t\vert Y_{t-1}, Y_{t-2}\dots) =& \, E(Y_{t-1}\vert Y_{t-1}, Y_{t-2}\dots) + E(u_t\vert Y_{t-1}, Y_{t-2}\dots) \\
  =& \, Y_{t-1}
\end{align*}

so the best forecast for \(Y_t\), todays value of \(Y\), is \(Y_{t-1}\),
the observation made yesterday so the difference between \(Y_t\) and
\(Y_{t-1}\) is unpredictable. One can shows that the path followed by
\(Y_t\) consists of random steps \(u_t\), hence it is called a random
walk.

Assume that \(Y_0\), the starting of a random walk is \(0\). Another way
to write out \eqref{eq:randomwalk} is

\begin{align*}
  Y_0 =& \, 0 \\
  Y_1 =& \, 0 + u_1 \\
  Y_2 =& \, 0 + u_1 + u_2 \\
  \vdots & \, \\
  Y_t =& \, \sum_{i=1}^t u_i.
\end{align*}

Therefore we have that

\begin{align*}
  var(Y_t) =& \, var(u_1 + u_2 + \dots + u_t) \\
           =& \, t \sigma_u^2.
\end{align*}

Thus the variance of a random walk depends on time which violates the
assumption presented in Key Concept 14.5: a random walk is
nonstationary.

Obviously, \eqref{eq:randomwalk} is a special case of an AR(\(1\)) model
where the \(\beta_1 = 1\). One can show that a time series that follows
an AR(\(1\)) model is stationary if \(\lvert\beta_1\rvert < 1\). In a
general AR(\(p\)) model, stationarity is linked to the roots of the
polynomial
\[1-\beta_1 z - \beta_2 z^2 - \beta_3 z^3 - \dots - \beta_p z^p.\] If
all roots are greater than \(1\) in absolute value, the AR(\(p\)) series
is stationary. If at least one root equals \(1\), the AR(\(p\)) is said
to have a \emph{unit root} and thus has a stochastic trend.

It is straightforward to simulate random walks in R using
\texttt{arima.sim()}. The function \texttt{matplot()} is covenient for
simple plots of the columns of a matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate and plot random walks starting at 0}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{RWs <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}
  \KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =} \DecValTok{4}\NormalTok{, }
            \KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{ ,}\DecValTok{0}\NormalTok{)), }\DataTypeTok{n =} \DecValTok{100}\NormalTok{)}
\NormalTok{            )}
\NormalTok{  )}

\KeywordTok{matplot}\NormalTok{(RWs, }
        \DataTypeTok{type =}\StringTok{"l"}\NormalTok{, }
        \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"steelblue"}\NormalTok{, }\StringTok{"darkgreen"}\NormalTok{, }\StringTok{"darkred"}\NormalTok{, }\StringTok{"orange"}\NormalTok{), }
        \DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }
        \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
        \DataTypeTok{main =} \StringTok{"Four Random Walks"}\NormalTok{,}
        \DataTypeTok{xlab =} \StringTok{"Time"}\NormalTok{,}
        \DataTypeTok{ylab =} \StringTok{"Value"}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-362-1} \end{center}

Adding a constant to \eqref{eq:randomwalk} yields

\begin{align}
  Y_t = \beta_0 + Y_{t-1} + u_t \label{eq:randomwalkdrift},
\end{align}

a \emph{random walk with drift}. This allows to model the tendency of a
series to move in one direction or the other. If \(\beta_0\) is
positive, the series drifts upwards and it follows a downward trend if
\(\beta_0\) is negative.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate and plot random walks with drift starting at 0 }
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{RWsd <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}
  \KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =} \DecValTok{4}\NormalTok{, }
            \KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)), }\DataTypeTok{n =} \DecValTok{100}\NormalTok{)}
\NormalTok{            )}
\NormalTok{  )}

\KeywordTok{matplot}\NormalTok{(RWsd, }
        \DataTypeTok{type=}\StringTok{"l"}\NormalTok{, }
        \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"steelblue"}\NormalTok{,}\StringTok{"darkgreen"}\NormalTok{,}\StringTok{"darkred"}\NormalTok{,}\StringTok{"orange"}\NormalTok{), }
        \DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }
        \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
        \DataTypeTok{main =} \StringTok{"Four Random Walks with Drift"}\NormalTok{,}
        \DataTypeTok{xlab =} \StringTok{"Time"}\NormalTok{,}
        \DataTypeTok{ylab =} \StringTok{"Value"}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-363-1} \end{center}

\subsubsection*{Problems Caused by Stochastic
Trends}\label{problems-caused-by-stochastic-trends}
\addcontentsline{toc}{subsubsection}{Problems Caused by Stochastic
Trends}

OLS estimation of the coefficients on regressors that have a stochastic
trend is problematic because the distribution of the estimator and its
\(t\)-statistic is nonnormal, even asymptotically. This has various
consequences:

\begin{itemize}
\item
  Downward bias of autoregressive coefficients

  If \(Y_t\) is a random walk, the coefficient \(\beta_1\) can be
  consistently estimated by OLS but the estimator is biased toward zero.
  This bias is roughly \(E(\widehat{\beta}_1) = 1 - 5.3/T\) which is
  substantial for sample sizes encountered in macroeconomics. This
  estimation bias causes forecasts of \(Y_t\) to perform worse than a
  pure random walk model.
\item
  Nonnormally distributed \(t\)-statistics

  The nonnormal distributions of OLS estimates of the coefficient on a
  stochastic regressors translates to a nonnormal distributions of its
  \(t\)-statistic so that normal critical values are invalid and
  therefore usual confidence intervals and hypothesis tests are invalid,
  too, and the true distribution of the \(t\)-statistic cannot be
  readily determined.
\item
  Spurious Regression

  When a time series that exhibits a stochastic trend is regressed on
  another time series that does have a stochastic trend too, the
  estimated relationship may appear highly significant. This is what
  econometricians call a \emph{spurious} relationship.
\end{itemize}

As an example for spurious regression, consider again the green and the
red random walks that we have simulated above. We know that there is no
relationship between both series: they are purely random and independent
of each other.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# spurious relationship}
\KeywordTok{matplot}\NormalTok{(RWs[,}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)], }
        \DataTypeTok{lty =} \DecValTok{1}\NormalTok{,}
        \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
        \DataTypeTok{type =} \StringTok{"l"}\NormalTok{,}
        \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkgreen"}\NormalTok{,}\StringTok{"darkred"}\NormalTok{),}
        \DataTypeTok{xlab =} \StringTok{"Time"}\NormalTok{,}
        \DataTypeTok{ylab =} \StringTok{""}\NormalTok{,}
        \DataTypeTok{main =} \StringTok{"A Spurious Relationship"}
\NormalTok{        )    }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-364-1} \end{center}

Imagine we did not have this information and instead conject that the
green series is useful for predicting the red series and thus end up
estimating the ADL(\(0\),\(1\)) model

\begin{align*}
  Red_t = \beta_0 + \beta_1 Green_{t-1} + u_t.
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate spurious AR model}
\KeywordTok{summary}\NormalTok{(}
  \KeywordTok{dynlm}\NormalTok{(RWs[,}\DecValTok{2}\NormalTok{] }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(RWs[,}\DecValTok{3}\NormalTok{]))}
\NormalTok{  )}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              Estimate Std. Error   t value     Pr(>|t|)
## (Intercept) -3.459488  0.3635104 -9.516889 1.354156e-15
## L(RWs[, 3])  1.047195  0.1450874  7.217687 1.135828e-10
\end{verbatim}

The result is obviously spurious: the coefficient on \(Green_{t-1}\) is
estimated to be about \(1\) and the \(p\)-value of
\(1.14 \cdot 10^{-10}\) of the corresponding \(t\)-test indicates that
the coefficient is highly significant while its true value is in fact
zero.

As an empirical example, consider the U.S. unemployment rate and the
Japanese industrial production. Both series show an upward trending
behaviour from the mod-1960s through the early 1980s.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot U.S. unemployment rate & Japanese industrial production}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{merge}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(USUnemp), }\KeywordTok{as.zoo}\NormalTok{(JPIndProd)), }
     \DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{, }
     \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{,}\StringTok{"steelblue"}\NormalTok{),}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{""}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Spurious Regression: Macroeconomic Time series"}
\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"USUnemp"}\NormalTok{,}\StringTok{"JPIndProd"}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{, }\StringTok{"steelblue"}\NormalTok{),}
       \DataTypeTok{lwd =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-366-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate regression using data from 1962 to 1985}
\NormalTok{SR_Unemp1 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(}\KeywordTok{ts}\NormalTok{(USUnemp[}\StringTok{"1962::1985"}\NormalTok{]) }\OperatorTok{~}\StringTok{ }\KeywordTok{ts}\NormalTok{(JPIndProd[}\StringTok{"1962::1985"}\NormalTok{]))}
\KeywordTok{coeftest}\NormalTok{(SR_Unemp1, }\DataTypeTok{vcov =}\NormalTok{ sandwich)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)                 -2.37452    1.12041 -2.1193    0.0367 *  
## ts(JPIndProd["1962::1985"])  2.22057    0.29233  7.5961 2.227e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

A simple regression of the U.S. unemployment rate on Japanese industrial
production using data from 1962 to 1985 yields

\begin{align}
  \widehat{U.S. UR}_t = \underset{(1.12)}{-2.37} + \underset{(0.29)}{2.22} \log(JapaneseIP)_t. \label{eq:urjpip1}
\end{align}

This appears to be a significant relationship: the \(t\)-statistic of
the coefficient on \(\log(JapaneseIP)_t\) is bigger than 7.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate regression using data from 1986 to 2012}
\NormalTok{SR_Unemp2 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(}\KeywordTok{ts}\NormalTok{(USUnemp[}\StringTok{"1986::2012"}\NormalTok{]) }\OperatorTok{~}\StringTok{ }\KeywordTok{ts}\NormalTok{(JPIndProd[}\StringTok{"1986::2012"}\NormalTok{]))}
\KeywordTok{coeftest}\NormalTok{(SR_Unemp2, }\DataTypeTok{vcov =}\NormalTok{ sandwich)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)                  41.7763     5.4066  7.7270 6.596e-12 ***
## ts(JPIndProd["1986::2012"])  -7.7771     1.1714 -6.6391 1.386e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

By estimating the same model, this time with data from 1986 to 2012, we
obtain

\begin{align}
  \widehat{U.S. UR}_t = \underset{(5.41)}{41.78} + \underset{(1.17)}{-7.78} \log(JapaneseIP)_t \label{eq:urjpip2}
\end{align}

which is suprisingly quite different from \eqref{eq:urjpip1} which
indicates a moderate postive relationship, in contrast to the strong
negative \eqref{eq:urjpip2}. This phenomenon can be attributed to
stochastic trends in the series. Since, there is no economic reasoning
that relates both trends, both regressions are spurious.

\subsubsection*{Testing for a Unit AR
Root}\label{testing-for-a-unit-ar-root}
\addcontentsline{toc}{subsubsection}{Testing for a Unit AR Root}

A formal test for a stochastic trend has been proposed by Dickey and
Fuller (1979) and is therefore termend the \emph{Dickey-Fuller test}. As
discussed above, a time series that follows an AR(\(1\)) model with
\(\beta_1 = 1\) has a stochastic trend. Thus, the testing problem is

\begin{align*}
  H_0: \beta_1 = 1 \ \ \ \text{vs.} \ \ \ H_1: \beta_1 < 1.
\end{align*}

so the null hypothesis is that the AR(\(1\)) has a unit root and the
alternative hypothesis is that it is stationary. One often rewrites the
AR(\(1\)) by substracting \(Y_{t-1}\) on both sides:

\begin{align*}
  Y_t = \beta_0 + \beta_1 Y_{t-1} + u_t \ \ \Leftrightarrow \ \ \Delta Y_t = \beta_0 + \delta_1 Y_{t-1} +
\end{align*}

where \(\delta_1 = \beta_1 - 1\). The testing problem then becomes

\begin{align*}
  H_0: \delta_1 = 0 \ \ \ \text{vs.} \ \ \ H_1: \beta_1 < 0
\end{align*}

which is convenient since the corresponding test statistic by default
reported by relevant R functions.\footnote{The \(t\)-statistic of the
  Dickey-Fuller test is computed using homoskedasticity-only standard
  errors since under the null hypothesis, the usual \(t\)-statistic is
  robust to heteroskedasticity.}

The Dickey-Fuller test can also be applied in an AR(\(p\)) model. The
\emph{Augmented Dickey-Fuller (ADF) test} is summarized in Key Concept
14.8.

Key Concept 14.8

The ADF Test for a Unit Root

Consider the regression

\begin{align}
  \Delta Y_t = \beta_0 + \delta Y_{t-1} + \gamma_1 \Delta_1 Y_{t-1} + \gamma_2 \Delta Y_{t-2} + \dots + \gamma_p \Delta Y_{t-p} + u_t. \label{eq:ADFreg1}
\end{align}

The ADF test for a unit autoregressive root test the hypothesis
\(H_0: \delta = 0\) (stochastic trend) against the one-sided alternative
\(H_1: \delta < 0\) (stationarity) using the usual OLS \(t\)-statistic.

If it is assumed that \(Y_t\) is stationary around a deterministic
linear time trend, the model is augmented by the regressor \(t\), that
is \(Y_t\) becomes

\begin{align}
  \Delta Y_t = \beta_0 + at + \delta Y_{t-1} + \gamma_1 \Delta_1 Y_{t-1} + \gamma_2 \Delta Y_{t-2} + \dots + \gamma_p \Delta Y_{t-p} + u_t  \label{eq:ADFreg2}
\end{align}

where again \(H_0: \delta = 0\) is tested against \(H_1: \delta < 0\).

The optimal lag length \(p\) can be estimated using information
criteria. Notice that in the regression \eqref{eq:ADFreg1}, \(p=0\) (that
is no lags of \(\Delta Y_t\) are used as regressors) corresponds to a
simple AR(\(1\)).

Under the null hypothesis, the \(t\)-statistic for \(H_0: \delta = 0\)
does not have a normal distribution. The cricital values can only be
obtained from simulation and differ for regressions \eqref{eq:ADFreg1} and
\eqref{eq:ADFreg2} since the distribution of the ADF test statistic is
sensitive to whether a deterministic time trend is included or not.

\subsubsection*{Critical Values for the ADF
Statistic}\label{critical-values-for-the-adf-statistic}
\addcontentsline{toc}{subsubsection}{Critical Values for the ADF
Statistic}

Key Concept 14.8 states that the critical values for the ADF test in the
regressions \eqref{eq:ADFreg1} and \eqref{eq:ADFreg2} can only be determined
using simulation. The idea of the simulation study is to simulate a
large number of ADF test test statistics and use them to estimate
quantiles of their distribution. This sections shows how this is
feasible whithin R.

First, consider an AR(\(1\)) model with drift. The procedure is as
follows:

\begin{itemize}
\item
  Simulate \(N\) random walks with \(n\) observations using the data
  generating process

  \begin{align*}
    Y_t =& \, \beta_0 + \beta_1 Y_{t-1} + u_t,
  \end{align*}

  \(t=1,\dots,n\) where \(N\) and \(n\) are large numbers.
\item
  For each random walk, estimate the regression

  \begin{align*}
    \Delta Y_t =& \, \beta_0 + \beta_1 Y_{t-1} + u_t,
  \end{align*}

  compute ADF test statistic. Save all \(N\) test statistics in a
  vector.
\item
  Estimate quantiles of the distribution of the ADF test statistics
  using the \(N\) test statistics obtained from the simulation.
\end{itemize}

For the case with drift and linear time trend we replace the data
generating process by

\begin{align*}
  Y_t =& \, \beta_0 + \alpha t + \beta_1 Y_{t-1} + u_t
\end{align*}

and estimate

\begin{align*}
  \Delta Y_t =& \, \beta_0 + \alpha t + \beta_1 Y_{t-1} + u_t,
\end{align*}

Loosely speaking, the precision of the estimated quantiles depends on
two factors: \(n\), the length of the underlying series and \(N\), the
number of test statistics used. Since we are interested in estimating
quantiles of the \emph{asymptotic} distribution (the Dickey-Fuller
distribution) of the ADF test statistic so both using many observations
and large number of simulated test statistics will increase the
precision of the estimated quantiles. We choose \(n=N=1000\) as the
computational burden grows quickly with \(n\) and \(N\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# repititions}
\NormalTok{N <-}\StringTok{ }\DecValTok{1000}

\CommentTok{# observations}
\NormalTok{n <-}\StringTok{ }\DecValTok{1000}

\CommentTok{# define drift an trend }
\NormalTok{drift <-}\StringTok{ }\FloatTok{0.5}
\NormalTok{trend <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\NormalTok{n}

\CommentTok{# simulate N random walks with drift }
\NormalTok{RWD <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ N, }
\NormalTok{            drift }\OperatorTok{+}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)),}
                              \DataTypeTok{n =}\NormalTok{ n }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{            )}
\NormalTok{          )}

\CommentTok{# compute ADF test statistics and store them in 'ADFD'}
\NormalTok{ADFD <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(N)}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(RWD)) \{}
\NormalTok{  ADFD[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}
    \KeywordTok{dynlm}\NormalTok{(}\KeywordTok{diff}\NormalTok{(RWD[,i],}\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(RWD[,i],}\DecValTok{1}\NormalTok{))}
\NormalTok{    )}\OperatorTok{$}\NormalTok{coef[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{]}
\NormalTok{\}}

\CommentTok{# simulate N random walks with drift + trend}
\NormalTok{RWDT <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ N, }
\NormalTok{                    trend }\OperatorTok{+}\StringTok{ }\NormalTok{drift }\OperatorTok{+}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)), }
                                              \DataTypeTok{n =}\NormalTok{ n }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{                    )}
\NormalTok{           )}

\CommentTok{# compute ADF test statistics and store them in 'ADFDT'}
\NormalTok{ADFDT <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(N)}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(RWDT)) \{}
\NormalTok{  ADFDT[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}
    \KeywordTok{dynlm}\NormalTok{(}\KeywordTok{diff}\NormalTok{(RWDT[,i],}\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(RWDT[,i],}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{trend}\NormalTok{(RWDT[,i], }\DataTypeTok{scale =}\NormalTok{ F))}
\NormalTok{  )}\OperatorTok{$}\NormalTok{coef[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate quantiles for ADF regression with drift}
\KeywordTok{round}\NormalTok{(}\KeywordTok{quantile}\NormalTok{(ADFD, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.01}\NormalTok{)),}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   10%    5%    1% 
## -2.62 -2.83 -3.39
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate quantiles for ADF regression with drift + trend}
\KeywordTok{round}\NormalTok{(}\KeywordTok{quantile}\NormalTok{(ADFDT, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.01}\NormalTok{)),}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   10%    5%    1% 
## -3.11 -3.43 -3.97
\end{verbatim}

The estimated quantiles are close to the large sample critical values of
the ADF test statistic reported in Table 14.4 of the book.

\begin{longtable}[]{@{}llll@{}}
\caption{\label{tab:DFcrits} Large Sample Critical Values of ADF
Test}\tabularnewline
\toprule
Deterministic Regressors & 10\% & 5\% & 1\%\tabularnewline
\midrule
\endfirsthead
\toprule
Deterministic Regressors & 10\% & 5\% & 1\%\tabularnewline
\midrule
\endhead
Intercept only & -2.57 & -2.86 & -3.43\tabularnewline
Intercept and time trend & -3.12 & -3.41 & -3.96\tabularnewline
\bottomrule
\end{longtable}

The results show that using standard normal critical values might be
fatal: the 5\% critical value of the standard normal distribution is
\(-1.64\) but for the Dickey-Fuller distributions the estimated critical
values are \(-2.87\) (drift) and \(-3.43\) (drift and linear time
trend). This implies that a true null hypothesis (the series has a
stochastic trend) would be rejected far to often if the inappropriate
normal critical values were used.

We may use the simulated test statistics for a graphical comparison of
the standard normal density and (estimates of) both Dickey-Fuller
densities.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot standard normal density}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x), }
      \DataTypeTok{from =} \OperatorTok{-}\DecValTok{6}\NormalTok{, }\DataTypeTok{to =} \DecValTok{3}\NormalTok{, }
      \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.6}\NormalTok{), }
      \DataTypeTok{lty =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{"t-Statistic"}\NormalTok{,}
      \DataTypeTok{main =} \StringTok{"Distributions of ADF Test Statistics"}\NormalTok{,}
      \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{# plot density estimates of both Dickey-Fuller distributions}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(ADFD), }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(ADFDT), }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{"N(0,1)"}\NormalTok{,}\StringTok{"Drift"}\NormalTok{,}\StringTok{"Drift+Trend"}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{,}\StringTok{"darkgreen"}\NormalTok{,}\StringTok{"blue"}\NormalTok{),}
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-371-1} \end{center}

The deviations from the standard normal distribution are significant:
both Dickey-Fuller distributions are skewed to the left and have a
havier left tail.

\subsubsection*{Does U.S. GDP Have a Unit
Root?}\label{does-u.s.-gdp-have-a-unit-root}
\addcontentsline{toc}{subsubsection}{Does U.S. GDP Have a Unit Root?}

As an empirical example, we use the ADF test to assess whether there is
a stochastic trend in U.S. GDP using the regression

\begin{align*}
  \Delta\log(GDP_t) = \beta_0 + \alpha t + \beta_1 \log(GDP_{t-1}) + \beta_2 \Delta \log(GDP_{t-1}) + \beta_3 \Delta \log(GDP_{t-2}) + u_t.
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# log GDP series}
\NormalTok{LogGDP <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{log}\NormalTok{(GDP[}\StringTok{"1962::2012"}\NormalTok{]))}

\CommentTok{# estimate model}
\KeywordTok{coeftest}\NormalTok{(}
  \KeywordTok{dynlm}\NormalTok{(}\KeywordTok{diff}\NormalTok{(LogGDP) }\OperatorTok{~}\StringTok{ }\KeywordTok{trend}\NormalTok{(LogGDP, }\DataTypeTok{scale =}\NormalTok{ F) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(LogGDP) }\OperatorTok{+}\StringTok{ }\KeywordTok{diff}\NormalTok{(}\KeywordTok{L}\NormalTok{(LogGDP)) }\OperatorTok{+}\StringTok{ }\KeywordTok{diff}\NormalTok{(}\KeywordTok{L}\NormalTok{(LogGDP),}\DecValTok{2}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                             Estimate  Std. Error t value Pr(>|t|)   
## (Intercept)               0.27877045  0.11793233  2.3638 0.019066 * 
## trend(LogGDP, scale = F)  0.00023818  0.00011090  2.1476 0.032970 * 
## L(LogGDP)                -0.03332452  0.01441436 -2.3119 0.021822 * 
## diff(L(LogGDP))           0.08317976  0.11295542  0.7364 0.462371   
## diff(L(LogGDP), 2)        0.18763384  0.07055574  2.6594 0.008476 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimation yields

\begin{align*}
  \Delta\log(GDP_t) =& \, \underset{(0.118)}{0.28} + \underset{(0.0001)}{0.0002} t + \underset{(0.014)}{-0.033} \log(GDP_{t-1}) \\
  & \, + \underset{(0.113)}{0.083} \Delta \log(GDP_{t-1}) + \underset{(0.071)}{0.188} \Delta \log(GDP_{t-2}) + u_t,
\end{align*}

so the ADF test statistic is \(t=-0.033/0.014 = - 2.35\). The
corresponding \(5\%\) critical value from table \ref{tab:DFcrits} is
\(-3.41\) so we cannot reject the null hypothesis that \(\log(GDP=\)has
a stochastic trend in favour of the alternative that it is stationary
around a deterministic linear time trend.

\section{Nonstationarity II: Breaks}\label{nonstationarity-ii-breaks}

When there are discrete (at a distinct date) or gradual changes (over
time) in the population regression ceofficients, the series is
nonstationary. These changes are called \emph{breaks}. There is a
variety of reasons why breaks can occur in macroeconomic time series but
most often they are related to changes in economic policy or major
changes in the structure of the economy. See Chapter 14.7 for a
discussion of examples.

If breaks are not accounted for in the regression model, OLS estimates
will reflect the average relationship. Since these estimates might be
strongly misleading and result in poor forecast quality, we are
interested in testing for breaks. One distinguishes between testing for
a break when the date is known and testing for a break with an unknown
break data.

Let \(\tau\) denote a known break date and let \(D_t(\tau)\) be a binary
variable indicating time periods before and after the break.
Incorporating the break in an ADL(\(1\),\(1\)) regression model yields

\begin{align*}
  Y_t =& \beta_0 + \beta_1 Y_{t-1} + \delta_1 X_{t-1} + \gamma_0 D_t(\tau) \left[ D_t(\tau) Y_{t-1}\right] \\ &+ \, \gamma_2\left[ D_t(\tau) X_{t-1} \right] + u_t
\end{align*}

where we allows for discrete changes in \(\beta_0\), \(\beta_1\) and
\(\beta_2\) at the break date \(\tau\). The null hypothesis of no break,
\[H_0: \gamma_0=\gamma_1=\gamma_2=0,\] can be tested against the
alternative that at least one of the \(\gamma\)'s is not zero using an
\(F\)-Test. This idea is called a Chow test after Gregory Chow (1960).

When the break date is unknown the \emph{Quandt likelihood ratio} (QLR)
test (Quandt, 1960) may be used. This is a modified version of the Chow
test which uses the largest of all \(F\)-statistic obtained for
application of the Chow test for all possible break dates in a
predetermined range \(\left[\tau_0,\tau_1\right]\). The QLR test is
summarized in Key Concept 14.9.

Key Concept 14.9

The QLR Test for Coefficient Stability

The QLR test can be used to test for a break in the population
regression function if the date of the break is unknown. The QLR test
statistic is the largest (Chow) \(F(\tau)\)-statistic computed over a
range of eligible break dates \(\tau_0 \leq \tau \leq \tau_1\):

\begin{align}
  QLR = \max\left[F(\tau_0),F(\tau_0 +1),\dots,F(\tau_1)\right] \label{eq:QLRstatistic}
\end{align}

The most important properties are:

\begin{itemize}
\item
  The QLR test also be applied to test whether a subset of the
  coefficients in the population regression function breaks but the test
  also rejects if there is a slow evolution of the regression function.
\item
  When there is a single discrete break in the population regression
  function, the \(QLR\) test statistic is \(F(\widehat{\tau})\) and
  \(\widehat{\tau}/T\) is a consistent estimator of the true break date.
\item
  The large-sample distribution of \(QLR\) depends on \(q\), the number
  of restrictions beeing tested and both ratios of end points to the
  sample size, \(\tau_0/T, \tau_1/T\).
\item
  Similar as the for the ADF test, the large-sample distribution of
  \(QLR\) is nonstandard. Cricital values are presented in Table 14.5 of
  the book.
\end{itemize}

\subsubsection*{Has the Predictive Power of the term spread been
stable?}\label{has-the-predictive-power-of-the-term-spread-been-stable}
\addcontentsline{toc}{subsubsection}{Has the Predictive Power of the
term spread been stable?}

Using the QLR statistic we may test whether there is a break in the
coefficients of terms spread in \eqref{eq:gdpgradl22}, the
ADL(\(2\),\(2\)) regression model of GDP growth. Following Key Concept
14.9 we modify the specification of \eqref{eq:gdpgradl22} by adding a
break dummy \(D(\tau)\) and its interactions with both lags of term
spread and choose the range of break points to be tested as 1970:Q1 -
2005:Q2 (these periods are the center 70\% of the sample data from
1962:Q2 - 2012:Q4). Thus, the model becomes

\begin{align*}
    GDPGR_t =& \, \beta_0 + \beta_1 GDPGR_{t-1} + \beta_2 GDPGR_{t-2} \\
            &\,+  \beta_3  TSpread_{t-1} + \beta_4 TSpread_{t-2} \\
            &\,+ \gamma_1 D(\tau) + \gamma_2 (D(\tau) \cdot TSpread_{t-1}) \\
            &\,+ \gamma_3 (D(\tau) \cdot TSpread_{t-2}) \\
            &\,+ u_t.
\end{align*}

Next, we estimate the model for each break point and compute the
\(F\)-statistic corresponding to the null hypothesis
\(H_0: \gamma_1=\gamma_2=\gamma_3=0\). The \(QLR\)-statistic is the
largest of the \(F\)-statistics obtained in this manner.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up a range of possible break dates}
\NormalTok{tau <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1970}\NormalTok{, }\DecValTok{2005}\NormalTok{, }\FloatTok{0.25}\NormalTok{)}

\CommentTok{# initialize vector of F-statistics}
\NormalTok{Fstats <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\KeywordTok{length}\NormalTok{(tau))}

\CommentTok{# estimation loop over break dates}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(tau)) \{}

  \CommentTok{# set up dummy variable}
\NormalTok{  D <-}\StringTok{ }\KeywordTok{time}\NormalTok{(GDPGrowth_ts) }\OperatorTok{>}\StringTok{ }\NormalTok{tau[i]}

  \CommentTok{# estimate ADL(2,2) model with intercations}
\NormalTok{  test <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(GDPGrowth_ts }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts, }\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{                  }\NormalTok{D}\OperatorTok{*}\KeywordTok{L}\NormalTok{(TSpread_ts) }\OperatorTok{+}\StringTok{ }\NormalTok{D}\OperatorTok{*}\KeywordTok{L}\NormalTok{(TSpread_ts, }\DecValTok{2}\NormalTok{),}
                \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}
  
  \CommentTok{# compute and save F-statistic}
\NormalTok{  Fstats[i] <-}\StringTok{ }\KeywordTok{linearHypothesis}\NormalTok{(test, }
                                \KeywordTok{c}\NormalTok{(}\StringTok{"DTRUE=0"}\NormalTok{, }\StringTok{"DTRUE:L(TSpread_ts)"}\NormalTok{, }\StringTok{"DTRUE:L(TSpread_ts, 2)"}\NormalTok{),}
                                \DataTypeTok{vcov. =}\NormalTok{ sandwich)}\OperatorTok{$}\NormalTok{F[}\DecValTok{2}\NormalTok{]}

\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We determine the \(QLR\) statistic using \texttt{max()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# identify QLR statistic}
\NormalTok{QLR <-}\StringTok{ }\KeywordTok{max}\NormalTok{(Fstats)}
\NormalTok{QLR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6.651156
\end{verbatim}

It is straightforward to check that the \(QLR\)-statistic is the
\(F\)-statistic obtained for the regression where 1980:Q4 is chosen as
the break date.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# identify time period where QLR-statistic is observed}
\KeywordTok{as.yearqtr}\NormalTok{(}
\NormalTok{  tau[}\KeywordTok{which.max}\NormalTok{(Fstats)]}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "1980 Q4"
\end{verbatim}

Since \(q=3\) hypotheses are tested and the central \(70\%\) of the
sample are considered to contain breaks, the corresponding \(1\%\)
critical value of the \(QLR\) test is \(6.02\). We reject the null
hypothesis that all coefficients (the coefficients on both lags of term
spread and the intercept) are stable since the computed
\(QLR\)-statistic exceeds this threshold. Thus evidence from the \(QLR\)
test suggests that there is a break in the ADL(\(2\),\(2\)) model of GDP
growth in the early 1980s.

To reproduce Figure 14.5 of the book, we convert the vector of
sequential break-point \(F\)-statistics into a time series object and
then generate a simple plot with some annotations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# series of F-statistics}
\NormalTok{Fstatsseries <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(Fstats, }
                   \DataTypeTok{start =}\NormalTok{ tau[}\DecValTok{1}\NormalTok{], }
                   \DataTypeTok{end =}\NormalTok{ tau[}\KeywordTok{length}\NormalTok{(tau)], }
                   \DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}

\CommentTok{# plot the F-statistics }
\KeywordTok{plot}\NormalTok{(Fstatsseries, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{, }\DecValTok{2015}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{7.5}\NormalTok{),}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"F-Statistic"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Break Date"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Testing for a Break in GDP ADL(2,2) Regression at Different Dates"}
\NormalTok{     )}

\CommentTok{# dashed horizontal lines for critical values and QLR statistic}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \FloatTok{4.71}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \FloatTok{6.02}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\KeywordTok{segments}\NormalTok{(}\DecValTok{0}\NormalTok{, QLR, }\FloatTok{1980.75}\NormalTok{, QLR, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{2010}\NormalTok{, }\FloatTok{6.2}\NormalTok{, }\StringTok{"1% Critical Value"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{2010}\NormalTok{, }\FloatTok{4.9}\NormalTok{, }\StringTok{"5% Critical Value"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{1980.75}\NormalTok{, QLR}\OperatorTok{+}\FloatTok{0.2}\NormalTok{, }\StringTok{"QLR Statistic"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-376-1} \end{center}

\subsubsection*{Pseudo Out-of-Sample
Forecasting}\label{pseudo-out-of-sample-forecasting}
\addcontentsline{toc}{subsubsection}{Pseudo Out-of-Sample Forecasting}

Pseudo out-of-sample forecasts are used to simulate the out-of-sample
performance (the real time forecast performance) of a time series
regression model. In particular, pseudo out-of-sample forecast allows
estimation of the \(RMSFE\) of the model and enable researchers to
compare different model specifications with respect to their
reliability. Key Concept 14.10 summarizes the idea.

Key Concept 14.10

Pseudo Out-of-Sample Forecasting

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Divide the sample data into \(s=T-P\) and \(P\) subsequent
  overservations. The \(P\) observations are used as
  pseudo-out-of-sample observations.
\item
  Estimate the model using the first \(s\) observations.
\item
  Compute the pseudo-forecast \(\overset{\sim}{Y}_{s+1\vert s}\).
\item
  Compute the pseudo-forecast-error
  \(\overset{\sim}{u}_{s+1} = Y_{s+1} - \overset{\sim}{Y}_{s+1\vert s}\).
\item
  Repeat stepts 2 trough 4 for all reamaining pseudo-out-of-sample
  dates.
\end{enumerate}

\subsubsection*{Did the Predictive Power of the Term Spread Change
During the
2000s?}\label{did-the-predictive-power-of-the-term-spread-change-during-the-2000s}
\addcontentsline{toc}{subsubsection}{Did the Predictive Power of the
Term Spread Change During the 2000s?}

The insight gained in the previous section gives reason to presume that
the pseudo-out-of-sample performance of ADL(\(2\),\(2\)) models
estimated using data after the break in the early 1980s should not
deteriorate: provided that the coefficients of the population regression
function are stable after the potential break in 1980:Q4, these models
should have good predictive power. We check this by computing
pseudo-out-of-sample forecasts for the period 2003:Q1 - 2012:Q4, a range
covering 40 periods, where the forecast for 2003:Q1 is done using data
from 1981:Q1 - 2002:Q4, the forecast for 2003:Q2 is based on data frome
1981:Q1 - 2003:Q1 and so on.

Similarly as for the \(QLR\)-test we use a \texttt{for()} loop for
estimation of all 40 models and gather their \(SER\)s and the obtained
forecasts in a vector which is then used to compute pseudo-out-of-sample
forecast errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# end of sample dates}
\NormalTok{EndOfSample <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{2002.75}\NormalTok{, }\FloatTok{2012.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{)}

\NormalTok{forecasts <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\KeywordTok{length}\NormalTok{(EndOfSample))}
\NormalTok{SER <-}\StringTok{ }\NormalTok{forecasts}

\CommentTok{# estimation loop over end of sample dates}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(EndOfSample)) \{}

  \CommentTok{# estimate ADL(2,2) model}
\NormalTok{  m <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(GDPGrowth_ts }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth_ts, }\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread_ts) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread_ts, }\DecValTok{2}\NormalTok{), }
                \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1981}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                \DataTypeTok{end =}\NormalTok{ EndOfSample[i])}
  
\NormalTok{  SER[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(m)}\OperatorTok{$}\NormalTok{sigma}
  
  \CommentTok{# sample data for one-period ahead forecast}
\NormalTok{  t <-}\StringTok{ }\KeywordTok{window}\NormalTok{(ADLdata, EndOfSample[i]}\OperatorTok{-}\FloatTok{0.25}\NormalTok{, EndOfSample[i])}
  
  \CommentTok{# compute forecast}
\NormalTok{  forecasts[i] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(m) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, t[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], t[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{], t[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], t[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]) }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute psuedo-out-of-sample forecast errors}
\NormalTok{POOSFCE <-}\StringTok{ }\KeywordTok{window}\NormalTok{(GDPGrowth_ts, }\KeywordTok{c}\NormalTok{(}\DecValTok{2003}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\OperatorTok{-}\StringTok{ }\NormalTok{forecasts}
\end{Highlighting}
\end{Shaded}

We translate the pseudo-out-of-sample forecasts into an object of class
ts and plot the real GDP growth rate against the forecasted series.
Using

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# series of pseudo-out-of-sample forecasts}
\NormalTok{PSOSSFc <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(forecasts, }
              \DataTypeTok{start =} \DecValTok{2003}\NormalTok{, }
              \DataTypeTok{end =} \FloatTok{2012.75}\NormalTok{, }
              \DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}

\CommentTok{# plot GDP growth time series}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{window}\NormalTok{(GDPGrowth_ts, }\KeywordTok{c}\NormalTok{(}\DecValTok{2003}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Pseudo-Out-Of-Sample Forecasts of GDP Growth"}
\NormalTok{     )}

\CommentTok{# add series of pseudo-out-of-sample forecasts}
\KeywordTok{lines}\NormalTok{(PSOSSFc, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lty =} \DecValTok{2}
\NormalTok{      )}

\CommentTok{# shade area between curves (the pseudo forecast error)}
\KeywordTok{polygon}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{time}\NormalTok{(PSOSSFc), }\KeywordTok{rev}\NormalTok{(}\KeywordTok{time}\NormalTok{(PSOSSFc))), }
        \KeywordTok{c}\NormalTok{(}\KeywordTok{window}\NormalTok{(GDPGrowth_ts, }\KeywordTok{c}\NormalTok{(}\DecValTok{2003}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)), }\KeywordTok{rev}\NormalTok{(PSOSSFc)),}
        \DataTypeTok{col =} \KeywordTok{alpha}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{),}
        \DataTypeTok{border =} \OtherTok{NA}
\NormalTok{        )}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomleft"}\NormalTok{, }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{),}
       \DataTypeTok{lwd =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{10}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"steelblue"}\NormalTok{,}\StringTok{"black"}\NormalTok{,}\KeywordTok{alpha}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{)), }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Actual GDP growth rate"}\NormalTok{,}
         \StringTok{"Forecasted GDP growth rate"}\NormalTok{,}
         \StringTok{"Pseudo forecast Error"}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-379-1} \end{center}

Apparently, the pseudo forecasts track the actual GDP growth rate quite
well, except for the kink in 2009 which can be attributed to the recent
financial crisis.

The \(SER\) of the first model (estimated using data from 1981:Q1 to
2002:Q4) is \(2.39\) so based on the in sample fit we would expect out
of sample forecast errors to have mean zero and a mean squared forecast
error of about \(2.39\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# SER of ADL(2,2) mode using data from 1981:Q1 - 2002:Q4}
\NormalTok{SER[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.389773
\end{verbatim}

The root mean squared forecast error of the pseudo-out-of-sample
forecasts is somewhat larger.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute root mean squared forecast error}
\KeywordTok{sd}\NormalTok{(POOSFCE)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.667612
\end{verbatim}

An interesting hypothesis is whether the mean forecast error, is zero
that is the ADL(\(2\),\(2\)) forecasts are right, on average. This
hypothesis is easily tested using the function \texttt{t.test()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test if mean forecast error is zero}
\KeywordTok{t.test}\NormalTok{(POOSFCE)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  POOSFCE
## t = -1.5523, df = 39, p-value = 0.1287
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -1.5078876  0.1984001
## sample estimates:
##  mean of x 
## -0.6547438
\end{verbatim}

The hypothesis cannot be rejected at the \(10\%\) significance level.
Alltogther the analysis suggests that the ADL(\(2\),\(2\)) model
coefficients have been stable since the presumed break in the early
1980s.

\section{Can You Beat the Market? Part
II}\label{can-you-beat-the-market-part-ii}

The dividend yield --- the ratio of current dividens to the stock price
--- can be considered as an indicator of future dividends: if a stock
has a high current dividend yield, it can be considered undervalued and
it can be presumed that the price of the stock goes up in the future,
meaning that future excess returns go up.

This presumption can be examined using ADL models of exess returns where
lags of the logarithm of the stock's dividend yield serve as additional
regressors.

Unfortunatly, a graphical inspection of the time series of the logarithm
of the dividend yield casts doubt on the assumption that the series is
stationary which, as has been discussed in Chapter 14.7, is necessary to
obtain meaningful results in a regression analysis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot logarithm of dividend yield series}
\KeywordTok{plot}\NormalTok{(StockReturns[,}\DecValTok{2}\NormalTok{], }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"Logarithm"}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Dividend Yield for CRSP Index"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-383-1} \end{center}

The Dickey-Fuller test statistic for an autoregressive unit root in an
AR(\(1\)) model with drift provides further evidence that the series
might be nonstationary.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# DF-Regression for log dividend yield}
\NormalTok{LDivYield <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(}\KeywordTok{d}\NormalTok{(ln_DivYield) }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(ln_DivYield), }\DataTypeTok{data =}\NormalTok{ StockReturns,}
      \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{,}\DecValTok{12}\NormalTok{)}
\NormalTok{      )}

\KeywordTok{summary}\NormalTok{(LDivYield)}\OperatorTok{$}\NormalTok{coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    Estimate  Std. Error   t value  Pr(>|t|)
## (Intercept)    -2.728786496 2.083152348 -1.309931 0.1908042
## L(ln_DivYield) -0.007656929 0.005998017 -1.276577 0.2023282
\end{verbatim}

Since the \(t\)-value for the coefficient on the logarithm of the
dividend yield is \(-1.27\) so the hypothesis that the true coefficient
is zero cannot be rejected, even at the \(10\%\) significance level.

However, it is possible to examine whether the dividend yield has
predictive power for excess returns by using its differences in an
ADL(\(1\),\(1\)) and an ADL(\(2\),\(2\)) model (differencing a series
with a unit root yield a stationary series) though these model
specifications do not correspond to the ecnonomic reasoning mentioned
above. Thus we also estimate a ADL(\(1\),\(1\)) regression using the
level of the logarithm of the dividend yield.

Thus we estimate three model specifications:

\begin{align*}
  excess \, returns_t =& \, \beta_0 + \beta_1 excess \, returns_{t-1} + \beta_3 \Delta \log(dividend yield_t-1) + u_t \\
  \\
  excess \, returns_t =& \, \beta_0 + \beta_1 excess \, returns_{t-1} + \beta_2 excess \, returns_{t-2} \\ &+ \, \beta_3 \Delta \log(dividend yield_t-1) + \beta_4 \Delta \log(dividend yield_t-2) + u_t \\
  \\
  excess \, returns_t =& \, \beta_0 + \beta_1 excess \, returns_{t-1} + \beta_5 \log(dividend yield_t-1) + u_t \\
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ADL(1,1) (1st difference of log dividend yield)}
\NormalTok{CRSP_ADL_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ExReturn }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn) }\OperatorTok{+}\StringTok{ }\KeywordTok{d}\NormalTok{(}\KeywordTok{L}\NormalTok{(ln_DivYield)), }\DataTypeTok{data =}\NormalTok{ StockReturns,}
      \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{,}\DecValTok{12}\NormalTok{)}
\NormalTok{      )}

\CommentTok{# ADL(2,2) (1st & 2nd differences of log dividend yield)}
\NormalTok{CRSP_ADL_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ExReturn }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn, }\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{d}\NormalTok{(}\KeywordTok{L}\NormalTok{(ln_DivYield)) }\OperatorTok{+}\StringTok{ }\KeywordTok{d}\NormalTok{(}\KeywordTok{L}\NormalTok{(ln_DivYield, }\DecValTok{2}\NormalTok{)), }\DataTypeTok{data =}\NormalTok{ StockReturns,}
      \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{,}\DecValTok{12}\NormalTok{)}
\NormalTok{      )}

\CommentTok{# ADL(1,1) (levels of log dividend yield)}
\NormalTok{CRSP_ADL_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ExReturn }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(ln_DivYield), }\DataTypeTok{data =}\NormalTok{ StockReturns,}
      \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{1992}\NormalTok{,}\DecValTok{12}\NormalTok{)}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rob_se_CRSP_ADL <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{sandwich}\NormalTok{(CRSP_ADL_}\DecValTok{1}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{sandwich}\NormalTok{(CRSP_ADL_}\DecValTok{2}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{sandwich}\NormalTok{(CRSP_ADL_}\DecValTok{3}\NormalTok{)))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

ADL Models of Monthly Excess Stock Returns

Excess returns on the CSRP value-weighted index

ADL(1,1)

ADL(2,2)

ADL(1,1)

(1)

(2)

(3)

excess returnt-1

0.059

0.042

0.078

(0.158)

(0.162)

(0.057)

excess returnt-2

-0.213

(0.193)

1st diff log(dividend yieldt-1)

0.009

-0.012

(0.157)

(0.163)

1st diff log(dividend yieldt-2)

-0.161

(0.185)

log(dividend yieldt-1)

0.026**

(0.012)

Constant

0.309

0.372*

8.987**

(0.199)

(0.208)

(3.912)

Observations

516

516

396

R2

0.003

0.007

0.018

Adjusted R2

-0.001

-0.001

0.013

Residual Std. Error

4.338 (df = 513)

4.337 (df = 511)

4.407 (df = 393)

F Statistic

0.653 (df = 2; 513)

0.897 (df = 4; 511)

3.683** (df = 2; 393)

Note:

\emph{p\textless{}0.1; \textbf{p\textless{}0.05; }}p\textless{}0.01

For models (1) and (2), notice that none of the individual
\(t\)-statistics suggests that the coefficients are different from zero.
Also we cannot reject the hypothesis that none of the lags have
predictive power for excess returns at any common level of significance
(an \(F\)-test that the lags have predictive power is reject for both
models).

Things are different for model (3). The coefficient on the level of the
logarithm of the devidend yield is different from zero at the \(5\%\)
level and the \(F\)-test does not reject at this level either. But we
should be suspicious: the high degree of persistence in the devidend
yield series probably renders this inference useless because \(t\)- and
\(F\)-statistics may follow distributions that deviate considerably from
their theoretical large-sample distributions such that the usual
critical values cannot be applied.

If model (3) would be of any use for predicting excess returns,
pseudo-out-of-sample forecasts based on (3) should outperform forecasts
of an intercept-only model in terms of lower sample RMSFE. We can
perform this type of comparison using R code in the fashion of the
applications of Chapter 14.8.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# end of sample dates}
\NormalTok{EndOfSample <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{window}\NormalTok{(}\KeywordTok{time}\NormalTok{(StockReturns), }\KeywordTok{c}\NormalTok{(}\DecValTok{1992}\NormalTok{,}\DecValTok{12}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{,}\DecValTok{11}\NormalTok{)))}

\CommentTok{# initialize matrix  forecasts}
\NormalTok{forecasts <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{, }
                    \DataTypeTok{ncol =} \KeywordTok{length}\NormalTok{(EndOfSample))}

\CommentTok{# estimation loop over end of sample dates}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(EndOfSample)) \{}

  \CommentTok{# estimate model (3)}
\NormalTok{  mod3 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ExReturn }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(ExReturn) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(ln_DivYield), }\DataTypeTok{data =}\NormalTok{ StockReturns, }
                \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                \DataTypeTok{end =}\NormalTok{ EndOfSample[i])}
  
  \CommentTok{# estimate intercept only model}
\NormalTok{  modconst <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(ExReturn }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ StockReturns, }
                \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1960}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                \DataTypeTok{end =}\NormalTok{ EndOfSample[i])}
  
  \CommentTok{# sample data for one-period ahead forecast}
\NormalTok{  t <-}\StringTok{ }\KeywordTok{window}\NormalTok{(StockReturns, EndOfSample[i], EndOfSample[i])}
  
  \CommentTok{# compute forecast}
\NormalTok{  forecasts[,i] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
    \KeywordTok{coef}\NormalTok{(mod3) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, t[}\DecValTok{1}\NormalTok{], t[}\DecValTok{2}\NormalTok{]),}
    \KeywordTok{coef}\NormalTok{(modconst)}
\NormalTok{  )}
                     
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# gather data}
\NormalTok{d <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}
  \StringTok{"Excess Returns"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{window}\NormalTok{(StockReturns[,}\DecValTok{1}\NormalTok{], }\KeywordTok{c}\NormalTok{(}\DecValTok{1993}\NormalTok{, }\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{, }\DecValTok{12}\NormalTok{))),}
  \StringTok{"Model (3)"}\NormalTok{ =}\StringTok{ }\NormalTok{forecasts[}\DecValTok{1}\NormalTok{,], }
  \StringTok{"Intercept Only"}\NormalTok{ =}\StringTok{ }\NormalTok{forecasts[}\DecValTok{2}\NormalTok{,], }
  \StringTok{"Always Zero"}\NormalTok{ =}\StringTok{  }\DecValTok{0}\NormalTok{)}

\CommentTok{# Compute RMSFEs}
\KeywordTok{c}\NormalTok{(}
  \StringTok{"ADL model (3)"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(d[,}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{d[,}\DecValTok{2}\NormalTok{]),}
  \StringTok{"Intercept-only model"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(d[,}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{d[,}\DecValTok{3}\NormalTok{]),}
  \StringTok{"Always zero"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(d[,}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{d[,}\DecValTok{4}\NormalTok{])}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        ADL model (3) Intercept-only model          Always zero 
##             4.043757             4.000221             3.995428
\end{verbatim}

The comparison indicates that model (3) has no predictive power since it
is outperformed in terms of sample RMSFE by the intercept-only model. A
model forecasting excess returns always by zero has an even lower sample
RMSFE. This finding is consistent with the strong-form efficiency
hypothesis which states that \emph{all} publicly available information
is accounted for in stock prices such that there is no way to predict
future stock prices or excess returns, implying that the perceived
significant relationship indicated by model (3) is wrong.

\subsubsection*{Summary}\label{summary-6}
\addcontentsline{toc}{subsubsection}{Summary}

Chapter 14 dealt with introductory topics in time series regression
analysis where variables are generally correlated from one observation
to the next, a concept termed serial correlation. At the beginning,
several ways of storing and plotting time series data using R have been
presented and used for informal analysis of economic data.

We have introduced AR and ADL models and applied them in the context of
forecasting of macroeconomic and financial time series using R. The
discussion also included the topic of lag length selection. It was shown
how to set up a simple function that computes the BIC for a model object
supplied.

We have also seen how to write simple R code for performing and
evaluating forecasts and demonstrated some more sophisticated approaches
to conduct pseudo-out-of-sample forecasts for assessment of a model's
predictive power for unobserved future outcomes of a series, to check
model stability and to compare different models.

Furthermore, some more technical aspects like the concept of
stationarity were adressed. This included applications to testing for an
autoregressive unit root with the Dickey-Fuller test and the detection
of a break in the population regression function using the \(QLR\)
statistic. For both approaches, the distribution of the relevant test
statistic is nonnormal, even in large samples. Concerning the
Dickey-Fuller test we have used R's random number generation facilities
to produce evidence for this issue by means of Monte-Carlo simulation
and motivated usage of tabulated quantiles.

Also, empirical studies on the validity of the weak and the strong form
efficiency hypothesis which are presented in the applications \emph{Can
You Beat the Market? Part I \& II} in the book have been reproduced
using R.

In all applications presented in this chapter, the focus was layed on
forecasting future outcomes rather than estimation of causal
relationships between time series variables. However, the framework of
methods needed for the latter is very similar. Chapter 15 is devoted to
estimation of so called \emph{dynamic causal effects}.

\chapter{Estimation of Dynamic Causal
Effects}\label{estimation-of-dynamic-causal-effects}

For answering some questions in economic research it interesting to know
how large the effect on \(Y\) now and in the future of a change in \(X\)
is. This called the \emph{dynamic causal effect} on \(Y\) of a change in
\(X\). In this section we will see how to estimate dynamic causal
effects using several R applications where we investigate the dynamic
effect of cold weather in Florida on the price of orange juice
concentrate.

The discussion covers:

\begin{itemize}
\tightlist
\item
  Estimation of distributed lag models
\item
  Heteroskedasticity- and autocorrelation-consistent (HAC) standard
  errors
\item
  Generalized least squares (GLS) of ADL models
\end{itemize}

If you would like to reproduce code examples, install the R packages
listed below and make sure that the subsequent code chunk executes
without any errors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{library}\NormalTok{(quantmod)}
\KeywordTok{library}\NormalTok{(dynlm)}
\KeywordTok{library}\NormalTok{(orcutt)}
\KeywordTok{library}\NormalTok{(nlme)}
\KeywordTok{library}\NormalTok{(stargazer)}
\end{Highlighting}
\end{Shaded}

\section{The Orange Juice Data}\label{the-orange-juice-data}

The largest cultivation region for oranges in the U.S. is located in
state of Florida which usually has ideal climate for the fruit growth
and thus makes Florida the source of almost all frozen juice contentrate
produced in the country However, from time to time and depening on their
severeness, cold snaps cause loss of harvests such that the supply of
oranges decreases and consequently the price of frozen juice conzentrate
rises. The timing of the price increases is complicated: a cut in
today's supply of concentrate influences not today's price but also
future prices because supply in future periods will be lower, too.
Clearly, the magnitude of today's and future price increases due to a
freeze is an empirical question that can be investigated using a
distributed lag model --- a model that relates price changes to weather
conditions.

To begin with the analysis, we reproduce Figure 15.1 of the book which
displays plots of the price index for frozen concentrated orange juice,
percent changes in the price as well as monthly freezing degree days in
Orlando, the center of Florida's orange-growing region.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load the frozen orange juice data set}
\KeywordTok{data}\NormalTok{(}\StringTok{"FrozenJuice"}\NormalTok{)}

\CommentTok{# price index for frozen concentrated juice}
\NormalTok{FOJCPI <-}\StringTok{ }\NormalTok{FrozenJuice[,}\StringTok{"price"}\NormalTok{]}\OperatorTok{/}\NormalTok{FrozenJuice[,}\StringTok{"ppi"}\NormalTok{]}
\NormalTok{FOJC_pctc <-}\StringTok{ }\DecValTok{100} \OperatorTok{*}\StringTok{ }\KeywordTok{diff}\NormalTok{(}\KeywordTok{log}\NormalTok{(FOJCPI))}
\NormalTok{FDD <-}\StringTok{ }\NormalTok{FrozenJuice[, }\StringTok{"fdd"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{FOJCPI_xts <-}\StringTok{ }\KeywordTok{as.xts}\NormalTok{(FOJCPI)}
\NormalTok{FDD_xts <-}\StringTok{ }\KeywordTok{as.xts}\NormalTok{(FrozenJuice[,}\DecValTok{3}\NormalTok{])}

\CommentTok{# Plot orange juice price index}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(FOJCPI),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Price index"}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Frozen Concentrated Orange Juice"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-392-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot percent changes in prices}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(FOJC_pctc),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Monthly Changes in the Price of Frozen Conentrated Orange Juice"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-392-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot freezing degree days}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(FDD),}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Freezing degree days"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Monthly Freezing Degree Days in Orlando, FL"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-392-3} \end{center}

It is conspicious that periods with a high amount of freezing degree
days are followed by large month-to-month price changes. These
coindciding movements give rise to run a simple regression of price
changes \(\%ChgOJC_t\) on freezing degree days (\(FDD_t\)) to estimate
the effect of an additional freezing degree day one the price in the
current month. For this, as for all other regressions in this chapter,
we use \(T=611\) observations (January 1950 to December 200).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simple regression of percent changes on freezing degree days}
\NormalTok{orange_SR <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(FOJC_pctc }\OperatorTok{~}\StringTok{ }\NormalTok{FDD)}
\KeywordTok{coeftest}\NormalTok{(orange_SR, }\DataTypeTok{vcov. =}\NormalTok{ vcovHAC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) -0.42095    0.18683 -2.2531 0.0246064 *  
## FDD          0.46724    0.13385  3.4906 0.0005167 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Notice that the standard errors are computed using a HAC estimator of
the model variance-covariance matrix, a matter that will not be further
commented at this point, see Chapter 14.5.

\begin{align*}
  \widehat{\%ChgOJC_t} = \underset{(0.19)}{-0.42} + \underset{(0.13)}{0.47} FDD_t
\end{align*}

The estimated coefficient on \(FDD_t\) has the following interpretation:
an additional freezing degree day in month \(t\) leads to a price
increase 0f \(0.47\%\) in the same month.

To consider effects of cold snaps on the orange juice price over
following, we include lagged values of \(FDD_t\) in our model which
leads to a so-called \emph{distributed lag regression}. We estimate a
specification of a contemporeneous and six lagged values of \(FDD_t\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# distributed lag model with 6 lags of freezing degree days}
\NormalTok{orange_DLM <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(FOJC_pctc }\OperatorTok{~}\StringTok{ }\NormalTok{FDD }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(FDD, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{))}
\KeywordTok{coeftest}\NormalTok{(orange_DLM, }\DataTypeTok{vcov. =}\NormalTok{ vcovHAC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##               Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)  -0.692961   0.212445 -3.2618 0.0011700 ** 
## FDD           0.471433   0.135195  3.4871 0.0005242 ***
## L(FDD, 1:6)1  0.145021   0.081557  1.7782 0.0758853 .  
## L(FDD, 1:6)2  0.058364   0.058911  0.9907 0.3222318    
## L(FDD, 1:6)3  0.074166   0.047143  1.5732 0.1162007    
## L(FDD, 1:6)4  0.036304   0.029335  1.2376 0.2163670    
## L(FDD, 1:6)5  0.048756   0.031370  1.5543 0.1206535    
## L(FDD, 1:6)6  0.050246   0.045129  1.1134 0.2659919    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

As a result we obtain

\begin{align}
  \widehat{\%ChgOJC_t} =& \, \underset{(0.21)}{-0.69} + \underset{(0.14)}{0.47} FDD_t + \underset{(0.08)}{0.15} FDD_{t-1} + \underset{(0.06)}{0.06} FDD_{t-2} + \underset{(0.05)}{0.07} FDD_{t-3} \\ &\,+ \underset{(0.03)}{0.04} FDD_{t-4} + \underset{(0.03)}{0.05} FDD_{t-5} + \underset{(0.05)}{0.05} FDD_{t-6} \label{eq:orangemod1}
\end{align}

where the coefficient on \(FDD_{t-1}\) estimates the price increase in
period \(t\) caused by an additional freezing degree day in the
preceding month, the coefficient on \(FDD_{t-2}\) estimates the effect
of an additional freezing degree day two month ago and so on.
Consequently, the coefficients in \eqref{eq:orangemod1} can be interpreted
as price changes in current and future periods due to a unit increase in
the current month' freezing degree days.

\section{Dynamic Causal Effects}\label{dynamic-causal-effects}

This section of the book describes the general idea of a dynamic causal
effect and how the concept of a randomized controlled experiment which
(as has been discussed in Chapter 13) constitutes and ideal study design
can be translated to time series applications, using several examples.
For brevity we will not go into the details but note once again that the
distributed lag model mentionted above can often be used to estimate a
dynamic causal relationship.

In general, for empirical attempts to measure a dynamic causal effect,
the assumptions of stationarity (see Key Concept 14.5) and exogeneity
must hold. In the time series appliaction up until here we have assumed
that the model error term has conditional mean zero given current and
past values of the regressors. For estimation of a dynamic causal effect
using a distributed lag model, assuming a stronger form termed
\emph{strict exogeneity} may be useful. Strict exogeneity states that
the error term has mean zero conditional on past, present \emph{and
future} values of the independent variables.

The two concepts of exogeneity and the distributive lag model are
summarized in Key Concept 15.1

Key Concept 15.1

The Distributed Lag Model and Exogeneity

The general distributed lag model is

\begin{align}
  Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + \beta_3 X_{t-2} + \dots + \beta_{r+1} X_{t-r} + u_t \label{eq:dlm}
\end{align}

where it is assumed that

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(X\) is an exogenous variable,
  \[E(u_t\vert X_t, X_{t-1}, X_{t-2},\dots) = 0.\]
\item
  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \tightlist
  \item
    \(X_t,Y_t\) have a stationary distribution.
  \item
    \((Y_t,X_t)\) and \((Y_{t-j},X_{t-j})\) become independently
    distributed as \(j\) gets large.
  \end{enumerate}
\item
  Large outliers are unlikely. In particular, we need that the all
  variables have more than eight nonzero and finite moments --- a
  stronger assumption than before (four finite nonzero moments) that is
  required for computation of the HAC covariance matrix estimator.
\item
  There is no perfect multicollinearity.
\end{enumerate}

The distributed lag model may be extended to include contempoarneous and
past values of additional regressors.

\textbf{On the assumption of exogeneity}

\begin{itemize}
\item
  There is another form of exogeneity termed \emph{strict exogeneity}
  which assumes
  \[E(u_t\vert \dots X_{t+2},X_{t+1},X_t,X_{t-1},X_{t-2},\dots)=0,\]
  that is the error term is has mean zero conditional on past, present
  and future values of \(X\). Strict exogeneity implies exogeneity (as
  defined in 1.) but not the other way around. From this point we will
  therefore distinguish between exogeneity and strict exogeneity.
\item
  Exogeneity as in 1. suffices for OLS estimators of the coefficient in
  distributed lag models to be consistent. However, if the the
  assumption of strict exogeneity can be made, more efficient estimators
  can be applied.
\end{itemize}

\section{Dynamic Multipliers and Cumulative Dynamic
Multipliers}\label{dynamic-multipliers-and-cumulative-dynamic-multipliers}

The following terminoligy regarding the coefficients in the distributed
lag model \eqref{eq:dlm} are useful for upcoming applications:

\begin{itemize}
\item
  The dynamic causal effect is also termed the \emph{dynamic
  multiplier}. We say that \(\beta_{h+1}\) in \eqref{eq:dlm} is the
  \(h\)-period dynamic multiplier.
\item
  The contemporaneous effect of \(X\) ond \(Y\), \(\beta_1\), is termed
  the \emph{impact effect}.
\item
  The \(h\)-period \emph{cumulative dynamic multiplier} of a unit change
  in \(X\) and \(Y\) is defined as the cumulative sum of the dynamic
  multipliers. In particualr, \(\beta_1\) is the zero-period cumulative
  dynamic multiplier, \(\beta_1 + \beta_2\) is the one-period cumulative
  dynamic multiplier and so forth.

  Cumulative dynamic multipliers of \eqref{eq:dlm} are the coefficients
  \(\delta_1,\delta_2,\dots,\delta_r,\delta_{r+1}\) in the modified
  regression

  \begin{align}
    Y_t =& \, \delta_0 + \delta_1 \Delta X_t + \delta_2 \Delta X_{t-1} + \dots + \delta_r \Delta X_{t-r+1} + \delta_{r+1} X_{t-r} + u_t. \label{eq:DCMreg}
  \end{align}

  and thus can be directly estimated using OLS wich makes it convenient
  to compute their HAC standard errors. \(\delta_{r+1}\) is called the
  \emph{long-run dynamic multiplier}.
\end{itemize}

It is straightforward to compute the cumulative dynamic multipliers for
\eqref{eq:orangemod1}, the estimated distributed lag regression of changes
in orange juice concentrate prices on freezing degree days, using the
corresponding model object \texttt{orange\_DLM} and the function
\texttt{cumsum()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute cumulative multipliers}
\NormalTok{cum_mult <-}\KeywordTok{cumsum}\NormalTok{(orange_DLM}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\DecValTok{1}\NormalTok{])}

\CommentTok{# rename entries}
\KeywordTok{names}\NormalTok{(cum_mult) <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DataTypeTok{sep =} \StringTok{"-"}\NormalTok{, }\StringTok{"period CDM"}\NormalTok{)}

\NormalTok{cum_mult}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0-period CDM 1-period CDM 2-period CDM 3-period CDM 4-period CDM 
##    0.4714329    0.6164542    0.6748177    0.7489835    0.7852874 
## 5-period CDM 6-period CDM 
##    0.8340436    0.8842895
\end{verbatim}

Translating the distributed lag model with six lags of \(FDD\) to
\eqref{eq:DCMreg}, we see that the OLS coefficient estimates in this model
coincide with the multipliers stored in \texttt{cum\_mult}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate cumulative dynamic multipliers using the modified regression}
\NormalTok{cum_mult_reg <-}\KeywordTok{dynlm}\NormalTok{(FOJC_pctc }\OperatorTok{~}\StringTok{ }\KeywordTok{d}\NormalTok{(FDD) }\OperatorTok{+}\StringTok{ }\KeywordTok{d}\NormalTok{(}\KeywordTok{L}\NormalTok{(FDD,}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(FDD,}\DecValTok{6}\NormalTok{))}
\KeywordTok{coef}\NormalTok{(cum_mult_reg)[}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          d(FDD) d(L(FDD, 1:5))1 d(L(FDD, 1:5))2 d(L(FDD, 1:5))3 
##       0.4714329       0.6164542       0.6748177       0.7489835 
## d(L(FDD, 1:5))4 d(L(FDD, 1:5))5       L(FDD, 6) 
##       0.7852874       0.8340436       0.8842895
\end{verbatim}

As noted above, this spcification of model allows to obtain standard
errors for the estimated dynamic cumulative multipliers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coeftest}\NormalTok{(cum_mult_reg, }\DataTypeTok{vcov. =}\NormalTok{ vcovHAC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##                 Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)     -0.69296    0.23668 -2.9278 0.0035431 ** 
## d(FDD)           0.47143    0.13583  3.4709 0.0005562 ***
## d(L(FDD, 1:5))1  0.61645    0.13145  4.6896 3.395e-06 ***
## d(L(FDD, 1:5))2  0.67482    0.16009  4.2151 2.882e-05 ***
## d(L(FDD, 1:5))3  0.74898    0.17263  4.3387 1.682e-05 ***
## d(L(FDD, 1:5))4  0.78529    0.17351  4.5260 7.255e-06 ***
## d(L(FDD, 1:5))5  0.83404    0.18236  4.5737 5.827e-06 ***
## L(FDD, 6)        0.88429    0.19303  4.5810 5.634e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\section{HAC Standard Errors}\label{hac-standard-errors}

The error term \(u_t\) in the distributed lag model \eqref{eq:dlm} may be
serially correlated due to serially correlated determinants of \(Y_t\)
that are not included as regressors. When these factors are not
correlated with the regressors included in the model, serially
correlated errors do not violate the assumption of exogeneity such that
the OLS estimator remains unbiased and consistent.

Instead, autocorrelated standard errors render the usual
homoskedasticity-only \emph{and} heteroskedasticity-robust standard
errrors invalid and may lead to misleading statistical inference. HAC
errors are a remedy for this issue. Key Concept 15.2

Key Concept 15.2

HAC Standard errors

\textbf{Problem}:

If the error term \(u_t\) in the distributed lag model \eqref{eq:dlm} is
serially correlated, satistical inference that rests on usual
(heteroskedasticity-robust) standard errors can be strongly misleading.

\textbf{Solution}:

Heteroskedasticity- and autocorrelation-consistent (HAC) estimators of
the variance-covariance matrix circumvent this issue. There are R
functions like \texttt{vcovHAC()} from the package sandwich which are
convinient for computation of such estimators.

The package sandwich also contains the function \texttt{NeweyWest()}, an
implementation of the HAC variace-covariance estimator proposed by Newey
and West (1987).

Consider the distributed lag regression model with no lags and the
single regressor \(X_t\)

\begin{align*}
  Y_t = \beta_0 + \beta_1 X_t + u_t.
\end{align*}

with autocorrelated errors. A brief derivation of

\begin{align}
  \overset{\sim}{\sigma}^2_{\widehat{\beta}_1} = \widehat{\sigma}^2_{\widehat{\beta}_1} \widehat{f}_t, \label{eq:nwhac}
\end{align}

the so-called \emph{Newey-West variance estimator} for the variance of
the OLS estimator of \(\beta_1\) is presented in Chapter 15.4 of the
book. \(\widehat{\sigma}^2_{\widehat{\beta}_1}\) in \eqref{eq:nwhac} is
the heteroskedasticity-robust variance estimate of \(\widehat{\beta}_1\)
and

\begin{align}
  \widehat{f}_t = 1 + 2 \sum_{j=1}^{m-1} \left(\frac{m-j}{m}\right) \overset{\sim}{\rho}_j \label{eq:nwhacf}
\end{align}

is a correction factor that adjusts for serially correlated errors and
involves estimates of \(m-1\) autocovariances
\(\overset{\sim}{\rho}_j\), whereby \(m\) is a truncation parameter to
be chosen. A rule of thumb for choosing \(m\) is

\begin{align}
  m = \left \lceil{0.75 \cdot T^{1/3}}\right\rceil. \label{eq:hactruncrot}
\end{align}

In the following we simulate a time series that, as stated above,
follows a distributed lag model with autocorrelated errors and then show
how to compute the Newey West HAC estimate of \(SD(\widehat{\beta}_1)\)
using R. This is done in two seperat but, as we will see, identical
approches: at first we follow the derivation presented in the book
step-by-step and compute the estimate manually. We then prove that the
result is exactly the estimate obtained when using the function
\texttt{NeweyWest()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# function that computes rho tilde}
\NormalTok{acf_c <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x,j) \{}
  \KeywordTok{return}\NormalTok{(}
    \KeywordTok{t}\NormalTok{(x[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{j)]) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(}\KeywordTok{Lag}\NormalTok{(x, j)) }\OperatorTok{/}\StringTok{ }\KeywordTok{t}\NormalTok{(x) }\OperatorTok{%*%}\StringTok{ }\NormalTok{x}
\NormalTok{  )}
\NormalTok{\}}

\CommentTok{# simulate time series with serially correlated errors}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{eps <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{ma =} \FloatTok{0.5}\NormalTok{))}
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\FloatTok{0.5} \OperatorTok{*}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\NormalTok{eps}

\CommentTok{# compute OLS residuals}
\NormalTok{res <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}\OperatorTok{$}\NormalTok{res}

\CommentTok{# compute v}
\NormalTok{v <-}\StringTok{ }\NormalTok{(X }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(X)) }\OperatorTok{*}\StringTok{ }\NormalTok{res}

\CommentTok{# compute robust estimate of beta_1 variance}
\NormalTok{var_beta_hat <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\DecValTok{100} \OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{100}\OperatorTok{-}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{sum}\NormalTok{((X }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(X))}\OperatorTok{^}\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{res}\OperatorTok{^}\DecValTok{2}\NormalTok{) ) }\OperatorTok{/}\StringTok{ }
\StringTok{                        }\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{100} \OperatorTok{*}\StringTok{ }\KeywordTok{sum}\NormalTok{((X}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(X))}\OperatorTok{^}\DecValTok{2}\NormalTok{))}\OperatorTok{^}\DecValTok{2}

\CommentTok{# rule of thumb truncation parameter}
\NormalTok{m <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(}\FloatTok{0.75} \OperatorTok{*}\StringTok{ }\DecValTok{100}\OperatorTok{^}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{3}\NormalTok{))}

\CommentTok{# compute correction factor}
\NormalTok{f_hat_T <-}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{sum}\NormalTok{(}
\NormalTok{  (m }\OperatorTok{-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\NormalTok{(m}\OperatorTok{-}\DecValTok{1}\NormalTok{))}\OperatorTok{/}\NormalTok{m }\OperatorTok{*}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{(m}\OperatorTok{-}\DecValTok{1}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(i) }\KeywordTok{acf_c}\NormalTok{(}\DataTypeTok{x=}\NormalTok{v, }\DataTypeTok{j=}\NormalTok{i))}
\NormalTok{  ) }

\CommentTok{# compute Newey West HAC estimate of the standard error }
\KeywordTok{sqrt}\NormalTok{(var_beta_hat }\OperatorTok{*}\StringTok{ }\NormalTok{f_hat_T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.04036208
\end{verbatim}

By choosing \texttt{lag\ =\ m-1} it is ensured that the maximum order of
autocorrelations used is \(m-1\) --- just as in equation
\eqref{eq:nwhacf}. Notice that we further set the arguments
\texttt{prewhite\ =\ F} and \texttt{adjust\ =\ T} to ensure that the
formula \eqref{eq:nwhac} is used and finite sample adjustments are made.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Using NeweyWest():}
\NormalTok{NW_VCOV <-}\StringTok{ }\KeywordTok{NeweyWest}\NormalTok{(}\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X), }
              \DataTypeTok{lag =}\NormalTok{ m }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{prewhite =}\NormalTok{ F, }
              \DataTypeTok{adjust =}\NormalTok{ T}
\NormalTok{              )}

\CommentTok{# compute standard error}
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(NW_VCOV))[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          X 
## 0.04036208
\end{verbatim}

We find that the computed standard errors coincide. Of course, a
variance-covariance matrix estimate as computed by \texttt{NeweyWest()}
can be supplied as the argument \texttt{vcov} in \texttt{coeftest()}
such that HAC \(t\)-statistics and \(p\)-values are provided.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\KeywordTok{coeftest}\NormalTok{(example_mod, }\DataTypeTok{vcov =}\NormalTok{ NW_VCOV)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 0.542310   0.235423  2.3036  0.02336 *  
## X           0.423305   0.040362 10.4877  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\section{Estimation of Dynamic Causal Effects with Strictly Exogeneous
Regressors}\label{estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors}

In general, the errors in a distributed lag model are correlated which
necessitates usage of HAC standard errors for valid inference. If,
however, the assumption of exogeneity (the frist assumption stated in
Key Concept 15.1) is replaced by strict exogeneity, that is
\[E(u_t\vert \dots, X_{t+1}, X_{t}, X_{t-1}, \dots),\] more efficient
approaches than OLS estimation of the coefficients may be available. For
a general distributed lag model with \(r\) lags and AR(\(p\)) errors,
these approaches are summarized in Key Concept 15.4.

Key Concept 15.4

Estimation of Dynamic Multipliers Under Strict Exogeneity

Consider the general distributed lag model with \(r\) lags and assume
that the errors follow an \(AR(p)\) process,

\begin{align}
  Y_t =& \, \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + \dots + \beta_{r+1} X_{t-r} + u_t \\
  u_t =& \, \phi_1 u_{t-1} + \phi u_{t-2} + \dots + \phi_p u_{t-p} + \overset{\sim}{u}_t. \label{eq:dlmarerrors}
\end{align}

Assuming strict exogeneity of \(X_t\), one may rewrite the above model
in the ADL specification

\begin{align*}
  Y_t =& \, \alpha_0 + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} \\
      &+ \, \delta_0 X_t + \delta_1 X_{t-1} + \dots + \delta_q X_{t-q} + \overset{\sim}{u}_t
\end{align*}

where \(q=r+p\) and compute estimates of the dynamic multipliers
\(\beta_1, \beta_2, \dots, \beta_{r+1}\) using OLS estimates of
\(\phi_1, \phi_2, \dots, \phi_p, \delta_0, \delta_1, \dots, \delta_q\).

An alternative is to estimate the dynamic multipliers using feasible
GLS, that is to apply the OLS estimator to a quasi-differenced
specification of \eqref{eq:dlmarerrors}. Under strict exogeneity, the
feasible GLS approach is the BLUE estimator for the dynamic multipliers
in large samples.

On the one han, as demonstrated in Chapter 15.5 of the book, the OLS
estimation of the ADL representation can be beneficial for estimation of
dynamic multipliers in large distributed lag model beacause it allows
for a more parsimonious model that may be a good approximation to a
large distributed lag model. On the other hand, the GLS approach is more
efficient than the ADL estimator if the sample size is large.

In what follows we shortly review how different representations of a
distribted lag model can be obtained when the causal effect in \(Y\) of
a change in \(X\) lasts for only two periods and show how these
specification can be estimated by OLS and GLS using R.

The model is

\begin{align}
  Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + u_t \label{eq:dldynamic}
\end{align}

so a change in \(X\) has a contemporaneous effect on \(Y\) (\(\beta_1\))
and an effect in the next period (\(\beta_2\)). The error term \(u_t\)
follows an AR(\(1\))
process,\[u_t = \phi_1 u_{t-1} + \overset{\sim}{u_t}\] where
\(\overset{\sim}{u_t}\) is serially uncorrelated.

One can show that the ADL representation of this model is

\begin{align}
  Y_t = \alpha_0 + \phi_1 Y_{t-1} + \delta_0 X_t + \delta_1 X_{t-1} + \delta_2 X_{t-2} + \overset{\sim}{u}_t \label{eq:adl21dynamic}
\end{align}

with the restrictions

\begin{align*}
  \beta_1 =& \, \delta_0, \\
  \beta_2 =& \, \delta_1 + \phi_1 \delta_0.
\end{align*}

\paragraph{Quasi-Differences}\label{quasi-differences}
\addcontentsline{toc}{paragraph}{Quasi-Differences}

Another way of writing this ADL(\(1\),\(2\)) representation is the
\emph{quasi-difference} model

\begin{align}
  \overset{\sim}{Y}_t = \alpha_0 + \beta_1 \overset{\sim}{X}_t + \beta_2   \overset{\sim}{X}_{t-1} + \overset{\sim}{u}_t \label{eq:qdm}
\end{align}

where \(\overset{\sim}{Y}_t = Y_t - \phi_1 Y_{t-1}\) and
\(\overset{\sim}{X}_t = X_t - \phi_1 X_{t-1}\). Notice that the error
term \(\overset{\sim}{u}_t\) is uncorrelated in both models and, as
shown in Chapter 15.5 of the book,
\[E(u_t\vert X_{t+1}, X_t, X_{t-1}, \dots) = 0\] which is implied by the
assumption of strict exogeneity.

We continue by simulation a time series of \(500\) observations using
the model \eqref{eq:dldynamic} where \(\beta_1 = 0.1\),
\(\beta_2 = 0.25\), \(\phi = 0.5\) and
\(\overset{\sim}{u}_t \sim N(0,1)\) and start by estimating the
distributed lag model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate time series with serially correlated errors}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{obs <-}\StringTok{ }\DecValTok{501}

\NormalTok{eps <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ obs}\OperatorTok{-}\DecValTok{1}\NormalTok{ , }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{ar =} \FloatTok{0.5}\NormalTok{))}
\NormalTok{X <-}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ obs, }\DataTypeTok{model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{ar =} \FloatTok{0.25}\NormalTok{))}
\NormalTok{Y <-}\StringTok{ }\FloatTok{0.1} \OperatorTok{*}\StringTok{ }\NormalTok{X[}\OperatorTok{-}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\FloatTok{0.25} \OperatorTok{*}\StringTok{ }\NormalTok{X[}\OperatorTok{-}\NormalTok{obs] }\OperatorTok{+}\StringTok{ }\NormalTok{eps}
\NormalTok{X <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(X[}\OperatorTok{-}\DecValTok{1}\NormalTok{])}

\CommentTok{# estimate the distributed lag model}
\NormalTok{dlm <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(X))}
\end{Highlighting}
\end{Shaded}

It is straightforward to check whether the residuals of this model
exhibit autocorrelation by using \texttt{acf()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check that the residuals are serially correlated}
\KeywordTok{acf}\NormalTok{(}
  \KeywordTok{residuals}\NormalTok{(dlm)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-402-1} \end{center}

The plot indicates that the residuals are autocorrelated. In particular,
the pattern reveals that the residuals follow an autoregressive process,
as the sample autocorrelation function decays quickly for the first few
lags and is probably zero for higher lag orders. At any case, HAC
standard errors should be used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# coefficient summary using NeweyWest estimator}
\KeywordTok{coeftest}\NormalTok{(dlm, }\DataTypeTok{vcov =} \KeywordTok{NeweyWest}\NormalTok{(dlm, }\DataTypeTok{prewhite =}\NormalTok{ F, }\DataTypeTok{adjust =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 0.038340   0.073411  0.5223  0.601717    
## X           0.123661   0.046710  2.6474  0.008368 ** 
## L(X)        0.247406   0.046377  5.3347 1.458e-07 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\paragraph{OLS Estimation of the ADL
Model}\label{ols-estimation-of-the-adl-model}
\addcontentsline{toc}{paragraph}{OLS Estimation of the ADL Model}

Next, we estimate the ADL(\(1\),\(2\)) model \eqref{eq:adl21dynamic} using
OLS. Notice that the errors are uncorrelated in this representation of
the model. This statement is supported by a plot of the sample
autocorrelation function of the residual series.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate ADL(2, 1) representation}
\NormalTok{adl21_dynamic <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(Y) }\OperatorTok{+}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(X, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{))}

\CommentTok{# plot sample autocorrelaltions of residuals}
\KeywordTok{acf}\NormalTok{(adl21_dynamic}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-404-1} \end{center}

The estimated coefficients in \texttt{adl21\_dynamic\$coefficients}
\emph{are not} the dynamic multiplers we are interested in but can be
computed according to the restrictions in \eqref{eq:adl21dynamic} where
the true coefficients are replaced by their OLS estimates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t <-}\StringTok{ }\NormalTok{adl21_dynamic}\OperatorTok{$}\NormalTok{coefficients}

\CommentTok{# compute estimated dynamic effects using coefficient restrictions}
\CommentTok{# in the ADL(2,1) representation}
\KeywordTok{c}\NormalTok{(}
  \StringTok{"hat_beta_1"}\NormalTok{ =}\StringTok{ }\NormalTok{t[}\DecValTok{3}\NormalTok{],}
  \StringTok{"hat_beta_2"}\NormalTok{ =}\StringTok{ }\NormalTok{t[}\DecValTok{4}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{t[}\DecValTok{3}\NormalTok{] }\OperatorTok{*}\StringTok{ }\NormalTok{t[}\DecValTok{2}\NormalTok{]}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          hat_beta_1.X hat_beta_2.L(X, 1:2)1 
##             0.1176425             0.2478484
\end{verbatim}

\paragraph{GLS Estimation}\label{gls-estimation}
\addcontentsline{toc}{paragraph}{GLS Estimation}

Strict exogeneity allows OLS estimation of the quasi-difference model
\eqref{eq:qdm}. The idea of applying the OLS estimator to a model where
the variables are linearly transformed such that the model errors are
uncorrelated and homoskedastic is called \emph{generalized least
squares} (GLS)

The OLS estimator in \eqref{eq:qdm} is called the \emph{infeasible GLS}
estimator because we \(\overset{\sim}{Y}\) and \(\overset{\sim}{X}\)
cannot be computed without knowledge of \(\phi_1\), the autoregressive
coefficient in the error AR(\(1\)) model, which is generally unknown in
practice.

Suppose \(\phi = 0.5\) was known. The infeasible GLS estimates of the
dynamic multipliers in \eqref{eq:dldynamic} are obtained by applying OLS
to the transformed data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# GLS: estimate quasi-differenced specification by OLS}
\NormalTok{iGLS_dynamic <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(}\KeywordTok{I}\NormalTok{(Y}\OperatorTok{-}\StringTok{ }\FloatTok{0.5} \OperatorTok{*}\StringTok{ }\KeywordTok{L}\NormalTok{(Y)) }\OperatorTok{~}\StringTok{ }\KeywordTok{I}\NormalTok{(X }\OperatorTok{-}\StringTok{ }\FloatTok{0.5}\OperatorTok{*}\StringTok{ }\KeywordTok{L}\NormalTok{(X)) }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(}\KeywordTok{L}\NormalTok{(X) }\OperatorTok{-}\StringTok{ }\FloatTok{0.5}\OperatorTok{*}\StringTok{ }\KeywordTok{L}\NormalTok{(X,}\DecValTok{2}\NormalTok{)) )}

\KeywordTok{summary}\NormalTok{(iGLS_dynamic)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Time series regression with "ts" data:
## Start = 3, End = 500
## 
## Call:
## dynlm(formula = I(Y - 0.5 * L(Y)) ~ I(X - 0.5 * L(X)) + I(L(X) - 
##     0.5 * L(X, 2)))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0325 -0.6375 -0.0499  0.6658  3.7724 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(>|t|)    
## (Intercept)              0.01620    0.04564   0.355  0.72273    
## I(X - 0.5 * L(X))        0.12000    0.04237   2.832  0.00481 ** 
## I(L(X) - 0.5 * L(X, 2))  0.25266    0.04237   5.963 4.72e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.017 on 495 degrees of freedom
## Multiple R-squared:  0.07035,    Adjusted R-squared:  0.0666 
## F-statistic: 18.73 on 2 and 495 DF,  p-value: 1.442e-08
\end{verbatim}

The \emph{feasible GLS} estimator uses preliminary estimation of the
coefficients in the presumed error term model, computes the
quasi-differenced data and then estimates the model using OLS. This idea
was introduced by Cochrance and Orcutt (1949) and can be extended by
continuing this process iteratively. Such a procedure is implemented in
the function cochrane.orcutt() from the package orcutt.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X_t <-}\StringTok{ }\KeywordTok{c}\NormalTok{(X[}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\NormalTok{X_l1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(X[}\OperatorTok{-}\DecValTok{500}\NormalTok{])}
\NormalTok{Y_t <-}\StringTok{ }\KeywordTok{c}\NormalTok{(Y[}\OperatorTok{-}\DecValTok{1}\NormalTok{])}

\CommentTok{# iterated cochrance-orcutt procedure}
\KeywordTok{summary}\NormalTok{(}
  \KeywordTok{cochrane.orcutt}\NormalTok{(}\KeywordTok{lm}\NormalTok{(Y_t }\OperatorTok{~}\StringTok{ }\NormalTok{X_t }\OperatorTok{+}\StringTok{ }\NormalTok{X_l1))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lm(formula = Y_t ~ X_t + X_l1)
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 0.032885   0.085163   0.386   0.69956    
## X_t         0.120128   0.042534   2.824   0.00493 ** 
## X_l1        0.252406   0.042538   5.934 5.572e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.0165 on 495 degrees of freedom
## Multiple R-squared:  0.0704 ,  Adjusted R-squared:  0.0666
## F-statistic: 18.7 on 2 and 495 DF,  p-value: < 1.429e-08
## 
## Durbin-Watson statistic 
## (original):    1.06907 , p-value: 1.05e-25
## (transformed): 1.98192 , p-value: 4.246e-01
\end{verbatim}

Some more sophisticated methods are provided with the package nlme. The
function gls() can be used to fit linear models by maximum likelihood
estimation algorithms and allows to specify a correlation structure for
the error term.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# feasible GLS maximum likelihood estimation procedure}
\KeywordTok{summary}\NormalTok{(}
  \KeywordTok{gls}\NormalTok{(Y_t }\OperatorTok{~}\StringTok{ }\NormalTok{X_t }\OperatorTok{+}\StringTok{ }\NormalTok{X_l1, }\DataTypeTok{correlation =} \KeywordTok{corAR1}\NormalTok{())}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Generalized least squares fit by REML
##   Model: Y_t ~ X_t + X_l1 
##   Data: NULL 
##        AIC     BIC    logLik
##   1451.847 1472.88 -720.9235
## 
## Correlation Structure: AR(1)
##  Formula: ~1 
##  Parameter estimate(s):
##       Phi 
## 0.4668343 
## 
## Coefficients:
##                  Value  Std.Error  t-value p-value
## (Intercept) 0.03929124 0.08530544 0.460595  0.6453
## X_t         0.11986994 0.04252270 2.818963  0.0050
## X_l1        0.25287471 0.04252497 5.946500  0.0000
## 
##  Correlation: 
##      (Intr) X_t  
## X_t  0.039       
## X_l1 0.037  0.230
## 
## Standardized residuals:
##         Min          Q1         Med          Q3         Max 
## -3.00075518 -0.64255522 -0.05400347  0.69101814  3.28555793 
## 
## Residual standard error: 1.14952 
## Degrees of freedom: 499 total; 496 residual
\end{verbatim}

Feasible GLS is the BLUE of the dynamic multipliers when there is serial
correlation and/or heteroskedasticity. Notice that in this example, the
coefficient estimates produced by GLS are somewhat closer to their true
values and that the standard errors are the smallest for the GLS
estimator.

\section{Orange Juice Prices and Cold
Weather}\label{orange-juice-prices-and-cold-weather}

In this section we investigate the following two questions using the
time series regression methods discussed before:

\begin{itemize}
\item
  How persistent is the effect of a single freeze on prices?
\item
  Has this effect been stable over the whole time span considered?
\end{itemize}

We start by estimating dynamic causal effects with a distributed lag
model where \(\%ChgOJC_t\) is regressed on \(FDD_t\) and 18 lags of it.
A second model specification is a transformation of the the distributed
lag model which allows to estimate the 19 cumulative dynamic multipliers
using OLS. In a third model, we add 11 binary variables (one for each of
the months from February to December) to adjust for a possible omitted
variable bias arising from correllation of \(FDD_t\) and seasons by
adding \texttt{season(FDD)} to the right hand side of the formula of the
second model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate distributed lag models of frozen orange juice price changes}
\NormalTok{FOJC_mod_DM <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(FOJC_pctc }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(FDD, }\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{))}
\NormalTok{FOJC_mod_CM1 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(FOJC_pctc }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{d}\NormalTok{(FDD), }\DecValTok{0}\OperatorTok{:}\DecValTok{17}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(FDD, }\DecValTok{18}\NormalTok{))}
\NormalTok{FOJC_mod_CM2 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(FOJC_pctc }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{d}\NormalTok{(FDD), }\DecValTok{0}\OperatorTok{:}\DecValTok{17}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(FDD, }\DecValTok{18}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{season}\NormalTok{(FDD))}
\end{Highlighting}
\end{Shaded}

The models above include a large number of lags with default labels that
correspond to the degree of differencing and the lag orders which makes
it somewhat cumbersome to read the outcomes. The regressor labels of a
model object may be altered by overriding the attribute \texttt{names}
of the coefficient section using the function \texttt{attr()}. Thus, for
better readability we use the lag orders as regressor labels.

Next, we compute HAC standard errors standard errors for each model
using \(NeweyWest()\) and gather the results in a list which is then
supplied as the argument \texttt{se} to the function
\texttt{stargazer()}, see below. Note that the sample consists of 612
observations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(FDD)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 612
\end{verbatim}

According to \eqref{eq:hactruncrot}, the rule of thumb for choosing the
HAC standard error truncation parameter \(m\), we choose
\[m = \left\lceil0.75 \cdot 612^{1/3} \right\rceil = \lceil6.37\rceil = 7.\]
To check for sensitivity of the standard errors to different choices of
the truncation parameter in the model that is used to estimate the
cumulative multipliers, we also compute the Newey and West estimator for
\(m=14\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# gather hAC standard error errors in a list}
\NormalTok{SEs <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}
    \KeywordTok{diag}\NormalTok{(}
      \KeywordTok{NeweyWest}\NormalTok{(FOJC_mod_DM, }\DataTypeTok{lag =} \DecValTok{7}\NormalTok{, }\DataTypeTok{prewhite =}\NormalTok{ F)}
\NormalTok{    )  }
\NormalTok{  ), }
  \KeywordTok{sqrt}\NormalTok{(}
    \KeywordTok{diag}\NormalTok{(}
      \KeywordTok{NeweyWest}\NormalTok{(FOJC_mod_CM1, }\DataTypeTok{lag =} \DecValTok{7}\NormalTok{, }\DataTypeTok{prewhite =}\NormalTok{ F)}
\NormalTok{    )  }
\NormalTok{  ), }
   \KeywordTok{sqrt}\NormalTok{(}
    \KeywordTok{diag}\NormalTok{(}
      \KeywordTok{NeweyWest}\NormalTok{(FOJC_mod_CM1, }\DataTypeTok{lag =} \DecValTok{14}\NormalTok{, }\DataTypeTok{prewhite =}\NormalTok{ F)}
\NormalTok{    )  }
\NormalTok{  ),}
  \KeywordTok{sqrt}\NormalTok{(}
    \KeywordTok{diag}\NormalTok{(}
      \KeywordTok{NeweyWest}\NormalTok{(FOJC_mod_CM2, }\DataTypeTok{lag =} \DecValTok{7}\NormalTok{, }\DataTypeTok{prewhite =}\NormalTok{ F)}
\NormalTok{    )  }
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The results are then used to reproduce the outcomes presented in Table
15.1 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{stargazer}\NormalTok{(FOJC_mod_DM , FOJC_mod_CM1, FOJC_mod_CM1, FOJC_mod_CM2,}
  \DataTypeTok{title =} \StringTok{"Dynamic Effects of a Freezing Degree Day on the Price of Orange Juice"}\NormalTok{,}
  \DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }
  \DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }
  \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Dynamic Multipliers"}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\StringTok{"Dynamic Cumulative Multipliers"}\NormalTok{,}\DecValTok{3}\NormalTok{)),}
  \DataTypeTok{dep.var.caption  =} \StringTok{"Monthly Percentage Change in Orange Juice Price"}\NormalTok{,}
  \DataTypeTok{dep.var.labels.include =} \OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{covariate.labels =} \KeywordTok{as.character}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{),}
  \DataTypeTok{omit =} \StringTok{"season"}\NormalTok{,}
  \DataTypeTok{se =}\NormalTok{ SEs,}
  \DataTypeTok{no.space =}\NormalTok{ T,}
  \DataTypeTok{add.lines =} \KeywordTok{list}\NormalTok{(}
                   \KeywordTok{c}\NormalTok{(}\StringTok{"Monthly indicators?"}\NormalTok{,}\StringTok{"no"}\NormalTok{, }\StringTok{"no"}\NormalTok{, }\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{),}
                   \KeywordTok{c}\NormalTok{(}\StringTok{"HAC truncation"}\NormalTok{,}\StringTok{"7"}\NormalTok{, }\StringTok{"7"}\NormalTok{, }\StringTok{"14"}\NormalTok{, }\StringTok{"7"}\NormalTok{)}
\NormalTok{                   ),}
  \DataTypeTok{omit.stat =} \KeywordTok{c}\NormalTok{(}\StringTok{"rsq"}\NormalTok{, }\StringTok{"f"}\NormalTok{,}\StringTok{"ser"}\NormalTok{)}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

Dynamic Effects of a Freezing Degree Day on the Price of Orange Juice

Monthly Percentage Change in Orange Juice Price

Dynamic Multipliers

Dynamic Cumulative Multipliers

Dynamic Cumulative Multipliers

Dynamic Cumulative Multipliers

(1)

(2)

(3)

(4)

0

0.51***

0.51***

0.51***

0.52***

(0.14)

(0.14)

(0.14)

(0.14)

1

0.17**

0.68***

0.68***

0.72***

(0.09)

(0.13)

(0.13)

(0.14)

2

0.07

0.75***

0.75***

0.78***

(0.06)

(0.17)

(0.16)

(0.17)

3

0.07

0.82***

0.82***

0.86***

(0.04)

(0.18)

(0.18)

(0.19)

4

0.02

0.84***

0.84***

0.89***

(0.03)

(0.18)

(0.18)

(0.19)

5

0.03

0.87***

0.87***

0.90***

(0.03)

(0.19)

(0.19)

(0.20)

6

0.03

0.90***

0.90***

0.92***

(0.05)

(0.20)

(0.21)

(0.21)

7

0.02

0.91***

0.91***

0.94***

(0.02)

(0.20)

(0.21)

(0.21)

8

-0.04

0.87***

0.87***

0.90***

(0.03)

(0.21)

(0.22)

(0.22)

9

-0.01

0.86***

0.86***

0.88***

(0.05)

(0.24)

(0.24)

(0.24)

10

-0.12*

0.75***

0.75***

0.75***

(0.07)

(0.26)

(0.26)

(0.26)

11

-0.07

0.68**

0.68**

0.68**

(0.05)

(0.27)

(0.27)

(0.27)

12

-0.14*

0.54**

0.54**

0.55**

(0.08)

(0.27)

(0.27)

(0.27)

13

-0.08*

0.45*

0.45*

0.49*

(0.04)

(0.27)

(0.27)

(0.27)

14

-0.06

0.40

0.40

0.43

(0.03)

(0.27)

(0.28)

(0.28)

15

-0.03

0.37

0.37

0.41

(0.03)

(0.28)

(0.29)

(0.28)

16

-0.01

0.36

0.36

0.41

(0.05)

(0.28)

(0.29)

(0.29)

17

0.003

0.36

0.36

0.40

(0.02)

(0.29)

(0.29)

(0.29)

18

0.003

0.37

0.37

0.39

(0.02)

(0.29)

(0.30)

(0.29)

Constant

-0.34

-0.34

-0.34

-0.24

(0.27)

(0.27)

(0.26)

(0.93)

Monthly indicators?

no

no

no

yes

HAC truncation

7

7

14

7

Observations

594

594

594

594

Adjusted R2

0.11

0.11

0.11

0.10

Note:

\emph{p\textless{}0.1; \textbf{p\textless{}0.05; }}p\textless{}0.01

Acording to coloumn (1), the contemporaneous effect of a freezing degree
day is an increase of \(0.5\%\) in orange juice prices. The estimated
effect is only \(0.17\%\) for the next month and close to zero for
subsequent months. In fact, for all lags larger than 1, we cannot reject
the individual null hypotheses that the respective coefficients are
zero. Notice also that the model \texttt{FOJC\_mod\_DM} does only
explain little variation in the dependent variable
(\(\overline{R}^2 = 0.11\)).

Columns (2) and (3) present estimates of the dynamic cumulative
multipliers of model \texttt{FOJC\_mod\_CM1}. Apparently, it does not
matter whether we choose \(m=7\) or \(m=14\) when computing HAC standard
errors so we stick with \(m=7\) and the standard errors reported in
column (2).

If demand for orange juice is higher in winter, \(FDD_t\) would be
correlated with the error term since since freezes occur rather in
winter so we would face omitted variable bias. The third model estimate,
\texttt{FOJC\_mod\_CM2}, accounts for this possible issue by using an
additional set of 11 monthly dummies. For brevity, estimates of the
dummy coefficients are excluded from the output produced by stargazer
(this is achieved by setting \texttt{omit\ =\ "season"}). We may check
that the dummy for January was omitted to prevent perfect
multicollinearity.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimates on mothly dummies}
\NormalTok{FOJC_mod_CM2}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## season(FDD)Feb season(FDD)Mar season(FDD)Apr season(FDD)May season(FDD)Jun 
##     -0.9565759     -0.6358007      0.5006770     -1.0801764      0.3195624 
## season(FDD)Jul season(FDD)Aug season(FDD)Sep season(FDD)Oct season(FDD)Nov 
##      0.1951113      0.3644312     -0.4130969     -0.1566622      0.3116534 
## season(FDD)Dec 
##      0.1481589
\end{verbatim}

A comparison of the estimates presented in columns (3) and (4) indicates
that adding monthly dummies has a negligible effect. Further evidence
for this comes from a test of the hypothesis that the 11 dummy
coefficients are jointly zero. Instead of using
\texttt{linearHypothesis()} which requires that all 11 restrictions are
provided in a matrix, we use the function \texttt{waldtest()} and supply
two model objects instead: \texttt{unres\_model}, the unrestricted model
object which is the same as \texttt{FOJC\_mod\_CM2} (except for the
coefficient names since we have modified them above) and
\texttt{res\_model}, the model where the restriction that all dummy
coefficients are zero is imposed. \texttt{res\_model} is conveniently
obtained using the function \texttt{update()}. It extracts the argument
formula of a model object, updates it as specified and then re-fits the
model. By setting
\texttt{formula\ =\ .\ \textasciitilde{}\ .\ -\ season(FDD)} we impose
that the monthly dummies do not enter the model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test if coefficients on monthly dummies are zero}

\NormalTok{unres_model <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(FOJC_pctc }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{d}\NormalTok{(FDD), }\DecValTok{0}\OperatorTok{:}\DecValTok{17}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(FDD, }\DecValTok{18}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{season}\NormalTok{(FDD))}

\NormalTok{res_model <-}\StringTok{ }\KeywordTok{update}\NormalTok{(unres_model, }\DataTypeTok{formula =}\NormalTok{ . }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{-}\StringTok{ }\KeywordTok{season}\NormalTok{(FDD))}

\KeywordTok{waldtest}\NormalTok{(unres_model, }
\NormalTok{         res_model, }
         \DataTypeTok{vcov =} \KeywordTok{NeweyWest}\NormalTok{(unres_model, }\DataTypeTok{lag =} \DecValTok{7}\NormalTok{, }\DataTypeTok{prewhite =}\NormalTok{ F))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Wald test
## 
## Model 1: FOJC_pctc ~ L(d(FDD), 0:17) + L(FDD, 18) + season(FDD)
## Model 2: FOJC_pctc ~ L(d(FDD), 0:17) + L(FDD, 18)
##   Res.Df  Df      F Pr(>F)
## 1    563                  
## 2    574 -11 0.9683 0.4743
\end{verbatim}

The \(p\)-value is \(0.47\) so we cannot reject the hypothesis that the
coefficients on the monthly dummies are zero, even at the \(10\%\) level
of significance. We conclude that the seasonal fluctuations in demand
for orange juice do not pose a serious threat to internal validity of
the model.

It is convenient to use plots of dynamic multipliers and cumulative
dynamic multipliers. The following two code chunks reproduce Figures
15.2 (a) and 15.2 (b) of the book which display point estimates of
dynamic and cumulative multipliers along with upper and lower bounds of
their \(95\%\) confidence intervals computed using the HAC standard
errors from above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 95% CI bounds}
\NormalTok{point_estimates <-}\StringTok{ }\NormalTok{FOJC_mod_DM}\OperatorTok{$}\NormalTok{coefficients}

\NormalTok{CI_bounds <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}
   \StringTok{"lower"}\NormalTok{ =}\StringTok{ }\NormalTok{point_estimates }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\NormalTok{SEs[[}\DecValTok{1}\NormalTok{]],}
   \StringTok{"upper"}\NormalTok{ =}\StringTok{ }\NormalTok{point_estimates }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\NormalTok{SEs[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{ )[}\OperatorTok{-}\DecValTok{1}\NormalTok{,]}


\CommentTok{# plot estimated dynamic multipliers}
\KeywordTok{plot}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, point_estimates[}\OperatorTok{-}\DecValTok{1}\NormalTok{], }
     \DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.4}\NormalTok{, }\DecValTok{1}\NormalTok{),}
     \DataTypeTok{xlab =} \StringTok{"Lag"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Dynamic multiplier"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Dynamic Effect of FDD on Orange Juice Price"}
\NormalTok{     )}

\CommentTok{# add dashed line at 0}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add CI bounds}
\KeywordTok{lines}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, CI_bounds[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, CI_bounds[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{URFITE_files/figure-latex/DM-1} 

}

\caption{Dynamic Multipliers}\label{fig:DM}
\end{figure}

Notice that the \(95\%\) confidence intervals plotted in Figure
\ref{fig:DM} indeed include zero for lags larger than 1 such that the
null of a zero multiplier cannot be rejected for these lags.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 95% CI bounds}
\NormalTok{point_estimates <-}\StringTok{ }\NormalTok{FOJC_mod_CM1}\OperatorTok{$}\NormalTok{coefficients}

\NormalTok{CI_bounds <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}
   \StringTok{"lower"}\NormalTok{ =}\StringTok{ }\NormalTok{point_estimates }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\NormalTok{SEs[[}\DecValTok{2}\NormalTok{]],}
   \StringTok{"upper"}\NormalTok{ =}\StringTok{ }\NormalTok{point_estimates }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\NormalTok{SEs[[}\DecValTok{2}\NormalTok{]]}
\NormalTok{ )[}\OperatorTok{-}\DecValTok{1}\NormalTok{,]}


\CommentTok{# plot estimated dynamic multipliers}
\KeywordTok{plot}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, point_estimates[}\OperatorTok{-}\DecValTok{1}\NormalTok{], }
     \DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.4}\NormalTok{, }\FloatTok{1.6}\NormalTok{),}
     \DataTypeTok{xlab =} \StringTok{"Lag"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Cumulative dynamic multiplier"}\NormalTok{,}
    \DataTypeTok{main =} \StringTok{"Cumulative Dynamic Effect of FDD on Orange Juice Price"}
\NormalTok{     )}

\CommentTok{# add dashed line at 0}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add CI bounds}
\KeywordTok{lines}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, CI_bounds[, }\DecValTok{1}\NormalTok{], }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, CI_bounds[, }\DecValTok{2}\NormalTok{], }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{URFITE_files/figure-latex/DCM-1} 

}

\caption{Dynamic Cumulative Multipliers}\label{fig:DCM}
\end{figure}

As can be seen from Figure \ref{fig:DCM}, the estimated dynamic
cumulative multipliers grow through the seventh month up to a price
increase of about \(0.91\%\) and then decrease slighly to the estimated
long-run cumulative multiplier of \(0.37\%\) which, however, is not
significantly different from zero at the \(5\%\) level.

Have the dynamic multipliers been stable over time? One way to see this
is the estimate them for different subperiods of the sample. For
example, consider periods 1950 - 1966, 1967 - 1983 and 1984 - 2000. If
the multipliers are the same for all three periods the estimates should
be close and thus the estimated cumulative multipliers should be
similar, too. We investigate this by re-estimating
\texttt{FOJC\_mod\_CM1} for the three different time spans and then plot
the estimated cumulative dynamic multipliers for the comparison.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate cumulative multiplieres using different sample periods}
\NormalTok{FOJC_mod_CM1950 <-}\StringTok{ }\KeywordTok{update}\NormalTok{(FOJC_mod_CM1, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1950}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{1966}\NormalTok{,}\DecValTok{12}\NormalTok{))}

\NormalTok{FOJC_mod_CM1967 <-}\StringTok{ }\KeywordTok{update}\NormalTok{(FOJC_mod_CM1, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1967}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{1983}\NormalTok{,}\DecValTok{12}\NormalTok{))}

\NormalTok{FOJC_mod_CM1984 <-}\StringTok{ }\KeywordTok{update}\NormalTok{(FOJC_mod_CM1, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1984}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2000}\NormalTok{,}\DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot estimated dynamic cumulative multipliers (1950-1966)}
\KeywordTok{plot}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, FOJC_mod_CM1950}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\DecValTok{1}\NormalTok{], }
     \DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{, }\DecValTok{2}\NormalTok{),}
     \DataTypeTok{xlab =} \StringTok{"Lag"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Cumulative dynamic multiplier"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Cumulative Dynamic Effect for Different Sample Periods"}
\NormalTok{)}

\CommentTok{# plot estimated dynamic multipliers (1967-1983)}
\KeywordTok{lines}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, FOJC_mod_CM1967}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{# plot estimated dynamic multipliers (1984-2000)}
\KeywordTok{lines}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{, FOJC_mod_CM1984}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{)}

\CommentTok{# add dashed line at 0}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add annotations}
\KeywordTok{text}\NormalTok{(}\DecValTok{18}\NormalTok{, }\OperatorTok{-}\FloatTok{0.24}\NormalTok{, }\StringTok{"1984 - 2000"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{18}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\StringTok{"1967 - 1983"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{18}\NormalTok{, }\FloatTok{1.2}\NormalTok{, }\StringTok{"1950 - 1966"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-418-1} \end{center}

Clearly, the cumulative dynamic multipliers have changed considerarbly
over time. Notice that the effect of a freeze was stronger an more
persistent in the 1950s and 1960s. For the 1970s the magnitude of the
effect was lower but still highly persistent. We observe an even lower
magnitude for the final third of the sample (1984 - 2000) where the
long-run effect is much less persist and essentialy zero after a year.

Evidence of a QLR test for a break in the population distributed lag
regression of column (1) with \(15\%\) trimming and HAC
variance-covariance matrix estimator supports the conjection that the
population regression coefficients have changed over time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up a range of possible break dates}
\NormalTok{tau <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}
  \KeywordTok{window}\NormalTok{(}
    \KeywordTok{time}\NormalTok{(FDD), }
    \KeywordTok{time}\NormalTok{(FDD)[}\KeywordTok{round}\NormalTok{(}\DecValTok{612}\OperatorTok{/}\DecValTok{100}\OperatorTok{*}\DecValTok{15}\NormalTok{)], }
    \KeywordTok{time}\NormalTok{(FDD)[}\KeywordTok{round}\NormalTok{(}\DecValTok{612}\OperatorTok{/}\DecValTok{100}\OperatorTok{*}\DecValTok{85}\NormalTok{)]}
\NormalTok{    )}
\NormalTok{  )}

\CommentTok{# initialize vector of F-statistics}
\NormalTok{Fstats <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\KeywordTok{length}\NormalTok{(tau))}

\CommentTok{# restricted model}
\NormalTok{res_model <-}\StringTok{ }\KeywordTok{update}\NormalTok{(unres_model, }\DataTypeTok{formula =}\NormalTok{ . }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(FDD, }\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{))}


\CommentTok{# estimation loop over break dates}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(tau)) \{}
  
  \CommentTok{# set up dummy variable}
\NormalTok{  D <-}\StringTok{ }\KeywordTok{time}\NormalTok{(FOJC_pctc) }\OperatorTok{>}\StringTok{ }\NormalTok{tau[i]}
  
  \CommentTok{# estimate DL model with intercations}
\NormalTok{  unres_model <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(FOJC_pctc }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(FDD, }\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{) }\OperatorTok{+}\StringTok{ }\NormalTok{D}\OperatorTok{*}\KeywordTok{L}\NormalTok{(FDD, }\DecValTok{0}\OperatorTok{:}\DecValTok{18}\NormalTok{))}
                 
  \CommentTok{# compute and save F-statistic}
\NormalTok{  Fstats[i] <-}\StringTok{ }\KeywordTok{waldtest}\NormalTok{(unres_model, }
\NormalTok{                        res_model, }
                        \DataTypeTok{vcov =} \KeywordTok{NeweyWest}\NormalTok{(unres_model, }\DataTypeTok{lag =} \DecValTok{7}\NormalTok{, }\DataTypeTok{prewhite =}\NormalTok{ F))}\OperatorTok{$}\NormalTok{F[}\DecValTok{2}\NormalTok{]}
    
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note that this code takes a couple of seconds to run since a total of
429 regressions with 40 model coefficients each are estimated.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# QLR test statistic}
\KeywordTok{max}\NormalTok{(Fstats)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 36.76819
\end{verbatim}

The QLR statistic is \(36.77\). From table 14.5 of the book we see that
the \(1\%\) critical value for the QLR test with \(15\%\) trimming and
\(q=20\) restrictions is \(2.43\). Since this is a right-sided test, the
QLR statistic clearly lies in the region of rejection so we can discard
the null hypothesis of no break in the population regression function.

See Chapter 15.7 of the book for a discussion of empirical examples
where it is questionable whether the assumption of (past and present)
exogeneity of regressors is plausible.

\section{Summary}\label{summary-7}

\begin{itemize}
\item
  We have seen how R can be used to estimate the time path of the effect
  on \(Y\) of a change in \(X\) (the dynamic causal effect on \(Y\) of a
  change in \(X\)) using time series data on both. The corresponding
  model is called the \emph{distributed lag model}. Distributed lag
  models are conveniently estimated using the function dynlm() from the
  package dynlm.
\item
  The regression error in distributed lag models is often serially
  correlated such that standard errors which are robust to
  heteroskedasticity and autocorrelation should be used to obtain valid
  inference based on \(t\)-statistics and confidence intervals. The
  package sandwich has functions for compuation of so-called HAC
  covariance matrix estimators, for example vcovHAC() and NeweyWest().
\item
  When \(X\) is \emph{strictly exogeneous}, more efficient estimates can
  be obtained using an ADL model or by GLS estimation. Feasible GLS
  algorithms can be found in the R packages orcutt and nlme. We have
  stressed that the assumption of strict exogeneity is often implausible
  in empirical applications. See chapter 15.7 of the book for further
  examples.
\end{itemize}

\chapter{Additional Topics in Time Series
Regression}\label{additional-topics-in-time-series-regression}

This chapter introduces the following advanced topics in time series
regression and demonstrates how core techniques can be applied using R:

\begin{itemize}
\tightlist
\item
  Vector autoregressions (VARs). We focus on using VARs for forecasting.
  Another branch of the literature is concerned with so-called
  \emph{Structural VARs} which are, however, beyond the scope of this
  chapter.
\item
  Multiperiod forecasts. This includes a discussion of iterated and
  direct (multivariate) forecasts.
\item
  The DF-GLS test, a more powerful method for unit root testing than the
  ADF test.
\item
  Cointegration analysis with an application to short- and long-term
  interest rates. We demonstrate how to estimate a vector error
  correction model.
\item
  Autoregressive conditional heteroskedasticity. We show how a
  GARCH(\(1\),\(1\)) model can be used for quantifying the risk
  associated with investing in the stock market in terms of estimation
  and forecasting of the volatility of asset returns.
\end{itemize}

If you are intersted in reproducing the code examples on your own
device, install the R packages listed below and make sure that the
subsequent code chunk executes without any errors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{library}\NormalTok{(readxl)}
\KeywordTok{library}\NormalTok{(dynlm)}
\KeywordTok{library}\NormalTok{(vars)}
\KeywordTok{library}\NormalTok{(quantmod)}
\KeywordTok{library}\NormalTok{(scales)}
\KeywordTok{library}\NormalTok{(fGarch)}
\end{Highlighting}
\end{Shaded}

\section{Vector Autoregressions}\label{vector-autoregressions}

A Vector autoregressive (VAR) model is useful when one is interested in
predicting multiple time series variables using a single model. At its
core, the VAR model is an extension of the univariate autoregressive
model we have dealt with in Chapters 14 and 15. Key Concept 16.1
summarizes the essentails of VAR.

Key Concept 16.1

Vector Autoregressions

The vector auoregression (VAR) model extends the idea of univariate
autoregression to \(k\) time series regression where the lagged values
of \emph{all} \(k\) series appear as regressors. Put differently, in a
VAR model we regress a \emph{vector} of time series variables on lagged
vectors of these variables. As for AR(\(p\)) models, the lag order is
denoted by \(p\) so the VAR(p) model of two variables \(X_t\) and
\(Y_t\) (\(k=2\)) is given by the equations

\begin{align*}
  Y_t =& \, \beta_{10} + \beta_{11} Y_{t-1} + \dots + \beta_{1p} Y_{t-p} + \gamma_{11} X_{t-1} + \dots + \gamma_{1p} X_{t-p} + u_{1t} \\
  X_t =& \, \beta_{20} + \beta_{21} Y_{t-1} + \dots + \beta_{2p} Y_{t-p} + \gamma_{21} X_{t-1} + \dots + \gamma_{2p} X_{t-p} + u_{2t}.
\end{align*}

The \(\alpha\)s and \(\beta\)s can be estimated using OLS in each
equation. The assumptions for VARs are the time series assumptions
presented in Key Concept 14.6 applied to teach equation.

It is straightforward to estimate VAR models in R. A valid way is to
simply use lm() for estimation of the individual equation. Furthermore,
the R package VARs provides standard tools for estimation, diagnostic
testing and prediction using this type of models.

When the assumptions referred to by Key Concept 16.1 hold, the OLS
estimators of the VAR coefficients are consistent and jointly normal in
large samples so that usual inferential methods such as confidence
intervals und \(t\)-statistics can be used.

Notice that, owing to the structure of VARs, we may jointly test
restrictions across multiple equations. For instance, it may of interest
to test whether the coefficients on all regressors on the lag \(p\) are
zero. This corresponds to testing the null hypothesis that the lag
length \(p-1\) is correct. Large sample joint normality of the
coefficient estimates is a convenient thing because this implies that we
may use an \(F\)-test for this testing problem. The explicit formula for
such a test statistic is rather complicated but fortunately such
computations are easily done using R functions we will work with for the
course of this chapter. Another way for determining optimal lag lengths
are information criteria like the \(BIC\) we have introduced for
univariate time series regressions. For a multiple equation model, we
choose the specification which has the smallest \(BIC(p)\) where

\begin{align*}
  BIC(p) =& \, \log\left[\text{det}(\widehat{\Sigma}_u)\right] + k(kp+1) \frac{\log(T)}{T}.
\end{align*}

with \(\widehat{\Sigma}_u\) the \(k \times k\) estimate of the
covariance matrix of the VAR errors and \(\text{det}(\cdot)\) denotes
the determinant.

As for univariate distributed lag models, one should think carefully
which variables to include in a VAR because adding unrelated variables
reduces the forecast accuracy by increasing the estimation error. This
is particulary important because the number of parameters to be
estimated grows proportionally to the number of variables used. In the
application below we shall see that economic thoery and empirical
evidence are helpful for the decision.

\paragraph{A VAR Model of the Growth Rate of GDP and the Term
Spread}\label{a-var-model-of-the-growth-rate-of-gdp-and-the-term-spread}
\addcontentsline{toc}{paragraph}{A VAR Model of the Growth Rate of GDP
and the Term Spread}

In the following we show how to estimate a VAR model of the GDP growth
rate, \(GDPGR\), and the term spread, \(TSpread\). As a consequence to
the discussion on nonstationarity of GDP growth in Chapter 14.7
(remember the possible break in the early 1980s detected by the \(QLR\)
test statistic ), we use data from 1981:Q1 to 2012:Q4. The two model
equations are

\begin{align*}
 GDPGR_t =& \, \beta_{10} + \beta_{11} GDPGR_{t-1} + \beta_{12} GDPGR_{t-2} + \gamma_{11} TSpread_{t-1} + \gamma_{12} TSpread_{t-2} + u_{1t} \\
 TSpread_t =& \, \beta_{20} + \beta_{21} GDPGR_{t-1} + \beta_{22} GDPGR_{t-2} + \gamma_{21} TSpread_{t-1} + \gamma_{22} TSpread_{t-2} + u_{2t}
\end{align*}

The data set us\_macro\_quarterly.xlsx is provided by the authors and
can be downloaded
\href{http://wps.aw.com/aw_stock_ie_3/178/45691/11696965.cw/index.html}{here}.
It provides data on quarterly data on US real (i.e.~inflation adjusted)
GDP from years 1947 to 2004. We begin by importing the data set and do
some formatting (we already worked with this data set in Chapter 14 so
you may skip these steps if you have already loaded the data in your
working environment).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load US macroeconomic data}
\NormalTok{USMacroSWQ <-}\StringTok{ }\KeywordTok{read_xlsx}\NormalTok{(}\StringTok{"Data/us_macro_quarterly.xlsx"}\NormalTok{,}
                         \DataTypeTok{sheet =} \DecValTok{1}\NormalTok{,}
                         \DataTypeTok{col_types =} \KeywordTok{c}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\StringTok{"numeric"}\NormalTok{, }\DecValTok{9}\NormalTok{))}
\NormalTok{                        )}

\CommentTok{# set column names}
\KeywordTok{colnames}\NormalTok{(USMacroSWQ) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Date"}\NormalTok{, }\StringTok{"GDPC96"}\NormalTok{, }\StringTok{"JAPAN_IP"}\NormalTok{, }\StringTok{"PCECTPI"}\NormalTok{, }\StringTok{"GS10"}\NormalTok{, }\StringTok{"GS1"}\NormalTok{, }\StringTok{"TB3MS"}\NormalTok{, }\StringTok{"UNRATE"}\NormalTok{, }\StringTok{"EXUSUK"}\NormalTok{, }\StringTok{"CPIAUCSL"}\NormalTok{)}

\CommentTok{# formate date column}
\NormalTok{USMacroSWQ}\OperatorTok{$}\NormalTok{Date <-}\StringTok{ }\KeywordTok{as.yearqtr}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{Date, }\DataTypeTok{format =} \StringTok{"%Y:0%q"}\NormalTok{)}

\CommentTok{# define GDP as ts object}
\NormalTok{GDP <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{GDPC96,}
          \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1957}\NormalTok{, }\DecValTok{1}\NormalTok{), }
          \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{, }\DecValTok{4}\NormalTok{), }
          \DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}

\CommentTok{# define GDP growth as ts object}
\NormalTok{GDPGrowth <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\DecValTok{400}\OperatorTok{*}\KeywordTok{log}\NormalTok{(GDP[}\OperatorTok{-}\DecValTok{1}\NormalTok{]}\OperatorTok{/}\NormalTok{GDP[}\OperatorTok{-}\KeywordTok{length}\NormalTok{(GDP)]),}
                \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1957}\NormalTok{, }\DecValTok{2}\NormalTok{), }
                \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{, }\DecValTok{4}\NormalTok{), }
                \DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}

\CommentTok{# 3 months Treasury bill interest rate as ts object}
\NormalTok{TB3MS <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{TB3MS,}
            \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1957}\NormalTok{, }\DecValTok{1}\NormalTok{), }
            \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{, }\DecValTok{4}\NormalTok{), }
            \DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}

\CommentTok{# 10 years Treasury bonds interest rate as ts object}
\NormalTok{TB10YS <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{GS10, }
              \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1957}\NormalTok{, }\DecValTok{1}\NormalTok{), }
              \DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2013}\NormalTok{, }\DecValTok{4}\NormalTok{), }
              \DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}

\CommentTok{# term spread series}
\NormalTok{TSpread <-}\StringTok{ }\NormalTok{TB10YS }\OperatorTok{-}\StringTok{ }\NormalTok{TB3MS}
\end{Highlighting}
\end{Shaded}

We estimate both equations separately by OLS and use \texttt{coeftest()}
in conjunction with \texttt{NeweyWest()} to obtain HAC standard errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate both equations using 'dynlm()'}
\NormalTok{VAR_EQ1 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(GDPGrowth }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{), }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1981}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{VAR_EQ2 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(TSpread }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{), }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1981}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}

\CommentTok{# rename regressors for better readability}
\KeywordTok{names}\NormalTok{(VAR_EQ1}\OperatorTok{$}\NormalTok{coefficients) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Intercept"}\NormalTok{,}\StringTok{"Growth_t-1"}\NormalTok{,}\StringTok{"Growth_t-2"}\NormalTok{,}\StringTok{"TSpread_t-1"}\NormalTok{,}\StringTok{"TSpread_t-2"}\NormalTok{)}
\KeywordTok{names}\NormalTok{(VAR_EQ2}\OperatorTok{$}\NormalTok{coefficients) <-}\StringTok{ }\KeywordTok{names}\NormalTok{(VAR_EQ1}\OperatorTok{$}\NormalTok{coefficients)}

\CommentTok{# HAC coefficient summaries}
\KeywordTok{coeftest}\NormalTok{(VAR_EQ1, }\DataTypeTok{vcov. =} \KeywordTok{NeweyWest}\NormalTok{(VAR_EQ1, }\DataTypeTok{prewhite =}\NormalTok{ F, }\DataTypeTok{adjust =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(>|t|)   
## Intercept    0.51634    0.46481  1.1109 0.268790   
## Growth_t-1   0.28955    0.12519  2.3130 0.022386 * 
## Growth_t-2   0.21639    0.09169  2.3600 0.019846 * 
## TSpread_t-1 -0.90255    0.38939 -2.3178 0.022111 * 
## TSpread_t-2  1.32983    0.46117  2.8836 0.004643 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coeftest}\NormalTok{(VAR_EQ2, }\DataTypeTok{vcov. =} \KeywordTok{NeweyWest}\NormalTok{(VAR_EQ2, }\DataTypeTok{prewhite =}\NormalTok{ F, }\DataTypeTok{adjust =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##               Estimate Std. Error t value  Pr(>|t|)    
## Intercept    0.4557740  0.1292772  3.5256 0.0005945 ***
## Growth_t-1   0.0099785  0.0264497  0.3773 0.7066285    
## Growth_t-2  -0.0572451  0.0303573 -1.8857 0.0616927 .  
## TSpread_t-1  1.0582279  0.0924398 11.4478 < 2.2e-16 ***
## TSpread_t-2 -0.2191902  0.0840014 -2.6094 0.0101965 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We end up with the following results:

\begin{align*}
 GDPGR_t =& \, \underset{(0.46)}{0.52} + \underset{(0.13)}{0.29} GDPGR_{t-1} + \underset{(0.09)}{0.22} GDPGR_{t-2} -\underset{(0.39)}{0.90} TSpread_{t-1} + \underset{(0.46)}{1.33} TSpread_{t-2} \\
 TSpread_t =& \, \underset{(0.13)}{0.46} + \underset{(0.03)}{0.01} GDPGR_{t-1} -\underset{(0.03)}{0.06} GDPGR_{t-2} + \underset{(0.09)}{1.06} TSpread_{t-1} -\underset{(0.08)}{0.22} TSpread_{t-2} 
\end{align*}

The function VAR() can be used to obtain the same coefficient estimates
as presented above since it applies OLS per equation, too.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up data for estimation using 'VAR()'}
\NormalTok{VAR_data <-}\StringTok{ }\KeywordTok{window}\NormalTok{(}\KeywordTok{ts.union}\NormalTok{(GDPGrowth, TSpread), }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1980}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{))}

\CommentTok{# estimate model coefficients using 'VAR()'}
\NormalTok{VAR_est <-}\StringTok{ }\KeywordTok{VAR}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ VAR_data, }\DataTypeTok{p =} \DecValTok{2}\NormalTok{)}
\NormalTok{VAR_est}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## VAR Estimation Results:
## ======================= 
## 
## Estimated coefficients for equation GDPGrowth: 
## ============================================== 
## Call:
## GDPGrowth = GDPGrowth.l1 + TSpread.l1 + GDPGrowth.l2 + TSpread.l2 + const 
## 
## GDPGrowth.l1   TSpread.l1 GDPGrowth.l2   TSpread.l2        const 
##    0.2895533   -0.9025493    0.2163919    1.3298305    0.5163440 
## 
## 
## Estimated coefficients for equation TSpread: 
## ============================================ 
## Call:
## TSpread = GDPGrowth.l1 + TSpread.l1 + GDPGrowth.l2 + TSpread.l2 + const 
## 
## GDPGrowth.l1   TSpread.l1 GDPGrowth.l2   TSpread.l2        const 
##  0.009978489  1.058227945 -0.057245123 -0.219190243  0.455773969
\end{verbatim}

Notice that VAR() returns a list of \texttt{lm} objects which can be
passed to the usual function, for example summary() and so it is
straightforward to obtain model statistics for the individual equations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# obtain the adj. R^2 from the output of 'VAR()'}
\KeywordTok{summary}\NormalTok{(VAR_est}\OperatorTok{$}\NormalTok{varresult}\OperatorTok{$}\NormalTok{GDPGrowth)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2887223
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(VAR_est}\OperatorTok{$}\NormalTok{varresult}\OperatorTok{$}\NormalTok{TSpread)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8254311
\end{verbatim}

We may use the individual model objects to conduct granger causality
tests.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# granger causality test:}

\CommentTok{# test if term spread has no power in explaining GDP growth}
\KeywordTok{linearHypothesis}\NormalTok{(VAR_EQ1, }
                 \DataTypeTok{hypothesis.matrix =} \KeywordTok{c}\NormalTok{(}\StringTok{"TSpread_t-1"}\NormalTok{, }\StringTok{"TSpread_t-2"}\NormalTok{),}
                 \DataTypeTok{vcov. =} \KeywordTok{NeweyWest}\NormalTok{(VAR_EQ1, }\DataTypeTok{prewhite =}\NormalTok{ F, }\DataTypeTok{adjust =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## TSpread_t - 0
## TSpread_t - 2 = 0
## 
## Model 1: restricted model
## Model 2: GDPGrowth ~ L(GDPGrowth, 1:2) + L(TSpread, 1:2)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(>F)  
## 1    125                    
## 2    123  2 4.5584 0.01231 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test if GDP growth has no power in explaining term spread}
\KeywordTok{linearHypothesis}\NormalTok{(VAR_EQ2, }
                 \DataTypeTok{hypothesis.matrix =} \KeywordTok{c}\NormalTok{(}\StringTok{"Growth_t-1"}\NormalTok{, }\StringTok{"Growth_t-2"}\NormalTok{),}
                 \DataTypeTok{vcov. =} \KeywordTok{NeweyWest}\NormalTok{(VAR_EQ2, }\DataTypeTok{prewhite =}\NormalTok{ F, }\DataTypeTok{adjust =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## Growth_t - 0
## Growth_t - 2 = 0
## 
## Model 1: restricted model
## Model 2: TSpread ~ L(GDPGrowth, 1:2) + L(TSpread, 1:2)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(>F)  
## 1    125                    
## 2    123  2 3.4806 0.03386 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We conclude that both granger causality tests reject at the level of
\(5\%\).

\paragraph{Iterated Multivariate Forecasts using an iterated
VAR}\label{iterated-multivariate-forecasts-using-an-iterated-var}
\addcontentsline{toc}{paragraph}{Iterated Multivariate Forecasts using
an iterated VAR}

The idea of an iterated forecast for period \(T+2\) based on
observations up to period \(T\) is to use the one-period-ahead forecast
as an intermediate step, that is the forecast for period \(T+1\) is used
as an observation when predicting the level of a series for period
\(T+2\). This can be generalized to a \(h\) period ahead forecast where
all intervening periods between \(T\) and \(T+h\) must be forecasted as
they are used as observations in the process (see Chapter 16.2 of the
book for a more detailed argument with this topic). Iterated Multiperiod
forecasts are summarized in Key Concept 16.2.

Key Concept 16.2

Interated Multiperiod Forecasts

The steps fot an \textbf{interated multiperiod AR forecast} are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate the AR(\(p\)) model using OLS and compute the
  one-period-ahead forecast.
\item
  Use the one-period-ahead forecast to obtain the two-period-ahead
  forecast.
\item
  Continue by iterating to obtain forecasts further into the future.
\end{enumerate}

An \textbf{interated multiperiod VAR forecast} is done as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate the VAR(\(p\)) model unsing OLS per equation and compute the
  one-period-ahead forecast for \emph{all} variables in the VAR.
\item
  Use the one-period-ahead forecasts to obtain the two-period-ahead
  forecasts.
\item
  Continue by iterating to obtain forecasts of all variables in the VAR
  further into the future.
\end{enumerate}

Since in a VAR the variables are modeled using lags of the repsective
other variables, we need to compute forecasts for \emph{all} variables.
This can be cumbersome when the VAR is large but furtnunately there are
R functions that facilitate this. For example, the function predict()
can be used to obtain iterated multivariate forecasts for VAR models
estimated by the function VAR().

The following code chunk shows how to compute forecasts for GDP growth
and the term spread up to period 2015:Q1, that is \(h=10\), using the
model object \texttt{VAR\_est}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute iterated forecasts for GDP growth and term spread for the next 10 periods}
\NormalTok{forecasts <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(VAR_est)}
\NormalTok{forecasts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $GDPGrowth
##           fcst     lower    upper       CI
##  [1,] 1.738653 -3.006124 6.483430 4.744777
##  [2,] 1.692193 -3.312731 6.697118 5.004925
##  [3,] 1.911852 -3.282880 7.106583 5.194731
##  [4,] 2.137070 -3.164247 7.438386 5.301317
##  [5,] 2.329667 -3.041435 7.700769 5.371102
##  [6,] 2.496815 -2.931819 7.925449 5.428634
##  [7,] 2.631849 -2.846390 8.110088 5.478239
##  [8,] 2.734819 -2.785426 8.255064 5.520245
##  [9,] 2.808291 -2.745597 8.362180 5.553889
## [10,] 2.856169 -2.722905 8.435243 5.579074
## 
## $TSpread
##           fcst        lower    upper        CI
##  [1,] 1.676746  0.708471226 2.645021 0.9682751
##  [2,] 1.884098  0.471880228 3.296316 1.4122179
##  [3,] 1.999409  0.336348101 3.662470 1.6630609
##  [4,] 2.080836  0.242407507 3.919265 1.8384285
##  [5,] 2.131402  0.175797245 4.087008 1.9556052
##  [6,] 2.156094  0.125220562 4.186968 2.0308738
##  [7,] 2.161783  0.085037834 4.238528 2.0767452
##  [8,] 2.154170  0.051061544 4.257278 2.1031082
##  [9,] 2.138164  0.020749780 4.255578 2.1174139
## [10,] 2.117733 -0.007139213 4.242605 2.1248722
\end{verbatim}

From this we can tell that the two-quarter-ahead forecast of GDP growth
in 2013:Q2 using data through 2012:Q4 is \(1.69\). For the same peiod,
the iterated VAR forecast for the term spread is \(1.88\).

Notice that the matrices returned by \texttt{predict(VAR\_est)} also
include \(95\%\) prediction intervals (however, the function does not
adjust for autocorrelation or heteroskedasticity of the errors!).

We may also plot the iterated forecasts for both variables by simply
calling \texttt{plot()} on the output of \texttt{predict(VAR\_est)}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# visualize the iterated forecasts}
\KeywordTok{plot}\NormalTok{(forecasts)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-429-1} \end{center}

\paragraph{Direct Multiperiod
Forecasts}\label{direct-multiperiod-forecasts}
\addcontentsline{toc}{paragraph}{Direct Multiperiod Forecasts}

A direct multiperiod forecast takes the model as a starting point where
the predictor variables are lagged appropriately such that available
observation can be used \emph{directly} to do the forecast. The idea of
direct multiperiod forecasting is summarized in Key Concept 16.3.

For example, to obtain two-quarter-ahead forecasts of GDP growth and
term spread we first estimate the models

\begin{align*}
 GDPGR_t =& \, \beta_{10} + \beta_{11} GDPGR_{t-2} + \beta_{12} GDPGR_{t-3} + \gamma_{11} TSpread_{t-2} + \gamma_{12} TSpread_{t-3} + u_{1t} \\
 TSpread_t =& \, \beta_{20} + \beta_{21} GDPGR_{t-2} + \beta_{22} GDPGR_{t-3} + \gamma_{21} TSpread_{t-2} + \gamma_{22} TSpread_{t-3} + u_{2t}
\end{align*}

and then substitute the values of \(GDPGR_{2012:Q4}\),
\(GDPGR_{2012:Q3}\), \(TSpread_{2012:Q4}\) and \(TSpread_{2012:Q3}\)
into both equations. This is easily done manually.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate models for direct two-quarter-ahead forecasts}
\NormalTok{VAR_EQ1_direct <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(GDPGrowth }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{), }
                        \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1981}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}

\NormalTok{VAR_EQ2_direct <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(TSpread }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(GDPGrowth, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(TSpread, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{), }
                        \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1981}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{, }\DecValTok{4}\NormalTok{))}

\CommentTok{# compute direct two-quarter-ahead forecasts}
\KeywordTok{coef}\NormalTok{(VAR_EQ1_direct) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }
                           \KeywordTok{window}\NormalTok{(GDPGrowth, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)), }
                           \KeywordTok{window}\NormalTok{(TSpread, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]
## [1,] 2.439497
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(VAR_EQ2_direct) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }
                           \KeywordTok{window}\NormalTok{(GDPGrowth, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)), }
                           \KeywordTok{window}\NormalTok{(TSpread, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         [,1]
## [1,] 1.66578
\end{verbatim}

Key Concept 16.3

Direct Multipreiod Forecasts

A \textbf{direct multiperiod forecast} that forecasts \(h\) periods into
the future using a model of \(Y_t\) and an additional predictor \(X_t\)
with \(p\) lags is done by first estimating

\begin{align*}
  Y_t =& \, \delta_0 + \delta_1 Y_{t-h} + \dots + \delta_{p} Y_{t-p-h+1} + \delta_{p+1} X_{t-h} \\
  &+ \dots + \delta_{2p} Y_{t-p-h+1} + u_t  
\end{align*}

which is then used to compute the forecast of \(Y_{T+h}\) based on
observations throuth period \(T\).

Applied economists often use the iterated method since this forecasts
are more reliable in terms of \(MSFE\) provided the one-period-ahead
model is correctly specified. If this is not the case, for example
because one equation in a VAR is believed to be misspecified, it can be
beneficial to use direct forecasts since the iterated method will then
be biased and thus have a higher \(MSFE\) then the direct method. See
Chapter 16.2 for a more detailed discussion on advantages and
disadvantages of both methods.

\section{Orders of Integration and the DF-GLS Unit Root
Test}\label{orders-of-integration-and-the-df-gls-unit-root-test}

Some economic variables have smoother trends than variables that can be
described by random walk models. A way to model these time series is
\[\Delta Y_t = \beta_0 + \Delta Y_{t-1} + u_t\], where \(u_t\) is a
serially uncorrelated error term. This model states that the first
differences of a series follow a random walk. Consequently, the series
of second differences of \(Y_t\) is stationary. Key Concept 16.4
summarizes the notation.

Key Concept 16.4

Orders of Integration, Differencing and Stationarity

\begin{itemize}
\item
  When a time series \(Y_t\) has a unit autoregressive root, we say that
  \(Y_t\) is integrated of order one. This is often denoted by
  \(Y_t \sim I(1)\). We simply say that \(Y_t\) is \(I(1)\). If \(Y_t\)
  is \(I(1)\), its first difference \(\Delta Y_t\) is stationary.
\item
  We say that \(Y_t\) is \(I(2)\) when \(Y_t\) needs to be differenced
  to time to obtain a stationary series. Using the notation introduced
  here we say that if \(Y_t\) is \(I(2)\), it difference first
  \(\Delta Y_t\) is \(I(1)\) and its second difference \(\Delta^2 Y_t\)
  is stationary. We say that \(Y_t\) is \(I(d)\) when \(Y_t\) must be
  differenced \(d\) times to obtain a stationary series.
\item
  When \(Y_t\) is stationary, we say it is integrated of order \(0\) so
  \(Y_t\) is \(I(0)\).
\end{itemize}

It is fairly easy to obtain differences of time series in R. For
example, the function diff() returns suitably lagged and iterated
differences of numeric vectors, matrices and time series objects of the
class ts.

Following the book, we take the price level of the united states
measured by the personal consumption expenditures price index as an
example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define ts object of the United States PCE Price Index}
\NormalTok{PCECTPI <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{log}\NormalTok{(USMacroSWQ}\OperatorTok{$}\NormalTok{PCECTPI), }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1957}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{), }\DataTypeTok{freq =} \DecValTok{4}\NormalTok{)}

\CommentTok{# plot logarithm of the PCE Price Index}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{log}\NormalTok{(PCECTPI),}
     \DataTypeTok{main =} \StringTok{"Log of United States PCE Price Index"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Logarithm"}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-431-1} \end{center}

We see that the logarithm of the price level has a smoothly varying
trend. This is typical for an \(I(2)\). If the price level is indeed
\(I(2)\), the first differences of this series should be \(I(1)\). Since
we are considering the logarithm of the price level, we obtain growth
rates by taking first differences, so the differenced price level series
is the series of quarterly inflation rates. This is quickly done in R
using the function Delt() from the package quantmod As explained in
Chapter 14.2, multiplying the quarterly inflation rates by \(400\)
yields the quarterly rate of inflation, measuared in percentage points
at an annual rate.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot United States PCE price inflation}
\KeywordTok{plot}\NormalTok{(}\DecValTok{400}\OperatorTok{*}\KeywordTok{Delt}\NormalTok{(PCECTPI),}
     \DataTypeTok{main =} \StringTok{"United States PCE Price Index"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent per annum"}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-432-1} \end{center}

Obviously, the rate of inflation behaves much more erratic than the
smooth graph of the logarithm of the PCE price index.

\paragraph{The DF-GLS Test for a Unit
Root}\label{the-df-gls-test-for-a-unit-root}
\addcontentsline{toc}{paragraph}{The DF-GLS Test for a Unit Root}

The DF-GLS test for a unit root has been developed by Elliot, Rothenberg
and Stock (1996) and has higher power than the ADF test when the
autoregressive root is large but less than one. That is, the DF-GLS has
a higher probability of rejecting the false null hypothesis of a
stochastic trend when the sample data stems from time series that is
close to being integrated.

The idea of the DF-GLS test is to test for an autoregressive unit root
in the detrended series whereby GLS estimates of the deterministic
components are used to obtain the detrended version of the original
series. See Chapter 16.3 of the book for a more detailed explanation of
the approach.

A function that performs the DF-GLS test is implemented in the package
urca (this package is a dependency of the package vars so it should be
already loaded if vars is attached). The function that computes the test
statistic is ur.ers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# DF-GLS test for unit root in GDP}
\KeywordTok{summary}\NormalTok{(}
 \KeywordTok{ur.ers}\NormalTok{(}\KeywordTok{log}\NormalTok{(}\KeywordTok{window}\NormalTok{(GDP, }\DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{end =} \KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{))),}
        \DataTypeTok{model =} \StringTok{"trend"}\NormalTok{, }
        \DataTypeTok{lag.max =} \DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ############################################### 
## # Elliot, Rothenberg and Stock Unit Root Test # 
## ############################################### 
## 
## Test of type DF-GLS 
## detrending of series with intercept and trend 
## 
## 
## Call:
## lm(formula = dfgls.form, data = data.dfgls)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.025739 -0.004054  0.000017  0.004619  0.033620 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## yd.lag       -0.01213    0.01012  -1.199  0.23207    
## yd.diff.lag1  0.28583    0.07002   4.082 6.47e-05 ***
## yd.diff.lag2  0.19320    0.07058   2.737  0.00676 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.007807 on 198 degrees of freedom
## Multiple R-squared:  0.1504, Adjusted R-squared:  0.1376 
## F-statistic: 11.69 on 3 and 198 DF,  p-value: 4.392e-07
## 
## 
## Value of test-statistic is: -1.1987 
## 
## Critical values of DF-GLS are:
##                  1pct  5pct 10pct
## critical values -3.48 -2.89 -2.57
\end{verbatim}

Inspecting the summary of the test we find that the test statistic is
about -1.2 which is larger than the \(10\%\) critical value of \(-2.57\)
(this is the appropriate critical value for the ADF test when an
intercept and a time trend are included in the Dickey-Fuller regression
and the test is left-sided) so we cannot reject the null hypothesis that
inflation is nonstationary, using the DF-GLS test.

\section{Cointegration}\label{cointegration}

Key Concept 16.5

Cointegration

When \(X_t\) and \(Y_t\) are \(I(1)\) and if there is a \(\theta\) such
that \(Y_t - \theta X_t\) is \(I(0)\), we say that \(X_t\) and \(Y_t\)
are cointegrated. Put differently, cointegration of \(X_t\) and \(Y_t\)
means that \(X_t\) and \(Y_t\) have the same or a common stochastic
trend and this trend can be eliminated by some difference of the series
such that the result is stationary.

R functions for cointegration testing are implemented in the package
urca.

As an example, reconsider the the relation between short- and long-term
interest rates by the example of U.S. 3-month treasury bills, U.S.
10-years treasury bonds and the spread in their interest rates which
have been introduced in Chapter 14.4. The next code chunks shows how to
reproduce figure 16.2 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# reproduce figure 16.2 of the book}

\CommentTok{# plot both interest series}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{merge}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(TB3MS),}\KeywordTok{as.zoo}\NormalTok{(TB10YS)), }
     \DataTypeTok{plot.type =} \StringTok{"single"}\NormalTok{, }
     \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{),}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent per annum"}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{, }\DecValTok{17}\NormalTok{),}
     \DataTypeTok{main =} \StringTok{"Interest Rates"}
\NormalTok{)}

\CommentTok{# add the term spread}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{as.zoo}\NormalTok{(TSpread), }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent per annum"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Term Spread"}
\NormalTok{)}

\CommentTok{# shade term spread}
\KeywordTok{polygon}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{time}\NormalTok{(TB3MS), }\KeywordTok{rev}\NormalTok{(}\KeywordTok{time}\NormalTok{(TB3MS))), }
        \KeywordTok{c}\NormalTok{(TB10YS, }\KeywordTok{rev}\NormalTok{(TB3MS)),}
        \DataTypeTok{col =} \KeywordTok{alpha}\NormalTok{(}\StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{),}
        \DataTypeTok{border =} \OtherTok{NA}
\NormalTok{        )}

\CommentTok{# add horizontal line add 0}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"TB3MS"}\NormalTok{,}\StringTok{"TB10YS"}\NormalTok{,}\StringTok{"Term Spread"}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"steelblue"}\NormalTok{),}
       \DataTypeTok{lwd =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-434-1} \end{center}

The plot suggests that long-term and short-term interest rates are
cointegrated: notice that both interest series seem to have the same
long-run behaviour. In fact, they share a common stochastic trend. The
term spread which is obtained by taking the difference between long-term
and short-term interest rates seems to be stationary. In fact, the
expectations theory of the term structure suggests the cointegrating
coefficient \(\theta\) to be 1. This is consistent with the visual
results.

\paragraph{Testing for Cointegration}\label{testing-for-cointegration}
\addcontentsline{toc}{paragraph}{Testing for Cointegration}

Folloing Key Concept 16.5, it seems natural to construct a test for
cointegration of two series in the following manner: if two series
\(X_t\) and \(Y_t\) are cointegrated, the series obtained by taking the
difference \(Y_t - \theta X_t\) must be stationary. If the are not
cointegrated, \(Y_t - \theta X_t\) is nonstationary. This is an
assumption that can be tested using a unit root test. We generally have
to distinguish between two cases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{\(\theta\) is known.}

  Knowledge of \(\theta\) enables us to compute differences
  \(z_t = Y_t - \theta X_t\) so that Dickey-Fuller and DF-GLS unit root
  tests can be applied to \(z_t\). For these tests, the critical values
  are the critical values of the ADF test.
\item
  \textbf{\(\theta\) is unknown.}

  If \(\theta\) is unknown it must be estimated before the unit root
  test can be applied. This is done by estimating the regression
  \[Y_t = \alpha + \theta X_t + z_t\] using OLS (this is refered to as
  the first stage regression). Then, a Dickey-Fuller test is used for
  testing the hypothesis that \(z_t\) is a nonstationary series. This is
  known as the Engle-Granger Augmented Dickey-Fuller test for
  cointegration (or \textbf{EG-ADF test}) after Engle and Granger
  (1987). The critical values for this test are special as the
  associated null distribution is nonnormal and depends on the number of
  \(I(1)\) variables used as regressors in the first stage regression.
  You may look them up in Table 16.2 of the book. When there are only
  two cointegrated variables (and thus a single \(I(1)\) variable used
  in the first stage OLS regression) the critical values for the levels
  \(10\%\), \(5\%\) and \(1\%\) are \(-3.12\), \(-3.41\) and \(-3.96\).
\end{enumerate}

\subsubsection*{Application to Interest
Rates}\label{application-to-interest-rates}
\addcontentsline{toc}{subsubsection}{Application to Interest Rates}

As has been mentioned above, the theory of the term structure suggests
that long-term and short-term interest rates are cointegrated with a
cointegration coefficient of \(\theta = 1\). In the previous section we
have seen that there is visual evidence for this conjecture since the
spread of 10-year and 3-month interest rates looks stationary.

We now continue by using formal tests (the ADF and the DF-GLS test) to
see whether the individual interest rate series are integrated and if
their difference is stationary (for now, we assume that \(\theta = 1\)
is known). Both is conveniently done by using the functions ur.df() for
computation of the ADF test and ur.ers for the DF-GLS test. Following
the book we use data from 1962:Q1 to 2012:Q4 use models that include a
drift component. We set the maximum lag order to five and use the
\(AIC\) for selection of the optimal lag length. Remember that the
Dickey-Fuller critical values from table \ref{tab:DFcrits} are
appropriate for both tests.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test for nonstationarity of 3-month treasury bills using ADF test}
\KeywordTok{ur.df}\NormalTok{(}\KeywordTok{window}\NormalTok{(TB3MS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)), }
      \DataTypeTok{lags =} \DecValTok{6}\NormalTok{, }
      \DataTypeTok{selectlags =} \StringTok{"AIC"}\NormalTok{, }
      \DataTypeTok{type =} \StringTok{"drift"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ############################################################### 
## # Augmented Dickey-Fuller Test Unit Root / Cointegration Test # 
## ############################################################### 
## 
## The value of the test statistic is: -2.1004 2.2385
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test for nonstationarity of 10-years treasury bonds using ADF test}
\KeywordTok{ur.df}\NormalTok{(}\KeywordTok{window}\NormalTok{(TB10YS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)), }
      \DataTypeTok{lags =} \DecValTok{6}\NormalTok{, }
      \DataTypeTok{selectlags =} \StringTok{"AIC"}\NormalTok{, }
      \DataTypeTok{type =} \StringTok{"drift"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ############################################################### 
## # Augmented Dickey-Fuller Test Unit Root / Cointegration Test # 
## ############################################################### 
## 
## The value of the test statistic is: -1.0079 0.5501
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test for nonstationarity of 3-month treasury bills using DF-GLS test}
\KeywordTok{ur.ers}\NormalTok{(}\KeywordTok{window}\NormalTok{(TB3MS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)),}
       \DataTypeTok{model =} \StringTok{"constant"}\NormalTok{, }
       \DataTypeTok{lag.max =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ############################################################### 
## # Elliot, Rothenberg and Stock Unit Root / Cointegration Test # 
## ############################################################### 
## 
## The value of the test statistic is: -1.8042
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test for nonstationarity of 10-years treasury bonds using DF-GLS test}
\KeywordTok{ur.ers}\NormalTok{(}\KeywordTok{window}\NormalTok{(TB10YS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)),}
       \DataTypeTok{model =} \StringTok{"constant"}\NormalTok{, }
       \DataTypeTok{lag.max =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ############################################################### 
## # Elliot, Rothenberg and Stock Unit Root / Cointegration Test # 
## ############################################################### 
## 
## The value of the test statistic is: -0.942
\end{verbatim}

The corresponding \(10\%\) critical value for both tests is \(-2.57\) so
we cannot reject the null hypotheses of nonstationary for either the
series, even at the \(10\%\) level of significance.\footnote{Note:
  ur.df() reports two test statistics when there is a drift in the ADF
  regression. The first of which (the one we are interested in here) is
  the \(t\)-statistic for the test that the coefficient on the first lag
  of the series is 0. The second one is the \(t\)-statistic for the
  hypothesis test that the drift term equals \(0\).} We conclude that it
is plausible to model both interest rate series as \(I(1)\).

Next, we may apply the ADF and the DF-GLS test to test for
nonstationarity of the term spread series, which means we test for
non-cointegration of long- and short-term interest rates with
cointegration coefficient \(\theta = 1\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test if term spread is stationairy (cointegration of interest rates) using ADF}
\KeywordTok{ur.df}\NormalTok{(}\KeywordTok{window}\NormalTok{(TB10YS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\OperatorTok{-}\StringTok{ }\KeywordTok{window}\NormalTok{(TB3MS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)), }
      \DataTypeTok{lags =} \DecValTok{6}\NormalTok{, }
      \DataTypeTok{selectlags =} \StringTok{"AIC"}\NormalTok{, }
      \DataTypeTok{type =} \StringTok{"drift"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ############################################################### 
## # Augmented Dickey-Fuller Test Unit Root / Cointegration Test # 
## ############################################################### 
## 
## The value of the test statistic is: -3.9308 7.7362
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test if term spread is stationairy (cointegration of interest rates) using DF-GLS test}
\KeywordTok{ur.ers}\NormalTok{(}\KeywordTok{window}\NormalTok{(TB10YS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\OperatorTok{-}\StringTok{ }\KeywordTok{window}\NormalTok{(TB3MS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)),}
       \DataTypeTok{model =} \StringTok{"constant"}\NormalTok{, }
       \DataTypeTok{lag.max =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ############################################################### 
## # Elliot, Rothenberg and Stock Unit Root / Cointegration Test # 
## ############################################################### 
## 
## The value of the test statistic is: -3.8576
\end{verbatim}

The following table summarizes the results.

\begin{longtable}[]{@{}lcc@{}}
\toprule
Series & ADF Test Statistic & DF-GLS Test Statistic\tabularnewline
\midrule
\endhead
TB3MS & \(-2.10\) & \(-1.80\)\tabularnewline
TB10YS & \(-1.01\) & \(-0.94\)\tabularnewline
TB10YS - TB3MS & \(-3.93\) & \(-3.93\)\tabularnewline
\bottomrule
\end{longtable}

We find that both test reject the hypothesis on nonstationarity of the
term spread series at the \(1\%\) level of significance which is strong
evidence in favour of the hypothesis that the term spread is stationary,
implying cointegration of long- and short-term interest rates.

Since theory suggests that \(\theta=1\) there is no need to estimate
\(\theta\) so it is not necessary to use the EG-ADF test which assumes
\(\theta\) to be unknown. However, since it is instructive to do so we
follow the book and compute the test statistic. The first stage OLS
regression is \[TB10YS_t = \beta_0 + \beta_1 TB3MS_t + z_t.\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate first stage regression of EG-ADF test}
\NormalTok{FS_EGADF <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(}\KeywordTok{window}\NormalTok{(TB10YS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)) }\OperatorTok{~}\StringTok{ }\KeywordTok{window}\NormalTok{(TB3MS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{)))}
\NormalTok{FS_EGADF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Time series regression with "ts" data:
## Start = 1962(1), End = 2012(4)
## 
## Call:
## dynlm(formula = window(TB10YS, c(1962, 1), c(2012, 4)) ~ window(TB3MS, 
##     c(1962, 1), c(2012, 4)))
## 
## Coefficients:
##                           (Intercept)  
##                                2.4642  
## window(TB3MS, c(1962, 1), c(2012, 4))  
##                                0.8147
\end{verbatim}

So we have

\begin{align*}
  \widehat{TB10YS}_t = 2.46 + 0.81 TB3MS_t
\end{align*}

where \(\widehat{\theta} = 0.81\). Next, we take the residual series
\(\{\widehat{z_t}\}\) and compute the ADF test statistic.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute residuals}
\NormalTok{z_hat <-}\StringTok{ }\KeywordTok{resid}\NormalTok{(FS_EGADF)}

\CommentTok{# compute ADF test statistic}
\KeywordTok{ur.df}\NormalTok{(z_hat, }\DataTypeTok{lags=}\DecValTok{6}\NormalTok{, }\DataTypeTok{type =} \StringTok{"drift"}\NormalTok{, }\DataTypeTok{selectlags =} \StringTok{"AIC"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ############################################################### 
## # Augmented Dickey-Fuller Test Unit Root / Cointegration Test # 
## ############################################################### 
## 
## The value of the test statistic is: -3.1882 5.0859
\end{verbatim}

The test statistic is \(-3.19\) which is smaller than the \(10\%\)
critical value but larger than the \(5\%\) critical value (see table
16.2 of the book). Thus, the null of no cointegration can be rejected at
the \(10\%\) level but not at the \(5\%\) level. As pointed out by Stock
\& Watson, this indicates lower power of the EG-ADF test due to the
estimation of \(\theta\): when \(\theta=1\) is the correct value, we
expect the power of the ADF test for a unit root in the residuals series
\(\widehat{z} = TB10YS_t - \widehat{\beta}_0 - TB3MS\) to be higher than
when some estimate \(\widehat{\theta}\) is used.

\paragraph{\texorpdfstring{A Vector Error correction model for
\(TB10YS_t\) and
\(TB3MS\)}{A Vector Error correction model for TB10YS\_t and TB3MS}}\label{a-vector-error-correction-model-for-tb10ys_t-and-tb3ms}
\addcontentsline{toc}{paragraph}{A Vector Error correction model for
\(TB10YS_t\) and \(TB3MS\)}

If two \(I(1)\) time series \(X_t\) and \(Y_t\) are cointegrated, their
differences are stationary and can modeled in a VAR which is augmented
by the regressor \(Y_{t-1} - \theta X_{t-1}\). This is called a
\textbf{vector error correction model} (VECM) and
\(Y_{t} - \theta X_{t}\) is called the \textbf{error correction term}.
Lagged values of the error correction term are useful for predicting
\(\Delta X_t\) and/or \(\Delta Y_t\).

A VECM can be used to model the two interest rates considered in the
previous sections. Following the book we specify the VECM to include 2
lags of both series as regressors and choose \(\theta = 1\), as theory
suggests (see above.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TB10YS <-}\StringTok{ }\KeywordTok{window}\NormalTok{(TB10YS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\NormalTok{TB3MS <-}\StringTok{ }\KeywordTok{window}\NormalTok{(TB3MS, }\KeywordTok{c}\NormalTok{(}\DecValTok{1962}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{4}\NormalTok{))}

\CommentTok{# set up error correction term}
\NormalTok{VECM_ECT <-}\StringTok{ }\NormalTok{TB10YS }\OperatorTok{-}\StringTok{ }\NormalTok{TB3MS}

\CommentTok{# estimate both equations of the VECM using 'dynlm()'}
\NormalTok{VECM_EQ1 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(}\KeywordTok{d}\NormalTok{(TB10YS) }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{d}\NormalTok{(TB3MS), }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{d}\NormalTok{(TB10YS), }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(VECM_ECT))}
\NormalTok{VECM_EQ2 <-}\StringTok{ }\KeywordTok{dynlm}\NormalTok{(}\KeywordTok{d}\NormalTok{(TB3MS) }\OperatorTok{~}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{d}\NormalTok{(TB3MS), }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(}\KeywordTok{d}\NormalTok{(TB10YS), }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{L}\NormalTok{(VECM_ECT))}

\CommentTok{# rename regressors for better readability}
\KeywordTok{names}\NormalTok{(VECM_EQ1}\OperatorTok{$}\NormalTok{coefficients) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Intercept"}\NormalTok{,}\StringTok{"D_TB3MS_l1"}\NormalTok{,}\StringTok{"D_TB3MS_l2"}\NormalTok{,}
                                  \StringTok{"D_TB10YS_l1"}\NormalTok{,}\StringTok{"D_TB10YS_l2"}\NormalTok{,}\StringTok{"ect_l1"}\NormalTok{)}
\KeywordTok{names}\NormalTok{(VECM_EQ2}\OperatorTok{$}\NormalTok{coefficients) <-}\StringTok{ }\KeywordTok{names}\NormalTok{(VECM_EQ1}\OperatorTok{$}\NormalTok{coefficients)}

\CommentTok{# HAC coefficient summaries}
\KeywordTok{coeftest}\NormalTok{(VECM_EQ1, }\DataTypeTok{vcov. =} \KeywordTok{NeweyWest}\NormalTok{(VECM_EQ1, }\DataTypeTok{prewhite =}\NormalTok{ F, }\DataTypeTok{adjust =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##               Estimate Std. Error t value Pr(>|t|)   
## Intercept    0.1227089  0.0551419  2.2253 0.027205 * 
## D_TB3MS_l1  -0.0016601  0.0727060 -0.0228 0.981807   
## D_TB3MS_l2  -0.0680845  0.0435059 -1.5649 0.119216   
## D_TB10YS_l1  0.2264878  0.0957071  2.3665 0.018939 * 
## D_TB10YS_l2 -0.0734486  0.0703476 -1.0441 0.297740   
## ect_l1      -0.0878871  0.0285644 -3.0768 0.002393 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coeftest}\NormalTok{(VECM_EQ2, }\DataTypeTok{vcov. =} \KeywordTok{NeweyWest}\NormalTok{(VECM_EQ2, }\DataTypeTok{prewhite =}\NormalTok{ F, }\DataTypeTok{adjust =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value Pr(>|t|)  
## Intercept   -0.060746   0.107937 -0.5628  0.57422  
## D_TB3MS_l1   0.240003   0.111611  2.1504  0.03276 *
## D_TB3MS_l2  -0.155883   0.153845 -1.0132  0.31220  
## D_TB10YS_l1  0.113740   0.125571  0.9058  0.36617  
## D_TB10YS_l2 -0.147519   0.112630 -1.3098  0.19182  
## ect_l1       0.031506   0.050519  0.6236  0.53359  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Thus the two estimated equations of the VECM are

\begin{align*}
 \widehat{\Delta TB3MS}_t =& \, -\underset{(0.11)}{0.06} + \underset{(0.11)}{0.24} \Delta TB3MS_{t-1} -\underset{(0.15)}{0.16} \Delta TB3MS_{t-2} \\ &+ \underset{(0.13)}{0.11} \Delta TB10YS_{t-1} -\underset{(0.11)}{0.15} \Delta TB10YS_{t-2} + \underset{(0.05)}{0.03} ECT_{t-1} \\
 \widehat{\Delta TB10YS}_t =& \, \underset{(0.06)}{0.12} -\underset{(0.07)}{0.00} \Delta TB3MS_{t-1} -\underset{(0.04)}{0.07} \Delta TB3MS_{t-2} \\ &+ \underset{(0.10)}{0.23} \Delta TB10YS_{t-1} -\underset{(0.07)}{0.07} \Delta TB10YS_{t-2} -\underset{(0.03)}{0.09} ECT_{t-1}.
\end{align*}

The output produced by coeftest() shows that there is little evidence
that lagged values of the differenced interest series' are useful for
prediction. This finding is more pronounced for the equation of the
differenced series of the 3-month treasury bill rate where the error
correction term (the lagged term spread) is not significantly different
from zero at any common level of significance. However, for the model
equation of the differenced 10-years treasury bonds rate we find that
the error correction term is statistically significant at the \(1\%\)
level with an estimate of \(-0.09\). This can be interpreted as follows:
although both interest rates are nonstationary, their conintegrating
relationship allows to predict the \emph{change} in the 10-years
treasury bonds rate using the VECM. In particular, the negative
coefficient estimate on the error correction term indicates that there
will be a negative change in the next period's 10-years treasury bonds
rate when the 10-years treasury bonds rate is higher than the 3-month
treasury bill rate in the current period.

\section{Volatility Clustering and Autoregressive Conditional
Heteroskedasticity}\label{volatility-clustering-and-autoregressive-conditional-heteroskedasticity}

In particular financial time series often exhibit a behaviour that is
known as \textbf{volatility clustering}: the volatility changes over
time but its degree shows a tendency to persist i.e.~there are periods
of low volatility and some where volatility is high. Econometricians
call this the conditional heteroskedasticity. Conditional
heteroskedasticity is an intersting property because it can be exploited
for forecasting the variance of future periods. A good forecast of a
series' variance allows to quantify uncertainty (for example by
computing accrurate forecast confidence intervals) which is especially
important when it is hard to come up with a good prediction of future
realisations (see section 14.4).

As an example, we consider daily changes in the Whilshire 5000 stock
index. The data is available for download at the
\href{https://fred.stlouisfed.org/series/WILL5000INDFC}{Federal Reserve
Economic data Base}. For consistency with the book we download data from
1989-29-12 to 2013-12-31 (choosing this somewhat larger time span is
necessary since later on we will be working with daily changes of the
series).

The following code chunk shows how to format the data accordingly and
how to reproduce Figure 16.3 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# import data on Wilshire 5000 index}
\NormalTok{W5000 <-}\StringTok{ }\KeywordTok{read.csv2}\NormalTok{(}\StringTok{"data/Wilshire5000.csv"}\NormalTok{, }\DataTypeTok{stringsAsFactors =}\NormalTok{ F, }\DataTypeTok{header =}\NormalTok{ T, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{, }\DataTypeTok{na.strings =} \StringTok{"."}\NormalTok{)}

\CommentTok{# convert columns}
\NormalTok{W5000}\OperatorTok{$}\NormalTok{DATE <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(W5000}\OperatorTok{$}\NormalTok{DATE)}
\NormalTok{W5000}\OperatorTok{$}\NormalTok{WILL5000INDFC <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(W5000}\OperatorTok{$}\NormalTok{WILL5000INDFC)}

\CommentTok{# remove NAs}
\NormalTok{W5000 <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(W5000)}

\CommentTok{# daily percentage changes}
\NormalTok{W5000_PC <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"Date"}\NormalTok{ =}\StringTok{ }\NormalTok{W5000}\OperatorTok{$}\NormalTok{DATE, }\StringTok{"Value"}\NormalTok{ =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{Delt}\NormalTok{(W5000}\OperatorTok{$}\NormalTok{WILL5000INDFC)}\OperatorTok{*}\DecValTok{100}\NormalTok{))}
\NormalTok{W5000_PC <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(W5000_PC)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot percentage changes}
\KeywordTok{plot}\NormalTok{(W5000_PC, }
     \DataTypeTok{ylab =} \StringTok{"Percent"}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Daily Percentage Changes in the Wilshire Index"}\NormalTok{,}
     \DataTypeTok{type=}\StringTok{"l"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{0.5}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-441-1} \end{center}

The level of the series of daily percentage changes in the Wilshire
Index seems to randomly fluctuate near zero, meaning there is little
amount autocorrelation so that it is difficult to predict future
outcomes using e.g.~an autoregressive model. However there is visual
evidence that the series exhibits conditional heteroskedasticity since
we observe volatility clustering and for some applications it is useful
to measure and forecast these patterns. This can be done using models
which assume that the volatility can be described by an autoregressive
processe.

\subsection*{ARCH and GARCH Models}\label{arch-and-garch-models}
\addcontentsline{toc}{subsection}{ARCH and GARCH Models}

Consider \[Y_t = \beta_0 + \beta_1 Y_{t-1} + \gamma_1 X_{t-1} + u_t\], a
simple ADL(\(1\),\(1\)) regression model. The econometrician Robert
Engle (1982) proposed to model \(\sigma^2_t\), the variance of the
normally distributed error \(u_t\) by an order \(p\) distributive lag
model

\begin{align}
 \sigma^2_t = \alpha_0 + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_p u_{t-p}^2, \label{eq:archmodel}
\end{align}

which is called the \textbf{autoregressive conditional
heteroskedasticity} (ARCH) model of order \(p\), or short
ARCH(\(p\)).\footnote{Although we introduce the ARCH model as a
  component in an ADL(\(1\),\(1\)) model (a model of the conditional
  error variances), it can be used for modelling the conditional
  zero-mean error term of any time series model.} The general idea is
apparent from the model structure: positive coefficients
\(\alpha_0,\alpha_1,\dots,\alpha_p\) imply that recent large squared
errors lead to a large variance, and thus large squared errors, in the
current period.

The generalized ARCH (GARCH) model, developed by Tim Bollerslev (1986)
is an extension of the ARCH model where \(\sigma^2_t\) is allowed to
depend on its on lags and lags of the squared error term. The
GARCH(\(p\),\(q\)) model is given by

\begin{align}
 \sigma^2_t = \alpha_0 + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_p u_{t-p}^2 + \phi_1 \sigma^2_{t-1} + \dots + \phi_p \sigma^2_{t-q}. \label{eq:garchmodel}
\end{align}

The GARCH model is an ADL(\(p\),\(q\)) model and thus can provide more
parsimonious parameterizations than an ARCH model (see the discussion in
Appendix 15.2 of the book).

\subsection*{Application to Stock Price
Volatility}\label{application-to-stock-price-volatility}
\addcontentsline{toc}{subsection}{Application to Stock Price Volatility}

Maximum likelihood estimates of ARCH and GARCH models are efficient and
have normal distributions in large samples such that the usual methods
for conducting inference about the unknown parameters can be applied.
The R package fGARCh is a collection of function for analyzing and
modelling the heteroskedastic behaviour in time series models. In the
following application we reproduce the results presented in Chapter 16.5
of the book by means of the function garchFit(). This function is
somewhat more sophisticated. It allows for different specifications of
the optimization procedure, different error distributions and much more
(use \texttt{?GarchFit} for a detailed description of the arguments), in
particular, the reported standard errors are robust.

The GARCH(\(1\),\(1\)) model of daily changes in the Wilshire 5000 index
we estimate is

\begin{align}
  R_t =& \, \beta_0 + u_t \ , \ u_t \sim \mathcal{N}(0,\sigma^2_t) \\
  \sigma^2_t =& \, \alpha_0 + \alpha_1 u_{t-1}^2 + \phi_1 \sigma_{t-1}^2 + \nu_t \label{eq:w5000g11}
\end{align}

where \(R_t\) are is the percentage change in period \(t\), \(\beta_1\),
\(\alpha_0\), \(\alpha_1\) and \(\phi_1\) are unknown coefficients and
\(u_t\) and \(\nu_t\) are error terms with conditional mean of zero. We
do not include any lagged predictors in the model of \(R_t\) because the
daily changes in the Wilshire 5000 index reflect daily stock returns
wich are essentially unpredictable. Note that \(u_t\) is assumed to be
normally distributed and the variance \(\sigma^2_t\) depends on \(t\) as
it follows the GARCH(\(1\),\(1\)) recursion \eqref{eq:w5000g11}.

It is straightforward to estimate this model using garchFit().

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate GARCH(1,1) model of daily percentage changes}
\NormalTok{GARCH_Wilshire <-}\StringTok{ }\KeywordTok{garchFit}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ W5000_PC}\OperatorTok{$}\NormalTok{Value, }\DataTypeTok{trace =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

We obtain

\begin{align}
  \widehat{R}_t =& \, \underset{(0.010)}{0.068} \label{eq:w5000g11est1} \\
  \widehat{\sigma}^2_t =& \, \underset{(0.002)}{0.011} + \underset{(0.007)}{0.081} u_{t-1}^2 + \underset{(0.008)}{0.909} \sigma_{t-1}^2 \label{eq:w5000g11est2}
\end{align}

so the coefficient on \(u_{t-1}^2\) and \(\sigma^2_{t-1}\) are
statistically significant at any common level of significance. One can
show that the persistence of movements in \(\sigma^2_t\) is determined
by the sum of both coefficients which is \(0.99\) here. This indicates
that movements in the conditional variance are highly persistent,
implying long-lasting periods of high volatility which is consistent
with the visual evidence for volatility clustering presented above.

The estimated conditional variance \(\widehat{\sigma}^2_t\) can be
computed by plugging the residuals from \eqref{eq:w5000g11est1} into
equation \eqref{eq:w5000g11est2}. This is performed automatically by
garchFit() so to obtain a vector of the estimated conditional standard
deviations all we have to do is to read out the values from
\texttt{GARCH\_Wilshire} by appending \texttt{@sigma.t}.

Using the vector containing the \(\widehat{\sigma}_t\) we plot bands of
\(\pm\) one conditional standard deviation along with deviations of the
series of daily percentage changes in the Wilshire 5000 index from its
mean. The following code chunk reproduces Figure 16.4 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute deviations of the percentage changes from their mean}
\NormalTok{dev_mean_W5000_PC <-}\StringTok{ }\NormalTok{W5000_PC}\OperatorTok{$}\NormalTok{Value }\OperatorTok{-}\StringTok{ }\NormalTok{GARCH_Wilshire}\OperatorTok{@}\NormalTok{fit}\OperatorTok{$}\NormalTok{coef[}\DecValTok{1}\NormalTok{]}

\CommentTok{# plot deviation of percentage changes from mean}
\KeywordTok{plot}\NormalTok{(W5000_PC}\OperatorTok{$}\NormalTok{Date, dev_mean_W5000_PC, }
     \DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Percent"}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"Date"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Estimated Bands of +- One Conditional Standard Deviation"}\NormalTok{,}
     \DataTypeTok{lwd =} \FloatTok{0.2}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{# add GARCH(1,1) confidence bands (one standard deviation) to the plot}
\KeywordTok{lines}\NormalTok{(W5000_PC}\OperatorTok{$}\NormalTok{Date, GARCH_Wilshire}\OperatorTok{@}\NormalTok{fit}\OperatorTok{$}\NormalTok{coef[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{GARCH_Wilshire}\OperatorTok{@}\NormalTok{sigma.t, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{0.5}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(W5000_PC}\OperatorTok{$}\NormalTok{Date, GARCH_Wilshire}\OperatorTok{@}\NormalTok{fit}\OperatorTok{$}\NormalTok{coef[}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{GARCH_Wilshire}\OperatorTok{@}\NormalTok{sigma.t, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-443-1} \end{center}

We see that the bands of estimated conditional standard deviation
describe the observed heteroskedasticity in the series of daily changes
of the Wilshire 5000 index quite well and are useful for quantifying
time-varying volatility and the emerging risk for investors holding
stocks summarized by the index. This estimated GARCH model may also be
used to produce forecast intervals whose widths depend on the volatility
of the most recent regression residuals.

\section{Summary}\label{summary-8}

\begin{itemize}
\item
  We have discussed how vector autoregressions are conveniently
  estimated and used for forecasting in R by means of functions from the
  VARS package.
\item
  The package urca which is a dependency of VARS supplies advanced
  methods for unit root and cointegration analysis like the DF-GLS and
  the EG-ADF tests. In an application we have found evidence that
  3-months and 10-year interest rates have a common stochastic trend
  (that is they are cointegrated) and thus can be modelled using a
  vector error correction model.
\item
  At last we have introduced the concept of volatility clusering and
  demonstrated how the function garchFit() from the package fGarch can
  be employed to estimate a GARCH(\(1\),\(1\)) model of the conditional
  heteroskedasticity inherent to returns of the Wilshire 5000 stock
  index.
\end{itemize}

\bibliography{book.bib,packages.bib}


\end{document}
