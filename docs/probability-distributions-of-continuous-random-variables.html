<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Using R for Introduction to Econometrics</title>
  <meta name="description" content="Using R for Introduction to Econometrics">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Using R for Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Using R for Introduction to Econometrics" />
  
  
  

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2017-12-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="random-variables-and-probability-distributions.html">
<link rel="next" href="random-sampling-and-the-distribution-of-sample-averages.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="MathJax-2.7.2/MathJax.js?config=default"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<!-- <script src="js/d3.v3.min.js"></script>
<script src="js/leastsquares.js"></script>
<script src="js/d3functions.js"></script>
<script src="d3.slider.js"></script>
<script src="slrm.js"></script> -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">URFITE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#expected-values-mean-and-variance"><i class="fa fa-check"></i>Expected Values, Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html"><i class="fa fa-check"></i><b>2.2</b> Probability Distributions of Continuous Random Variables</a><ul>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li><a href="probability-distributions-of-continuous-random-variables.html#thetdist">The Student <span class="math inline">\(t\)</span> Distribution</a></li>
<li><a href="probability-distributions-of-continuous-random-variables.html#the-f-distribution">The <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html"><i class="fa fa-check"></i><b>2.3</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="estimation-of-the-population-mean.html"><a href="estimation-of-the-population-mean.html"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-the-sample-mean.html"><a href="properties-of-the-sample-mean.html"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#p-value"><span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-sigma_y-is-known">Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#sample-variance-sample-standard-deviation-and-standard-error"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-sigma_y-is-unknown">Calculating the <span class="math inline">\(p\)</span>-value When <span class="math inline">\(\sigma_Y\)</span> is Unknown</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#the-t-statistic">The <span class="math inline">\(t\)</span>-statistic</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confidence-intervals-for-the-population-mean.html"><a href="confidence-intervals-for-the-population-mean.html"><i class="fa fa-check"></i><b>3.4</b> Confidence intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="comparing-means-from-different-populations.html"><a href="comparing-means-from-different-populations.html"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="an-application-to-the-gender-gap-of-earnings.html"><a href="an-application-to-the-gender-gap-of-earnings.html"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="scatterplots-sample-covariance-and-sample-correlation.html"><a href="scatterplots-sample-covariance-and-sample-correlation.html"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="estimating-the-coefficients-of-the-linear-regression-model.html"><a href="estimating-the-coefficients-of-the-linear-regression-model.html"><i class="fa fa-check"></i><b>4.1</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="estimating-the-coefficients-of-the-linear-regression-model.html"><a href="estimating-the-coefficients-of-the-linear-regression-model.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="measures-of-fit.html"><a href="measures-of-fit.html"><i class="fa fa-check"></i><b>4.2</b> Measures of Fit</a><ul>
<li><a href="measures-of-fit.html#the-r2">The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#standard-error-of-the-regression"><i class="fa fa-check"></i>Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html"><i class="fa fa-check"></i><b>4.3</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption #1: The Error Term has Conditional Mean of Zero</a></li>
<li><a href="the-least-squares-assumptions.html#assumption-2-all-x_i-y_i-are-independently-and-identically-distributed">Assumption #2: All <span class="math inline">\((X_i, Y_i)\)</span> are Independently and Identically Distributed</a></li>
<li class="chapter" data-level="" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption #3: Large outliers are unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tsdotoe.html"><a href="tsdotoe.html"><i class="fa fa-check"></i><b>4.4</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-1"><i class="fa fa-check"></i>R Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-2"><i class="fa fa-check"></i>R Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-3"><i class="fa fa-check"></i>R Simulation Study 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="testing-two-sided-hypotheses-concerning-beta-1.html"><a href="testing-two-sided-hypotheses-concerning-beta-1.html"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="5.2" data-path="confidence-intervals-for-regression-coefficients.html"><a href="confidence-intervals-for-regression-coefficients.html"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="confidence-intervals-for-regression-coefficients.html"><a href="confidence-intervals-for-regression-coefficients.html#r-simulation-study-5.1"><i class="fa fa-check"></i>R Simulation Study 5.1</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regression-when-x-is-a-binary-variable.html"><a href="regression-when-x-is-a-binary-variable.html"><i class="fa fa-check"></i><b>5.3</b> Regression when <span class="math inline">\(X\)</span> is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html#r-simulation-study-blue-estimator"><i class="fa fa-check"></i>R Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><a href="using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><i class="fa fa-check"></i><b>5.6</b> Using the <span class="math inline">\(t\)</span>-Statistic in Regression When the Sample Size Is Small</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="the-multiple-regression-model.html"><a href="the-multiple-regression-model.html"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="measures-of-fit-in-multiple-regression.html"><a href="measures-of-fit-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="the-distribution-of-the-ols-estimators-in-multiple-regression.html"><a href="the-distribution-of-the-ols-estimators-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><a href="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="an-application-to-test-scores-and-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="an-application-to-test-scores-and-the-student-teacher-ratio.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="joint-hypothesis-testing-using-the-f-statistic.html"><a href="joint-hypothesis-testing-using-the-f-statistic.html"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the <span class="math inline">\(F\)</span>-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="confidence-sets-for-multiple-coefficients.html"><a href="confidence-sets-for-multiple-coefficients.html"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="model-specification-for-multiple-regression.html"><a href="model-specification-for-multiple-regression.html"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="model-specification-for-multiple-regression.html"><a href="model-specification-for-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="analysis-of-the-test-score-data-set.html"><a href="analysis-of-the-test-score-data-set.html"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="a-general-strategy-for-modeling-nonlinear-regression-functions.html"><a href="a-general-strategy-for-modeling-nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modeling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="interactions-between-independent-variables.html"><a href="interactions-between-independent-variables.html"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><a href="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="assessing-studies-based-on-multiple-regression.html"><a href="assessing-studies-based-on-multiple-regression.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="internal-and-external-validity.html"><a href="internal-and-external-validity.html"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="threats-to-internal-validity-of-multiple-regression-analysis.html"><a href="threats-to-internal-validity-of-multiple-regression-analysis.html"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><a href="internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity When the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="example-test-scores-and-class-size.html"><a href="example-test-scores-and-class-size.html"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regression-with-panel-data.html"><a href="regression-with-panel-data.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="panel-data-with-two-time-periods-before-and-afer-comparisons.html"><a href="panel-data-with-two-time-periods-before-and-afer-comparisons.html"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and Afer” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="regression-with-time-fixed-effects.html"><a href="regression-with-time-fixed-effects.html"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="the-fixed-effects-regression-assumptions-and-standard-errors-for-fixed-effects-regression.html"><a href="the-fixed-effects-regression-assumptions-and-standard-errors-for-fixed-effects-regression.html"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="drunk-driving-laws-and-traffic-deaths.html"><a href="drunk-driving-laws-and-traffic-deaths.html"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using R for Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="probability-distributions-of-continuous-random-variables" class="section level2">
<h2><span class="header-section-number">2.2</span> Probability Distributions of Continuous Random Variables</h2>
<p>Since a continuous random variable takes on a continuum of possible values, we cannot use the concept of a probability distribution as used for discrete random variables. Instead, the probability distribution of a continuous random variable is summarized by its <em>probability density function</em> (PDF).</p>
<p>The cumulative probability distribution function (CDF) for a continuous random variable is defined just as in the discrete case. Hence, the cumulative probability distribution of a continuous random variables states the probability that the random variable is less than or equal to a particular value.</p>
<p>For completeness, we present revisions of Key Concepts 2.1 and 2.2 for the continuous case.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.3
</h3>
<h3 class="left">
Probabilities, Expected Value and Variance of a Continuous Random Variable
</h3>
<p>
<p>Let <span class="math inline">\(f_Y(y)\)</span> denote the probability density function of <span class="math inline">\(Y\)</span>. Because probabilities cannot be negative, we have <span class="math inline">\(f_Y\geq 0\)</span> for all <span class="math inline">\(y\)</span>. The Probability that <span class="math inline">\(Y\)</span> falls between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, <span class="math inline">\(a &lt; b\)</span> is <span class="math display">\[ P(a \leq Y \leq b) = \int_a^b f_Y(y) \mathrm{d}y. \]</span> We further have that <span class="math inline">\(P(-\infty \leq Y \leq \infty) = 1\)</span> and therefore <span class="math inline">\(\int_{-\infty}^{\infty} f_Y(y) \mathrm{d}y = 1\)</span>.</p>
<p>As for the discrete case, the expected value of <span class="math inline">\(Y\)</span> is the probability weighted average of its values. Due to continuity, we use intergrals instead of sums.</p>
<p>The expected value of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[ E(Y) =  \mu_Y = \int y f_Y(y) \mathrm{d}y. \]</span></p>
<p>The variance is the expected value of <span class="math inline">\((Y - \mu_Y)^2\)</span>. We thus have</p>
<p><span class="math display">\[ \text{Var}(Y) =  \sigma_Y^2 = \int (y - \mu_Y)^2 f_Y(y) \mathrm{d}y. \]</span></p>
</p>
</div>
<p>Let us discuss an example:</p>
<p>Consider the continuous random variable <span class="math inline">\(X\)</span> with propability density function</p>
<p><span class="math display">\[ f_X(x) = \frac{3}{x^4}, x&gt;1. \]</span></p>
<ul>
<li>We can show analytically that the integral of <span class="math inline">\(f_X(x)\)</span> over the real line equals <span class="math inline">\(1\)</span>.</li>
</ul>
<span class="math display">\[\begin{align}
 \int f_X(x) \mathrm{d}x =&amp;  \int_{1}^{\infty} \frac{3}{x^4} \mathrm{d}x \\
  =&amp; \lim_{t \rightarrow \infty} \int_{1}^{t} \frac{3}{x^4} \mathrm{d}x \\
  =&amp; \lim_{t \rightarrow \infty}  -x^{-3} \rvert_{x=1}^t \\
  =&amp; -\left(\lim_{t \rightarrow \infty}\frac{1}{t^3} - 1\right) \\
  =&amp; 1
\end{align}\]</span>
<ul>
<li>The expectation of <span class="math inline">\(X\)</span> can be computed as follows:</li>
</ul>
<span class="math display">\[\begin{align}
 E(X) = \int x \cdot f_X(x) \mathrm{d}x =&amp;  \int_{1}^{\infty} x \cdot \frac{3}{x^4} \mathrm{d}x \\
  =&amp; - \frac{3}{2} x^{-2} \rvert_{x=1}^{\infty} \\
  =&amp; -\frac{3}{2} \left( \lim_{t \rightarrow \infty} \frac{1}{t^2} - 1 \right) \\
  =&amp; \frac{3}{2}
\end{align}\]</span>
<ul>
<li>Note that the variance of <span class="math inline">\(X\)</span> can be expressed as <span class="math inline">\(\text{Var}(X) = E(X^2) - E(X)^2\)</span>. Since <span class="math inline">\(E(X)\)</span> has been computed in the previous step, we seek <span class="math inline">\(E(X^2)\)</span>:</li>
</ul>
<span class="math display">\[\begin{align}
 E(X^2)= \int x^2 \cdot f_X(x) \mathrm{d}x =&amp;  \int_{1}^{\infty} x^2 \cdot \frac{3}{x^4} \mathrm{d}x \\
  =&amp; -3 x^{-1} \rvert_{x=1}^{\infty} \\
  =&amp; -3 \left( \lim_{t \rightarrow \infty} \frac{1}{t} - 1 \right) \\
  =&amp; 3
\end{align}\]</span>
<p>So we have shown that the area under the curve equals one, that the expectation is <span class="math inline">\(E(X)=\frac{3}{2} \ \)</span> and we found the variance to be <span class="math inline">\(\text{Var}(X) = \frac{3}{4}\)</span>. However, this was quite tedious and, as we shall see soon, an analytic approach is not applicable for some probability density functions e.g. if integrals have no closed form solutions.</p>
<p>Luckily, R enables us to find the results derived above in an instant. The tool we use for this is the function <code>integrate()</code>. First, we have to define the functions we want to calculate integrals for as R functions, i.e. the PDF <span class="math inline">\(f_X(x)\)</span> as well as the expressions <span class="math inline">\(x\cdot f_X(x)\)</span> and <span class="math inline">\(x^2\cdot f_X(x)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define functions</span>
f &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="dv">3</span><span class="op">/</span>x<span class="op">^</span><span class="dv">4</span>
g &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">*</span><span class="kw">f</span>(x)
h &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span><span class="op">*</span><span class="kw">f</span>(x)</code></pre></div>
<p>Next, we use <code>integrate()</code> and set lower and upper limits of integration to <span class="math inline">\(1\)</span> and <span class="math inline">\(\infty\)</span> using arguments <code>lower</code> and <code>upper</code>. By default, <code>integrate()</code> prints the result along with an estimate of the calculation error to the console. However, the outcome is not a numeric value one can do further calculation with readily. In order to get only a numeric value of the integral, we need to use the <code>$</code> operator in conjunction with <code>value</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate area under curve</span>
AUC &lt;-<span class="st"> </span><span class="kw">integrate</span>(f, 
                 <span class="dt">lower =</span> <span class="dv">1</span>, 
                 <span class="dt">upper =</span> <span class="ot">Inf</span>
                 )
AUC </code></pre></div>
<pre><code>## 1 with absolute error &lt; 1.1e-14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate E(X)</span>
EX &lt;-<span class="st"> </span><span class="kw">integrate</span>(g,
                <span class="dt">lower =</span> <span class="dv">1</span>,
                <span class="dt">upper =</span> <span class="ot">Inf</span>)
EX</code></pre></div>
<pre><code>## 1.5 with absolute error &lt; 1.7e-14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate Var(X)</span>
VarX &lt;-<span class="st"> </span><span class="kw">integrate</span>(h,
                  <span class="dt">lower =</span> <span class="dv">1</span>,
                  <span class="dt">upper =</span> <span class="ot">Inf</span>
                  )<span class="op">$</span>value <span class="op">-</span><span class="st"> </span>EX<span class="op">$</span>value<span class="op">^</span><span class="dv">2</span> 
VarX</code></pre></div>
<pre><code>## [1] 0.75</code></pre>
<p>Although there is a wide variety of distributions, the ones most often encountered in econometrics are the normal, chi-squared, Student <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> distributions. Therefore we will discuss some core R functions that allow to do calculations involving densities, probabilities and quantiles of these distributions.</p>
<p>Every probability distribution that R handles has four basic functions whose names consist of a prefix followed by a root name. As an example, take the normal distribution. The root name of all four functions associated with the normal distribution is <tt>norm</tt>. The four prefixes are</p>
<ul>
<li><tt>d</tt> for “density” - probability function / probability density function</li>
<li><tt>p</tt> for “probability” - cumulative distribution function</li>
<li><tt>q</tt> for “quantile” - quantile function (inverse cumulative distribution function)</li>
<li><tt>r</tt> for “random” - random number generator</li>
</ul>
<p>Thus, for the normal distribution we have the R functions <code>dnorm()</code>, <code>pnorm()</code>, <code>qnorm()</code> and <code>rnorm()</code>.</p>
<div id="the-normal-distribution" class="section level3 unnumbered">
<h3>The Normal Distribution</h3>
<p>The probably most important probability distribution considered here is the normal distribution. This is not least due to the special role of the standard normal distribution and the Central Limit Theorem which is treated shortly during the course of this section. Distributions of the normal family have a familiar symmetric, bell-shaped probability density. A normal distribution is characterized by its mean <span class="math inline">\(\mu\)</span> and its standard deviation <span class="math inline">\(\sigma\)</span> what is concisely expressed by <span class="math inline">\(N(\mu,\sigma^2)\)</span>. The normal distribution has the PDF</p>
<p><span class="math display">\[ f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x - μ)^2/(2 σ^2)}. \]</span></p>
<p>For the standard normal distribution we have <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>. Standard normal variates are often denoted <span class="math inline">\(Z\)</span>. Usually, the standard normal PDF is denoted by <span class="math inline">\(\phi\)</span> and the standard normal CDF is denoted by <span class="math inline">\(\Phi\)</span>. Hence,</p>
<p><span class="math display">\[ \phi(c) = \Phi&#39;(c) \ \ , \ \ \Phi(c) = P(Z \leq c) \ \ , \ \ Z \sim N(0,1).
\]</span> In R, we can conveniently obtain density values of normal distributions using the function <code>dnorm()</code>. Let us draw a plot of the standard normal density function using <code>curve()</code> and <code>dnorm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw a plot of the N(0,1) pdf</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x),
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>),
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, 
      <span class="dt">main =</span> <span class="st">&quot;Standard Normal Density Function&quot;</span>
      ) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can obtain the density at different positions by passing a vector of quantiles to <code>dnorm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute denstiy at x=-1.96, x=0 and x=1.96</span>
<span class="kw">dnorm</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">1.96</span>, <span class="dv">0</span>, <span class="fl">1.96</span>))</code></pre></div>
<pre><code>## [1] 0.05844094 0.39894228 0.05844094</code></pre>
<p>Similary as for the PDF, we can plot the standard normal CDF using <code>curve()</code> and <code>pnorm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the standard normal CDF</span>
<span class="kw">curve</span>(<span class="kw">pnorm</span>(x), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), 
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, 
      <span class="dt">main =</span> <span class="st">&quot;Standard Normal Cumulative Distribution Function&quot;</span>
      )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can also use R to calculate the probability of events associated with a standard normal random variate.</p>
<p>Let us say we are interested in <span class="math inline">\(P(Z \leq 1.337)\)</span>. For some general continuous random variable <span class="math inline">\(Z\)</span> on <span class="math inline">\([-\infty,\infty]\)</span> with density function <span class="math inline">\(g(x)\)</span> we would have to determine <span class="math inline">\(G(x)\)</span>, the antiderivative of <span class="math inline">\(g(x)\)</span> since</p>
<p><span class="math display">\[ P(Z \leq 1,337 ) = G(1,337) = \int_{-\infty}^{1,337} g(x) \mathrm{d}x.  \]</span></p>
<p>However, if <span class="math inline">\(Z \sim N(0,1)\)</span>, we have <span class="math inline">\(g(x)=\phi(x)\)</span> so there is no analytic solution to the integral above and it is cumbersome to come up with an approximation. However, we may circumvent this using R in different ways. <br> The first approach makes use of the function <code>integrate()</code> which allows to solve one-dimensional integration problems using a numerical method. For this, we first define the function we want to compute the integral of as a R function <code>f</code>. In our example, <code>f</code> needs to be the standard normal density function and hence takes a single argument <code>x</code>. Following the definition of <span class="math inline">\(\phi(x)\)</span> we define <code>f</code> as</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the standard normal PDF as a R function</span>
f &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="dv">1</span><span class="op">/</span>(<span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi)) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>)
}</code></pre></div>
<p>Let us check if this function enables us to compute standard normal density values by passing it a vector of quantiles.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define vector of quantiles</span>
quants &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.96</span>,<span class="dv">0</span>,<span class="fl">1.96</span>)

<span class="co"># compute density values</span>
<span class="kw">f</span>(quants)</code></pre></div>
<pre><code>## [1] 0.05844094 0.39894228 0.05844094</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compare to results produced by dnorm()</span>
<span class="kw">f</span>(quants) <span class="op">==</span><span class="st"> </span><span class="kw">dnorm</span>(quants)</code></pre></div>
<pre><code>## [1] TRUE TRUE TRUE</code></pre>
<p>Notice that the results produces by <code>f()</code> are indeed equivalent to those given by <code>dnorm()</code>.</p>
<p>Next, we call <code>integrate()</code> on <code>f()</code> and further specify the arguments <code>lower</code> and <code>upper</code>, the lower and upper limits of integration.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># integrate f()</span>
<span class="kw">integrate</span>(f, 
          <span class="dt">lower =</span> <span class="op">-</span><span class="ot">Inf</span>, 
          <span class="dt">upper =</span> <span class="fl">1.337</span>
          )</code></pre></div>
<pre><code>## 0.9093887 with absolute error &lt; 1.7e-07</code></pre>
<p>We find that the probability of observing <span class="math inline">\(Z \leq 1,337\)</span> is about <span class="math inline">\(0.9094\%\)</span>.</p>
<p>A second and much more convenient way is to use the function <code>pnorm()</code> which also allows calculus involving the standard normal cumulative distribution function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute a probability using pnorm()</span>
<span class="kw">pnorm</span>(<span class="fl">1.337</span>)</code></pre></div>
<pre><code>## [1] 0.9093887</code></pre>
<p>The result matches the outcome of the approach using <code>ìntegrate()</code>.</p>
<p>Let us discuss some further examples:</p>
<p>A commonly known result is that <span class="math inline">\(95\%\)</span> probability mass of a standard normal lies in the intervall <span class="math inline">\([-1.96, 1.96]\)</span>, that is in a distance of about <span class="math inline">\(2\)</span> standard deviations to the mean. We can easily confirm this by calculating</p>
<p><span class="math display">\[ P(-1.96 \leq Z \leq 1.96) = 1-2\times P(Z \leq -1.96) \]</span> due to symmetry of the standard normal PDF. Thanks to R, we can abondon the table of the standard normal CDF again and instead solve this by means of the function <code>pnorm()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the probability</span>
<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">pnorm</span>(<span class="op">-</span><span class="fl">1.96</span>)) </code></pre></div>
<pre><code>## [1] 0.9500042</code></pre>
<p>Now consider a random variable <span class="math inline">\(Y\)</span> with <span class="math inline">\(Y \sim N(5,25)\)</span>. As You should already know from Your statistics courses it is not possible to make any statement of probability without prior standardizing as shown in Key Concept 2.4.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 2.4
</h3>
<h3 class="left">
Computing Probabilities Involving Normal Random Variables
</h3>
<p>
<p>Suppose <span class="math inline">\(Y\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[Y
\sim N(\mu, \sigma^2)\]</span> Then <span class="math inline">\(Y\)</span> is standardized by substracting its mean and dividing by its standard deviation: <span class="math display">\[ Z = \frac{Y -\mu}{\sigma} \]</span> Let <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> denote two numbers whereby <span class="math inline">\(c_1 &lt; c_2\)</span> and further <span class="math inline">\(d_1 = (c_1 - \mu) / \sigma\)</span> and <span class="math inline">\(d_2 = (c_2 - \mu)/\sigma\)</span>. Then</p>
<span class="math display">\[\begin{align} 
P(Y \leq c_2) =&amp; \, P(Z \leq d_2) = \Phi(d_2) \\ 
P(Y \geq c_1) =&amp; \, P(Z \geq d_1) = 1 - \Phi(d_1) \\ 
P(c_1 \leq Y \leq c_2) =&amp; \, P(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1) 
\end{align}\]</span>
</p>
</div>
<p>R functions that handle the normal distribution can perform this standardization. If we are interested in <span class="math inline">\(P(3 \leq Y \leq 4)\)</span> we can use <code>pnorm()</code> and adjust for a mean and/or a standard deviation that deviate from <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma = 1\)</span> by specifying the arguments <code>mean</code> and <code>sd</code> accordingly. <strong>Attention</strong>: <code>pnorm()</code> requires the argument <code>sd</code> which is the standard deviation, not the variance!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnorm</span>(<span class="dv">4</span>, <span class="dt">mean =</span> <span class="dv">5</span>, <span class="dt">sd =</span> <span class="dv">5</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">3</span>, <span class="dt">mean =</span> <span class="dv">5</span>, <span class="dt">sd =</span> <span class="dv">5</span>) </code></pre></div>
<pre><code>## [1] 0.07616203</code></pre>
<p>An extension of the normal distribution in a univariate setting is the multivariate normal distribution. The PDF of two random normal variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by</p>
<span class="math display" id="eq:bivnorm">\[\begin{align}
g_{X,Y}(x,y) =&amp; \, \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho_{XY}^2}} \\ 
\cdot &amp; \, \exp \left\{ \frac{1}{-2(1-\rho_{XY}^2)} \left[ \left( \frac{x-\mu_x}{\sigma_x} \right)^2 - 2\rho_{XY}\left( \frac{x-\mu_X}{\sigma_X} \right)\left( \frac{y-\mu_Y}{\sigma_Y} \right) + \left( \frac{y-\mu_Y}{\sigma_Y} \right)^2 \right]  \right\}. \tag{2.1}
\end{align}\]</span>
<p>Equation <a href="probability-distributions-of-continuous-random-variables.html#eq:bivnorm">(2.1)</a> contains the bivariate normal PDF. Admittedly, it is hard to gain insights from this complicated expression. Instead, let us consider the special case where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated standard normal random variables with density functions <span class="math inline">\(f_X(x)\)</span> and <span class="math inline">\(f_Y(y)\)</span> and we assume that they have a joint normal distribution. We then have the parameters <span class="math inline">\(\sigma_X = \sigma_Y = 1\)</span>, <span class="math inline">\(\mu_X=\mu_Y=0\)</span> (due to marginal standard normality) and <span class="math inline">\(\rho_{XY}=0\)</span> (due to uncorrelatedness). The joint probability density function of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> then becomes</p>
<p><span class="math display">\[ g_{X,Y}(x,y) = f_X(x) f_Y(y) = \frac{1}{2\pi} \cdot \exp \left\{ -\frac{1}{2} \left[x^2 + y^2 \right]  \right\}, \tag{2.2}  \]</span></p>
<p>the PDF of the bivariate standard normal distribution. The next plot provides an interactive three dimensional plot of (<a href="#mjx-eqn-2.2">2.2</a>). By moving the mouse curser over the plot You can see that the density is rotationally invariant.</p>
<center>
<iframe scrolling="no" seamless="seamless" style="border:none" src="https://plot.ly/~mca_unidue/22.embed?width=550&amp;height=550?showlink=false/800/1200" width="600" height="400">
</iframe>
<center>
<p><a name="chisquare"></a></p>
</div>
<div id="the-chi-squared-distribution" class="section level3 unnumbered">
<h3>The Chi-Squared Distribution</h3>
<p>Another distribution relevant in econometric day-to-day work is the chi-squared distribution. It is often needed when testing special types of hypotheses frequently ecountered when dealing with regression models.</p>
<p>The sum of <span class="math inline">\(M\)</span> squared independent standard normal distributed random variables follows a chi-squared distribution with <span class="math inline">\(M\)</span> degrees of freedom.</p>
<p><span class="math display">\[ Z_1^2 + \dots + Z_M^2 = \sum_{m=1}^M Z_m^2 \sim \chi^2_M \ \ \text{with} \ \ Z_m \overset{i.i.d.}{\sim} N(0,1) \label{eq:chisq}\]</span></p>
<p>A <span class="math inline">\(\chi^2\)</span> distributed random variable with <span class="math inline">\(M\)</span> degrees of freedom has expectation <span class="math inline">\(M\)</span>, mode at <span class="math inline">\(M-2\)</span> for <span class="math inline">\(n \geq 2\)</span> and variance <span class="math inline">\(2 \cdot M\)</span>.</p>
<p>For example, if we have</p>
<p><span class="math display">\[ Z_1,Z_2,Z_3 \overset{i.i.d.}{\sim} N(0,1) \]</span></p>
<p>it holds that</p>
<p><span class="math display">\[ Z_1^2+Z_2^2+Z_3^3 \sim \chi^2_3. \tag{2.3} \]</span> By means of the code below, we can display the PDF and the CDF of a <span class="math inline">\(\chi^2_3\)</span> random variable in a single plot. This is achieved by setting the argument <code>add = TRUE</code> in the second call of <code>curve()</code>. Further we adjust limits of both axes using <code>xlim</code> and <code>ylim</code> and choose different colors to make both functions better distinguishable. The plot is completed by adding a legend with help of the function <code>legend()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the PDF</span>
<span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df=</span><span class="dv">3</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>), 
      <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), 
      <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, 
      <span class="dt">main=</span><span class="st">&quot;p.d.f. and c.d.f of Chi-Squared Distribution, m = 3&quot;</span>
      )

<span class="co"># add the CDF to the plot</span>
<span class="kw">curve</span>(<span class="kw">pchisq</span>(x, <span class="dt">df=</span><span class="dv">3</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>), 
      <span class="dt">add =</span> <span class="ot">TRUE</span>, 
      <span class="dt">col=</span><span class="st">&quot;red&quot;</span>
      )

<span class="co"># add a legend to the plot</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, 
       <span class="kw">c</span>(<span class="st">&quot;PDF&quot;</span>,<span class="st">&quot;CDF&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), 
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-27-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice that, since the outcomes of a <span class="math inline">\(\chi^2_M\)</span> distributed random variable are always positive, the domain of the related PDF and CDF is <span class="math inline">\(\mathbb{R}_{\geq0}\)</span>.</p>
<p>As expectation and variance depend (solely) on the degrees of freedom, the distribution’s shape changes drastically if we vary the number of squared standard normals that are summed up. This relation is often depicted by overlaying densities for different <span class="math inline">\(M\)</span>, see e.g. the <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">Wikipedia Article</a>.</p>
<p>Of course, one can easily reproduce such a plot using R. Again we start by plotting the density of the <span class="math inline">\(\chi_1^2\)</span> distribution on the intervall <span class="math inline">\([0,15]\)</span> with <code>curve()</code>. In the next step, we loop over degrees of freedom <span class="math inline">\(m=2,...,7\)</span> and add a density curve for each <span class="math inline">\(m\)</span> to the plot. We also adjust the line color for each iteration of the loop by setting <code>col = m</code>. At last, we add a legend that displays degrees of freedom and the associated colors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the density for m=1</span>
<span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df=</span><span class="dv">1</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>), 
      <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, 
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, 
      <span class="dt">main=</span><span class="st">&quot;Chi-Square Distributed Random Variables&quot;</span>
      )

<span class="co"># add densities for m=2,...,7 to the plot using a for loop </span>
<span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">7</span>) {
  <span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df =</span> m),
        <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>), 
        <span class="dt">add =</span> T, 
        <span class="dt">col =</span> m
        )
}

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="kw">as.character</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>), 
       <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span> , 
       <span class="dt">lty =</span> <span class="dv">1</span>, 
       <span class="dt">title =</span> <span class="st">&quot;D.f.&quot;</span>
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-28-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It is evident that increasing the degrees of freedom shifts the distribution to the right (the modus becomes larger) and increases its dispersion (the distribution’s variance grows).</p>
</div>
<div id="thetdist" class="section level3 unnumbered">
<h3>The Student <span class="math inline">\(t\)</span> Distribution</h3>
<p><a name="tdist"></a></p>
<p>Let <span class="math inline">\(Z\)</span> be a standard normal variate, <span class="math inline">\(W\)</span> a random variable that follows a <span class="math inline">\(\chi^2_M\)</span> distribution with <span class="math inline">\(M\)</span> degrees of freedom and further assume that <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> are independently distributed. Then it holds that</p>
<p><span class="math display">\[ \frac{Z}{\sqrt{W/M}} =:X \sim t_M \]</span> and we say that <span class="math inline">\(X\)</span> follows a student <span class="math inline">\(t\)</span> distribution (or simple <span class="math inline">\(t\)</span> distribution) with <span class="math inline">\(M\)</span> degrees of freedom.</p>
<p>As for the <span class="math inline">\(\chi^2_M\)</span> distribution, a <span class="math inline">\(t\)</span> distribution depends on the degrees of freedom <span class="math inline">\(M\)</span>. <span class="math inline">\(t\)</span> distributions are symmetric, bell-shaped and look very similar to a normal distribution, especially when <span class="math inline">\(M\)</span> is large. This is not a coincidence: for a sufficient large <span class="math inline">\(M\)</span>, a <span class="math inline">\(t_M\)</span> distribution can be approximated by the standard normal distribution. This approximation works reasonably well for <span class="math inline">\(M\geq 30\)</span>. As we will show later by means of a small simulation study, the <span class="math inline">\(t_{\infty}\)</span> distribution <em>is</em> the standard normal distribution.</p>
<p>A <span class="math inline">\(t_M\)</span> distributed random variable has an expectation value if <span class="math inline">\(M&gt;1\)</span> and a variance if <span class="math inline">\(n&gt;2\)</span>.</p>
<span class="math display">\[\begin{align}
  E(X) =&amp; 0 \ , \ M&gt;1 \\
  \text{Var}(X) =&amp; \frac{M}{M-2} \ , \ M&gt;2
\end{align}\]</span>
<p>Let us graph some <span class="math inline">\(t\)</span> distributions with different <span class="math inline">\(M\)</span> and compare them with the standard normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the standard normal density</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), 
      <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, 
      <span class="dt">lty=</span><span class="dv">2</span>, 
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, 
      <span class="dt">main=</span><span class="st">&quot;Theoretical Densities of t-Distributions&quot;</span>
      )

<span class="co"># plot the t density for m=2</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df=</span><span class="dv">2</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), 
      <span class="dt">col=</span><span class="dv">2</span>, 
      <span class="dt">add =</span> T
      )

<span class="co"># plot the t density for m=4</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df=</span><span class="dv">4</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), 
      <span class="dt">col=</span><span class="dv">3</span>, 
      <span class="dt">add=</span>T
      )

<span class="co"># plot the t density for m=25</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df=</span><span class="dv">25</span>), 
      <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), 
      <span class="dt">col=</span><span class="dv">4</span>, 
      <span class="dt">add=</span>T
      )

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="kw">c</span>(<span class="st">&quot;N(0,1)&quot;</span>,<span class="st">&quot;M=2&quot;</span>,<span class="st">&quot;M=4&quot;</span>,<span class="st">&quot;M=25&quot;</span>), 
       <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, 
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot indicates what has been claimed in the previous paragraph: as the degrees of freedom increase, the shape of the <span class="math inline">\(t\)</span> distribution comes closer to that of a standard normal bell. Already for <span class="math inline">\(M=25\)</span> we find little difference to the dashed line which belongs to the standard normal density curve. If <span class="math inline">\(M\)</span> is small, we find the distribution to have slightly havier tails than a standard normal, i.e. it has a “fatter” bell shape.</p>
</div>
<div id="the-f-distribution" class="section level3 unnumbered">
<h3>The <span class="math inline">\(F\)</span> Distribution</h3>
<p>Another ratio of random variables important to econometricians is the ratio of two indpendently <span class="math inline">\(\chi^2\)</span> distributed random variables that are divided by their degrees of freedom. Such a quantity follows a <span class="math inline">\(F\)</span> distribution with numerator degrees of freedom <span class="math inline">\(M\)</span> and denominator degrees of freedom <span class="math inline">\(n\)</span>, denoted <span class="math inline">\(F_{M,n}\)</span>. The distribution was first derived by George Snedecor but was named in honor of <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Sir Ronald Fisher</a>.</p>
<p><span class="math display">\[ \frac{W/M}{V/n} \sim F_{M,n} \ \ \text{with} \ \ W \sim \chi^2_M \ \ , \ \ V \sim \chi^2_n \]</span></p>
<p>By definition, the domain of both PDF and CDF of a <span class="math inline">\(F_{M,n}\)</span> distributed random variable is <span class="math inline">\(\mathbb{R}_{\geq0}\)</span>.</p>
<p>Say we have a <span class="math inline">\(F\)</span> distributed random variable <span class="math inline">\(Y\)</span> with numerator degrees of freedom <span class="math inline">\(3\)</span> and denominator degrees of freedom <span class="math inline">\(14\)</span> and are interested in <span class="math inline">\(P(Y \geq 2)\)</span>. This can be computed with help of the function <code>pf()</code>. By setting the argument <code>lower.tail</code> to <code>TRUE</code> we ensure that R computes <span class="math inline">\(1- P(Y \leq 2)\)</span>, i.e. the probability mass in the tail right of <span class="math inline">\(2\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pf</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">13</span>, <span class="dt">lower.tail =</span> F)</code></pre></div>
<pre><code>## [1] 0.1638271</code></pre>
<p>We can visualize this probability by drawing a line plot of the related density function and adding a color shading with <code>polygon()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define coordinate vectors for vertices of the polygon</span>
x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="fl">0.01</span>), <span class="dv">10</span>)
y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">df</span>(<span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="fl">0.01</span>), <span class="dv">3</span>, <span class="dv">14</span>), <span class="dv">0</span>)

<span class="co"># draw density of F_{3, 14}</span>
<span class="kw">curve</span>(<span class="kw">df</span>(x ,<span class="dv">3</span> ,<span class="dv">14</span>), 
      <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.8</span>), 
      <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>), 
      <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>,
      <span class="dt">main =</span> <span class="st">&quot;Density Function&quot;</span>
      )

<span class="co"># draw the polygon</span>
<span class="kw">polygon</span>(x, y, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The <span class="math inline">\(F\)</span> distribution is related to many other distributions. An important special case encountered in econometrics arises if the denominator degrees of freedom are large such that the <span class="math inline">\(F_{M,n}\)</span> distribution can be approximated by the <span class="math inline">\(F_{M,\infty}\)</span> distribution which turns out to be simply the distribution of a <span class="math inline">\(\chi^2_M\)</span> random variable divided by its degrees of freedom <span class="math inline">\(M\)</span>.</p>
<p><span class="math display">\[ W/M \sim F_{M,\infty} \ \ , \ \ W \sim \chi^2_M \]</span></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-variables-and-probability-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="random-sampling-and-the-distribution-of-sample-averages.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-ch2.Rmd",
"text": "Edit"
},
"download": ["URFITE.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

<!-- </html> -->
