<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Using R for Introduction to Econometrics</title>
  <meta name="description" content="Using R for Introduction to Econometrics">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Using R for Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Using R for Introduction to Econometrics" />
  
  
  

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2017-11-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="a-review-of-statistics-using-r.html">
<link rel="next" href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="MathJax-2.7.2/MathJax.js?config=default"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/SVG"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- <script src="js/d3.v3.min.js"></script> 
<script src="js/leastsquares.js"></script>
<script src="js/d3functions.js"></script>
<script src="d3.slider.js"></script>
<script src="slrm.js"></script> -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">URFITE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i>Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#expected-values-mean-and-variance"><i class="fa fa-check"></i>Expected Values, Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li><a href="probability-theory.html#thetdist">The Student <span class="math inline">\(t\)</span> Distribution</a></li>
<li><a href="probability-theory.html#the-f-distribution">The <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#random-sampling-and-the-distribution-of-sample-averages"><i class="fa fa-check"></i>Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#estimation-of-the-population-mean"><i class="fa fa-check"></i>Estimation of the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#properties-of-the-population-mean"><i class="fa fa-check"></i>Properties of the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i>Hypothesis Tests Concerning the Population Mean</a><ul>
<li><a href="a-review-of-statistics-using-r.html#p-value"><span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="a-review-of-statistics-using-r.html#calculating-the-p-value-when-sigma_y-is-known">Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#sample-variance-sample-standard-deviation-and-standard-error"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li><a href="a-review-of-statistics-using-r.html#calculating-the-p-value-when-sigma_y-is-unknown">Calculating the <span class="math inline">\(p\)</span>-value When <span class="math inline">\(\sigma_Y\)</span> is Unknown</a></li>
<li><a href="a-review-of-statistics-using-r.html#the-t-statistic">The <span class="math inline">\(t\)</span>-statistic</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i>Confidence intervals for the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#comparing-means-from-different-populations"><i class="fa fa-check"></i>Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#an-application-to-the-gender-gap-of-earnings"><i class="fa fa-check"></i>An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i>Scatterplots, Sample Covariance and Sample Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i>Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i>Measures of fit</a><ul>
<li><a href="lrwor.html#the-r2">The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#standard-error-of-the-regression"><i class="fa fa-check"></i>Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-least-squares-assumptions"><i class="fa fa-check"></i>The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption #1: The Error Term has Conditional Mean of Zero</a></li>
<li><a href="lrwor.html#assumption-2-all-x_i-y_i-are-independently-and-identically-distributed">Assumption #2: All <span class="math inline">\((X_i, Y_i)\)</span> are Independently and Identically Distributed</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption #3: Large outliers are unlikely</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i>The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-1"><i class="fa fa-check"></i>R Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-2"><i class="fa fa-check"></i>R Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-3"><i class="fa fa-check"></i>R Simulation Study 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#testing-two-sided-hypotheses-concerning-beta_1">Testing Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#confidence-intervals-for-regression-coefficients"><i class="fa fa-check"></i>Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#r-simulation-study-5.1"><i class="fa fa-check"></i>R Simulation Study 5.1</a></li>
</ul></li>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#regression-when-x-is-a-binary-variable">Regression when <span class="math inline">\(X\)</span> is a Binary Variable</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#heteroskedasticity-and-homoskedasticity"><i class="fa fa-check"></i>Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#the-gauss-markov-theorem"><i class="fa fa-check"></i>The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#r-simulation-study-blue-estimator"><i class="fa fa-check"></i>R Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small">Using the <span class="math inline">\(t\)</span>-Statistic in Regression When the Sample Size Is Small</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#omitted-variable-bias"><i class="fa fa-check"></i>Omitted Variable Bias</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#the-multiple-regression-model"><i class="fa fa-check"></i>The Multiple Regression Model</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#measures-of-fit-in-multiple-regression"><i class="fa fa-check"></i>Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i>OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i>The Distribution of the OLS Estimators in Multiple Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the <span class="math inline">\(F\)</span>-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using R for Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lrwor" class="section level1">
<h1><span class="header-section-number">4</span> Linear Regression with One Regressor</h1>
<p>This chapter introduces the basics in linear regression and shows how to perform regression analysis in R. In linear regression, the aim is to model the relationship between a dependent variable <span class="math inline">\(Y\)</span> and one or more explanatory variables denoted as <span class="math inline">\(X_1, X_2, \dots, X_k\)</span>. Following the book we will focus on the concept of simple linear regression throughout the whole chapter. In simple linear regression, there is just one explanatory variable <span class="math inline">\(X_1\)</span>. <br> If for example a school cuts the class sizes by hiring new teachers, that is the school lowers the student-teacher ratios of their classes, <span class="math inline">\(X_1\)</span>, how would this affect the performance of the students involved in a standardized test,<span class="math inline">\(Y\)</span>? With linear regression we can not only examine whether the student-teacher ratio <em>does have</em> an impact on the test results but we can also learn about the <em>direction</em> and the <em>strength</em> of this effect.</p>
<p>To start with an easy example, consider the following combinations of average test score and the average student-teacher ratio in some fictional schools.</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
5
</th>
<th style="text-align:right;">
6
</th>
<th style="text-align:right;">
7
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
TestScore
</td>
<td style="text-align:right;">
680
</td>
<td style="text-align:right;">
640
</td>
<td style="text-align:right;">
670
</td>
<td style="text-align:right;">
660
</td>
<td style="text-align:right;">
630
</td>
<td style="text-align:right;">
660.0
</td>
<td style="text-align:right;">
635
</td>
</tr>
<tr>
<td style="text-align:left;">
STR
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
23.5
</td>
<td style="text-align:right;">
25
</td>
</tr>
</tbody>
</table>
<p>To work with these data in R we begin by creating two vectors: one for the student-teacher ratios (<code>STR</code>) and one for test scores (<code>TestScore</code>), both containing the data from the table above.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create sample data</span>
STR &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>, <span class="dv">17</span>, <span class="dv">19</span>, <span class="dv">20</span>, <span class="dv">22</span>, <span class="fl">23.5</span>, <span class="dv">25</span>)
TestScore &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">680</span>, <span class="dv">640</span>, <span class="dv">670</span>, <span class="dv">660</span>, <span class="dv">630</span>, <span class="dv">660</span>, <span class="dv">635</span>) 

<span class="co"># Print out sample data</span>
STR</code></pre></div>
<pre><code>## [1] 15.0 17.0 19.0 20.0 22.0 23.5 25.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TestScore</code></pre></div>
<pre><code>## [1] 680 640 670 660 630 660 635</code></pre>
</div>
<p>If we use a simple linear regression model, we assume that the true relationship between both variables can be represented by a straight line, formally</p>
<p><span class="math display">\[ Y = b \cdot X + a. \]</span></p>
<p>For now, let us suppose that the true function which relates test score and student-teacher ratio to each other is</p>
<p><span class="math display">\[TestScore = 713 - 3 \times STR.\]</span></p>
<p>If possible, it is always a good idea to visualize the data You work with in an appropriate way. For our purpose it is suitable to use the function <code>plot()</code> to produce a scatterplot with <code>STR</code> on the <span class="math inline">\(X\)</span>-axis and <code>TestScore</code> on the <span class="math inline">\(Y\)</span> axis. An easy way to do so is to call <code>plot(y_variable ~ x_variable)</code> whereby <code>y_variable</code> and <code>x_variable</code> are placeholders for the vectors of observations we want to plot. Furthermore, we might want to add the true relationship to the plot. To draw a straight line, R provides the function <code>abline()</code>. We just have to call this function with arguments <code>a</code> (representing the intercept) and <code>b</code> (representing the slope) after executing <code>plot()</code> in order to add the line to our scatterplot.</p>
<p>The following code reproduces figure 4.1 from the textbook.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a scatter plot of the data</span>
<span class="kw">plot</span>(TestScore <span class="op">~</span><span class="st"> </span>STR)
<span class="co"># add the true relationship to the plot</span>
<span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">713</span>, <span class="dt">b =</span> <span class="op">-</span><span class="dv">3</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-73-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We find that our line does not touch any of the points although we claimed that it represents the true relationship. The reason for this is the core problem of statistics, <em>randomness</em>. Most of the time there are influences which cannot be explained in a purely deterministic fashion and thus exacerbate finding the true relationship.</p>
<p>In order to account for these differences between observed data and the true relationship, we extend our model from above by an <em>error term</em> <span class="math inline">\(u\)</span> which covers these random effects. Put differently, <span class="math inline">\(u\)</span> accounts for all the differences between the true regression line and the actual observed data. Beside pure randomness, these deviations could also arise from measerment errors or, as will be discussed later, could be the consequence of leaving out other factors that are relevant in explaining the dependent variable.<br> Which other factor are plausible in our example? For one thing, the test scores might be driven by the teachers quality and the background of the students. It is also imaginable that in some classes, the students were lucky on the test days and thus achieved higher scores. For now, we will summarize such influences by an additive component:</p>
<p><span class="math display">\[ TestScore = \beta_0 + \beta_1 \times STR + \text{other factors} \]</span></p>
<p>Of course this idea is very general as it can be easily extented to other situations that can be described with a linear model. The basic linear regression function we will work with hence is</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + u_i. \]</span></p>
<p>Key Concept 4.1 summarizes the terminology of the simple linear regression model.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.1
</h3>
<h3 class="left">
Terminology for the Linear Regression Model with a Single Regressor
</h3>
<p>
<p>The linear regression model is</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_1 + u_i \]</span></p>
<p>where</p>
<ul>
<li>the subscript <span class="math inline">\(i\)</span> runs over the observations, <span class="math inline">\(i = 1\)</span>, …, <span class="math inline">\(n\)</span></li>
<li><span class="math inline">\(Y_i\)</span> is the <em>dependent variable</em>, the <em>regressand</em>, or simply the <em>left-hand variable</em></li>
<li><span class="math inline">\(X_i\)</span> is the <em>independent variable</em>, the <em>regressor</em>, or simply the <em>right-hand variable</em></li>
<li><span class="math inline">\(Y = \beta_0 + \beta_1 X\)</span> is the <em>population regression line</em> also called the <em>population regression function</em></li>
<li><span class="math inline">\(\beta_0\)</span> is the <em>intercept</em> of the population regression line</li>
<li><span class="math inline">\(\beta_1\)</span> is the <em>slope</em> of the population regression line</li>
<li><span class="math inline">\(u_i\)</span> is the <em>error term</em></li>
</ul>
</p>
</div>
<div id="estimating-the-coefficients-of-the-linear-regression-model" class="section level2 unnumbered">
<h2>Estimating the Coefficients of the Linear Regression Model</h2>
<p>In practice, the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span>of the population regression line are unknown. Therefore, we must employ data to estimate both unknown parameters. In the following a real world example will be used to demonstrate how this is achieved. We want to relate test scores to student-teacher ratios measured in californian schools. The test score is the district-wide average of reading and math scores for fifth graders. Again, the class size is measured as the number of students divided by the number of teachers (the student-teacher ratio). As for the data, the California School dataset (<code>CASchools</code>) comes with a R package called AER, an acronym for <a href="https://cran.r-project.org/web/packages/AER/AER.pdf">Applied Econometrics with R</a>. After installing the package with <code>install.packages(&quot;AER&quot;)</code> and attaching it with <code>library(&quot;AER&quot;)</code> the dataset can be loaded using the <code>data</code> function.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install the AER package (once)</span>
<span class="kw">install.packages</span>(<span class="st">&quot;AER&quot;</span>)

<span class="co"># load the AER package </span>
<span class="kw">library</span>(AER)   

<span class="co"># load the the data set in the workspace</span>
<span class="kw">data</span>(CASchools) </code></pre></div>
</div>
<p>Note that once a package has been installed it is available for use at further occasions when invoked with <code>library()</code> — there is no need to run <code>install.packages(&quot;...&quot;)</code> again!</p>
<p>For several reasons it is interesting to know what kind of object we are dealing with. <code>class(object_name)</code> returns the type (class) of an object. Depending on the class of an object some functions (such as <code>plot()</code> and <code>summary()</code>) behave differently.</p>
<p>Let us check the class of the object <code>CASchools</code>.</p>
<div class="undfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(CASchools)</code></pre></div>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
</div>
<p>It turns out that <code>CASchools</code> is of class <code>data.frame</code> which is a convienient format to work with.</p>
<p>With help of the function <code>head()</code> we get a first overview of our data. This function shows only the first 6 rows of the data set which prevents an overcrowded console output.</p>

<div class="rmdnote">
Press <tt>ctrl + L</tt> to clear the console. This command deletes any code that has been typed in and executed by You or printed to the console by R functions. Good news is: anything else is left untouched. You neither loose defined variables and alike nor the code history. It is still possible to recall previously executed R commands using the up and down keys. If You are working in RStudio, press <tt> ctrl + Up </tt> on Your keyboard (<tt>CMD + Up </tt> on a mac) to review a list of previously entered commands.
</div>

<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(CASchools)</code></pre></div>
<pre><code>##   district                          school  county grades students
## 1    75119              Sunol Glen Unified Alameda  KK-08      195
## 2    61499            Manzanita Elementary   Butte  KK-08      240
## 3    61549     Thermalito Union Elementary   Butte  KK-08     1550
## 4    61457 Golden Feather Union Elementary   Butte  KK-08      243
## 5    61523        Palermo Union Elementary   Butte  KK-08     1335
## 6    62042         Burrel Union Elementary  Fresno  KK-08      137
##   teachers calworks   lunch computer expenditure    income   english  read
## 1    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6
## 2    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5
## 3    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3
## 4    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9
## 5    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8
## 6     6.40  12.3188 86.9565       25    5580.147 10.415000 12.408759 605.7
##    math
## 1 690.0
## 2 661.9
## 3 650.9
## 4 643.5
## 5 639.9
## 6 605.4</code></pre>
</div>
<p>We find that the dataset consists of plenty of variables and most of them are numeric.</p>
<p>By the way: an alternative to <code>class()</code> and <code>head()</code> is <code>str()</code> which is deduced from ‘structure’ and gives a comprehensive overview of the object. Try this!</p>
<p>Turning back to <code>CASchools</code>, the two variables we are intersted in (i.e. average test score and the student-teacher ratio) are <em>not</em> included. However, it is possible to calculate both from the provided data. To obtain the student-teacher ratios, we simply divide the number of students by the number of teachers. The avarage test score is the arithmetic mean of the test score for reading and the score of the math test. The next code chunk shows how the two variables can be constructed and how they are appended to <code>CASchools</code> which is a data.frame.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute STR and append it to CASchools</span>
CASchools<span class="op">$</span>STR &lt;-<span class="st"> </span>CASchools<span class="op">$</span>students<span class="op">/</span>CASchools<span class="op">$</span>teachers 

<span class="co"># compute TestScore and append it to CASchools</span>
CASchools<span class="op">$</span>score &lt;-<span class="st"> </span>(CASchools<span class="op">$</span>read <span class="op">+</span><span class="st"> </span>CASchools<span class="op">$</span>math)<span class="op">/</span><span class="dv">2</span>     </code></pre></div>
</div>
<p>If we ran <code>head(CASchools)</code> again we would find the two variables of interest as additional columns named <code>STR</code> and <code>score</code> (check this!).</p>
<p>Table 4.1 from the text book summarizes the distribution of test scores and student-teacher ratios. There are several functions which can be used to produce similar results within R:</p>
<ul>
<li><p><code>mean()</code> (computes the arithmetic mean of the provided numbers)</p></li>
<li><p><code>sd()</code> (computes the sample standard deviation)</p></li>
<li><p><code>quantile()</code> (returns a vector of the specified quantiles for the data)</p></li>
</ul>
<p>The next code chunk shows how to achieve this. First, we compute summary statistics on the coloumns <code>STR</code> and <code>score</code> of <code>CASchools</code>. In order to have a nice display format we gather the computed measures in a <code>data.frame</code> object named <code>DistributionSummary</code>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute sample averages of STR and score</span>
avg_STR &lt;-<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>STR) 
avg_score &lt;-<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>score)

<span class="co"># compute sample standard deviations of STR and score</span>
sd_STR &lt;-<span class="st"> </span><span class="kw">sd</span>(CASchools<span class="op">$</span>STR) 
sd_score &lt;-<span class="st"> </span><span class="kw">sd</span>(CASchools<span class="op">$</span>score)

<span class="co"># set up a vector of percentiles and compute the quantiles </span>
quantiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.25</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.75</span>, <span class="fl">0.9</span>)
quant_STR &lt;-<span class="st"> </span><span class="kw">quantile</span>(CASchools<span class="op">$</span>STR, quantiles)
quant_score &lt;-<span class="st"> </span><span class="kw">quantile</span>(CASchools<span class="op">$</span>score, quantiles)

<span class="co"># gather everything in a data.frame </span>
DistributionSummary &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
                                  <span class="dt">Average =</span> <span class="kw">c</span>(avg_STR, avg_score), 
                                  <span class="dt">StandardDeviation =</span> <span class="kw">c</span>(sd_STR, sd_score), 
                                  <span class="dt">quantile =</span> <span class="kw">rbind</span>(quant_STR, quant_score)
                                  )

<span class="co"># print the summary to the console</span>
DistributionSummary</code></pre></div>
<pre><code>##               Average StandardDeviation quantile.10. quantile.25.
## quant_STR    19.64043          1.891812      17.3486     18.58236
## quant_score 654.15655         19.053347     630.3950    640.05000
##             quantile.40. quantile.50. quantile.60. quantile.75.
## quant_STR       19.26618     19.72321      20.0783     20.87181
## quant_score    649.06999    654.45000     659.4000    666.66249
##             quantile.90.
## quant_STR       21.86741
## quant_score    678.85999</code></pre>
</div>
<p>The standard distribution (<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html">the Base R package</a>) of R already contains a <code>summary</code> function which can be applied to objects of class <code>data.frame</code>. Type and execute <code>summary(STR)</code>!</p>
<p>As done for the sample data, we use <code>plot()</code> for a visual survey. This allows us to detect specific characteristics of our data, such as outliers which are hard to discover by looking at mere numbers. This time we add some additional arguments to the <code>plot()</code> function.</p>
<p>The first argument in our call of <code>plot()</code>, <code>score ~ STR</code>, is again a formula that states the dependent variable and the regressor. However, this time the two variables are not saved in seperate vectors but are columns of <code>CASchools</code>. Therefore, R would not find the variables without the argument <code>data</code> beeing correctly specified. <code>data</code> must be in accordance with the name of the <code>data.frame</code> to which the variables belong, in this case <code>CASchools</code>. Further arguments are used to change the appearance of the plot: while <code>main</code> adds a title, <code>xlab</code> and <code>ylab</code> are adding custom labels to both axes.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(score <span class="op">~</span><span class="st"> </span>STR, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">main =</span> <span class="st">&quot;Scatterplot of TestScore and STR&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;STR (X)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score (Y)&quot;</span>
)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-79-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The plot (figure 4.2 in the book) shows the scatterplot of all observations on student-teacher ratio and Test score. We see that the points are strongly scatterd and an apparent relationship cannot be detected by only looking at them. Yet it can be assumed that both variables are negatively correlated, that is we expect to observe lower test scores in bigger classes.</p>
<p>The function <code>cor()</code> (type and execute <code>?cor</code> for further info), can be used to compute the correlation between 2 <em>numerical</em> vectors.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(CASchools<span class="op">$</span>STR, CASchools<span class="op">$</span>score)</code></pre></div>
<pre><code>## [1] -0.2263627</code></pre>
</div>
<p>As the scatterplot already suggests, the correlation is negative but rather weak.</p>
<p>The task we are facing now is to find a line which fits best to the data. Of course we could simply stick with graphical inspection and correlation analysis and then select the best fitting line by eyeballing. However, this is pretty unscientific and prone to subjective perception: different students would draw different regression lines. On this account, we are interested in techniques that are more sophisticated. Such a technique is ordinary least squares (OLS) estimation.</p>
<div id="the-ordinary-least-squares-estimator" class="section level3 unnumbered">
<h3>The Ordinary Least Squares Estimator</h3>
<p>The OLS estimator chooses the regression coefficients such that the estimated regression line is as close as possible to the observed data points. Thereby closeness is measured by the sum of the squared mistakes made in predicting <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. Let <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> be some estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Then the sum of squared estimation mistakes can be expressed as</p>
<p><span class="math display">\[ \sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2. \]</span></p>
<p>The OLS estimator in the simple regression model is the pair of estimators for intercept and slope which minimizes the expression above. The derivation of the OLS estimators for both parameters are presented in Appendix 4.1 of the book. The results are summarized in Key Concept 4.2.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.2
</h3>
<h3 class="left">
The OLS Estimator, Predicted Values, and Residuals
</h3>
<p>
<p>The OLS estimators of the slope <span class="math inline">\(\beta_1\)</span> and the intercept <span class="math inline">\(\beta_0\)</span> in the simple linear regression model are</p>
<span class="math display">\[\begin{align}
  \hat\beta_1 &amp; = \frac{ \sum_{i = 1}^n (X_i - \overline{X})(Y_i - \overline{Y}) } { \sum_{i=1}^n (X_i - \overline{X})^2}  \\
  \\
  \hat\beta_0 &amp; =  \overline{Y} - \hat\beta_1 \overline{X} 
\end{align}\]</span>
<p>The OLS predicted values <span class="math inline">\(\widehat{Y}_i\)</span> and residuals <span class="math inline">\(\hat{u}_i\)</span> are</p>
<span class="math display">\[\begin{align}
  \widehat{Y}_i &amp; =  \hat\beta_0 + \hat\beta_1 X_i,\\
  \\
  \hat{u}_i &amp; =  Y_i - \widehat{Y}_i. 
\end{align}\]</span>
<p>The estimated intercept <span class="math inline">\(\hat{\beta}_0\)</span>, the slope parameter <span class="math inline">\(\hat{\beta}_1\)</span>, and the residuals <span class="math inline">\(\left(\hat{u}_i\right)\)</span> are computed from a sample of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(i\)</span>, <span class="math inline">\(...\)</span>, <span class="math inline">\(n\)</span>. These are <em>estimates</em> of the unkown true population intercept <span class="math inline">\(\left(\beta_0 \right)\)</span>, slope <span class="math inline">\(\left(\beta_1\right)\)</span>, and error term <span class="math inline">\((u_i)\)</span>.</p>
</p>
</div>
<p>We are aware that the results presented in Key Concept 4.2 are not very intuitive at first glance. The following interactice application aims to help You understand the mechanics of OLS. You can add observations by clicking into the coordinate system where the data are represented by points. If two or more observations are available, the application computes a regression line using OLS and some statistics which are displayed in the right panel. The results are updated as You add further observations to the left panel. A double-click resets the application i.e. all data are removed.</p>
<iframe height="410" width="900" frameborder="0" scrolling="no" src="SimpleRegression.html">
</iframe>
<p>There are many possible ways to compute <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> in R. For example, we could implement the formulas presented in Key Concept 4.2 with two of R’s most basic functions: <code>mean()</code> and <code>sum()</code>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(CASchools) <span class="co">#allows to use the variables contained in CASchools directly</span>

<span class="co"># compute beta_1 </span>
beta_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">sum</span>((STR <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(STR))<span class="op">*</span>(score <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(score))) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>((STR <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(STR))<span class="op">^</span><span class="dv">2</span>)

<span class="co"># compute beta_0</span>
beta_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(score) <span class="op">-</span><span class="st"> </span>beta_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(STR)

<span class="co"># print the results to the console</span>
beta_<span class="dv">1</span></code></pre></div>
<pre><code>## [1] -2.279808</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta_<span class="dv">0</span></code></pre></div>
<pre><code>## [1] 698.9329</code></pre>
</div>
<p>Of course there are also other and even more manual ways to do the same tasks. Luckily, OLS is one of the most widely-used estimation techniques. Being a statistical programming language, R already contains a built-in function named <code>lm()</code> (<strong>l</strong>inear <strong>m</strong>odel) which can be used to carry out regression analysis.</p>
<p>The first argument of the function to be specified is, similar as in <code>plot()</code>, the regression formula with the basic syntax <code>y ~ x</code> where <code>y</code> is the dependent variable and <code>x</code> the explanatory variable. The argument <code>data</code> sets the data set to be used in the regression. We now revisit the example from the book where the relationship between the test scores and the class sizes is analysed. The following code uses <code>lm()</code> to replicate the results presented in figure 4.3 in the book.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the model and assign the result to linear_model</span>
linear_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR, <span class="dt">data =</span> CASchools)

<span class="co"># Print the standard output of the estimated lm object to the console </span>
linear_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28</code></pre>
</div>
<p>Let us add the estimated regression line to the plot. This time we also enlarge ranges of both axes by setting the arguments <code>xlim</code> and <code>ylim</code>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data</span>
<span class="kw">plot</span>(score <span class="op">~</span><span class="st"> </span>STR, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">main =</span> <span class="st">&quot;Scatterplot of TestScore and STR&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;STR (X)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score (Y)&quot;</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">30</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">600</span>, <span class="dv">720</span>)
     )

<span class="co"># add the regression line</span>
<span class="kw">abline</span>(linear_model) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-83-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Did you notice that this time, we did not pass the intercept and slope parameters to <code>abline</code>? If you call <code>abline</code> on an object of class <code>lm</code> that only contains a single regressor variable, R draws the regression line automatically!</p>
</div>
</div>
<div id="measures-of-fit" class="section level2 unnumbered">
<h2>Measures of fit</h2>
<p>After estimating a linear regression, the question occurs how well that regression line describes the data. Are the observations tightly clustered arround the regression line, or are they spread out? Both, the <span class="math inline">\(R^2\)</span> and the <em>standard error of the regression</em> (<span class="math inline">\(SER\)</span>) measure how well the OLS Regression line fits the data.</p>
<div id="the-r2" class="section level3 unnumbered">
<h3>The <span class="math inline">\(R^2\)</span></h3>
<p>The <span class="math inline">\(R^2\)</span> is the fraction of sample variance of <span class="math inline">\(Y_i\)</span> that is explained by <span class="math inline">\(X_i\)</span>. Mathemethically, the <span class="math inline">\(R^2\)</span> can be written as the ratio of the explained sum of squares to the total sum of squares. The <em>explained sum of squares</em> (<span class="math inline">\(ESS\)</span>) is the sum of squared deviations of the predicted values, <span class="math inline">\(\hat{Y_i}\)</span>, from the average of the <span class="math inline">\(Y_i\)</span>. The <em>total sum of squares</em> (<span class="math inline">\(TSS\)</span>) is the sum of squared deviations of the <span class="math inline">\(Y_i\)</span> from their average.</p>
<span class="math display">\[\begin{align}
  ESS &amp; =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2   \\
  \\
  TSS &amp; =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2   \\
  \\
  R^2 &amp; = \frac{ESS}{TSS}
\end{align}\]</span>
<p>Since <span class="math inline">\(TSS = ESS + SSR\)</span> we can also write</p>
<p><span class="math display">\[ R^2 = 1- \frac{SSR}{TSS} \]</span></p>
<p>where <span class="math inline">\(SSR\)</span> is the sum of squared residuals, a measure for the errors made when predicting the <span class="math inline">\(Y\)</span> by <span class="math inline">\(X\)</span>. The <span class="math inline">\(SSR\)</span> is defined as</p>
<p><span class="math display">\[ SSR = \sum_{i=1}^n \hat{u}_i^2. \]</span></p>
<p><span class="math inline">\(R^2\)</span> lies between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. It is easy to see that a perfect fit, i.e. no errors made when fitting the regression line, implies <span class="math inline">\(R^2 = 1\)</span> since then we have <span class="math inline">\(SSR=0\)</span>. On the contrary, if our estimated regression line does not explain any variation in the <span class="math inline">\(Y_i\)</span>, we have <span class="math inline">\(ESS=0\)</span> and consequently <span class="math inline">\(R^2=0\)</span>.</p>
</div>
<div id="standard-error-of-the-regression" class="section level3 unnumbered">
<h3>Standard Error of the Regression</h3>
<p>The <em>Standard Error of the Regression</em> (<span class="math inline">\(SER\)</span>) is an estimator of the standard deviation of the regression error <span class="math inline">\(\hat{u}_i\)</span>. As such it measure the magnitude of a typical deviation from the regression, i.e. the magnitude of a typical regression error.</p>
<p><span class="math display">\[ SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{where} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2} \]</span></p>
<p>Remember that the <span class="math inline">\(u_i\)</span> are <em>unobserved</em>. That is why we use their estimated counterparts, the residuals <span class="math inline">\(\hat{u}_i\)</span> instead. See chapter 4.3 of the book for a more detailed comment on the <span class="math inline">\(SER\)</span>.</p>
</div>
<div id="application-to-the-test-score-data" class="section level3 unnumbered">
<h3>Application to the Test Score Data</h3>
<p>Both measures of fit can be obtained by using the function <code>summary()</code> with the <code>lm</code> object provided as the only argument. Whereas the function <code>lm()</code> only prints out the estimated coefficients to the console, <code>summary</code> provides additional predefined information such as the regression’s <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(SER\)</span>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_summary &lt;-<span class="st"> </span><span class="kw">summary</span>(linear_model)
mod_summary</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.727 -14.251   0.483  12.822  48.540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 698.9329     9.4675  73.825  &lt; 2e-16 ***
## STR          -2.2798     0.4798  -4.751 2.78e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.58 on 418 degrees of freedom
## Multiple R-squared:  0.05124,    Adjusted R-squared:  0.04897 
## F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06</code></pre>
</div>
<p>The <span class="math inline">\(R^2\)</span> in the output is called ‘Multiple R-squared’ and has the value <span class="math inline">\(0.051\)</span>. Hence, <span class="math inline">\(5.1 \%\)</span> of the variance of the dependent variable <span class="math inline">\(score\)</span> is explained by the explanatory variable <span class="math inline">\(STR\)</span>. That is the regression explains some of the variance but much of the variation in test scores remains unexplained (cf. figure 4.3 in the book).</p>
<p>The <span class="math inline">\(SER\)</span> is called ‘Residual standard error’ and takes the value <span class="math inline">\(18.58\)</span>. The unit of the <span class="math inline">\(SER\)</span> is the same as the unit of the dependent variable. In our context we can interpret the value as follows: on average the deviation of the actual achieved test score and the regression line is <span class="math inline">\(18.58\)</span> points.</p>
<p>Now, let us check whether the <code>summary()</code> function uses the same definitions for <span class="math inline">\(R^2\)</span> and <span class="math inline">\(SER\)</span> as we do by computing them manually.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute R^2 manually</span>
SSR &lt;-<span class="st"> </span><span class="kw">sum</span>(mod_summary<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)
TSS &lt;-<span class="st"> </span><span class="kw">sum</span>((score <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(score))<span class="op">^</span><span class="dv">2</span>)
R2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>SSR<span class="op">/</span>TSS

<span class="co"># print the value to the console</span>
R2</code></pre></div>
<pre><code>## [1] 0.05124009</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute SER manually</span>
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(CASchools)
SER &lt;-<span class="st"> </span><span class="kw">sqrt</span>(SSR <span class="op">/</span><span class="st"> </span>(n<span class="op">-</span><span class="dv">2</span>))

<span class="co"># print the value to the console</span>
SER</code></pre></div>
<pre><code>## [1] 18.58097</code></pre>
<p>We find that the results coincide. Note that the values provided by <code>summary()</code> are rounded to two decimal places. Can You Do this using R?</p>
</div>
</div>
<div id="the-least-squares-assumptions" class="section level2 unnumbered">
<h2>The Least Squares Assumptions</h2>
<p>OLS performs well under a quite broad variety of different circumstances. However, there are some assumptions which are posed on the data which need to be satisfied in order to achieve reliable results.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.3
</h3>
<h3 class="left">
The Least Squares Assumptions
</h3>
<p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i \text{, } i = 1, ...,n\]</span> where</p>
<ol style="list-style-type: decimal">
<li>The error term <span class="math inline">\(u_i\)</span> has conditional mean zero given <span class="math inline">\(X_i\)</span>: <span class="math inline">\(E(u_i|X_i) = 0\)</span></li>
<li><span class="math inline">\((X_i,Y_i), i = 1,...,n\)</span> are independent and identically distributed (i.i.d.) draws from their joint distribution</li>
<li>Large outliers are unlikely: <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> have nonzero finite fourth moments</li>
</ol>
</p>
</div>
<div id="assumption-1-the-error-term-has-conditional-mean-of-zero" class="section level3 unnumbered">
<h3>Assumption #1: The Error Term has Conditional Mean of Zero</h3>
<p>This means that no matter which value we choose for <span class="math inline">\(X\)</span>, the error term <span class="math inline">\(u\)</span> must not show any systematic pattern and must have a mean of <span class="math inline">\(0\)</span>. Consider the case that <span class="math inline">\(E(u) = 0\)</span> but for low and high values of <span class="math inline">\(X\)</span>, the error term tends to be positive and for midrange values of <span class="math inline">\(X\)</span> the error tends to be negative. We can use R to construct such an example. To do so we generate our own data using R’s build in random number generators.</p>
<p>We will use the following functions You should be familiar with:</p>
<ul>
<li><code>runif()</code> (generates uniformly distributed random numbers)</li>
<li><code>rnorm()</code> (generates nomally distributed random numbers)</li>
<li><code>predict()</code> (does predictions based on the results of model fitting functions like <code>lm()</code>)</li>
<li><code>lines()</code> (adds line segments to an existing plot)</li>
</ul>
<p>We start by creating a vector containing values that are randomly scattered on the domain <span class="math inline">\([-5,5]\)</span>. For our example we decide to generate uniformly distributed random numbers. This can be done with the function <code>runif()</code>. We also need to simulate the error term. For this we generate normally distributed random numbers with a mean equal to <span class="math inline">\(0\)</span> and a variance of <span class="math inline">\(1\)</span> using <code>rnorm()</code>. The <span class="math inline">\(Y\)</span> values are obtained as a quadratic function of the <span class="math inline">\(X\)</span> values and the error.<br> After generating the data we estimate both a simple regression model and a quadratic model that also includes the regressor <span class="math inline">\(X^2\)</span>. Finally, we plot the simulated data and add a the estimated regression line of a simple regression model as well as the predictions made with a quadratic model to compare the fit graphically.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set a random seed to make the results reproducible</span>
<span class="kw">set.seed</span>(<span class="dv">321</span>)

<span class="co"># simulate the data </span>
X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>, <span class="dt">min =</span> <span class="op">-</span><span class="dv">5</span>, <span class="dt">max =</span> <span class="dv">5</span>)
u &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>, <span class="dt">sd =</span> <span class="dv">5</span>)  
## the true relation  
Y &lt;-<span class="st"> </span>X<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>X <span class="op">+</span><span class="st"> </span>u                

<span class="co"># estimate a simple regression model </span>
mod_simple &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)

<span class="co"># predict using a quadratic model </span>
prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X <span class="op">+</span><span class="st">  </span><span class="kw">I</span>(X<span class="op">^</span><span class="dv">2</span>)), <span class="kw">data.frame</span>(<span class="dt">X =</span> <span class="kw">sort</span>(X)))

<span class="co"># plot the results</span>
<span class="kw">plot</span>(Y <span class="op">~</span><span class="st"> </span>X)
<span class="kw">abline</span>(mod_simple, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">lines</span>(<span class="kw">sort</span>(X), prediction)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-86-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>This shows what is meant by <span class="math inline">\(E(u_i|X_i) = 0\)</span>:</p>
<p>Using the quadratic model (represented by the black curve) we see that there are no systematic deviations of the observation from the predicted relation. It is credible that the assumption is not violated when such a model is employed. However, using a simple linear regression model we see that the assumption is probably violated as <span class="math inline">\(E(u_i|X_i)\)</span> varies with the <span class="math inline">\(X_i\)</span>.</p>
</div>
<div id="assumption-2-all-x_i-y_i-are-independently-and-identically-distributed" class="section level3 unnumbered">
<h3>Assumption #2: All <span class="math inline">\((X_i, Y_i)\)</span> are Independently and Identically Distributed</h3>
<p>Most common sampling schemes used when collecting data from populations produce i.i.d. samples. For example, we could use R’s random number generator to randomly select student IDs from a university’s enrollment list and record age <span class="math inline">\(X\)</span> and earnings <span class="math inline">\(Y\)</span> of the corresponding students. This is a typical example of simple random sampling and ensures that all the <span class="math inline">\((X_i,Y_i)\)</span> are drawn randomly from the same population.</p>
<p>A prominent example where the i.i.d. assumption is not fulfilled is time series data where we have observations on the same unit over time. For example, take <span class="math inline">\(X\)</span> as the number of workers employed by a production company over the course of time. Due to technological change, the company makes job cuts periodically but there are also some non-deterministic influences that relate to economics, politics and alike. Using R we can simulate such a process and plot it.</p>
<p>We start the series with a total of 5000 workers and simulate the reduction of employment with a simple autoregressive process that exhibits a downward trend and has normal distributed errors:<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display">\[ employment_t = 0.98 \cdot employment_{t-1} + u_t \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)

<span class="co"># initialize the employment vector</span>
X &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5000</span>,<span class="kw">rep</span>(<span class="ot">NA</span>,<span class="dv">99</span>))

<span class="co"># generate a date vector</span>
Date &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">as.Date</span>(<span class="st">&quot;1951/1/1&quot;</span>), <span class="kw">as.Date</span>(<span class="st">&quot;2050/1/1&quot;</span>), <span class="st">&quot;years&quot;</span>)

<span class="co"># generate time series observations with random influences</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">100</span>) X[i] &lt;-<span class="st"> </span><span class="fl">0.98</span><span class="op">*</span>X[i<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">sd=</span><span class="dv">200</span>)

<span class="co">#plot the results</span>
<span class="kw">plot</span>(Date, X, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Workers&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Time (t)&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-87-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It is evident that the observations on <span class="math inline">\(X\)</span> cannot be independnet in this example: the level of today’s employment is correlated with tomorrows employment level. Thus, the i.i.d. assumption is violated for <span class="math inline">\(X\)</span>.</p>
</div>
<div id="assumption-3-large-outliers-are-unlikely" class="section level3 unnumbered">
<h3>Assumption #3: Large outliers are unlikely</h3>
<p>It is easy to come up with situations where extreme observations, i.e. observations that deviate considerably from the usual range of the data, may occur. Such observations are called outliers. Technically speaking, assumption #3 requires that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a finite kurtosis.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Common cases where we want to exclude or (if possible) correct such outliers is when they are apperently typos, conversion errors or measurement errors. Even if it seems that extreme observations have been recorded correctly, it is advisable to exclude them before estimating a model since OLS suffers from <em>sensitivity to outliers</em>.</p>
<p>What does this mean? One can show that extreme observation receive heavy weighting in the computation done with OLS. Therefore, outliers can lead to strongly distorted estimates of regression coefficient. To get a better impression of this, consider the following application where we have placed some sample data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> which are highly correlated. The relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> seems to be explained pretty good by the plotted regression line: all of the blue dots lie close to the red line and we have <span class="math inline">\(R^2=0.92\)</span>.</p>
<p>Now go ahead and add a further observation at, say, <span class="math inline">\((18,2)\)</span>. This clearly is an outlier. The result is quite striking: the estimated regression line differs greatly from the one we adjudged to fit the data well. The slope is heavily downward biased and <span class="math inline">\(R^2\)</span> decreased to a mere <span class="math inline">\(29\%\)</span>! <br> Double-click inside the coordinate system to reset the app. Feel free to experiment. Choose different coordinates for the outlier or add additional ones.</p>
<iframe height="410" width="900" frameborder="0" scrolling="no" src="Outlier.html">
</iframe>
<p>The following code roughly reproduces what is shown in figure 4.5 in the book. As done above we use sample data generated using R’s random number functions <code>rnorm()</code> and <code>runif()</code>. We estimate simple regression models based on the original data set and a modified set where one observation is change to be an outlier and plot the results. In order to understand the complete code You should be familiar with the function <code>sort()</code> which sorts the entries of a numeric vector in ascending order.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># generate the data</span>
X &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">runif</span>(<span class="dv">10</span>, <span class="dt">min =</span> <span class="dv">30</span>, <span class="dt">max =</span> <span class="dv">70</span> ))
Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10</span> , <span class="dt">mean =</span> <span class="dv">200</span>, <span class="dt">sd =</span> <span class="dv">50</span>)
Y[<span class="dv">9</span>] &lt;-<span class="st"> </span><span class="dv">2000</span>

<span class="co"># fit model with outlier</span>
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)

<span class="co"># fit model without outlier</span>
fitWithoutOutlier &lt;-<span class="st"> </span><span class="kw">lm</span>(Y[<span class="op">-</span><span class="dv">9</span>] <span class="op">~</span><span class="st"> </span>X[<span class="op">-</span><span class="dv">9</span>])

<span class="co"># plot the results</span>
<span class="kw">plot</span>(Y <span class="op">~</span><span class="st"> </span>X)
<span class="kw">abline</span>(fit)
<span class="kw">abline</span>(fitWithoutOutlier, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-88-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="tsdotoe" class="section level2 unnumbered">
<h2>The Sampling Distribution of the OLS Estimator</h2>
<p>Because the OLS estimators <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are computed from a randomly drawn sample, the estimators themselves are random variables with a probability distribution — the so-called sampling distribution of the estimators — which describes the values they could take over different random samples. Although the sampling distribution of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> can be complicated when the sample size is small and generally differs with the number of observation, <span class="math inline">\(n\)</span>, it is possible to make certain statements about it that hold for all <span class="math inline">\(n\)</span>. In particular <span class="math display">\[ E(\hat{\beta_0}) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta_1}) = \beta_1,\]</span> that is, <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the true parameters. If the sample is sufficiently large, by the central limit theorem the <em>joint</em> sampling distribution of the estimators is well approximated by the bivariate normal distribution (<a href="#mjx-eqn-2.1">2.1</a>). This implies that the marginal distributions are also normal in large samples. Core facts on the large-sample distribution of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are presented in Key Concept 4.4.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.4
</h3>
<h3 class="left">
Large Sample Distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>
</h3>
<p>
<p>If the least squares assumptions in Key Concept 4.3 hold, then in large samples <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> have a jointly normal sampling distribution. The large sample normal distribution of <span class="math inline">\(\hat\beta_1\)</span> is <span class="math inline">\(N(\beta_1, \sigma^2_{\hat\beta_1})\)</span>, where the variance of the distribution, <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span>, is</p>
<p><span class="math display">\[ \sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2} \tag{4.1}. \]</span></p>
<p>The large sample normal distribution of <span class="math inline">\(\hat\beta_0\)</span> is <span class="math inline">\(N(\beta_0, \sigma^2_{\hat\beta_0})\)</span>, where</p>
<p><span class="math display">\[ \sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{4.2} \]</span></p>
</p>
</div>
<iframe height="470" width="800" frameborder="0" scrolling="no" src="SmallSampleDIstReg.html">
</iframe>
<div id="r-simulation-study-1" class="section level3 unnumbered">
<h3>R Simulation Study 1</h3>
<p>Whether Key Koncept 4.4 really holds can be verified using R. First we build our own population of <span class="math inline">\(100000\)</span> observations in total. To do this we need values for our independent variable <span class="math inline">\(X\)</span>, for the error term <span class="math inline">\(u\)</span>, and the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. With all this combined in a simple regression model, we can compute our dependent variable <span class="math inline">\(Y\)</span>. <br> In our example we generate the numbers <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i = 1\)</span>, … ,<span class="math inline">\(100000\)</span> by drawing a random sample from a uniform distribution on the interval <span class="math inline">\([0,20]\)</span>. The realisations of the error terms <span class="math inline">\(u_i\)</span> are drawn from a standard normal distribution with parameters <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 100\)</span> (note that <code>rnorm()</code> requires <span class="math inline">\(\sigma\)</span> as input for the argument <code>sd</code>, see <code>?rnorm</code>). Furthermore we chose <span class="math inline">\(\beta_0 = -2\)</span> and <span class="math inline">\(\beta_1 = 3.5\)</span> so the true model is</p>
<p><span class="math display">\[ Y_i = -2 + 3.5 \cdot X_i. \]</span></p>
<p>Finally, we store the results in a data.frame.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate data</span>
N &lt;-<span class="st"> </span><span class="dv">100000</span>
X &lt;-<span class="st"> </span><span class="kw">runif</span>(N, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">20</span>)
u &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">sd =</span> <span class="dv">10</span>)

<span class="co"># population regression</span>
Y &lt;-<span class="st"> </span><span class="op">-</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">3.5</span> <span class="op">*</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>u
population &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X, Y)</code></pre></div>
</div>
<p>From now on we will consider the previously generated data as the true population (which of course would be <em>unknown</em> in a real world application, otherwise there would not be a reason to do draw a random sample in the first place). The knowledge about the true population and the true relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> can be used to verify the statements made in Key Concept 4.4.</p>
<p>First, let us calculate the true variances <span class="math inline">\(\sigma^2_\hat{\beta_0}\)</span> and <span class="math inline">\(\sigma^2_\hat{\beta_1}\)</span> for a randomly drawn sample of size <span class="math inline">\(n = 100\)</span>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set sample size</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>

<span class="co"># compute the variance of hat_beta_0</span>
H_i &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X) <span class="op">/</span><span class="st"> </span><span class="kw">mean</span>(X<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>X
var_b0 &lt;-<span class="st"> </span><span class="kw">var</span>(H_i <span class="op">*</span><span class="st"> </span>u) <span class="op">/</span><span class="st"> </span>(n <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(H_i<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span> )

<span class="co"># compute the variance of hat_beta_1</span>
var_b1 &lt;-<span class="st"> </span><span class="kw">var</span>( ( X <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X) ) <span class="op">*</span><span class="st"> </span>u ) <span class="op">/</span><span class="st"> </span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="kw">var</span>(X)<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print variances to the console</span>
var_b0</code></pre></div>
<pre><code>## [1] 4.045066</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">var_b1</code></pre></div>
<pre><code>## [1] 0.03018694</code></pre>
<p>Now let us assume that we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and that it is not possible to observe the whole population. However, we can observe a random sample of <span class="math inline">\(n\)</span> observations. Then, it would not be possible to compute the true parameters but we could obtain estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> from the sample data using OLS. However, we know that these estimates are outcomes of random variables themselves since the observations are randomly sampled from the population. Key Concept 4.4. describes their distributions for large <span class="math inline">\(n\)</span>. When drawing a single sample of size <span class="math inline">\(n\)</span> it is not possible to make any statement about these distributions. Things change if we repeat the sampling scheme many times and compute the estimates for each sample: using such a procedure we simulate outcomes of the respective distributions.</p>
<p>To achieve this in R, we employ the following approach:</p>
<ul>
<li>We assign the number of repetitions, say <span class="math inline">\(10000\)</span>, to <code>reps</code>. Then we initialize a matrix <code>fit</code> were the estimates obtained in each sampling iteration shall be stored row-wise. Thus <code>fit</code> has to be an array of dimensions <code>reps</code><span class="math inline">\(\times2\)</span>.</li>
<li>In the next step we draw <code>reps</code> random sample of size <code>n</code> from the population and obtain the OLS estimates for each sample. The results are stored as row entries in the outcome matrix <code>fit</code>. This is done using a <code>for()</code> loop.</li>
<li>At last, we estimate variances of both coefficient estimators using the sampled outcomes and plot histograms of the latter. We also add plot of the density functions belonging to the distributions that follow from Key Concept 4.4. The function <code>bquote()</code> is used to obtain math expressions in the titels and labels of both plots. See <code>?bquote</code>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set repetitions and sample size</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>
reps &lt;-<span class="st"> </span><span class="dv">10000</span>

<span class="co"># initialize the matrix of outcomes</span>
fit &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> reps)

<span class="co"># loop sampling and estimating of the coefficients</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps){
 sample &lt;-<span class="st"> </span>population[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, n),]
 fit[i, ] &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> sample)<span class="op">$</span>coefficients
}

<span class="co"># compute variance estimates using outcomes</span>
<span class="kw">var</span>(fit[ ,<span class="dv">1</span>])</code></pre></div>
<pre><code>## [1] 4.057089</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(fit[ ,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 0.03021784</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot histograms of beta_0 estimates</span>
<span class="kw">hist</span>(fit[ ,<span class="dv">1</span>], 
     <span class="dt">main =</span> <span class="kw">bquote</span>(The <span class="op">~</span><span class="st"> </span>Distribution  <span class="op">~</span><span class="st"> </span>of <span class="op">~</span><span class="st"> </span><span class="dv">10000</span> <span class="op">~</span><span class="st"> </span>beta[<span class="dv">0</span>] <span class="op">~</span><span class="st"> </span>Estimates), 
     <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">0</span>]), 
     <span class="dt">freq =</span> F)
<span class="co"># add true distribution to plot</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x,<span class="op">-</span><span class="dv">2</span>,<span class="kw">sqrt</span>(var_b0)), <span class="dt">add =</span> T, <span class="dt">col=</span><span class="st">&quot;darkred&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-92-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot histograms of beta_1 estimates</span>
<span class="kw">hist</span>(fit[ ,<span class="dv">2</span>], 
     <span class="dt">main =</span> <span class="kw">bquote</span>(The <span class="op">~</span><span class="st"> </span>Distribution  <span class="op">~</span><span class="st"> </span>of <span class="op">~</span><span class="st"> </span><span class="dv">10000</span> <span class="op">~</span><span class="st"> </span>beta[<span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>Estimates), 
     <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]), 
     <span class="dt">freq =</span> F)

<span class="co"># add true distribution to plot</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x,<span class="fl">3.5</span>,<span class="kw">sqrt</span>(var_b1)), <span class="dt">add =</span> T, <span class="dt">col=</span><span class="st">&quot;darkred&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-92-2.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We are now able to say the following: first, our variance estimates are in favour of the claims made in Key Concept 4.4 since they come close to the computed theoretical values. Second, the histograms suggest that the estimators distributions indeed follow normal distributions which can be fairly approximated by the respective normal distributions stated in Key Concept 4.4.</p>
</div>
<div id="r-simulation-study-2" class="section level3 unnumbered">
<h3>R Simulation Study 2</h3>
<p>A further result implied by Key Concept 4.4 is that both estimators are consistent i.e. they converge in probability to their true value. This is since their variances converge to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\)</span> increases. We can check this by repeating the simulation above for an increasing sequence of sample sizes. This means we no langer assign the sample size but a <em>vector</em> of sample sizes: <code>n &lt;- c(...)</code>. <br> Let us look at the distributions of <span class="math inline">\(\beta_1\)</span>. The idea here is to add an additional call of <code>for()</code> to the code. This is done in order to loop over the vector of sample sizes <code>n</code>. For each of the sample sizes we carry out the same simulation as before but plot a density estimate for the outcomes of each iteration over <code>n</code>. Notice that we have to change <code>n</code> to <code>n[j]</code> in the inner loop to ensure that the <code>j</code><span class="math inline">\(^{th}\)</span> element of <code>n</code> is used. In the simulation, we use sample sizes <span class="math inline">\(100, 250, 1000\)</span> and <span class="math inline">\(3000\)</span>. Consequently we have a total of four distinct simulations using different sample sizes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># set repetitions and the vector of sample sizes</span>
reps &lt;-<span class="st"> </span><span class="dv">1000</span>
n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>, <span class="dv">250</span>, <span class="dv">1000</span>, <span class="dv">3000</span>)

<span class="co"># initialize the matrix of outcomes</span>
fit &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> reps)

<span class="co"># devide the plot panel in a 2-by-2 array</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))

#### Loop sampling and plotting ####

<span class="co"># outer loop over n</span>
<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n)) {
  
  <span class="co"># inner loop: sampling and estimating of the coefficients</span>
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps){
    sample &lt;-<span class="st"> </span>population[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, n[j]), ]
    fit[i, ] &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> sample)<span class="op">$</span>coefficients
  }
  
  <span class="co"># draw density estimates</span>
  <span class="kw">plot</span>(<span class="kw">density</span>(fit[,<span class="dv">2</span>]), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">2.5</span>,<span class="fl">4.5</span>), <span class="dt">col=</span>j, 
       <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;n=&quot;</span>, n[j]), <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]))
  
}</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-93-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We find that, as <span class="math inline">\(n\)</span> increases, the distribution of <span class="math inline">\(\hat\beta_1\)</span> concentrates around its mean, i.e. its variance decreases. Put differently, the likelihood of observering estimates close to the true value of <span class="math inline">\(\beta_1 = 3.5\)</span> grows as we increase the sample size. The same behaviour could be observed if we would analyze the distribution of <span class="math inline">\(\hat\beta_0\)</span> instead.</p>
</div>
<div id="r-simulation-study-3" class="section level3 unnumbered">
<h3>R Simulation Study 3</h3>
<p>Furthermore, (<a href="#mjx-eqn-4.1">4.1</a>) reveals that the variance of the OLS estimator for <span class="math inline">\(\beta_1\)</span> decreases as the variance of the <span class="math inline">\(X_i\)</span> increases. In other words, as we increase the amount of information provided by the regressor, that is increasing <span class="math inline">\(Var(X)\)</span>, which is used to estimate <span class="math inline">\(\beta_1\)</span>, we are more confident that the estimate is close to the true value (i.e. <span class="math inline">\(Var(\hat\beta_1)\)</span> decreases).<br> We can visualize this by reproducing figure 4.6 from the book. To do this, we sample <span class="math inline">\(100\)</span> observations <span class="math inline">\((X,Y)\)</span> from a bivariate normal distribution with</p>
<p><span class="math display">\[E(X)=E(Y)=5,\]</span> <span class="math display">\[Var(X)=Var(Y)=5\]</span> and <span class="math display">\[Cov(X,Y)=4.\]</span></p>
<p>Formally, this is written down as</p>
<span class="math display">\[\begin{align}
  \begin{pmatrix}
    X \\
    Y \\
  \end{pmatrix}
  \overset{i.i.d.}{\sim} &amp; \ \mathcal{N} 
  \left[
    \begin{pmatrix}
      5 \\
      5 \\
    \end{pmatrix}, \ 
    \begin{pmatrix}
      5 &amp; 4 \\
      4 &amp; 5 \\
    \end{pmatrix}
  \right]. \tag{4.3}
\end{align}\]</span>
<p>To carry out the random sampling, we make use of the function <code>mvtnorm()</code> from the package <code>MASS</code> which allows to draw random samples from multivariate normal distributions, see <code>?mvtnorm</code>. Next, we use the <code>subset()</code> function to split the sample into two subsets such that the first set, <code>set1</code>, consists of observations that fulfill the condition <span class="math inline">\(\lvert X - \overline{X} \rvert &gt; 1\)</span> and the second set, <code>set2</code>, includes the remainder of the sample. We then plot both sets and use different colors to make them distinguishable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the MASS package</span>
<span class="kw">library</span>(MASS)

<span class="co"># set random seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">4</span>)

<span class="co"># simulate bivarite normal data</span>
bvndata &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">100</span>, 
                <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>), 
                <span class="dt">Sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">4</span>),<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">5</span>))
                ) 

<span class="co"># assign column names / convert to data.frame</span>
<span class="kw">colnames</span>(bvndata) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>,<span class="st">&quot;Y&quot;</span>)
bvndata &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(bvndata)

<span class="co"># subset the data</span>
set1 &lt;-<span class="st"> </span><span class="kw">subset</span>(bvndata, <span class="kw">abs</span>(<span class="kw">mean</span>(X) <span class="op">-</span><span class="st"> </span>X) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>)
set2 &lt;-<span class="st"> </span><span class="kw">subset</span>(bvndata, <span class="kw">abs</span>(<span class="kw">mean</span>(X) <span class="op">-</span><span class="st"> </span>X) <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>)

<span class="co"># plot both data sets</span>
<span class="kw">plot</span>(set1, <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>)
<span class="kw">points</span>(set2, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-94-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It is clear that observations that are close to the sample average of the <span class="math inline">\(X_i\)</span> have less variance than those that are farther away. Now, if we were to draw a line as accurately as possible through either of the two sets it is obvious that choosing the observations indicated by the black dots, i.e. using the set of observations which has larger variance than the blue ones, would result in a more precise line. Now, let us use OLS to estimate and draw the regression lines for both sets of observations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate both regression lines</span>
lm.set1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> set1)
lm.set2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> set2)

<span class="co"># add both lines to the plot</span>
<span class="kw">abline</span>(lm.set1, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>)
<span class="kw">abline</span>(lm.set2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-95-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Evidently, the green regression line does far better in describing data sampled from the bivariate normal distribution stated in (<a href="#mjx-eqn-4.3">4.3</a>) than the red line. This is a nice example why we are interested in a high variance of the regressor <span class="math inline">\(X\)</span>: more variance in the <span class="math inline">\(X_i\)</span> means more information from which the precision of the estimation benefits.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>See chapter 14 in the book for more on autoregressive processes and time series analysis in general.<a href="lrwor.html#fnref1">↩</a></p></li>
<li id="fn2"><p>See chapter 4.4 in the book.<a href="lrwor.html#fnref2">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-review-of-statistics-using-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-ch4.Rmd",
"text": "Edit"
},
"download": ["URFITE.pdf", "URFITE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
