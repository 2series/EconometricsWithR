\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Using R for Introduction to Econometrics},
            pdfauthor={Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Using R for Introduction to Econometrics}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2018-05-29}

\usepackage{booktabs}
%\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{rotating, graphicx}
\usepackage{multirow}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\makeatletter % undo the wrong changes made by mathspec
\let\RequirePackage\original@RequirePackage
\let\usepackage\RequirePackage
\makeatother

\newenvironment{rmdknit}
    {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    }
    {
    \\\\\hline
    \end{tabular}
    \end{center}
    }

\newenvironment{rmdnote}
    {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    }
    {
    \\\\\hline
    \end{tabular}
    \end{center}
    }
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Introduction}\label{introduction}

\begin{center}\includegraphics[width=0.45\linewidth]{images/URFITE_logo} \end{center}

\noindent\rule{\textwidth}{1pt}

The interest in the freely available statistical programming language
and software environment \texttt{R} is soaring. By the time we wrote
first drafts for this project, more than 11000 addons (many of them
providing cutting-edge methods) were made available on the Comprehensive
\texttt{R} Archive Network (\href{https://cran.r-project.org/}{CRAN}),
an extensive network of FTP servers around the world that store
identical and up-to-date versions of \texttt{R} code and its
documentation. \texttt{R} dominates other (commercial) software for
statistical computing in most fields of research in applied statistics
but the benefits of it being freely available, open source and having a
large and constantly growing community of users that contribute to CRAN
render \texttt{R} more and more appealing for empirical economists and
econometricians.

A striking advantage of using \texttt{R} in econometrics courses is that
it enables students to explicitly document their analysis step-by-step
such that it is easy to update and to expand. This allows to re-use code
for similar applications with different data. Furthermore, \texttt{R}
programs are fully reproducible which makes it straightforward for
others to comprehend and validate the results.

Over the recent years, \texttt{R} has thus become an integral part of
the curricula of econometrics classes we teach at University of
Duisburg-Essen. In some sense, learning to code is comparable to
learning a foreign language and continuous practice is essential for the
learning success. Of course, presenting bare \texttt{R} code chunks on
slides has mostly a deterring effect for the students to engage with
programming on their own. This is why we offer tutorials where both
econometric theory and its applications using \texttt{R} are introduced,
for some time now. As for accompanying literature, there are some
excellent books that deal with \texttt{R} and its applications to
econometrics like Kleiber \& Zeilis (2008) and Hetekar (2010). However,
we have found that these works are somewhat difficult to access,
especially for undergraduate students in economics having little
understanding of econometric methods and predominantly no experience in
programming at all. Consequently, we have started to compile a
collection of reproducible reports for use in class. These reports
provide guidance on how to implement selected applications from the
textbook \emph{Introduction to Econometrics} by Stock \& Watson (2014)
which serves as a basis for the lecture and the accompanying tutorials.
The process has been facilitated considerably with the release of
\texttt{knitr} (2018) in 2012. \texttt{knitr} is an \texttt{R} package
for dynamic report generation which allows to seamlessly combine pure
text, LaTeX, \texttt{R} code and its output in a variety of formats,
including PDF and HTML. Being inspired by \emph{Using R for Introductory
Econometrics} (Heiss, 2016)\footnote{Heiss (2016) builds on the popular
  \emph{Introductory Econometrics} by Wooldridge (2015) and demonstrates
  how to replicate the applications discussed therein using \texttt{R}.}
and with this powerful toolkit at hand we decided to write up our own
empirical companion to Stock \& Watson (2014) which resulted in
\textbf{U}sing \textbf{R} \textbf{f}or \textbf{I}ntroduction \textbf{t}o
\textbf{E}conometrics (\emph{URFITE}).

Similarly to the book by Heiss (2016) this project is neither a
comprehensive econometrics textbook nor is it intended to be a general
introduction \texttt{R}. \emph{URFITE} is best described as an
interactive script in the style of a reproducible research report which
aims to provide students of economic sciences with a
platform-independent e-learning arrangement by seamlessly intertwining
theoretical core knowledge and empirical skills in undergraduate
econometrics. Of course the focus is set on empirical applications with
\texttt{R}; we leave out tedious derivations and formal proofs wherever
we can. \emph{URFITE} is closely aligned on Stock \& Watson (2014) which
does very well in motivating theory by real-world applications. However,
we take it a step further and enable students not only to learn how
results of case studies can be replicated with \texttt{R} but we also
intend to strengthen their ability in using the newly acquired skills in
other empirical applications. To support this, each chapter contains
interactive \texttt{R} programming exercises. These exercises are used
as supplements to code chunks that display how previously discussed
techniques can be implemented within \texttt{R}. They are generated
using the \href{https://github.com/datacamp/datacamp-light}{DataCamp
light widget} and are backed by an \texttt{R}-session which is
maintained on \href{https://www.datacamp.com/home}{DataCamp}'s servers.
You may play around with the example exercise presented below.

\begin{center}\textit{This interactive application is only available in the HTML version.}\end{center}

As you can see above, the widget consists of two tabs. \texttt{script.R}
mimics an \texttt{.R}-file, a file format that is commonly used for
storing \texttt{R} code. Lines starting with a \# are commented out,
that is they are not recognized as code. Furthermore, \texttt{script.R}
works like an exercise sheet where you may write down the solution you
come up with. If you hit the button \emph{Run}, the code will be
executed, submission correctness tests are run and you will be notified
whether your approach is correct. If it is not correct, you will receive
feedback suggesting improvements or hints. The other tab,
\texttt{R Console}, is a fully functional \texttt{R} console that can be
used for trying out solutions to exercises before submitting them. Of
course you may submit (almost any) arbitrary \texttt{R} code and use the
console to play around and explore. Simply type a command and hit the
enter key on your keyboard.

As an example, consider the following line of code presented in chunk
below. It tells \texttt{R} to compute the number of packages available
on \texttt{CRAN}. The code chunk is followed by the output produced.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the number of packages available on CRAN}
\KeywordTok{nrow}\NormalTok{(}\KeywordTok{available.packages}\NormalTok{(}\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12564
\end{verbatim}

Each code chunk is equipped with a button on the outer right hand side
which copies the code to your clipboard. This makes it convenient to
work with larger code segments. In the widget above, you may click on
\texttt{R Console} and type \texttt{nrow(available.packages())} (the
command from the code chunk above) and execute it by hitting
\emph{Enter} on your keyboard\footnote{The \texttt{R} session is
  initialized by clicking anywhere into the widget. This might take a
  few seconds. Just wait for the indicator next to the button \emph{Run}
  to turn green}.

As you might have noticed, there are some out-commented lines in the
widget that ask you to assign a numeric value to a variable and then to
print the variable's content to the console. You may enter your solution
approach to \texttt{script.R} and hit the button \emph{Run} in order to
get the feedback described further above. In case you do not know how to
solve this sample exercise (don't panic, that is probably why you are
reading this), a click on \emph{Hint} will prompt you with some advice.
If you still can't find a solution, a click on \emph{solution} will
provide you with another tab, \texttt{Solution.R} which contains sample
solution code. It will often be the case that exercises can be solved in
many different ways and \texttt{Solution.R} presents what we consider as
comprehensible and idiomatic.

\subsubsection*{Conventions Used in this
Book}\label{conventions-used-in-this-book}
\addcontentsline{toc}{subsubsection}{Conventions Used in this Book}

\begin{itemize}
\item
  \emph{Italic} text indicates new terms, names, buttons and alike.
\item
  \texttt{Constant width text}, is generally used in paragraphs to refer
  to \texttt{R} code. This includes commands, variables, functions, data
  types, databases and file names.
\item
  Constant width text on gray background is used to indicate \texttt{R}
  code that can be typed literally by you. It may appear in paragraphs
  for better distinguishability among executable and non-executable code
  statements but it will mostly be encountered in shape of large blocks
  of \texttt{R} code. These blocks are referred to as code chunks (see
  above).
\end{itemize}

\subsubsection*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{subsubsection}{Acknowledgements}

We thank Alexander Blasberg and Kim Hermann for proofreading and their
constructive criticism.

\section{\texorpdfstring{A Very Short Introduction to \texttt{R} and
\emph{RStudio}}{A Very Short Introduction to  and RStudio}}\label{a-very-short-introduction-to-and-rstudio}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/rstudio} 

}

\caption{*RStudio*: the four panes}\label{fig:unnamed-chunk-6}
\end{figure}

\subsubsection*{\texorpdfstring{\texttt{R}
Basics}{ Basics}}\label{basics}
\addcontentsline{toc}{subsubsection}{\texttt{R} Basics}

This section is meant for those who have never worked with \texttt{R} or
\emph{RStudio}. If you at least know how to create objects and call
functions, you can skip it. If you would like to refresh your memories
or get a feeling for how to work with \emph{RStudio}, keep reading.

First of all start \emph{RStudio} and create a new R Script by selecting
\emph{File}, \emph{New File}, \emph{R Script}. In the editor pane type

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

and click on the button labeled \emph{Run} in the top right corner of
the editor. By doing so, your line of code is send to the console and
the result of this operation should be displayed right underneath it. As
you can see, \texttt{R} works just like a calculator. You can do all the
arithmetic calculations by using the corresponding operator (+, - , *, /
or \^{}). If you are not sure what the last operator does, try it out
and check the results.

\subsubsection*{Vectors}\label{vectors}
\addcontentsline{toc}{subsubsection}{Vectors}

\texttt{R} is of course more sophisticated than that. We can work with
variables or more generally objects. Objects are defined by using the
assignment operator \texttt{<-}. To create a variable named \texttt{x}
which contains the value \texttt{10} type \texttt{x\ \textless{}-\ 10}
and click the button \emph{Run} yet again. The new variable should have
appeared in the environment pane on the top right. The console however
did not show any results, because our line of code did not contain any
call that creates output. When you now type \texttt{x} in the console
and hit return, you ask \texttt{R} to show you the value of \texttt{x}
and the corresponding value should be printed in the console.

\texttt{x} is a scalar, a vector of length \(1\). You can easily create
longer vectors by using the function \texttt{c()} (\emph{c} for
``concatenate'' or ``combine''). To create a vector \texttt{y}
containing the numbers \(1\) to \(5\) and print it, do the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5
\end{verbatim}

You can also create a vector of letters or words. For now just remember
that characters have to be surrounded by quotes, else wise they will be
parsed as object names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hello <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Hello"}\NormalTok{, }\StringTok{"World"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here we have created a vector of length 2 containing the words
\texttt{Hello} and \texttt{World}.

Do not forget to save your script! To do so, select \emph{File},
\emph{Save}.

\subsubsection*{Functions}\label{functions}
\addcontentsline{toc}{subsubsection}{Functions}

You have seen the function \texttt{c()} that can be used to combine
objects. In general, function calls look all the same, a function name
is always followed by round parentheses. Sometimes, the parentheses
include arguments

Here are two simple examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{1}\NormalTok{, }\DataTypeTok{to =} \DecValTok{5}\NormalTok{, }\DataTypeTok{by =} \DecValTok{1}\NormalTok{)}

\KeywordTok{mean}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

In the first line we use a function called \texttt{seq} to create the
exact same vector as we did in the previous section but naming it
\texttt{z}. The function takes on the arguments \texttt{from},
\texttt{to} and \texttt{by} which should be self-explaining. The
function \texttt{mean()} computes the arithmetic mean of its argument
\texttt{x}. Since we pass the vector \texttt{z} as the argument
\texttt{x} to \texttt{mean()}, the result is \texttt{3}!

If you are not sure what argument a function expects you may consult the
function's documentation. Let's say we are not sure how the arguments
required for \texttt{seq()} work. Then we can type \texttt{?seq} in the
console and by hitting return the documentation page for that function
pops up in the lower right pane of \emph{RStudio}. In there, the section
\emph{Arguments} holds the information we seek.

On the bottom of almost every help page you find examples on how to use
the corresponding functions. This is very helpful for beginners and we
recommend to look out for those.

\chapter{Probability Theory}\label{probability-theory}

This chapter reviews some basic concepts of probability theory and
demonstrates how they can be applied in \texttt{R}.

Most of the statistical functionalities in \texttt{R}'s standard version
are collected in the \texttt{stats} package. It provides simple
functions which compute descriptive measures and facilitate calculus
involving a variety of probability distributions. However, it also holds
more sophisticated routines that e.g.~enable the user to estimate a
large number of models based on the same data or help to conduct
extensive simulation studies. \texttt{stats} is part of the base
distribution of \texttt{R}, meaning that it is installed by default so
there is no need to run \texttt{install.packages("stats")} or
\texttt{library("stats")}. Simply execute
\texttt{library(help\ =\ "stats")} in the console to view the
documentation and a complete list of all functions gathered in
\texttt{stats}.

In what follows, we lay our focus on (some of) the probability
distributions that are handled by \texttt{R} and show how to use the
relevant functions to solve simple problems. Thereby we will repeat some
core concepts of probability theory. Among other things, you will learn
how to draw random numbers, how to compute densities, probabilities,
quantiles and alike. As we shall see, it is very convenient to rely on
these routines, especially when writing your own functions.

\section{Random Variables and Probability
Distributions}\label{random-variables-and-probability-distributions}

For a start, let us briefly review some basic concepts of probability
theory.

\begin{itemize}
\tightlist
\item
  The mutually exclusive results of a random process are called the
  \emph{outcomes}. `Mutually exclusive' means that only one of the
  possible outcomes is observed.
\item
  We refer to the \emph{probability} of an outcome as the proportion of
  the time that the outcome occurs in the long run, that is if the
  experiment is repeated very often.
\item
  The set of all possible outcomes of a random variable is called the
  \emph{sample space}.
\item
  An \emph{event} is a subset of the sample space and consists of one or
  more outcomes.
\end{itemize}

These ideas are unified in the concept of a \emph{random variable} which
is a numerical summary of random outcomes. Random variables can be
\emph{discrete} or \emph{continuous}.

\begin{itemize}
\tightlist
\item
  Discrete random variables have discrete outcomes, e.g. \(0\) and
  \(1\).
\item
  A continuous random variable takes on a continuum of possible values.
\end{itemize}

\subsection*{Probability Distributions of Discrete Random
Variables}\label{probability-distributions-of-discrete-random-variables}
\addcontentsline{toc}{subsection}{Probability Distributions of Discrete
Random Variables}

A typical example for a discrete random variable \(D\) is the result of
a die roll: in terms of a random experiment this is nothing but randomly
selecting a sample of size \(1\) from a set of numbers which are
mutually exclusive outcomes. Here, the sample space is
\(\{1,2,3,4,5,6\}\) and we can think of many different events, e.g. `the
observed outcome lies between \(2\) and \(5\)'.

A basic function to draw random samples from a specified set of elements
is the the function \texttt{sample()}, see \texttt{?sample}. We can use
it to simulate the random outcome of a die roll. Let's role the die!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

The probability distribution of a discrete random variable is the list
of all possible values of the variable and their probabilities which sum
to \(1\). The cumulative probability distribution function states the
probability that the random variable is less than or equal to a
particular value.

For the die roll, this is straightforward to set up

\begin{longtable}[]{@{}lllllll@{}}
\toprule
Outcome & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\midrule
\endhead
Probability distribution & 1/6 & 1/6 & 1/6 & 1/6 & 1/6 &
1/6\tabularnewline
Cumulative probability distribution & 1/6 & 2/6 & 3/6 & 4/6 & 5/6 &
1\tabularnewline
\bottomrule
\end{longtable}

We can easily plot both functions using R. Since the probability equals
\(1/6\) for each outcome, we set up the vector \texttt{probability} by
using the \texttt{rep()} function which replicates a given value a
specified number of times.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate the vector of probabilities }
\NormalTok{probability <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{) }

\CommentTok{# plot the probabilites }
\KeywordTok{plot}\NormalTok{(probability, }\DataTypeTok{xlab =} \StringTok{"outcomes"}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Probability Distribution"}
\NormalTok{     ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-12-1} \end{center}

For the cumulative probability distribution we need the cumulative
probabilities i.e.~we need the cumulative sums of the vector
\texttt{probability}. These sums can be computed using
\texttt{cumsum()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#generate the vector of cumulative probabilities }
\NormalTok{cum_probability <-}\StringTok{ }\KeywordTok{cumsum}\NormalTok{(probability) }

\CommentTok{# plot the probabilites }
\KeywordTok{plot}\NormalTok{(cum_probability, }
     \DataTypeTok{xlab =} \StringTok{"outcomes"}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Cumulative Probability Distribution"}
\NormalTok{     ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-13-1} \end{center}

\subsection*{Bernoulli Trials}\label{bernoulli-trials}
\addcontentsline{toc}{subsection}{Bernoulli Trials}

The set of elements from which \texttt{sample()} draws outcomes does not
have to consist of numbers only. We might as well simulate coin tossing
with outcomes \(H\) (heads) and \(T\) (tails).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"H"}\NormalTok{,}\StringTok{"T"}\NormalTok{),}\DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "T"
\end{verbatim}

The result of a single coin toss is a \emph{Bernoulli} distributed
random variable i.e.~a variable with two possible distinct outcomes.

Imagine you are about to toss a coin \(10\) times in a row and wonder
how likely it is to end up with a sequence of outcomes like

\[ H \, H \, T \, T \,T \,H \,T \,T \, H \, H .\]

This is a typical example of what we call a \emph{Bernoulli experiment}
as it consists of \(n=10\) Bernoulli trials that are independent of each
other and we are interested in the likelihood of observing \(k=5\)
successes \(H\) that occur with probability \(p=0.5\) (assuming a fair
coin) in each trial. Note that the order of the outcomes does not matter
here.

It is a well known result that the number of successes \(k\) in a
Bernoulli experiment follows a binomial distribution. We denote this as

\[k \sim B(n,p).\]

The probability of observing \(k\) successes in the experiment
\(B(n,p)\) is given by

\[f(k)=P(k)=\begin{pmatrix}n\\ k \end{pmatrix} \cdot p^k \cdot
q^{n-k}=\frac{n!}{k!(n-k)!} \cdot p^k \cdot q^{n-k}\]

where \(\begin{pmatrix}n\\ k \end{pmatrix}\) means the binomial
coefficient.

In \texttt{R}, we can solve the problem stated above by means of the
function \texttt{dbinom()} which calculates the probability of the
binomial distribution given the parameters \texttt{x}, \texttt{size},
and \texttt{prob}, see \texttt{?binom}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =} \DecValTok{5}\NormalTok{,}
       \DataTypeTok{size =} \DecValTok{10}\NormalTok{,}
       \DataTypeTok{prob =} \FloatTok{0.5}
\NormalTok{       ) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2460938
\end{verbatim}

We conclude that the probability of observing Head \(k=5\) times when
tossing the coin \(n=10\) times is about \(24.6\%\).

Now assume we are interested in \(P(4 \leq k \leq 7)\) i.e.~the
probability of observing \(4\), \(5\), \(6\) or \(7\) successes for
\(B(10,0.5)\). This is easily computed by providing a vector as the
argument \texttt{x} in our call of \texttt{dbinom()} and summing up
using \texttt{sum()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}
  \KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =} \DecValTok{4}\OperatorTok{:}\DecValTok{7}\NormalTok{, }
         \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }
         \DataTypeTok{prob =} \FloatTok{0.5}
\NormalTok{         )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7734375
\end{verbatim}

The probability distribution of a discrete random variable is nothing
but a list of all possible outcomes that can occur and their respective
probabilities. In the coin tossing example we face \(11\) possible
outcomes for \(k\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up vector of possible outcomes}
\NormalTok{k <-}\StringTok{ }\DecValTok{0}\OperatorTok{:}\DecValTok{10}
\end{Highlighting}
\end{Shaded}

To visualize the probability distribution function of \(k\) we may
therefore do the following:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# assign probabilities}
\NormalTok{probability <-}\StringTok{ }\KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ k,}
                      \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }
                      \DataTypeTok{prob =} \FloatTok{0.5}
\NormalTok{                      )}

\CommentTok{# plot outcomes against probabilities}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ k, }
     \DataTypeTok{y =}\NormalTok{ probability,}
     \DataTypeTok{main =} \StringTok{"Probability Distribution Function"}
\NormalTok{     ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-18-1} \end{center}

In a similar fashion we may plot the cumulative distribution function of
\(k\) by executing the following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute cumulative probabilities}
\NormalTok{prob <-}\StringTok{ }\KeywordTok{cumsum}\NormalTok{(}
              \KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =} \DecValTok{0}\OperatorTok{:}\DecValTok{10}\NormalTok{, }
                     \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{prob =} \FloatTok{0.5}
\NormalTok{                     )}
\NormalTok{              )}

\CommentTok{# plot the cumulative probabilities}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ k, }
     \DataTypeTok{y =}\NormalTok{ prob,}
     \DataTypeTok{main =} \StringTok{"Cumulative Distribution Function"}
\NormalTok{     ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-19-1} \end{center}

\subsection*{Expected Value, Mean and
Variance}\label{expected-value-mean-and-variance}
\addcontentsline{toc}{subsection}{Expected Value, Mean and Variance}

The expected value of a random variable is the long-run average value of
its outcomes when the number of repeated trials is large. For a discrete
random variable, the expected value is computed as a weighted average of
its possible outcomes whereby the weights are the related probabilities.
This is formalized in Key Concept 2.1.

Key Concept 2.1

Expected Value and the Mean

Suppose the random variable \(Y\) takes on \(k\) possible values,
\(y_1, \dots, y_k\), where \(y_1\) denotes the first value, \(y_2\)
denotes the second value, and so forth, and that the probability that
\(Y\) takes on \(y_1\) is \(p_1\), the probability that \(Y\) takes on
\(y_2\) is \(p_2\) and so forth. The expected value of \(Y\), \(E(Y)\)
is defined as

\[ E(Y) = y_1 p_1 + y_2 p_2 + \cdots + y_k p_k = \sum_{i=1}^k y_i p_i \]

where the notation \(\sum_{i=1}^k y_i p_i\) means ``the sum of \(y_i\)
\(p_i\) for \(i\) running from \(1\) to \(k\)''. The expected value of
\(Y\) is also called the mean of \(Y\) or the expectation of \(Y\) and
is denoted by \(\mu_y\).

In the die example, the random variable, \(D\) say, takes on \(6\)
possible values \(d_1 = 1, d_2 = 2, \dots, d_6 = 6\). Assuming a fair
die, each of the \(6\) outcomes occurs with a probability of \(1/6\). It
is therefore easy to calculate the exact value of \(E(D)\) by hand:

\[ E(D) = 1/6 \sum_{i=1}^6 d_i = 3.5 \]

\(E(D)\) is simply the average of the natural numbers from \(1\) to
\(6\) since all wights \(p_i\) are \(1/6\). Convince yourself that this
can be easily calculated using the function \texttt{mean()} which
computes the arithmetic mean of a numeric vector.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5
\end{verbatim}

An example of sampling with replacement is rolling a die three times in
a row.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# rolling a die three times in a row}
\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 4
\end{verbatim}

Of course we could also consider a much bigger number of trials,
\(10000\) say. Doing so, it would be pointless to simply print the
results to the console: by default \texttt{R} displays up to \(1000\)
entries of large vectors and omits the remainder (give it a go).
Eyeballing the numbers does not reveal too much. Instead let us
calculate the sample average of the outcomes using \texttt{mean()} and
see if the result comes close to the expected value \(E(D)=3.5\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# compute the sample mean of 10000 die rolls}
\KeywordTok{mean}\NormalTok{(}
    \KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }
           \DecValTok{10000}\NormalTok{, }
           \DataTypeTok{replace =}\NormalTok{ T}
\NormalTok{           )}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5039
\end{verbatim}

We find the sample mean to be fairly close to the expected value. This
result will be discussed in Chapter \ref{RSATDOSA} in more detail.

Other frequently encountered measures are the variance and the standard
deviation. Both are measures of the \emph{dispersion} of a random
variable.

Key Concept 2.2

Variance and Standard Deviation

The Variance of the discrete \emph{random variable} \(Y\), denoted
\(\sigma^2_Y\), is
\[ \sigma^2_Y = \text{Var}(Y) = E\left[(Y-\mu_y)^2\right] = \sum_{i=1}^k (y_i - \mu_y)^2 p_i \]
The standard deviation of \(Y\) is \(\sigma_Y\), the square root of the
variance. The units of the standard deviation are the same as the units
of \(Y\).

The variance as defined in Key Concept 2.2 \emph{is not} implemented as
a function in R. Instead we have the function \texttt{var()} which
computes the \emph{sample variance}

\[ s^2_Y = \frac{1}{n-1} \sum_{i=1}^n (y_i - \overline{y})^2. \]

Remember that \(s^2_Y\) is different from the so called \emph{population
variance} of \(Y\),

\[ \text{Var}(Y) = \frac{1}{N} \sum_{i=1}^N (y_i - \mu_Y)^2, \]

since it measures how the data is dispersed around the sample average
\(\overline{y}\) instead of the population mean \(\mu_Y\). This becomes
clear when we look at our die rolling example. For \(D\) we have

\[ \text{Var}(D) = 1/6 \sum_{i=1}^6 (d_i - 3.5)^2 = 2.92  \] which is
obviously different from the result of \(s^2\) as computed by
\texttt{var()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5
\end{verbatim}

The sample variance as computed by \texttt{var()} is an \emph{estimator}
of the population variance.

 \# You may use this widget to play around with the functions presented
above

\section{Probability Distributions of Continuous Random
Variables}\label{probability-distributions-of-continuous-random-variables}

Since a continuous random variable takes on a continuum of possible
values, we cannot use the concept of a probability distribution as used
for discrete random variables. Instead, the probability distribution of
a continuous random variable is summarized by its \emph{probability
density function} (PDF).

The cumulative probability distribution function (CDF) for a continuous
random variable is defined just as in the discrete case. Hence, the
cumulative probability distribution of a continuous random variables
states the probability that the random variable is less than or equal to
a particular value.

For completeness, we present revisions of Key Concepts 2.1 and 2.2 for
the continuous case.

Key Concept 2.3

Probabilities, Expected Value and Variance of a Continuous Random
Variable

Let \(f_Y(y)\) denote the probability density function of \(Y\). Because
probabilities cannot be negative, we have \(f_Y\geq 0\) for all \(y\).
The Probability that \(Y\) falls between \(a\) and \(b\) where \(a < b\)
is \[ P(a \leq Y \leq b) = \int_a^b f_Y(y) \mathrm{d}y. \] We further
have that \(P(-\infty \leq Y \leq \infty) = 1\) and therefore
\(\int_{-\infty}^{\infty} f_Y(y) \mathrm{d}y = 1\).

As for the discrete case, the expected value of \(Y\) is the probability
weighted average of its values. Due to continuity, we use integrals
instead of sums.

The expected value of \(Y\) is defined as

\[ E(Y) =  \mu_Y = \int y f_Y(y) \mathrm{d}y. \]

The variance is the expected value of \((Y - \mu_Y)^2\). We thus have

\[ \text{Var}(Y) =  \sigma_Y^2 = \int (y - \mu_Y)^2 f_Y(y) \mathrm{d}y. \]

Let us discuss an example:

Consider the continuous random variable \(X\) with probability density
function

\[ f_X(x) = \frac{3}{x^4}, x>1. \]

\begin{itemize}
\tightlist
\item
  We can show analytically that the integral of \(f_X(x)\) over the real
  line equals \(1\).
\end{itemize}

\begin{align}
 \int f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} \frac{3}{x^4} \mathrm{d}x \\
  =& \lim_{t \rightarrow \infty} \int_{1}^{t} \frac{3}{x^4} \mathrm{d}x \\
  =& \lim_{t \rightarrow \infty}  -x^{-3} \rvert_{x=1}^t \\
  =& -\left(\lim_{t \rightarrow \infty}\frac{1}{t^3} - 1\right) \\
  =& 1
\end{align}

\begin{itemize}
\tightlist
\item
  The expectation of \(X\) can be computed as follows:
\end{itemize}

\begin{align}
 E(X) = \int x \cdot f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} x \cdot \frac{3}{x^4} \mathrm{d}x \\
  =& - \frac{3}{2} x^{-2} \rvert_{x=1}^{\infty} \\
  =& -\frac{3}{2} \left( \lim_{t \rightarrow \infty} \frac{1}{t^2} - 1 \right) \\
  =& \frac{3}{2}
\end{align}

\begin{itemize}
\tightlist
\item
  Note that the variance of \(X\) can be expressed as
  \(\text{Var}(X) = E(X^2) - E(X)^2\). Since \(E(X)\) has been computed
  in the previous step, we seek \(E(X^2)\):
\end{itemize}

\begin{align}
 E(X^2)= \int x^2 \cdot f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} x^2 \cdot \frac{3}{x^4} \mathrm{d}x \\
  =& -3 x^{-1} \rvert_{x=1}^{\infty} \\
  =& -3 \left( \lim_{t \rightarrow \infty} \frac{1}{t} - 1 \right) \\
  =& 3
\end{align}

So we have shown that the area under the curve equals one, that the
expectation is \(E(X)=\frac{3}{2} \ \) and we found the variance to be
\(\text{Var}(X) = \frac{3}{4}\). However, this was quite tedious and, as
we shall see soon, an analytic approach is not applicable for some
probability density functions e.g.~if integrals have no closed form
solutions.

Luckily, \texttt{R} enables us to find the results derived above in an
instant. The tool we use for this is the function \texttt{integrate()}.
First, we have to define the functions we want to calculate integrals
for as \texttt{R} functions, i.e.~the PDF \(f_X(x)\) as well as the
expressions \(x\cdot f_X(x)\) and \(x^2\cdot f_X(x)\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define functions}
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\DecValTok{3}\OperatorTok{/}\NormalTok{x}\OperatorTok{^}\DecValTok{4}
\NormalTok{g <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{*}\KeywordTok{f}\NormalTok{(x)}
\NormalTok{h <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{^}\DecValTok{2}\OperatorTok{*}\KeywordTok{f}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

Next, we use \texttt{integrate()} and set lower and upper limits of
integration to \(1\) and \(\infty\) using arguments \texttt{lower} and
\texttt{upper}. By default, \texttt{integrate()} prints the result along
with an estimate of the approximation error to the console. However, the
outcome is not a numeric value one can do further calculation with
readily. In order to get only a numeric value of the integral, we need
to use the \$ operator in conjunction with \texttt{value}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate area under curve}
\NormalTok{AUC <-}\StringTok{ }\KeywordTok{integrate}\NormalTok{(f, }
                 \DataTypeTok{lower =} \DecValTok{1}\NormalTok{, }
                 \DataTypeTok{upper =} \OtherTok{Inf}
\NormalTok{                 )}
\NormalTok{AUC }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1 with absolute error < 1.1e-14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate E(X)}
\NormalTok{EX <-}\StringTok{ }\KeywordTok{integrate}\NormalTok{(g,}
                \DataTypeTok{lower =} \DecValTok{1}\NormalTok{,}
                \DataTypeTok{upper =} \OtherTok{Inf}
\NormalTok{                )}
\NormalTok{EX}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1.5 with absolute error < 1.7e-14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate Var(X)}
\NormalTok{VarX <-}\StringTok{ }\KeywordTok{integrate}\NormalTok{(h,}
                  \DataTypeTok{lower =} \DecValTok{1}\NormalTok{,}
                  \DataTypeTok{upper =} \OtherTok{Inf}
\NormalTok{                  )}\OperatorTok{$}\NormalTok{value }\OperatorTok{-}\StringTok{ }\NormalTok{EX}\OperatorTok{$}\NormalTok{value}\OperatorTok{^}\DecValTok{2} 
\NormalTok{VarX}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.75
\end{verbatim}

Although there is a wide variety of distributions, the ones most often
encountered in econometrics are the normal, chi-squared, Student \(t\)
and \(F\) distributions. Therefore we will discuss some core \texttt{R}
functions that allow to do calculations involving densities,
probabilities and quantiles of these distributions.

Every probability distribution that \texttt{R} handles has four basic
functions whose names consist of a prefix followed by a root name. As an
example, take the normal distribution. The root name of all four
functions associated with the normal distribution is \texttt{norm}. The
four prefixes are

\begin{itemize}
\tightlist
\item
  \texttt{d} for ``density'' - probability function / probability
  density function
\item
  \texttt{p} for ``probability'' - cumulative distribution function
\item
  \texttt{q} for ``quantile'' - quantile function (inverse cumulative
  distribution function)
\item
  \texttt{r} for ``random'' - random number generator
\end{itemize}

Thus, for the normal distribution we have the \texttt{R} functions
\texttt{dnorm()}, \texttt{pnorm()}, \texttt{qnorm()} and
\texttt{rnorm()}.

\subsection*{The Normal Distribution}\label{the-normal-distribution}
\addcontentsline{toc}{subsection}{The Normal Distribution}

The probably most important probability distribution considered here is
the normal distribution. This is not least due to the special role of
the standard normal distribution and the Central Limit Theorem which is
treated shortly during the course of this section. Distributions of the
normal family have a familiar symmetric, bell-shaped probability
density. A normal distribution is characterized by its mean \(\mu\) and
its standard deviation \(\sigma\) what is concisely expressed by
\(N(\mu,\sigma^2)\). The normal distribution has the PDF

\[ f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x - μ)^2/(2 σ^2)}. \]

For the standard normal distribution we have \(\mu=0\) and \(\sigma=1\).
Standard normal variates are often denoted by \(Z\). Usually, the
standard normal PDF is denoted by \(\phi\) and the standard normal CDF
is denoted by \(\Phi\). Hence,

\[ \phi(c) = \Phi'(c) \ \ , \ \ \Phi(c) = P(Z \leq c) \ \ , \ \ Z \sim N(0,1).
\] In \texttt{R}, we can conveniently obtain density values of normal
distributions using the function \texttt{dnorm()}. Let us draw a plot of
the standard normal density function using \texttt{curve()} in
conjunction with \texttt{dnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# draw a plot of the N(0,1) PDF}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x),}
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{, }\FloatTok{3.5}\NormalTok{),}
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
      \DataTypeTok{main =} \StringTok{"Standard Normal Density Function"}
\NormalTok{      ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-26-1} \end{center}

We can obtain the density at different positions by passing a vector of
quantiles to \texttt{dnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute denstiy at x=-1.96, x=0 and x=1.96}
\KeywordTok{dnorm}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.96}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{1.96}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05844094 0.39894228 0.05844094
\end{verbatim}

Similar to the PDF, we can plot the standard normal CDF using
\texttt{curve()} and \texttt{pnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal CDF}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{pnorm}\NormalTok{(x), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{, }\FloatTok{3.5}\NormalTok{), }
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
      \DataTypeTok{main =} \StringTok{"Standard Normal Cumulative Distribution Function"}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-28-1} \end{center}

We can also use \texttt{R} to calculate the probability of events
associated with a standard normal variate.

Let us say we are interested in \(P(Z \leq 1.337)\). For some continuous
random variable \(Z\) on \([-\infty,\infty]\) with density function
\(g(x)\) we would have to determine \(G(x)\) which is the anti
derivative of \(g(x)\) so that

\[ P(Z \leq 1,337 ) = G(1,337) = \int_{-\infty}^{1,337} g(x) \mathrm{d}x.  \]

If \(Z \sim N(0,1)\), we have \(g(x)=\phi(x)\). There is no analytic
solution to the integral above and it is cumbersome to come up with an
approximation. However, we may circumvent this using \texttt{R} in
different ways. The first approach makes use of the function
\texttt{integrate()} which allows to solve one-dimensional integration
problems using a numerical method. For this, we first define the
function we want to compute the integral of as a \texttt{R} function
\texttt{f}. In our example, \texttt{f} needs to be the standard normal
density function and hence takes a single argument \texttt{x}. Following
the definition of \(\phi(x)\) we define \texttt{f} as

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the standard normal PDF as a R function}
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \DecValTok{1}\OperatorTok{/}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{pi)) }\OperatorTok{*}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5} \OperatorTok{*}\StringTok{ }\NormalTok{x}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let us check if this function enables us to compute standard normal
density values by passing it a vector of quantiles.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define vector of quantiles}
\NormalTok{quants <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.96}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{1.96}\NormalTok{)}

\CommentTok{# compute density values}
\KeywordTok{f}\NormalTok{(quants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05844094 0.39894228 0.05844094
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compare to results produced by dnorm()}
\KeywordTok{f}\NormalTok{(quants) }\OperatorTok{==}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(quants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE TRUE TRUE
\end{verbatim}

Notice that the results produced by \texttt{f()} are indeed equivalent
to those given by \texttt{dnorm()}.

Next, we call \texttt{integrate()} on \texttt{f()} and specify the
arguments \texttt{lower} and \texttt{upper}, the lower and upper limits
of integration.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# integrate f()}
\KeywordTok{integrate}\NormalTok{(f, }
          \DataTypeTok{lower =} \OperatorTok{-}\OtherTok{Inf}\NormalTok{, }
          \DataTypeTok{upper =} \FloatTok{1.337}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.9093887 with absolute error < 1.7e-07
\end{verbatim}

We find that the probability of observing \(Z \leq 1.337\) is about
\(0.9094\%\).

A second and much more convenient way is to use the function
\texttt{pnorm()} which also allows calculus involving the standard
normal cumulative distribution function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the probability using pnorm()}
\KeywordTok{pnorm}\NormalTok{(}\FloatTok{1.337}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9093887
\end{verbatim}

The result matches the outcome of the approach using
\texttt{integrate()}.

Let us discuss some further examples:

A commonly known result is that \(95\%\) probability mass of a standard
normal lies in the interval \([-1.96, 1.96]\), that is in a distance of
about \(2\) standard deviations to the mean. We can easily confirm this
by calculating

\[ P(-1.96 \leq Z \leq 1.96) = 1-2\times P(Z \leq -1.96) \] due to
symmetry of the standard normal PDF. Thanks to \texttt{R}, we can
abandon the table of the standard normal CDF again and instead solve
this by using the function \texttt{pnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the probability}
\DecValTok{1} \OperatorTok{-}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\FloatTok{1.96}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9500042
\end{verbatim}

Now consider a random variable \(Y\) with \(Y \sim N(5, 25)\). As you
should already know from your statistics courses it is not possible to
make any statement of probability without prior standardizing as shown
in Key Concept 2.4.

Key Concept 2.4

Computing Probabilities Involving Normal Random Variables

Suppose \(Y\) is normally distributed with mean \(\mu\) and variance
\(\sigma^2\): \[Y
\sim N(\mu, \sigma^2)\] Then \(Y\) is standardized by subtracting its
mean and dividing by its standard deviation:
\[ Z = \frac{Y -\mu}{\sigma} \] Let \(c_1\) and \(c_2\) denote two
numbers whereby \(c_1 < c_2\) and further \(d_1 = (c_1 - \mu) / \sigma\)
and \(d_2 = (c_2 - \mu)/\sigma\). Then

\begin{align} 
P(Y \leq c_2) =& \, P(Z \leq d_2) = \Phi(d_2) \\ 
P(Y \geq c_1) =& \, P(Z \geq d_1) = 1 - \Phi(d_1) \\ 
P(c_1 \leq Y \leq c_2) =& \, P(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1) 
\end{align}

\texttt{R} functions that handle the normal distribution can perform
this standardization. If we are interested in \(P(3 \leq Y \leq 4)\) we
can use \texttt{pnorm()} and adjust for a mean and/or a standard
deviation that deviate from \(\mu=0\) and \(\sigma = 1\) by specifying
the arguments \texttt{mean} and \texttt{sd} accordingly.
\textbf{Attention}: the argument \texttt{sd} requires the standard
deviation, not the variance!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{5}\NormalTok{) }\OperatorTok{-}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{5}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.07616203
\end{verbatim}

An extension of the normal distribution in a univariate setting is the
multivariate normal distribution. The PDF of two random normal variables
\(X\) and \(Y\) is given by

\begin{align}
g_{X,Y}(x,y) =& \, \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho_{XY}^2}} \\ 
\cdot & \, \exp \left\{ \frac{1}{-2(1-\rho_{XY}^2)} \left[ \left( \frac{x-\mu_x}{\sigma_x} \right)^2 - 2\rho_{XY}\left( \frac{x-\mu_X}{\sigma_X} \right)\left( \frac{y-\mu_Y}{\sigma_Y} \right) + \left( \frac{y-\mu_Y}{\sigma_Y} \right)^2 \right]  \right\}. \label{eq:bivnorm}
\end{align}

Equation \eqref{eq:bivnorm} contains the bivariate normal PDF. Admittedly,
it is hard to gain insights from this complicated expression. Instead,
let us consider the special case where \(X\) and \(Y\) are uncorrelated
standard normal random variables with density functions \(f_X(x)\) and
\(f_Y(y)\) and we assume that they have a joint normal distribution. We
then have the parameters \(\sigma_X = \sigma_Y = 1\), \(\mu_X=\mu_Y=0\)
(due to marginal standard normality) and \(\rho_{XY}=0\) (due to
independence). The joint probability density function of \(X\) and \(Y\)
then becomes

\[ g_{X,Y}(x,y) = f_X(x) f_Y(y) = \frac{1}{2\pi} \cdot \exp \left\{ -\frac{1}{2} \left[x^2 + y^2 \right]  \right\}, \tag{2.2}  \]

the PDF of the bivariate standard normal distribution. The next plot
provides an interactive three dimensional plot of (2.2). By moving the
cursor over the plot you can see that the density is rotationally
invariant.

\subsection*{The Chi-Squared
Distribution}\label{the-chi-squared-distribution}
\addcontentsline{toc}{subsection}{The Chi-Squared Distribution}

Another distribution relevant in econometric day-to-day work is the
chi-squared distribution. It is often needed when testing special types
of hypotheses frequently encountered when dealing with regression
models.

The sum of \(M\) squared independent standard normal distributed random
variables follows a chi-squared distribution with \(M\) degrees of
freedom.

\[ Z_1^2 + \dots + Z_M^2 = \sum_{m=1}^M Z_m^2 \sim \chi^2_M \ \ \text{with} \ \ Z_m \overset{i.i.d.}{\sim} N(0,1) \label{eq:chisq}\]

A \(\chi^2\) distributed random variable with \(M\) degrees of freedom
has expectation \(M\), mode at \(M-2\) for \(n \geq 2\) and variance
\(2 \cdot M\).

For example, if we have

\[ Z_1,Z_2,Z_3 \overset{i.i.d.}{\sim} N(0,1) \]

it holds that

\[ Z_1^2+Z_2^2+Z_3^3 \sim \chi^2_3. \tag{2.3} \] Using the code below,
we can display the PDF and the CDF of a \(\chi^2_3\) random variable in
a single plot. This is achieved by setting the argument
\texttt{add = TRUE} in the second call of \texttt{curve()}. Further we
adjust limits of both axes using \texttt{xlim} and \texttt{ylim} and
choose different colors to make both functions better distinguishable.
The plot is completed by adding a legend with help of the function
\texttt{legend()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the PDF}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{3}\NormalTok{), }
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }
      \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
      \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{""}\NormalTok{,}
      \DataTypeTok{main =} \StringTok{"p.d.f. and c.d.f of Chi-Squared Distribution, m = 3"}
\NormalTok{      )}

\CommentTok{# add the CDF to the plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{pchisq}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{3}\NormalTok{), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }
      \DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }
      \DataTypeTok{col =} \StringTok{"red"}
\NormalTok{      )}

\CommentTok{# add a legend to the plot}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{"PDF"}\NormalTok{, }\StringTok{"CDF"}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-36-1} \end{center}

Since the outcomes of a \(\chi^2_M\) distributed random variable are
always positive, the domain of the related PDF and CDF is
\(\mathbb{R}_{\geq0}\).

As expectation and variance depend (solely!) on the degrees of freedom,
the distribution's shape changes drastically if we vary the number of
squared standard normals that are summed up. This relation is often
depicted by overlaying densities for different \(M\), see e.g.~the
Wikipedia Article.

Of course, one can easily reproduce such a plot using \texttt{R}. Again
we start by plotting the density of the \(\chi_1^2\) distribution on the
interval \([0,15]\) with \texttt{curve()}. In the next step, we loop
over degrees of freedom \(m=2,...,7\) and add a density curve for each
\(m\) to the plot. We also adjust the line color for each iteration of
the loop by setting \texttt{col = m}. At last, we add a legend that
displays degrees of freedom and the associated colors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the density for m=1}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{1}\NormalTok{), }
      \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{), }
      \DataTypeTok{xlab =} \StringTok{"x"}\NormalTok{, }
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
      \DataTypeTok{main =} \StringTok{"Chi-Square Distributed Random Variables"}
\NormalTok{      )}

\CommentTok{# add densities for m=2,...,7 to the plot using a for loop }
\ControlFlowTok{for}\NormalTok{ (m }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
  \KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df =}\NormalTok{ m),}
        \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{), }
        \DataTypeTok{add =}\NormalTok{ T, }
        \DataTypeTok{col =}\NormalTok{ m}
\NormalTok{        )}
\NormalTok{\}}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \KeywordTok{as.character}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{), }
       \DataTypeTok{col =} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{ , }
       \DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }
       \DataTypeTok{title =} \StringTok{"D.f."}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-37-1} \end{center}

It is evident that increasing the degrees of freedom shifts the
distribution to the right (the mode becomes larger) and increases the
dispersion (the distribution's variance grows).

\hypertarget{thetdist}{\subsection*{\texorpdfstring{The Student \(t\)
Distribution}{The Student t Distribution}}\label{thetdist}}
\addcontentsline{toc}{subsection}{The Student \(t\) Distribution}

Let \(Z\) be a standard normal variate, \(W\) a random variable that
follows a \(\chi^2_M\) distribution with \(M\) degrees of freedom and
further assume that \(Z\) and \(W\) are independently distributed. Then
it holds that

\[ \frac{Z}{\sqrt{W/M}} =:X \sim t_M \] and we say that \(X\) follows a
\emph{Student \(t\) distribution} (or simply \(t\) distribution) with
\(M\) degrees of freedom.

As for the \(\chi^2_M\) distribution, the shape of a \(t_M\)
distribution depends on \(M\). \(t\) distributions are symmetric,
bell-shaped and look very similar to a normal distribution, especially
when \(M\) is large. This is not a coincidence: for a sufficient large
\(M\), the \(t_M\) distribution can be approximated by the standard
normal distribution. This approximation works reasonably well for
\(M\geq 30\). As we will show later by means of a small simulation
study, the \(t_{\infty}\) distribution \emph{is} the standard normal
distribution.

A \(t_M\) distributed random variable has an expectation if \(M>1\) and
it has a variance if \(n>2\).

\begin{align}
  E(X) =& 0 \ , \ M>1 \\
  \text{Var}(X) =& \frac{M}{M-2} \ , \ M>2
\end{align}

Let us graph some \(t\) distributions with different \(M\) and compare
them with the standard normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal density}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x), }
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{), }
      \DataTypeTok{xlab =} \StringTok{"x"}\NormalTok{, }
      \DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
      \DataTypeTok{main =} \StringTok{"Theoretical Densities of t-Distributions"}
\NormalTok{      )}

\CommentTok{# plot the t density for m=2}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{2}\NormalTok{), }
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
      \DataTypeTok{col =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{      )}

\CommentTok{# plot the t density for m=4}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{4}\NormalTok{), }
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
      \DataTypeTok{col =} \DecValTok{3}\NormalTok{, }
      \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{      )}

\CommentTok{# plot the t density for m=25}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{25}\NormalTok{), }
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
      \DataTypeTok{col =} \DecValTok{4}\NormalTok{, }
      \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{      )}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{"N(0,1)"}\NormalTok{,}\StringTok{"M=2"}\NormalTok{,}\StringTok{"M=4"}\NormalTok{,}\StringTok{"M=25"}\NormalTok{), }
       \DataTypeTok{col =} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-38-1} \end{center}

The plot indicates what has been claimed in the previous paragraph: as
the degrees of freedom increase, the shape of the \(t\) distribution
comes closer to that of a standard normal bell. Already for \(M=25\) we
find little difference to the dashed line which is the standard normal
density curve. If \(M\) is small, we find the distribution to have
slightly heavier tails than a standard normal, i.e.~it has a ``fatter''
bell shape.

\subsection*{\texorpdfstring{The \(F\)
Distribution}{The F Distribution}}\label{the-f-distribution}
\addcontentsline{toc}{subsection}{The \(F\) Distribution}

Another ratio of random variables important to econometricians is the
ratio of two independently \(\chi^2\) distributed random variables that
are divided by their degrees of freedom \(M\) and \(n\). The quantity

\[ \frac{W/M}{V/n} \sim F_{M,n} \ \ \text{with} \ \ W \sim \chi^2_M \ \ , \ \ V \sim \chi^2_n \]
follows an \(F\) distribution with numerator degrees of freedom \(M\)
and denominator degrees of freedom \(n\), denoted \(F_{M,n}\). The
distribution was first derived by George Snedecor but was named in honor
of \href{https://en.wikipedia.org/wiki/Ronald_Fisher}{Sir Ronald
Fisher}.

By definition, the domain of both PDF and CDF of an \(F_{M,n}\)
distributed random variable is \(\mathbb{R}_{\geq0}\).

Say we have an \(F\) distributed random variable \(Y\) with numerator
degrees of freedom \(3\) and denominator degrees of freedom \(14\) and
are interested in \(P(Y \geq 2)\). This can be computed with help of the
function \texttt{pf()}. By setting the argument \texttt{lower.tail} to
\texttt{TRUE} we ensure that \texttt{R} computes \(1- P(Y \leq 2)\),
i.e.~the probability mass in the tail right of \(2\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pf}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DataTypeTok{lower.tail =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1638271
\end{verbatim}

We can visualize this probability by drawing a line plot of the related
density function and adding a color shading with \texttt{polygon()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define coordinate vectors for vertices of the polygon}
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{10}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{df}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{3}\NormalTok{, }\DecValTok{14}\NormalTok{), }\DecValTok{0}\NormalTok{)}

\CommentTok{# draw density of F_\{3, 14\}}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{df}\NormalTok{(x ,}\DecValTok{3}\NormalTok{ ,}\DecValTok{14}\NormalTok{), }
      \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.8}\NormalTok{), }
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }
      \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{,}
      \DataTypeTok{main =} \StringTok{"Density Function"}
\NormalTok{      )}

\CommentTok{# draw the polygon}
\KeywordTok{polygon}\NormalTok{(x, y, }\DataTypeTok{col=}\StringTok{"orange"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-40-1} \end{center}

The \(F\) distribution is related to many other distributions. An
important special case encountered in econometrics arises if the
denominator degrees of freedom are large such that the \(F_{M,n}\)
distribution can be approximated by the \(F_{M,\infty}\) distribution
which turns out to be simply the distribution of a \(\chi^2_M\) random
variable divided by its degrees of freedom \(M\),

\[ W/M \sim F_{M,\infty} \ \ , \ \ W \sim \chi^2_M. \]

 \# You may use this widget to play around with the functions presented
above

\section{Random Sampling and the Distribution of Sample
Averages}\label{RSATDOSA}

To clarify the basic idea of random sampling, let us jump back to the
die rolling example:

Suppose we are rolling the die \(n\) times. This means we are interested
in the outcomes of \(n\) random processes \(Y_i, \ i=1,...,n\) which are
characterized by the same distribution. Since these outcomes are
selected randomly, they are \emph{random variables} themselves and their
realizations will differ each time we draw a sample, i.e.~each time we
roll the die \(n\) times. Furthermore, each observation is randomly
drawn from the same population, that is the numbers from \(1\) to \(6\),
and their individual distribution is the same. Hence we say that
\(Y_1,\dots,Y_n\) are identically distributed. Moreover, we know that
the value of any of the \(Y_i\) does not provide any information on the
remainder of the sample. In our example, rolling a six as the first
observation in our sample does not alter the distributions of
\(Y_2,\dots,Y_n\): all numbers are equally likely to occur. This means
that all \(Y_i\) are also independently distributed. Thus, we say that
\(Y_1,\dots,Y_n\) are independently and identically distributed
(\emph{i.i.d}). The die example uses the most simple sampling scheme.
That is why it is called \emph{simple random sampling}. This concept is
condensed in Key Concept 2.5.

Key Concept 2.5

Simple Random Sampling and i.i.d. Random Variables

In simple random sampling, \(n\) objects are drawn at random from a
population. Each object is equally likely to end up in the sample. We
denote the value of the random variable \(Y\) for the \(i^{th}\)
randomly drawn object as \(Y_i\). Since all objects are equally likely
to be drawn and the distribution of \(Y_i\) is the same for all \(i\),
the \(Y_i, \dots, Y_n\) are independently and identically distributed
(i.i.d.). This means the distribution of \(Y_i\) is the same for all
\(i=1,\dots,n\) and \(Y_1\) is distributed independently of
\(Y_2, \dots, Y_n\) and \(Y_2\) is distributed independently of
\(Y_1, Y_3, \dots, Y_n\) and so forth.

What happens if we consider functions of the sample data? Consider the
example of rolling a die two times in a row once again. A sample now
consists of two independent random draws from the set
\(\{1,2,3,4,5,6\}\). In view of the aforementioned, it is apparent that
any function of these two random variables is also random, e.g.~their
sum. Convince yourself by executing the code below several times.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

Clearly this sum, let us call it \(S\), is a random variable as it
depends on randomly drawn summands. For this example, we can completely
enumerate all outcomes and hence write down the theoretical probability
distribution of our function of the sample data, \(S\):

We face \(6^2=36\) possible pairs. Those pairs are

\begin{align}
  &(1,1)    (1,2)   (1,3)   (1,4)   (1,5)   (1,6) \\ 
  &(2,1)    (2,2)   (2,3)   (2,4)   (2,5)   (2,6) \\ 
  &(3,1)    (3,2)   (3,3)   (3,4)   (3,5)   (3,6) \\ 
  &(4,1)    (4,2)   (4,3)   (4,4)   (4,5)   (4,6) \\ 
  &(5,1)    (5,2)   (5,3)   (5,4)   (5,5)   (5,6) \\ 
  &(6,1)    (6,2)   (6,3)   (6,4)   (6,5)   (6,6)
\end{align}

Thus, possible outcomes for \(S\) are

\[ \left\{ 2,3,4,5,6,7,8,9,10,11,12 \right\} . \] Enumeration of
outcomes yields

\begin{align}
  P(S) = 
  \begin{cases} 
    1/36, \ & S = 2 \\ 
    2/36, \ & S = 3 \\
    3/36, \ & S = 4 \\
    4/36, \ & S = 5 \\
    5/36, \ & S = 6 \\
    6/36, \ & S = 7 \\
    5/36, \ & S = 8 \\
    4/36, \ & S = 9 \\
    3/36, \ & S = 10 \\
    2/36, \ & S = 11 \\
    1/36, \ & S = 12
  \end{cases}
\end{align}

We can also compute \(E(S)\) and \(\text{Var}(S)\) as stated in Key
Concept 2.1 and Key Concept 2.2.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Vector of outcomes}
\NormalTok{S <-}\StringTok{ }\DecValTok{2}\OperatorTok{:}\DecValTok{12}

\CommentTok{# Vector of probabilities}
\NormalTok{PS <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{5}\OperatorTok{:}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{36}

\CommentTok{# Expectation of S}
\NormalTok{ES <-}\StringTok{ }\NormalTok{S }\OperatorTok{%*%}\StringTok{ }\NormalTok{PS}
\NormalTok{ES}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]    7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Variance of S}
\NormalTok{VarS <-}\StringTok{ }\NormalTok{(S }\OperatorTok{-}\StringTok{ }\KeywordTok{c}\NormalTok{(ES))}\OperatorTok{^}\DecValTok{2} \OperatorTok{%*%}\StringTok{ }\NormalTok{PS}
\NormalTok{VarS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]
## [1,] 5.833333
\end{verbatim}

(The \%*\% operator is used to compute the scalar product of two
vectors.)

So the distribution of \(S\) is known. It is also evident that its
distribution differs considerably from the marginal distribution,
i.e.~the distribution of a single die roll's outcome, \(D\) . Let us
visualize this using bar plots.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# divide the plotting area in one row with two columns}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{# plot the distribution of S}
\KeywordTok{names}\NormalTok{(PS) <-}\StringTok{ }\DecValTok{2}\OperatorTok{:}\DecValTok{12}

\KeywordTok{barplot}\NormalTok{(PS, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }
        \DataTypeTok{xlab =} \StringTok{"S"}\NormalTok{, }
        \DataTypeTok{ylab =} \StringTok{"Probability"}\NormalTok{, }
        \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
        \DataTypeTok{space =} \DecValTok{0}\NormalTok{, }
        \DataTypeTok{main =} \StringTok{"Sum of Two Die Rolls"}
\NormalTok{        )}

\CommentTok{# plot the distribution of D }
\NormalTok{probability <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\KeywordTok{names}\NormalTok{(probability) <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{6}

\KeywordTok{barplot}\NormalTok{(probability, }
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }
        \DataTypeTok{xlab =} \StringTok{"D"}\NormalTok{, }
        \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
        \DataTypeTok{space =} \DecValTok{0}\NormalTok{, }
        \DataTypeTok{main =} \StringTok{"Outcome of a Single Die Roll"}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-43-1} \end{center}

Many econometric procedures deal with averages of sampled data. It is
almost always assumed that observations are drawn randomly from a
larger, unknown population. As demonstrated for the sample function
\(S\), computing an average of a random sample also has the effect to
make the average a random variable itself. This random variable in turn
has a probability distribution which is called the sampling
distribution. Knowledge about the sampling distribution of the average
is therefore crucial for understanding the performance of econometric
procedures.

The \emph{sample average} of a sample of \(n\) observations
\(Y_1, \dots, Y_n\) is

\[ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i = \frac{1}{n} (Y_1 + Y_2 + \cdots + Y_n). \]
\(\overline{Y}\) is also called the sample mean.

\subsection*{Mean and Variance of the Sample
Mean}\label{mean-and-variance-of-the-sample-mean}
\addcontentsline{toc}{subsection}{Mean and Variance of the Sample Mean}

Denote \(\mu_Y\) and \(\sigma_Y^2\) the mean and the variance of the
\(Y_i\) and suppose that all observations \(Y_1,\dots,Y_n\) are i.i.d.
such that in particular mean and variance are the same for all
\(i=1,\dots,n\). Then we have that

\[ E(\overline{Y}) = E\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) = \frac{1}{n} E\left(\sum_{i=1}^n Y_i\right) = \frac{1}{n} \sum_{i=1}^n E\left(Y_i\right) = \frac{1}{n} \cdot n \cdot \mu_Y = \mu_Y    \]
and

\begin{align}
  \text{Var}(\overline{Y}) =& \text{Var}\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) \\
  =& \frac{1}{n^2} \sum_{i=1}^n \text{Var}(Y_i) + \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1, j\neq i}^n \text{cov}(Y_i,Y_j) \\
  =& \frac{\sigma^2_Y}{n} \\
  =& \sigma_{\overline{Y}}^2.
\end{align}

Note that the second summand vanishes since \(\text{cov}(Y_i,Y_j)=0\)
for \(i\neq j\) due to independence of the observations.

Consequently, the standard deviation of the sample mean is given by

\[ \sigma_{\overline{Y}} = \frac{\sigma_Y}{\sqrt{n}}. \]

It is worthwhile to mention that these results hold irrespective of the
underlying distribution of the \(Y_i\).

\subsubsection*{\texorpdfstring{The Sampling Distribution of
\(\overline{Y}\) when \(Y\) Is Normally
Distributed}{The Sampling Distribution of \textbackslash{}overline\{Y\} when Y Is Normally Distributed}}\label{the-sampling-distribution-of-overliney-when-y-is-normally-distributed}
\addcontentsline{toc}{subsubsection}{The Sampling Distribution of
\(\overline{Y}\) when \(Y\) Is Normally Distributed}

If the \(Y_1,\dots,Y_n\) are i.i.d. draws from a normal distribution
with mean \(\mu_Y\) and variance \(\sigma_Y^2\), the following holds for
their sample average \(\overline{Y}\):

\[ \overline{Y} \sim N(\mu_y, \sigma_Y^2/n) \tag{2.4} \]

For example, if a sample \(Y_i\) with \(i=1,\dots,10\) is drawn from a
standard normal distribution with mean \(\mu_Y = 0\) and variance
\(\sigma_Y^2=1\) we have

\[ \overline{Y} \sim N(0,0.1).\]

We can use \texttt{R}'s random number generation facilities to verify
this result. The basic idea is to simulate outcomes of the true
distribution of \(\overline{Y}\) by repeatedly drawing random samples of
10 observation from the \(N(0,1)\) distribution and computing their
respective averages. If we do this for a large number of repetitions,
the simulated data set of averages should quite accurately reflect the
theoretical distribution of \(\overline{Y}\) if the theoretical result
holds.

The approach sketched above is an example of what is commonly known as
\emph{Monte Carlo Simulation} or \emph{Monte Carlo Experiment}. To
perform this simulation in \texttt{R}, we proceed as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a sample size \texttt{n} and the number of samples to be drawn
  \texttt{reps}.
\item
  Use the function \texttt{replicate()} in conjunction with
  \texttt{rnorm()} to draw \texttt{n} observations from the standard
  normal distribution \texttt{rep} times. \textbf{Note}: the outcome of
  \texttt{replicate()} is a matrix with dimensions \texttt{n} \(\times\)
  \texttt{rep}. It contains the drawn samples as \emph{columns}.
\item
  Compute sample means using \texttt{colMeans()}. This function computes
  the mean of each column i.e.~of each sample and returns a vector.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set sample size and number of samples}
\NormalTok{n <-}\StringTok{ }\DecValTok{10}
\NormalTok{reps <-}\StringTok{ }\DecValTok{10000}

\CommentTok{# Perform random sampling}
\NormalTok{samples <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(reps, }\KeywordTok{rnorm}\NormalTok{(n)) }\CommentTok{# 10 x 10000 sample matrix}

\CommentTok{# Compute sample means}
\NormalTok{sample.avgs <-}\StringTok{ }\KeywordTok{colMeans}\NormalTok{(samples)}
\end{Highlighting}
\end{Shaded}

After performing these steps we end up with a vector of sample averages.
You can check the vector property of \texttt{sample.avgs}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Check that sample.avgs is a vector}
\KeywordTok{is.vector}\NormalTok{(sample.avgs) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print the first few entries to the console}
\KeywordTok{head}\NormalTok{(sample.avgs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.12406767 -0.10649421 -0.01033423 -0.39905236 -0.41897968 -0.90883537
\end{verbatim}

A straightforward approach to examine the distribution of univariate
numerical data is to plot it as a histogram and compare it to some known
or assumed distribution. This comparison can be done with help of a
suitable statistical test or by simply eyeballing some graphical
representations of these distributions. For our simulated sample
averages, we will do the latter by means of the functions
\texttt{hist()} and \texttt{curve()}.

By default, \texttt{hist()} will give us a frequency histogram i.e.~a
bar chart where observations are grouped into ranges, also called bins.
The ordinate reports the number of observations falling into each of the
bins. Instead, we want it to report density estimates for comparison
purposes. This is achieved by setting the argument
\texttt{freq = FALSE}. The number of bins is adjusted by the argument
\texttt{breaks}.

Using \texttt{curve()}, we overlay the histogram with a red line which
represents the theoretical density of a \(N(0, 0.1)\) distributed random
variable. Remember to use the argument \texttt{add = TRUE} to add the
curve to the current plot. Otherwise \texttt{R} will open a new graphic
device and discard the previous plot!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot the density histogram}
\KeywordTok{hist}\NormalTok{(sample.avgs, }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.4}\NormalTok{), }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{ , }
     \DataTypeTok{freq =}\NormalTok{ F, }
     \DataTypeTok{breaks =} \DecValTok{20}
\NormalTok{     )}

\CommentTok{# overlay the theoretical distribution of sample averages on top of the histogram}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{sd =} \DecValTok{1}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)), }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd =} \StringTok{"2"}\NormalTok{, }
      \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-46-1} \end{center}

From inspection of the plot we can tell that the distribution of
\(\overline{Y}\) is indeed very close to that of a \(N(0, 0.1)\)
distributed random variable so that evidence obtained from the Monte
Carlo Simulation supports the theoretical claim.

Let us discuss another example where using simple random sampling in a
simulation setup helps to verify a well known result. As discussed
before, the \protect\hyperlink{chisquare}{Chi-squared} distribution with
\(m\) degrees of freedom arises as the distribution of the sum of \(m\)
independent squared standard normal distributed random variables.

To visualize the claim stated in equation (2.3), we proceed similarly as
in the example before:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose the degrees of freedom \texttt{DF} and the number of samples to
  be drawn \texttt{reps}.
\item
  Draw \texttt{reps} random samples of size \texttt{DF} from the
  standard normal distribution using \texttt{replicate()}.
\item
  For each sample, by squaring the outcomes and summing them up column
  wise. Store the results
\end{enumerate}

Again, we produce a density estimate for the distribution underlying our
simulated data using a density histogram and overlay it with a line
graph of the theoretical density function of the \(\chi^2_3\)
distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Number of repititions}
\NormalTok{reps <-}\StringTok{ }\DecValTok{10000}

\CommentTok{# Set degrees of freedom of a chi-Square Distribution}
\NormalTok{DF <-}\StringTok{ }\DecValTok{3} 

\CommentTok{# Sample 10000 column vectors à 3 N(0,1) R.V.S}
\NormalTok{Z <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(reps, }\KeywordTok{rnorm}\NormalTok{(DF)) }

\CommentTok{# Column sums of squares}
\NormalTok{X <-}\StringTok{ }\KeywordTok{colSums}\NormalTok{(Z}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\CommentTok{# Histogram of column sums of squares}
\KeywordTok{hist}\NormalTok{(X, }
     \DataTypeTok{freq =}\NormalTok{ F, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{breaks =} \DecValTok{40}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"Density"}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{""}
\NormalTok{     )}

\CommentTok{# Add theoretical density}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df =}\NormalTok{ DF), }
      \DataTypeTok{type =} \StringTok{'l'}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-47-1} \end{center}

\subsection*{Large Sample Approximations to Sampling
Distributions}\label{large-sample-approximations-to-sampling-distributions}
\addcontentsline{toc}{subsection}{Large Sample Approximations to
Sampling Distributions}

Sampling distributions as considered in the last section play an
important role in the development of econometric methods. In general,
there are two different approaches in characterizing sampling
distributions: an ``exact'' approach and an ``approximate'' approach.

The exact approach aims to find a general formula for the sampling
distribution that holds for any sample size \(n\). We call this the
\emph{exact distribution} or \emph{finite sample distribution}. In the
previous examples of die rolling and normal variates, we have dealt with
functions of random variables whose sample distributions are
\emph{excactly known} in the sense that we can write them down as
analytic expressions and do calculations. However, this is not always
possible. For \(\overline{Y}\), result (2.4) tells us that normality of
the \(Y_i\) implies normality of \(\overline{Y}\) (we demonstrated this
for the special case of \(Y_i \overset{i.i.d.}{\sim} N(0,1)\) with
\(n=10\) using a simulation study that involves simple random sampling).
Unfortunately, the \emph{exact} distribution of \(\overline{Y}\) is
generally unknown and often hard to derive (or even untraceable) if we
drop the assumption that the \(Y_i\) have a normal distribution.

Therefore, as can be guessed from its name, the ``approximate'' approach
aims to find an approximation to the sampling distribution whereby it is
required that the sample size \(n\) is large. A distribution that is
used as a large-sample approximation to the sampling distribution is
also called the \emph{asymptotic distribution}. This is due to the fact
that the asymptotic distribution \emph{is} the sampling distribution for
\(n \rightarrow \infty\) i.e.~the approximation becomes exact if the
sample size goes to infinity. However, there are cases where the
difference between the sampling distribution and the asymptotic
distribution is negligible for moderate or even small samples sizes so
that approximations using the asymptotic distribution are reasonably
good.

In this section we will discuss two well known results that are used to
approximate sampling distributions and thus constitute key tools in
econometric theory: the \emph{law of large numbers} and the
\emph{central limit theorem}. The law of large numbers states that in
large samples, \(\overline{Y}\) is close to \(\mu_Y\) with high
probability. The central limit theorem says that the sampling
distribution of the standardized sample average, that is
\((\overline{Y} - \mu_Y)/\sigma_{\overline{Y}}\) is asymptotically
normally distributed. It is particularly interesting that both results
do not depend on the distribution of \(Y\). In other words, being unable
to describe the complicated sampling distribution of \(\overline{Y}\) if
\(Y\) is not normal, approximations of the latter using the central
limit theorem simplify the development and applicability of econometric
procedures enormously. This is a key component underlying the theory of
statistical inference for regression models. Both results are summarized
in Key Concept 2.6 and Key Concept 2.7.

Key Concept 2.6

Convergence in Probability, Consistency and the Law of Large Numbers

The sample average \(\overline{Y}\) converges in probability to
\(\mu_Y\) --- we say that \(\overline{Y}\) is \emph{consistent} for
\(\mu_Y\) --- if the probability that \(\overline{Y}\) is in the range
\((\mu_Y - \epsilon)\) to \((\mu_Y + \epsilon)\) becomes arbitrary close
to \(1\) as \(n\) increases for any constant \(\epsilon > 0\). We write
this short as

\[ \overline{Y} \xrightarrow[]{p} \mu_Y. \]

Consider the independently and identically distributed random variables
\(Y_i, i=1,\dots,n\) with expectation \(E(Y_i)=\mu_Y\) and variance
\(\text{Var}(Y_i)=\sigma^2_Y\). Under the condition that
\(\sigma^2_Y< \infty\), that is large outliers are unlikely, the law of
large numbers states

\[ \overline{Y} \xrightarrow[]{p} \mu_Y. \]

The following application simulates a large number of coin tosses (you
may set the number of trials using the slider) with a fair coin and
computes the fraction of heads observed for each additional toss. The
result is a random path that, as stated by the law of large numbers,
shows a tendency to approach the value of \(0.5\) as \(n\) grows.

The core statement of the law of large numbers is that under quite
general conditions, the probability of obtaining a sample average
\(\overline{Y}\) that is close to \(\mu_Y\) is high if we have a large
sample size.

Consider the example of repeatedly tossing a coin where \(Y_i\) is the
result of the \(i^{th}\) coin toss. \(Y_i\) is a Bernoulli distributed
random variable with

\[ P(Y_i) = \begin{cases} p, & Y_i = 1 \\ 1-p, & Y_i = 0 \end{cases} \]
where \(p = 0.5\) as we assume a fair coin. It is straightforward to
show that

\[ \mu_Y = p = 0.5. \] Say \(p\) is the probability of observing head
and denote \(R_n\) the proportion of heads in the first \(n\) tosses,

\[ R_n = \frac{1}{n} \sum_{i=1}^n Y_i. \tag{2.5}\]

According to the law of large numbers, the observed proportion of heads
converges in probability to \(\mu_Y = 0.5\), the probability of tossing
head in a \emph{single} coin toss,

\[ R_n \xrightarrow[]{p} \mu_Y=0.5 \ \ \text{as} \ \ n \rightarrow \infty.  \]

We can use \texttt{R} to compute and illustrate such paths by
simulation. The procedure is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample \texttt{N} observations from the Bernoulli distribution
  e.g.~using \texttt{sample()}.
\item
  Calculate the proportion of heads \(R_n\) as in (2.5). A way to
  achieve this is to call \texttt{cumsum()} on the vector of
  observations \texttt{Y} to obtain its cumulative sum and then divide
  by the respective number of observations.
\end{enumerate}

We continue by plotting the path and also add a dashed line for the
benchmark probability \(R_n = p = 0.5\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# Set number of coin tosses and simulate}
\NormalTok{N <-}\StringTok{ }\DecValTok{30000}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, N, }\DataTypeTok{replace =}\NormalTok{T)}

\CommentTok{# Calculate R_n for 1:N}
\NormalTok{S <-}\StringTok{ }\KeywordTok{cumsum}\NormalTok{(Y)}
\NormalTok{R <-}\StringTok{ }\NormalTok{S}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N)}

\CommentTok{# Plot the path.}
\KeywordTok{plot}\NormalTok{(R, }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{), }
     \DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"n"}\NormalTok{, }
     \DataTypeTok{ylab =} \StringTok{"R_n"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Converging Share of Heads in Repeated Coin Tossing"}
\NormalTok{     )}

\CommentTok{# Add a dashed line for R_n = 0.5}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, N), }
      \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }
      \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }
      \DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{1}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-48-1} \end{center}

There are several things to be said about this plot.

\begin{itemize}
\item
  The blue graph shows the observed proportion of heads when tossing a
  coin \(n\) times.
\item
  Since the \(Y_i\) are random variables, \(R_n\) is a random variate,
  too. The path depicted is only one of many possible realizations of
  \(R_n\) as it is determined by the \(30000\) observations sampled from
  the Bernoulli distribution. Thus, the code chunk above produces a
  different path every time you execute it (try this below!).
\item
  If the number of coin tosses \(n\) is small, we observe the proportion
  of heads to be anything but close to its theoretical value,
  \(\mu_Y = 0.5\). However, as more and more observation are included in
  the sample we find that the path stabilizes in neighborhood of
  \(0.5\). This is the message to take away: the average of multiple
  trials shows a clear tendency to converge to its expected value as the
  sample size increases, just as claimed by the law of large numbers.
\end{itemize}

Key Concept 2.6

The Central Limit Theorem

Suppose that \(Y_1,\dots,Y_n\) are independently and identically
distributed random variables with expectation \(E(Y_i)=\mu_Y\) and
variance \(\text{Var}(Y_i)=\sigma^2_Y\) where \(0<\sigma^2_Y<\infty\).
The central Limit Theorem (CLT) states that, if the sample size \(n\)
goes to infinity, the distribution of the standardized sample average
\[ \frac{\overline{Y} - \mu_Y}{\sigma_{\overline{Y}}} = \frac{\overline{Y} - \mu_Y}{\sigma_Y/\sqrt{n}} \ \]
becomes arbitrarily well approximated by the standard normal
distribution.

The application below demonstrates the CLT for the sample average of
normally distributed random variables with mean \(5\) and variance
\(25^2\). You may check the following properties:

\begin{itemize}
\tightlist
\item
  The distribution of the sample average is normal.
\item
  As the sample size increases, the distribution of \(\overline{Y}\)
  tightens around the true mean of \(5\).
\item
  The distribution of the standardized sample average is close to the
  standard normal distribution for large \(n\).
\end{itemize}

According to the central limit theorem, the distribution of the sample
mean \(\overline{Y}\) of the Bernoulli distributed random variables
\(Y_i\), \(i=1,...,n\) is well approximated by the normal distribution
with parameters \(\mu_Y=p=0.5\) and
\(\sigma^2_{\overline{Y}} = p(1-p) = 0.25\) for large \(n\).
Consequently, for the standardized sample mean we conclude that the
ratio

\[ \frac{\overline{Y} - 0.5}{0.5/\sqrt{n}} \tag{2.6}\] should be well
approximated by the standard normal distribution \(N(0,1)\). We employ
another simulation study to demonstrate this graphically. The idea is as
follows.

Draw a large number of random samples, \(10000\) say, of size \(n\) from
the Bernoulli distribution and compute the sample averages. Standardize
the averages as shown in (2.6). Next, visualize the distribution of the
generated standardized sample averages by means of a density histogram
and compare to the standard normal distribution. Repeat this for
different sample sizes \(n\) to see how increasing the sample size \(n\)
impacts the simulated distribution of the averages.

In R, we realized this as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We start by defining that the next four subsequently generated figures
  shall be drawn in a \(2\times2\) array such that they can be easily
  compared. This is done by calling \texttt{par(mfrow = c(2, 2))} before
  the figures are generated.
\item
  We define the number of repetitions \texttt{reps} as \(10000\) and
  create a vector of sample sizes named \texttt{sample.sizes}. We
  consider samples of sizes \(2\), \(10\), \(50\) and \(100\).
\item
  Next, we combine two \texttt{for()} loops to simulate the data and
  plot the distributions. The inner loop generates \(10000\) random
  samples, each consisting of \texttt{n} observations that are drawn
  from the Bernoulli distribution, and computes the standardized
  averages. The outer loop executes the inner loop for the different
  sample sizes \texttt{n} and produces a plot for each iteration.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Subdivide the plot panel into a 2-by-2 array}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{# Set number of repetitions and the sample sizes}
\NormalTok{reps <-}\StringTok{ }\DecValTok{10000}
\NormalTok{sample.sizes <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{)}

\CommentTok{# outer loop (loop over the sample sizes)}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in}\NormalTok{ sample.sizes) \{}
    
\NormalTok{    samplemean <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, reps) }\CommentTok{#initialize the vector of sample menas}
\NormalTok{    stdsamplemean <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, reps) }\CommentTok{#initialize the vector of standardized sample menas}

\CommentTok{# inner loop (loop over repetitions)   }
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{reps) \{}
\NormalTok{      x <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{      samplemean[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)}
\NormalTok{      stdsamplemean[i] <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(n)}\OperatorTok{*}\NormalTok{(}\KeywordTok{mean}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\FloatTok{0.5}\NormalTok{)}\OperatorTok{/}\FloatTok{0.5}
\NormalTok{    \}}
    
\CommentTok{# plot the histogram and overlay it with the N(0,1) density for every iteration    }
    \KeywordTok{hist}\NormalTok{(stdsamplemean, }
         \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
         \DataTypeTok{breaks =} \DecValTok{40}\NormalTok{, }
         \DataTypeTok{freq =} \OtherTok{FALSE}\NormalTok{, }
         \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }
         \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.4}\NormalTok{), }
         \DataTypeTok{xlab =} \KeywordTok{paste}\NormalTok{(}\StringTok{"n ="}\NormalTok{, n), }
         \DataTypeTok{main =} \StringTok{""}
\NormalTok{         )}
    
    \KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x), }
          \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
          \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }
          \DataTypeTok{add =} \OtherTok{TRUE}
\NormalTok{          )}
\NormalTok{  \}  }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-49-1} \end{center}

We see that the simulated sampling distribution of the standardized
average tends to deviate strongly from the standard normal distribution
if the sample size is small, e.g.~for \(n=5\) and \(n=10\). However as
\(n\) grows, the histograms are approaching the bell shape of a standard
normal and we can be confident that the approximation works quite well
as seen for \(n=100\).

\section{Exercises}\label{exercises}

\begin{center}\textit{This interactive part of URFITE is only available in the HTML version}\end{center}

\chapter{A Review of Statistics using
R}\label{a-review-of-statistics-using-r}

This section reviews important statistical concepts:

\begin{itemize}
\item
  Estimation
\item
  Hypothesis Testing
\item
  Confidence Intervals
\end{itemize}

Since these types of statistical methods are heavily used in
econometrics, we will discuss them in the context of inference about an
unknown population mean and discuss several applications in \texttt{R}.

The \texttt{R} applications presented in this chapter rely on the
following packages which are not part of base \texttt{R}:

\begin{itemize}
\item
  \texttt{readxl} allows to import data from Excel to \texttt{R}.
\item
  \texttt{dplyr} provides a flexible grammar for data manipulation.
\item
  \texttt{MASS} is a collection of functions for applied statistics.
\end{itemize}

Make sure these are installed before you go ahead and replicate the
examples.

\section{Estimation of the Population
Mean}\label{estimation-of-the-population-mean}

Key Concept 3.1

Estimators and Estimates

\emph{Estimators} are functions of sample data that are drawn randomly
from an unknown population. \emph{Estimates} are numeric values computed
by estimators based on the sample data. Estimators are random variables
because they are functions of \emph{random} data. Estimates are
nonrandom numbers.

Think of some economic variable, for example hourly earnings of college
graduates, denoted by \(Y\). Suppose we are interested in \(\mu_Y\) the
mean of \(Y\). In order to exactly calculate \(\mu_Y\) we would have to
interview every graduated member of the working population in the
economy. We simply cannot do this for time and cost reasons. However, we
could draw a random sample of \(n\) i.i.d. observations
\(Y_1, \dots, Y_n\) and estimate \(\mu_Y\) using one of the simplest
estimators in the sense of Key Concept 3.1 one can think of, that is

\[ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i, \]

the sample mean of \(Y\). Then again, we could use an even simpler
estimator for \(\mu_Y\): the very first observation in the sample,
\(Y_1\). Is \(Y_1\) a good estimator? For now, assume that

\[ Y \sim \chi_{12}^2 \]

which is not too unreasonable as hourly income is non-negative and we
expect many hourly earnings to be in a range of \(5€\,\) to \(15€\).
Moreover, it is common for income distributions to be skewed to the
right --- a property of the \(\chi^2_{12}\) distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the chi_12^2 distribution}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dchisq}\NormalTok{(x, }\DataTypeTok{df=}\DecValTok{12}\NormalTok{), }
      \DataTypeTok{from =} \DecValTok{0}\NormalTok{, }
      \DataTypeTok{to =} \DecValTok{40}\NormalTok{, }
      \DataTypeTok{ylab =} \StringTok{"density"}\NormalTok{, }
      \DataTypeTok{xlab =} \StringTok{"hourly earnings in Euro"}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-64-1} \end{center}

We now draw a sample of \(n=100\) observations and take the first
observation \(Y_1\) as an estimate for \(\mu_Y\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# sample from the chi_12^2 distribution, keep only the first observation}
\KeywordTok{rchisq}\NormalTok{(}\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }\DataTypeTok{df =} \DecValTok{12}\NormalTok{)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.257893
\end{verbatim}

The estimate \(8.26\) is not too far away from \(\mu_Y = 12\) but it is
somewhat intuitive that we could do better: the estimator \(Y_1\)
discards a lot of information and its variance is the population
variance:

\[ \text{Var}(Y_1) = \text{Var}(Y) = 2 \cdot 12 = 24 \]

This brings us to the following question: What is a \emph{good}
estimator of an unknown parameter in the first place? This question is
tackled in Key Concepts 3.2 and 3.3

Key Concept 3.2

Bias, Consistency and Efficiency

Desirable characteristics of an estimator are unbiasedness, consistency
and efficiency.

\textbf{Unbiasedness:}\\
If the mean of the sampling distribution of some estimator \(\hat\mu_Y\)
for the population mean \(\mu_Y\) equals \(\mu_Y\)
\[ E(\hat\mu_Y) = \mu_Y \] we say that the estimator is unbiased for
\(\mu_Y\). The \emph{bias} of \(\hat\mu_Y\) is \(0\):

\[ E(\hat\mu_Y) - \mu_Y = 0\]

\textbf{Consistency:}

We want the uncertainty of the estimator \(\mu_Y\) to decrease as the
number of observations in the sample grows. More precisely, we want the
probability that the estimate \(\hat\mu_Y\) falls within a small
interval of the true value \(\mu_Y\) to get increasingly closer to \(1\)
as \(n\) grows. We write this as

\[ \hat\mu_Y \xrightarrow{p} \mu_Y. \]

\textbf{Variance and efficiency:}

We want the estimator to be efficient. Suppose we have two estimators,
\(\hat\mu_Y\) and \(\overset{\sim}{\mu}_Y\) and for some given sample
size \(n\) it holds that

\[ E(\hat\mu_Y) = E(\overset{\sim}{\mu}_Y) = \mu_Y \] but
\[\text{Var}(\hat\mu_Y) < \text{Var}(\overset{\sim}{\mu}_Y).\]

We then would prefer to use \(\hat\mu_Y\) as it has a lower variance
than \(\overset{\sim}{\mu}_Y\), meaning that \(\hat\mu_Y\) is more
\emph{efficient} in using the information provided by the observations
in the sample.

Key Concept 3.3

Efficiency of \(\overline{Y}\): The BLUE Property

Let \(\hat\mu_Y\) be a linear and unbiased estimator of \(\mu_Y\) in the
fashion of

\[ \hat\mu_Y = \frac{1}{n} \sum_{i=1}^n a_i Y_i\]

with nonrandom constants \(a_i\). We see that \(\hat\mu_Y\) is a
weighted average of the \(Y_i\) and the \(a_i\) are weights. For these
type of estimators, \(\overline{Y}\) with \(a_i = 1\) for all
\(i = 1, \dots, n\) is the most efficient estimator. We say that
\(\overline{Y}\) is the \textbf{B}est \textbf{L}inear \textbf{U}nbiased
\textbf{E}stimator (BLUE).

\section{Properties of the Sample
Mean}\label{properties-of-the-sample-mean}

To examine properties of the sample mean as an estimator for the
corresponding population mean, consider the following \texttt{R}example.

We generate a population \texttt{pop} which consists observations
\(Y_i \ , \ i=1,\dots,10000\) that origin from a normal distribution
with mean \(\mu = 10\) and variance \(\sigma^2 = 1\). To investigate how
the estimator \(\hat{\mu} = \bar{Y}\) behaves we can draw random samples
from this population and calculate \(\bar{Y}\) for each of them. This is
easily done by making use of the function \texttt{replicate()}. The
argument \texttt{expr} is evaluated \texttt{n} times. In this case we
draw samples of sizes \(n=5\) and \(n=25\), compute the sample means and
repeat this exactly \(n=25000\) times.

For comparison purposes we store results for the estimator \(Y_1\), the
first observation in a sample for a sample of size \(5\), separately.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate a fictive population}
\NormalTok{pop <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{# sample form pop and estimate the mean}
\NormalTok{est1 <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{expr =} \KeywordTok{mean}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pop, }\DataTypeTok{size =} \DecValTok{5}\NormalTok{)), }\DataTypeTok{n =} \DecValTok{25000}\NormalTok{)}

\NormalTok{est2 <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{expr =} \KeywordTok{mean}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pop, }\DataTypeTok{size =} \DecValTok{25}\NormalTok{)), }\DataTypeTok{n =} \DecValTok{25000}\NormalTok{)}

\NormalTok{fo <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{expr =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pop, }\DataTypeTok{size =} \DecValTok{5}\NormalTok{)[}\DecValTok{1}\NormalTok{], }\DataTypeTok{n =} \DecValTok{25000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Check that \texttt{est1} and \texttt{est2} are vectors of length
\(25000\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check if object type is vector}
\KeywordTok{is.vector}\NormalTok{(est1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.vector}\NormalTok{(est2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check length}
\KeywordTok{length}\NormalTok{(est1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(est2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25000
\end{verbatim}

The code chunk below produces a plot of the sampling distributions of
the estimators \(\bar{Y}\) and \(Y_1\) on the basis of the \(25000\)
samples in each case. We also plot the density function of the
\(N(10,1)\) distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot density estimate Y_1}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(fo), }
      \DataTypeTok{col =} \StringTok{'green'}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{),}
      \DataTypeTok{xlab =} \StringTok{'estimates'}\NormalTok{,}
      \DataTypeTok{main =} \StringTok{'Sampling Distributions of Unbiased Estimators'}
\NormalTok{      )}

\CommentTok{# add density estimate for the distribution of the sample mean with n=5 to the plot}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(est1), }
     \DataTypeTok{col =} \StringTok{'steelblue'}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{bty =} \StringTok{'l'}
\NormalTok{     )}

\CommentTok{# add density estimate for the distribution of the sample mean with n=25 to the plot}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(est2), }
      \DataTypeTok{col =} \StringTok{'red2'}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{      )}

\CommentTok{# add a vertical line marking the true parameter}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \DecValTok{10}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add N(10,1) density to the plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{mean =} \DecValTok{10}\NormalTok{), }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{lty =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{add =}\NormalTok{ T}
\NormalTok{     )}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"N(10,1)"}\NormalTok{,}
                  \KeywordTok{expression}\NormalTok{(Y[}\DecValTok{1}\NormalTok{]),}
                  \KeywordTok{expression}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ }\NormalTok{n }\OperatorTok{==}\StringTok{ }\DecValTok{5}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ }\NormalTok{n }\OperatorTok{==}\StringTok{ }\DecValTok{25}\NormalTok{)}
\NormalTok{                  ), }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{'black'}\NormalTok{,}\StringTok{'green'}\NormalTok{, }\StringTok{'steelblue'}\NormalTok{, }\StringTok{'red2'}\NormalTok{),}
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-68-1} \end{center}

At first, notice that \emph{all} sampling distributions (represented by
the solid lines) are centered around \(\mu = 10\). This is evidence for
the \emph{unbiasedness} of \(Y_1\), \(\overline{Y}_{n=5}\) and
\(\overline{Y}_{n=25}\). Of course, the theoretical density the
\(N(10,1)\) distribution is centered at \(10\), too.

Next, have a look at the spread of the sampling distributions. Several
things are remarkable:

\begin{itemize}
\item
  the sampling distribution of \(Y_1\) (green curve) tracks the density
  of the \(N(10,1)\) distribution (black dashed line) pretty closely In
  fact, the sampling distribution of \(Y_1\) is the \(N(10,1)\)
  distribution. This is less surprising if You keep in mind that the
  \(Y_1\) estimator does nothing but reporting an observation that is
  randomly selected from a population with \(N(10,1)\) distribution.
  Hence, \(Y_1 \sim N(10,1)\). Note that this result does not depend on
  the sample size \(n\): the sampling distribution of \(Y_1\) \emph{is
  always} the population distribution, no matter how large the sample
  is.
\item
  Both sampling distributions of \(\overline{Y}\) show less dispersion
  than the sampling distribution of \(Y_1\). This means that
  \(\overline{Y}\) has a lower variance than \(Y_1\). In view of Key
  Concepts 3.2 and 3.3, we find that \(\overline{Y}\) is a more
  efficient estimator than \(Y_1\). In fact, one can show that this
  holds for all \(n>1\).
\item
  \(\overline{Y}\) shows a behavior that is termed consistency (see Key
  Concept 3.2). Notice that the blue and the red density curves are much
  more concentrated around \(\mu=10\) then the green one. As the number
  of observations is increased from \(1\) to \(5\), the sampling
  distribution tightens around the true parameter. Increasing the sample
  size to \(25\) this effect becomes more dominant. This implies that
  the probability of obtaining estimates that are close to the true
  value increases with \(n\).
\end{itemize}

\BeginKnitrBlock{rmdknit}
A more precise way to express consitency of an estimator \(\hat\mu\) for
a parameter \(\mu\) is

\[ P(|\hat{\mu} - \mu|<\epsilon) \xrightarrow[n \rightarrow \infty]{p} 1 \quad \text{for any}\quad\epsilon>0.\]

This expression says that the probability of observing a deviation from
the true value \(\mu\) that is smaller than some arbitrary
\(\epsilon > 0\) converges to \(1\) as \(n\) grows. Note that
consistency does not require unbiasedness.
\EndKnitrBlock{rmdknit}

We encourage you to go ahead and modify the code. Try out different
values for the sample size and see how the sampling distribution of
\(\overline{Y}\) changes!

\subsubsection*{\texorpdfstring{\(\overline{Y}\) is the Least Squares
Estimator of
\(\mu_Y\)}{\textbackslash{}overline\{Y\} is the Least Squares Estimator of \textbackslash{}mu\_Y}}\label{overliney-is-the-least-squares-estimator-of-mu_y}
\addcontentsline{toc}{subsubsection}{\(\overline{Y}\) is the Least
Squares Estimator of \(\mu_Y\)}

Assume you have some observations \(Y_1,\dots,Y_n\) on
\(Y \sim N(10,1)\) (which is unknown) and would like to find an
estimator \(m\) that predicts the observations as good as possible. Good
means to choose \(m\) such that the total deviation between the
predicted value and the observed values is small. Mathematically this
means we want to find an \(m\) that minimizes

\begin{equation}
  \sum_{i=1}^n (Y_i - m)^2. \label{eq:sqm}
\end{equation}

Think of \(Y_i - m\) as the committed mistake when predicting \(Y_i\) by
\(m\). We could just as well minimize the sum of absolute deviations
from \(m\) but minimizing the sum of squared deviations is
mathematically more convenient (and may lead to a different result).
That is why the estimator we are looking for is called the \emph{least
squares estimator}. As It turns out \(m = \overline{Y}\), the estimator
of \(\mu_Y=10\) is this wanted estimator.

We can show this by generating a random sample of fair size and plotting
\eqref{eq:sqm} as a function of \(m\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the function and vectorize it}
\NormalTok{sqm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(m) \{}
 \KeywordTok{sum}\NormalTok{((y}\OperatorTok{-}\NormalTok{m)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\NormalTok{sqm <-}\StringTok{ }\KeywordTok{Vectorize}\NormalTok{(sqm)}

\CommentTok{# draw random sample and compute the mean}
\NormalTok{y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10.00543
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the objective function}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{sqm}\NormalTok{(x), }
      \DataTypeTok{from =} \OperatorTok{-}\DecValTok{50}\NormalTok{, }
      \DataTypeTok{to =} \DecValTok{70}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{"m"}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{"sqm(m)"}
\NormalTok{      )}

\CommentTok{# add vertical line at mean(y)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \KeywordTok{mean}\NormalTok{(y), }
       \DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }
       \DataTypeTok{col =} \StringTok{"darkred"}
\NormalTok{       )}

\CommentTok{# add annotation at mean(y)}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{mean}\NormalTok{(y), }
     \DataTypeTok{y =} \DecValTok{0}\NormalTok{, }
     \DataTypeTok{labels =} \KeywordTok{paste}\NormalTok{(}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(y), }\DecValTok{2}\NormalTok{))}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-70-1} \end{center}

Notice that \eqref{eq:sqm} is a quadratic function so there is only one
minimum. The plot shows that this minimum lies exactly at the sample
mean of the sample data.

\BeginKnitrBlock{rmdknit}
Some R functions can only interact with functions that take a vector as
an input and evaluate the function body on every values of the vector,
for example curve(). We call such functions vectorized functions and it
is often a good idea to write vectorized functions yourself although
this is cumbersome in some cases. Having a vectorized function in R is
never a drawback since these functions work on both single values and
vectors.

Let us look at the function sqm() which is nonvectorized

 sqm \textless{}- function(m) \{\\
\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}\hspace*{0.333em}
sum((y-m)\^{}2) \#body of the function\\
\}

Providing e.g. c(1,2,3) as the argument m would cause an error since
then the operation y-m is invalid: the vecors y and m are of
incompatible dimensions. This is why we cannot use sqm() in conjunction
with curve().

Here comes Vectorize() into play. It generates a vectorized version of a
non-vectorized function.
\EndKnitrBlock{rmdknit}

\subsubsection*{Why Random Sampling is
Important}\label{why-random-sampling-is-important}
\addcontentsline{toc}{subsubsection}{Why Random Sampling is Important}

So far, we assumed (sometimes implicitly) that observed data
\(Y_1, \dots, Y_n\) are the result of a sampling process that satisfies
the assumption of i.i.d. random sampling. It is very important that this
assumption is fulfilled when estimating a population mean using
\(\overline{Y}\). If this is not the case, estimates are biased.

Let us fall back to \texttt{pop}, the fictive population of \(10000\)
observations and compute the population mean \(\mu_{\texttt{pop}}\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the population mean of pop}
\KeywordTok{mean}\NormalTok{(pop)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.992604
\end{verbatim}

Next we sample \(10\) observations from \texttt{pop} with
\texttt{sample()} and estimate \(\mu_{\texttt{pop}}\) using
\(\overline{Y}\) repeatedly. However this time we use a sampling scheme
that deviates from simple random sampling: instead of ensuring that each
member of the population has the same chance to end up in a sample, we
assign a higher probability of being sampled to the \(2500\) smallest
observations of the population by setting the argument \texttt{prop} to
a suitable vector of probability weights:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate outcome for the sample mean when the i.i.d. assumption fails}
\NormalTok{est3 <-}\StringTok{  }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =} \DecValTok{25000}\NormalTok{, }
                   \DataTypeTok{expr =} \KeywordTok{mean}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{sort}\NormalTok{(pop), }
                                      \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }
                                      \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{2500}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{7500}\NormalTok{))}
\NormalTok{                                      )}
\NormalTok{                          )}
\NormalTok{                   )}

\CommentTok{# compute the sample mean of the outcomes}
\KeywordTok{mean}\NormalTok{(est3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.443454
\end{verbatim}

Next we plot the sampling distribution of \(\overline{Y}\) for this
non-i.i.d. case and compare it to the sampling distribution when the
i.i.d. assumption holds.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sampling distribution of sample mean, i.i.d. holds, n=25}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(est2), }
      \DataTypeTok{col =} \StringTok{'red2'}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{11}\NormalTok{),}
      \DataTypeTok{xlab =} \StringTok{'Estimates'}\NormalTok{,}
      \DataTypeTok{main =} \StringTok{'When the i.i.d. Assumption Fails'}
\NormalTok{     )}

\CommentTok{# sampling distribution of sample mean, i.i.d. fails, n=25}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(est3),}
      \DataTypeTok{col =} \StringTok{'steelblue'}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{      )}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\KeywordTok{expression}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ ","} \OperatorTok{~}\StringTok{ }\NormalTok{n }\OperatorTok{==}\StringTok{ }\DecValTok{25} \OperatorTok{~}\StringTok{ ", i.i.d. fails"}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y) }\OperatorTok{~}\StringTok{ ","} \OperatorTok{~}\StringTok{ }\NormalTok{n }\OperatorTok{==}\StringTok{ }\DecValTok{25} \OperatorTok{~}\StringTok{ ", i.i.d. holds"}\NormalTok{)}
\NormalTok{                  ), }
       \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{'red2'}\NormalTok{, }\StringTok{'steelblue'}\NormalTok{),}
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-73-1} \end{center}

We find that in this case failure of the i.i.d. assumption implies that,
on average, we \emph{underestimate} \(\mu_Y\) using \(\overline{Y}\):
the corresponding distribution of \(\overline{Y}\) is shifted to the
left. In other words, \(\overline{Y}\) is a \emph{biased} estimator for
\(\mu_Y\) if the i.i.d. assumption does not hold.

\section{Hypothesis Tests Concerning the Population
Mean}\label{hypothesis-tests-concerning-the-population-mean}

In this section we briefly review concepts in hypothesis testing and
discuss how to conduct hypothesis tests in \texttt{R}. We focus on
drawing inferences about an unknown population mean.

\subsubsection*{About Hypotheses and Hypothesis
Testing}\label{about-hypotheses-and-hypothesis-testing}
\addcontentsline{toc}{subsubsection}{About Hypotheses and Hypothesis
Testing}

In a significance test we want to exploit the information contained in a
random sample as evidence in favor or against a hypothesis. Essentially,
hypotheses are simple questions that can be answered by `yes' or `no'.
When conducting a hypothesis test we always deal with two different
hypotheses:

\begin{itemize}
\item
  The \textbf{null hypothesis}, denoted \(H_0\) is the hypothesis we are
  interested in testing.
\item
  The \textbf{alternative hypothesis}, denoted \(H_1\), is the
  hypothesis that holds if the null hypothesis is false.
\end{itemize}

The null hypothesis that the population mean of \(Y\) equals the value
\(\mu_{Y,0}\) is written down as

\[ H_0: E(Y) = \mu_{Y,0}. \]

The alternative hypothesis states what holds if the null hypothesis is
false. Often the alternative hypothesis chosen is the most general one,

\[ H_1: E(Y) \neq \mu_{Y,0}, \]

meaning that \(E(Y)\) may be anything but the value under the null
hypothesis. This is called a \emph{two-sided} alternative.

For the sake of brevity, we will only consider the case of a two-sided
alternative in the subsequent sections of this chapter.

\subsection*{The p-Value}\label{the-p-value}
\addcontentsline{toc}{subsection}{The p-Value}

Assume that the null hypothesis is \emph{true}. The \(p\)-value is the
probability of drawing data and observing a corresponding test statistic
that is at least as adverse to what is stated under the null hypothesis
as the test statistic actually computed using the sample data.

In the context of the population mean and the sample mean, this
definition can be stated mathematically in the following way:

\begin{equation}
p \text{-value} = P_{H_0}\left[ \lvert \overline{Y} - \mu_{Y,0} \rvert > \lvert \overline{Y}^{act} - \mu_{Y,0} \rvert \right] \label{eq:pvalue}
\end{equation}

In \eqref{eq:pvalue}, \(\overline{Y}^{act}\) is the actually computed mean
of the random sample. Visualized, the \(p\)-value is the area in the
part of tails of the distribution of \(\overline{Y}\) that lies beyond

\[ \mu_{Y,0} \pm \lvert \overline{Y}^{act} - \mu_{Y,0} \rvert. \]

Consequently, in order to compute the \(p\)-value as in \eqref{eq:pvalue},
knowledge about the sampling distribution of \(\overline{Y}\) when the
null hypothesis is true is required. However in most cases the sampling
distribution of \(\overline{Y}\) is unknown. Fortunately, due to the
large-sample normal approximation we know that under the null hypothesis

\[ \overline{Y} \sim N(\mu_{Y,0}, \, \sigma^2_{\overline{Y}}) \ \ , \ \ \sigma^2_{\overline{Y}} = \frac{\sigma_Y^2}{n} \]

and thus

\[ \frac{\overline{Y} - \mu_{Y,0}}{\sigma_Y/\sqrt{n}} \sim N(0,1). \]

So in large samples, the \(p\)-value can be computed \emph{without}
knowledge of the exact sampling distribution of \(\overline{Y}\).

\subsection*{\texorpdfstring{Calculating the \(p\)-Value When
\(\sigma_Y\) Is
Known}{Calculating the p-Value When \textbackslash{}sigma\_Y Is Known}}\label{calculating-the-p-value-when-sigma_y-is-known}
\addcontentsline{toc}{subsection}{Calculating the \(p\)-Value When
\(\sigma_Y\) Is Known}

For now, let us assume that \(\sigma_{\overline{Y}}\) is known. Then we
can rewrite \eqref{eq:pvalue} as

\begin{align}
p \text{-value} =& \, P_{H_0}\left[ \left\lvert \frac{\overline{Y} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert > \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert \right] \\
=& \, 2 \cdot \Phi \left[ - \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}}  \right\rvert\right].  \label{eq:pvaluenorm1}
\end{align}

The \(p\)-value can be seen as the area in the tails of the \(N(0,1)\)
distribution that lies beyond

\begin{equation}
\pm \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert \label{eq:pvaluenorm2}
\end{equation}

We now use \texttt{R} to visualize what is stated in
\eqref{eq:pvaluenorm1} and \eqref{eq:pvaluenorm2}. The next code chunk
replicates figure 3.1 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal density on the interval [-4,4]}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x),}
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{),}
      \DataTypeTok{main =} \StringTok{'Calculating a p-Value'}\NormalTok{,}
      \DataTypeTok{yaxs =} \StringTok{'i'}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{'z'}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{''}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{axes =} \StringTok{'F'}
\NormalTok{      )}

\CommentTok{# add x-axis}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }
     \DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{1.5}\NormalTok{), }
     \DataTypeTok{padj =} \FloatTok{0.75}\NormalTok{,}
     \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\KeywordTok{expression}\NormalTok{(}\OperatorTok{-}\KeywordTok{frac}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y)}\OperatorTok{^}\StringTok{"act"}\OperatorTok{~-}\ErrorTok{~}\KeywordTok{bar}\NormalTok{(mu)[Y,}\DecValTok{0}\NormalTok{], sigma[}\KeywordTok{bar}\NormalTok{(Y)])),}
                \DecValTok{0}\NormalTok{,}
                \KeywordTok{expression}\NormalTok{(}\KeywordTok{frac}\NormalTok{(}\KeywordTok{bar}\NormalTok{(Y)}\OperatorTok{^}\StringTok{"act"}\OperatorTok{~-}\ErrorTok{~}\KeywordTok{bar}\NormalTok{(mu)[Y,}\DecValTok{0}\NormalTok{], sigma[}\KeywordTok{bar}\NormalTok{(Y)])))}
\NormalTok{     )}

\CommentTok{# shade p-value/2 region in left tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\OperatorTok{-}\FloatTok{1.5}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\OperatorTok{-}\FloatTok{1.5}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\OperatorTok{-}\FloatTok{1.5}\NormalTok{, }\FloatTok{0.01}\NormalTok{)),}\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'steelblue'}
\NormalTok{        )}

\CommentTok{# shade p-value/2 region in right tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{6}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'steelblue'}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-74-1} \end{center}

\hypertarget{SVSSDASE}{\subsection*{Sample Variance, Sample Standard
Deviation and Standard Error}\label{SVSSDASE}}
\addcontentsline{toc}{subsection}{Sample Variance, Sample Standard
Deviation and Standard Error}

If \(\sigma^2_Y\) is unknown, it must be estimated. This can be done
efficiently using the sample variance

\begin{equation}
s_y^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2.
\end{equation}

Furthermore

\begin{equation}
s_y = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2}
\end{equation}

is a suitable estimator for the standard deviation of \(Y\). In R,
\(s_y\) is implemented in the function \texttt{sd()}, use \texttt{?sd}.

Using \texttt{R} we can get a notion that \(s_y\) is a consistent
estimator for \(\sigma_Y\), that is

\[ s_Y \overset{p}{\longrightarrow} \sigma_Y. \]

The idea here is to generate a large number of samples \(Y_1,\dots,Y_n\)
where, \(Y\sim N(10,10)\) say, estimate \(\sigma_Y\) using \(s_y\) and
investigate how the distribution of \(s_Y\) changes as \(n\) gets
larger.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# vector of sample sizes}
\NormalTok{n <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{5000}\NormalTok{, }\DecValTok{2000}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{500}\NormalTok{)}

\CommentTok{# sample observations, estimate using sd() and plot estimated distributions}
\NormalTok{s2_y <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10000}\NormalTok{, }\DataTypeTok{expr =} \KeywordTok{sd}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n[}\DecValTok{1}\NormalTok{], }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(s2_y),}
     \DataTypeTok{main =} \KeywordTok{expression}\NormalTok{(}\StringTok{'Sampling Distributions of'} \OperatorTok{~}\StringTok{ }\NormalTok{s[y]),}
     \DataTypeTok{xlab =} \KeywordTok{expression}\NormalTok{(s[y]),}
     \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{     )}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\KeywordTok{length}\NormalTok{(n)) \{}
\NormalTok{  s2_y <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DataTypeTok{n =} \DecValTok{10000}\NormalTok{, }\DataTypeTok{expr =} \KeywordTok{sd}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n[i], }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)))}
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(s2_y), }
        \DataTypeTok{col =}\NormalTok{ i, }
        \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{        )}
\NormalTok{\}}

\CommentTok{# add a legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\KeywordTok{expression}\NormalTok{(n }\OperatorTok{==}\StringTok{ }\DecValTok{10000}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(n }\OperatorTok{==}\StringTok{ }\DecValTok{5000}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(n }\OperatorTok{==}\StringTok{ }\DecValTok{2000}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(n }\OperatorTok{==}\StringTok{ }\DecValTok{1000}\NormalTok{),}
                  \KeywordTok{expression}\NormalTok{(n }\OperatorTok{==}\StringTok{ }\DecValTok{500}\NormalTok{)}
\NormalTok{       ), }
       \DataTypeTok{col =} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-75-1} \end{center}

The plot shows that the distribution of \(s_Y\) tightens around the true
value \(\sigma_Y = 10\) as \(n\) increases.

The function that estimates the standard deviation of an estimator is
called the \emph{standard error of the estimator}. Key Concept 3.4
summarizes the terminology in the context of the sample mean.

Key Concept 3.4

The Standard Error of \(\overline{Y}\)

Take an i.i.d. sample \(Y_1, \dots, Y_n\). The mean of \(Y\) can be
consistently estimated using \(\overline{Y}\), the sample mean of the
\(Y_i\). Since \(\overline{Y}\) is a random variable, it has a sampling
distribution with variance \(\frac{\sigma_Y^2}{n}\).

The standard error of \(\overline{Y}\), denoted \(SE(\overline{Y})\) is
an estimator of the standard deviation of \(\overline{Y}\):

\[ SE(\overline{Y}) = \hat\sigma_{\overline{Y}} = \frac{s_Y}{\sqrt{n}} \]

The caret (\^{}) over \(\sigma\) indicates that
\(\hat\sigma_{\overline{Y}}\) is an estimator for
\(\sigma_{\overline{Y}}\).

As an example to underpin Key Concept 3.4, consider a sample of
\(n=100\) i.i.d. observations of the Bernoulli distributed variable
\(Y\) with success probability \(p=0.1\) and thus \(E(Y)=p=0.1\) and
\(\text{Var}(Y)=p(1-p)\). \(E(Y)\) can be estimated by \(\overline{Y}\)
which then has variance

\[ \sigma^2_{\overline{Y}} = p(1-p)/n = 0.0009 \]

and standard deviation

\[ \sigma_{\overline{Y}} = \sqrt{p(1-p)/n} = 0.03. \]

In this case the standard error of \(\overline{Y}\) is given by

\[ SE(\overline{Y}) = \sqrt{\overline{Y}(1-\overline{Y})/n}. \]

Let us verify whether \(\overline{Y}\) and \(SE(\overline{Y})\) estimate
the respective true values, on average.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# draw 10000 samples of size 100 and estimate the mean of Y and}
\CommentTok{# estimate the standard error of the sample mean}

\NormalTok{mean_estimates <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}
\NormalTok{se_estimates <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
\NormalTok{  s <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }
              \DataTypeTok{size =} \DecValTok{100}\NormalTok{,  }
              \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
              \DataTypeTok{replace =}\NormalTok{ T}
\NormalTok{              )}
\NormalTok{  mean_estimates[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(s)}
\NormalTok{  se_estimates[i] <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(s)}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(s))}\OperatorTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{\}}

\KeywordTok{mean}\NormalTok{(mean_estimates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.099693
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(se_estimates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.02953467
\end{verbatim}

Both estimators seem to be unbiased for the true parameters.

\subsection*{Calculating the p-value When the Standard Deviation is
Unknown}\label{calculating-the-p-value-when-the-standard-deviation-is-unknown}
\addcontentsline{toc}{subsection}{Calculating the p-value When the
Standard Deviation is Unknown}

When \(\sigma_Y\) is unknown, the \(p\)-value for a hypothesis test
concerning \(\mu_Y\) using \(\overline{Y}\) can be computed by replacing
\(\sigma_{\overline{Y}}\) in \eqref{eq:pvaluenorm1} by the standard error
\(SE(\overline{Y}) = \hat\sigma_Y\). Then,

\[ p\text{-value} = 2\cdot\Phi\left(-\left\lvert \frac{\overline{Y}^{act}-\mu_{Y,0}}{SE(\overline{Y})} \right\rvert \right). \]

This is easily done in \texttt{R}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sample and estimate, compute standard error and make a hypothesis}
\NormalTok{samplemean_act <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}
  \KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }
         \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }
         \DataTypeTok{replace =}\NormalTok{ T, }
         \DataTypeTok{size =} \DecValTok{100}
\NormalTok{         )}
\NormalTok{  )}

\NormalTok{SE_samplemean <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(samplemean_act }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{-}\NormalTok{samplemean_act)}\OperatorTok{/}\DecValTok{100}\NormalTok{)}

\NormalTok{mean_h0 <-}\StringTok{ }\FloatTok{0.1} \CommentTok{#true null hypothesis}

\CommentTok{# compute the p-value}
\NormalTok{pvalue <-}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\KeywordTok{abs}\NormalTok{(samplemean_act }\OperatorTok{-}\StringTok{ }\NormalTok{mean_h0)}\OperatorTok{/}\NormalTok{SE_samplemean)}
\NormalTok{pvalue}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5382527
\end{verbatim}

\subsection*{The t-statistic}\label{the-t-statistic}
\addcontentsline{toc}{subsection}{The t-statistic}

In hypothesis testing, the standardized sample average

\begin{equation}
t = \frac{\overline{Y} - \mu_{Y,0}}{SE(\overline{Y})} \label{eq:tstat}
\end{equation}

is called \(t\)-statistic. This \(t\)-statistic plays an important role
in testing hypotheses about \(\mu_Y\). It is a prominent example of a
test statistic.

Implicitly, we already have computed a \(t\)-statistic for
\(\overline{Y}\) in the previous code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute a t-statistic for the sample mean}
\NormalTok{tstatistic <-}\StringTok{ }\NormalTok{(samplemean_act }\OperatorTok{-}\StringTok{ }\NormalTok{mean_h0) }\OperatorTok{/}\StringTok{ }\NormalTok{SE_samplemean}
\NormalTok{tstatistic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6154575
\end{verbatim}

Using \texttt{R}we can show that if \(\mu_{Y,0}\) equals the true value,
that is the null hypothesis is true, \eqref{eq:tstat} is approximately
distributed \(N(0,1)\) when \(n\) is large.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prepare empty vector for t-statistics}
\NormalTok{tstatistics <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}

\CommentTok{# set sample size}
\NormalTok{n <-}\StringTok{ }\DecValTok{300}

\CommentTok{# simulate 10000 t-statistics}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
\NormalTok{  s <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }
              \DataTypeTok{size =}\NormalTok{ n,  }
              \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
              \DataTypeTok{replace =}\NormalTok{ T}
\NormalTok{              )}
\NormalTok{  tstatistics[i] <-}\StringTok{ }\NormalTok{(}\KeywordTok{mean}\NormalTok{(s)}\OperatorTok{-}\FloatTok{0.1}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(s)}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(s))}\OperatorTok{/}\NormalTok{n))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot density and compare to N(0,1) density}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(tstatistics),}
     \DataTypeTok{xlab =} \StringTok{'t-statistic'}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{'Distribution of the t-statistic when n=300'}\NormalTok{,}
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{),}
     \DataTypeTok{col =} \StringTok{'steelblue'}
\NormalTok{     )}

\CommentTok{# N(0,1) density (dashed)}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x), }
      \DataTypeTok{add =}\NormalTok{ T, }
      \DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-80-1} \end{center}

Judging from the plot, the normal approximation works reasonably well
for the chosen sample size. This normal approximation has already been
used in the definition of the \(p\)-value, see \eqref{eq:tstat}.

\subsection*{Hypothesis Testing with a Prespecified Significance
Level}\label{hypothesis-testing-with-a-prespecified-significance-level}
\addcontentsline{toc}{subsection}{Hypothesis Testing with a Prespecified
Significance Level}

Key Concept 3.5

The Terminology of Hypothesis Testing

In hypothesis testing, two types of mistakes are possible:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The null hypothesis \emph{is} rejected although it is true
  (\(\alpha\)-error / type-I-error)
\item
  The null hypothesis \emph{is not} rejected although it is false
  (\(\beta\)-error / type-II-error)
\end{enumerate}

The \textbf{significance level} of the test is the probability to commit
a type-I-error we are willing to accept in advance. E.g. using a
prespecified significance level of \(0.05\), we reject the null
hypothesis if and only if the \(p\)-value is less than \(0.05\). The
significance level is chosen before the test is conducted.

An equivalent procedure is to reject the null hypothesis if the test
statistic observed is, in absolute value terms, larger than the
\textbf{critical value} of the test statistic. The critical value is
determined by the significance level chosen and defines two disjoint
sets of values which are called \textbf{acceptance region} and
\textbf{rejection region}. The acceptance region contains all values of
the test statistic for which the test does not reject while the
rejection region contains all the values for which the test does reject.

The \textbf{\(p\)-value} is the probability that, in repeated sampling
under the same conditions, meaning i.i.d. sampling, the same null
hypothesis and the same sample size, a test statistic is observed that
provides just as much evidence against the null hypothesis as the test
statistic actually observed.

The actual probability that the test rejects the true null hypothesis is
called the \textbf{size of the test}. In an ideal setting, the size does
not exceed the significance level.

The probability that the test correctly rejects a false null hypothesis
is called \textbf{power}.

Reconsider \texttt{pvalue} computed further above:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check whether p-value < 0.05}
\NormalTok{pvalue }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

The condition is not fulfilled so we do not reject the null hypothesis
(remember that the null hypothesis is true in this example).

When working with a \(t\)-statistic instead, it is equivalent to apply
the following rule:

\[ \text{Reject } H_0 \text{ if } \lvert t^{act} \rvert > 1.96 \]

We reject the null hypothesis at the significance level of \(5\%\) if
the computed \(t\)-statistic lies beyond the critical value of 1.96 in
absolute value terms. \(1.96\) is the \(0.05\)-quantile of the standard
normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check the critical value}
\KeywordTok{qnorm}\NormalTok{(}\DataTypeTok{p =} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -1.644854
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check whether the null is rejected using the t-statistic computed further above}
\KeywordTok{abs}\NormalTok{(tstatistic) }\OperatorTok{>}\StringTok{ }\FloatTok{1.96}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

Just like using the \(p\)-value, we cannot reject the null hypothesis
using the corresponding \(t\)-statistic. Key Concept 3.6 summarizes the
procedure of performing a two-sided hypothesis about the population mean
\(E(Y)\).

Key Concept 3.6

Testing the Hypothesis \(E(Y) = \mu_{Y,0}\) Against the Alternative
\(E(Y) \neq \mu_{Y,0}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate \(\mu_{Y}\) using \(\overline{Y}\) and compute the standard
  error of \(\overline{Y}\), \(SE(\overline{Y})\).
\item
  Compute the \(t\)-statistic.
\item
  Compute the \(p\)-value and reject the null hypothesis at the \(5\%\)
  level of significance if the \(p\)-value is smaller than \(0.05\) or
  equivalently, if
\end{enumerate}

\[ \left\lvert t^{act} \right\rvert > 1.96. \]

\subsection*{One-sided Alternatives}\label{one-sided-alternatives}
\addcontentsline{toc}{subsection}{One-sided Alternatives}

Sometimes we are interested in finding evidence that the mean is bigger
or smaller than some value hypothesized under the null. One can come up
with many examples here but, to stick to the book, take the presumed
wage gap between well and less educated working individuals. Since we
hope that this differential exists, a relevant alternative (to the null
hypothesis that there is no wage differential) is that good educated
individuals earn more, i.e.~that the average hourly wage for this group,
\(\mu_Y\) is \emph{bigger} than \(\mu_{Y,0}\) the know average wage of
less educated workers.

This is an example of a \emph{right-sided test} and the hypotheses pair
is chosen to be

\[ H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs} \ \ H_1: \mu_Y > \mu_{Y,0}. \]

We reject the null hypothesis if the computed test-statistic is larger
than the critical value \(1.64\), the \(0.95\)-quantile of the
\(N(0,1)\) distribution. This ensures that \(1-0.95=5\%\) probability
mass remains in the area to the right of the critical value. Similar as
before we can visualize this in \texttt{R} using the function
\texttt{polygon()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal density on the domain [-4,4]}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x),}
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{),}
      \DataTypeTok{main =} \StringTok{'Rejection Region of a Right-Sided Test'}\NormalTok{,}
      \DataTypeTok{yaxs =} \StringTok{'i'}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{'t-statistic'}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{''}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{axes =} \StringTok{'F'}
\NormalTok{)}

\CommentTok{# add x-axis}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }
     \DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{1.64}\NormalTok{, }\DecValTok{4}\NormalTok{), }
     \DataTypeTok{padj =} \FloatTok{0.5}\NormalTok{,}
     \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{''}\NormalTok{, }\DecValTok{0}\NormalTok{, }\KeywordTok{expression}\NormalTok{(Phi}\OperatorTok{^-}\DecValTok{1}\OperatorTok{~}\NormalTok{(.}\DecValTok{95}\NormalTok{)}\OperatorTok{==}\FloatTok{1.64}\NormalTok{), }\StringTok{''}\NormalTok{)}
\NormalTok{)}

\CommentTok{# shade rejection region in the left tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.64}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\FloatTok{1.64}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{4}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\FloatTok{1.64}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'darkred'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-83-1} \end{center}

Analogously for the left-sided test we have
\[H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs.} \ \ H_1: \mu_Y < \mu_{Y,0}.\]
The null is rejected if the observed test statistic falls short of the
critical value which, for a test at the \(0.05\) level of significance,
is given by \(-1.64\), the \(0.05\)-quantile of the \(N(0,1)\)
distribution. \(5\%\) probability mass lies to the left of the critical
value.

It is straightforward to adapt the code chunk above to the case of a
left-sided test. We only have to adjust the color shading and the tick
marks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the standard normal density on the domain [-4,4]}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x),}
      \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{),}
      \DataTypeTok{main =} \StringTok{'Rejection Region of a Left-Sided Test'}\NormalTok{,}
      \DataTypeTok{yaxs =} \StringTok{'i'}\NormalTok{,}
      \DataTypeTok{xlab =} \StringTok{'t-statistic'}\NormalTok{,}
      \DataTypeTok{ylab =} \StringTok{''}\NormalTok{,}
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{,}
      \DataTypeTok{axes =} \StringTok{'F'}
\NormalTok{)}

\CommentTok{# add x-axis}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }
     \DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\OperatorTok{-}\FloatTok{1.64}\NormalTok{, }\DecValTok{4}\NormalTok{), }
     \DataTypeTok{padj =} \FloatTok{0.5}\NormalTok{,}
     \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{''}\NormalTok{, }\DecValTok{0}\NormalTok{, }\KeywordTok{expression}\NormalTok{(Phi}\OperatorTok{^-}\DecValTok{1}\OperatorTok{~}\NormalTok{(.}\DecValTok{05}\NormalTok{)}\OperatorTok{==-}\FloatTok{1.64}\NormalTok{), }\StringTok{''}\NormalTok{)}
\NormalTok{)}

\CommentTok{# shade rejection region in right tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\OperatorTok{-}\FloatTok{1.64}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\OperatorTok{-}\FloatTok{1.64}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\OperatorTok{-}\FloatTok{1.64}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'darkred'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-84-1} \end{center}

\section{Confidence Intervals for the Population
Mean}\label{confidence-intervals-for-the-population-mean}

As stressed before, we will never estimate the \emph{exact} value of the
population mean of \(Y\) using a random sample. However, we can compute
confidence intervals for the population mean. In general, a confidence
interval for a unknown parameter is a set of values that contains the
true parameter with a prespecified probability, the \emph{confidence
level}. Confidence intervals are computed using the information
available in the sample. Since this information is the result of a
random process, confidence intervals are random variables themselves.

Key Concept 3.7 shows how to compute confidence intervals for the
unknown population mean \(E(Y)\).

Key Concept 3.7

Confidence Intervals for the Population Mean

A \(95\%\) confidence interval for \(\mu_Y\) is a
\texttt{random variable} that contains the true \(\mu_Y\) in \(95\%\) of
all possible random samples. When \(n\) is large we can use the normal
approximation. Then, \(99\%\), \(95\%\), \(90\%\) confidence intervals
are

\begin{align}
&99\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 2.58 \times SE(\overline{Y}) \right\}, \\
&95\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 1.96 \times SE(\overline{Y}) \right\}, \\
&90\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 1.64 \times SE(\overline{Y}) \right\}.
\end{align}

These confidence intervals are sets of null hypotheses we cannot reject
in a two-sided hypothesis test at the given level of confidence.

Now consider the following statements.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The interval
  \[ \left\{ \overline{Y} \pm 1.96 \times SE(\overline{Y}) \right\} \]
  covers the true value of \(\mu_Y\) with a probability of \(95\%\).
\item
  We have computed \(\overline{Y} = 5.1\) and \(SE(\overline{Y}=2.5\) so
  the interval
  \[ \left\{ 5.1  \pm 1.96 \times 2.5 \right\} = \left[0.2,10\right] \]
  covers the true value of \(\mu_Y\) with a probability of \(95\%\).
\end{enumerate}

While 1. is right (this is exactly in line with the definition above),
2. is completely wrong and none of your lecturers wants to read such a
sentence in a term paper, written exam or similar, believe us. The
difference is that, while 1. is the definition of a random variable, 2.
is one possible \emph{outcome} of this random variable so there is no
meaning in making any probabilistic statement about it. Either the
computed interval \emph{does cover} \(\mu_Y\) or it \emph{does not}!

In \texttt{R}, testing of hypotheses about the mean of a population on
the basis of a random sample is very easy due to functions like
\texttt{t.test()} from the \texttt{stats} package. It produces an object
of type \texttt{list}. Luckily, one of the most simple ways to use
\texttt{t.test()} is when you want to obtain a \(95\%\) confidence
interval for some population mean. We start by generating some random
data and calling \texttt{t.test()} in conjunction with \texttt{ls()} to
obtain a breakdown of the output components.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# generate some sample data}
\NormalTok{sampledata <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\CommentTok{# checke type}
\KeywordTok{typeof}\NormalTok{(}\KeywordTok{t.test}\NormalTok{(sampledata))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# display list elements produced by t.test}
\KeywordTok{ls}\NormalTok{(}
  \KeywordTok{t.test}\NormalTok{(sampledata)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "alternative" "conf.int"    "data.name"   "estimate"    "method"     
## [6] "null.value"  "p.value"     "parameter"   "statistic"
\end{verbatim}

Though we find that many items are reported, at the moment we are only
interested in computing a \(95\%\) confidence set for the mean.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(sampledata)}\OperatorTok{$}\StringTok{"conf.int"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  9.306651 12.871096
## attr(,"conf.level")
## [1] 0.95
\end{verbatim}

This tells us that the \(95\%\) confidence interval is

\[ \left[9.31, 12.87\right]. \]

In this example, the computed interval obviously does cover the true
\(\mu_Y\) which we know to be \(10\).

Let us have a look at the whole standard output produced by
\texttt{t.test()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(sampledata)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  sampledata
## t = 12.346, df = 99, p-value < 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##   9.306651 12.871096
## sample estimates:
## mean of x 
##  11.08887
\end{verbatim}

We see that \texttt{t.test()} does not only compute a \(95\%\)
confidence interval but automatically conducts a two-sided significance
test of the hypothesis \(H_0: \mu_Y = 0\) at the level of \(5\%\) and
reports relevant parameters thereof: the alternative hypothesis, the
estimated mean, the resulting \(t\)-statistic, the degrees of freedom of
the underlying \(t\) distribution (\texttt{t.test()} does not perform
the normal approximation) and the corresponding \(p\)-value. This is
very convenient!

In this example, we come to the conclusion that the population mean
\emph{is not} significantly different from \(0\) (which is correct) at
the level of \(5\%\), since \(\mu_Y = 0\) is element of the \(95\%\)
confidence interval

\[ 0 \in \left[-0.27,0.12\right]. \] We come to an equivalent result
when using the \(p\)-value rejection rule since

\[ p = 0.456 > 0.05. \]

\section{Comparing Means from Different
Populations}\label{comparing-means-from-different-populations}

Suppose you are interested in the means of two different populations,
denote them \(\mu_1\) and \(\mu_2\). More specifically you are
interested whether these population means are different from each other
and plan using a hypothesis test to verify this on the basis of
independent sample data from both populations. A suitable pair of
hypotheses is

\begin{equation}
H_0: \mu_1 - \mu_2 = d_0 \ \ \text{vs.} \ \ H_1: \mu_1 - \mu_2 \neq d_0 \label{eq:hypmeans}
\end{equation}

where \(d_0\) denotes the hypothesized difference in means. The book
teaches us that \(H_0\) can be tested with the \(t\)-statistic

\begin{equation}
t=\frac{(\overline{Y}_1 - \overline{Y}_2) - d_0}{SE(\overline{Y}_1 - \overline{Y}_2)} \label{eq:tstatmeans}
\end{equation}

where

\begin{equation}
SE(\overline{Y}_1 - \overline{Y}_2) = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}.
\end{equation}

This is called a two sample \(t\)-test. For large \(n_1\) and \(n_2\),
\eqref{eq:tstatmeans} is standard normal distributed under the null
hypothesis. Analogously to the simple \(t\)-test we can compute
confidence intervals for the true difference in population means:

\[ (\overline{Y}_1 - \overline{Y}_2) \pm 1.96 \times SE(\overline{Y}_1 - \overline{Y}_2) \]

is a \(95\%\) confidence interval for \(d\). In \texttt{R}, hypotheses
as in \eqref{eq:hypmeans} can be tested with \texttt{t.test()}, too. Note
that \texttt{t.test()} chooses \(d_0 = 0\) by default. This can be
changed by setting the argument \texttt{mu} accordingly.

The subsequent code chunk demonstrates how to perform a two sample
\(t\)-test in \texttt{R} using simulated data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# draw data from two different populations with equal mean}
\NormalTok{sample_pop1 <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{sample_pop2 <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{)}

\CommentTok{# perform a two sample t-test}
\KeywordTok{t.test}\NormalTok{(sample_pop1, sample_pop2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  sample_pop1 and sample_pop2
## t = 0.872, df = 140.52, p-value = 0.3847
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.338012  6.028083
## sample estimates:
## mean of x mean of y 
## 11.088874  9.243838
\end{verbatim}

We find that the two sample \(t\)-test does not reject the (true) null
hypothesis that \(d_0 = 0\).

\section{An Application to the Gender Gap of
Earnings}\label{an-application-to-the-gender-gap-of-earnings}

In this section discusses how to reproduce the results presented in the
box \emph{The Gender Gap of Earnings of College Graduates in the United
States} of the book.

In order to reproduce table 3.1 you need to download the replication
data which are hosted by Pearson and can be found and downloaded
\href{http://wps.aw.com/aw_stock_ie_3/178/45691/11696965.cw/index.html}{here}.
Download the data for chapter three as an excel spreadsheet
(\texttt{cps\_ch3.xlsx}). This data set contains data that range from
\(1992\) to \(2008\) and earnings are reported in prices of \(2008\).
There are several ways to import the \texttt{.xlsx}-files into
\texttt{R}. Our suggestion is the function \texttt{read\_excel()} from
the \texttt{readxl} package. The package is not part of \texttt{R}'s
base version and has to be installed manually.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the 'readxl' package}
\KeywordTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

You are now ready to import the data set. Make sure you use the correct
path to import the downloaded file! In our example, the file is saved in
a sub folder (\texttt{data}) of the working directory. If you are not
sure what your current working directory is, use \texttt{getwd()}, see
also \texttt{?getwd()}. This will give you the path that points to the
place \texttt{R}is currently looking for files.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# import the data into R}
\NormalTok{cps <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(}\DataTypeTok{path =} \StringTok{'data/cps_ch3.xlsx'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, install and load the package \texttt{dyplr}. This package provides
some handy functions that simplify data wrangling a lot. It makes use of
the \texttt{\%>\%} operator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the 'dplyr' package}
\KeywordTok{library}\NormalTok{(}\StringTok{'dplyr'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

First, get an overview over the data set. Next, use \texttt{\%>\%} and
some functions from the \texttt{dplyr} package to group the observations
by gender and year and compute descriptive statistics for both groups.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Get an overview of the data structure}
\KeywordTok{head}\NormalTok{(cps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   a_sex  year ahe08
##   <dbl> <dbl> <dbl>
## 1    1. 1992.  17.2
## 2    1. 1992.  15.3
## 3    1. 1992.  22.9
## 4    2. 1992.  13.3
## 5    1. 1992.  22.1
## 6    2. 1992.  12.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# group data by gender and year and compute the mean, standard deviation}
\CommentTok{# and number of observations for each group}
\NormalTok{avgs <-}\StringTok{ }\NormalTok{cps }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{group_by}\NormalTok{(a_sex, year) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{summarise}\NormalTok{(}\KeywordTok{mean}\NormalTok{(ahe08), }
                  \KeywordTok{sd}\NormalTok{(ahe08), }
                  \KeywordTok{n}\NormalTok{()}
\NormalTok{                  )}

\CommentTok{# print results to the console}
\KeywordTok{print}\NormalTok{(avgs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 5
## # Groups:   a_sex [?]
##    a_sex  year `mean(ahe08)` `sd(ahe08)` `n()`
##    <dbl> <dbl>         <dbl>       <dbl> <int>
##  1    1. 1992.          23.3       10.2   1594
##  2    1. 1996.          22.5       10.1   1379
##  3    1. 2000.          24.9       11.6   1303
##  4    1. 2004.          25.1       12.0   1894
##  5    1. 2008.          25.0       11.8   1838
##  6    2. 1992.          20.0        7.87  1368
##  7    2. 1996.          19.0        7.95  1230
##  8    2. 2000.          20.7        9.36  1181
##  9    2. 2004.          21.0        9.36  1735
## 10    2. 2008.          20.9        9.66  1871
\end{verbatim}

With the pipe operator \texttt{\%>\%} we simply chain different
\texttt{R} functions that produce compatible input and output. In the
code above, we take the data set \texttt{cps} and use it as an input for
the function \texttt{group\_by()}. The output of \texttt{group\_by} is
subsequently used as an input for \texttt{summarise()} and so forth.

Now that we have computed the statistics of interest for both genders,
we can investigate how the gap in earnings between both groups evolves
over time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split the data set by gender}
\NormalTok{male <-}\StringTok{ }\NormalTok{avgs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(a_sex }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }
\NormalTok{female <-}\StringTok{ }\NormalTok{avgs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(a_sex }\OperatorTok{==}\StringTok{ }\DecValTok{2}\NormalTok{)}

\CommentTok{# Rename columns of both splits}
\KeywordTok{colnames}\NormalTok{(male)   <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Y_bar_m"}\NormalTok{, }\StringTok{"s_m"}\NormalTok{, }\StringTok{"n_m"}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(female) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Y_bar_f"}\NormalTok{, }\StringTok{"s_f"}\NormalTok{, }\StringTok{"n_f"}\NormalTok{)}

\CommentTok{# Estimate gender gaps, compute standard errors and confidence intervals for all dates}
\NormalTok{gap <-}\StringTok{ }\NormalTok{male}\OperatorTok{$}\NormalTok{Y_bar_m }\OperatorTok{-}\StringTok{ }\NormalTok{female}\OperatorTok{$}\NormalTok{Y_bar_f}

\NormalTok{gap_se <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(male}\OperatorTok{$}\NormalTok{s_m}\OperatorTok{^}\DecValTok{2} \OperatorTok{/}\StringTok{ }\NormalTok{male}\OperatorTok{$}\NormalTok{n_m }\OperatorTok{+}\StringTok{ }\NormalTok{female}\OperatorTok{$}\NormalTok{s_f}\OperatorTok{^}\DecValTok{2} \OperatorTok{/}\StringTok{ }\NormalTok{female}\OperatorTok{$}\NormalTok{n_f)}

\NormalTok{gap_ci_l <-}\StringTok{ }\NormalTok{gap }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\NormalTok{gap_se}

\NormalTok{gap_ci_u <-}\StringTok{ }\NormalTok{gap }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\NormalTok{gap_se}

\NormalTok{result <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(male[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], female[,}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{)], gap, gap_se, gap_ci_l, gap_ci_u)}

\CommentTok{# print results to the console}
\KeywordTok{print}\NormalTok{(result, }\DataTypeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Year Y_bar_m  s_m  n_m Y_bar_f  s_f  n_f  gap gap_se gap_ci_l gap_ci_u
## 1 1992    23.3 10.2 1594    20.0 7.87 1368 3.23  0.332     2.58     3.88
## 2 1996    22.5 10.1 1379    19.0 7.95 1230 3.49  0.354     2.80     4.19
## 3 2000    24.9 11.6 1303    20.7 9.36 1181 4.14  0.421     3.32     4.97
## 4 2004    25.1 12.0 1894    21.0 9.36 1735 4.10  0.356     3.40     4.80
## 5 2008    25.0 11.8 1838    20.9 9.66 1871 4.10  0.354     3.41     4.80
\end{verbatim}

We observe virtually the same results as the ones presented in the book.
The computed statistics suggest that there \emph{is} a gender gap in
earnings. Note that we can reject the null hypothesis that the gap is
zero for all periods. Further, estimates of the gap and limits of the
\(95\%\) confidence intervals indicate that the gap has been quite
stable over the recent past.

\section{Scatterplots, Sample Covariance and Sample
Correlation}\label{scatterplots-sample-covariance-and-sample-correlation}

A scatter plot represents two dimensional data, for example \(n\)
observation on \(X_i\) and \(Y_i\), by points in a Cartesian coordinate
system. It is very easy to generate scatter plots using the
\texttt{plot()} function in \texttt{R}. Let us generate some fictional
data on age and earnings of workers and plot it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# generate data set}
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }
           \DataTypeTok{min =} \DecValTok{18}\NormalTok{, }
           \DataTypeTok{max =} \DecValTok{70}
\NormalTok{           )}
\NormalTok{Y <-}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n=}\DecValTok{100}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{15}\NormalTok{)}

\CommentTok{# plot observations}
\KeywordTok{plot}\NormalTok{(X, }
\NormalTok{     Y, }
     \DataTypeTok{type =} \StringTok{"p"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"A Scatterplot of X and Y"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Age"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Earnings"}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{,}
     \DataTypeTok{pch =} \DecValTok{19}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-94-1} \end{center}

The plot shows positive correlation between age and earnings. This is in
line with the assumption that older workers earn more than those who
joined the working population recently.

\subsubsection*{Sample Covariance and
Correlation}\label{sample-covariance-and-correlation}
\addcontentsline{toc}{subsubsection}{Sample Covariance and Correlation}

By now you should be familiar with the concepts of variance and
covariance. If not, we recommend you to work your way through chapter 2
of the book (again).

Just like the variance, covariance and correlation of two variables are
properties that relate to the (unknown) joint probability distribution
of these variables. We can estimate covariance and correlation by means
of suitable estimators using a random sample \((X_i,Y_i)\),
\(i=1,\dots,n\).

The sample covariance

\[ s_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) \]

is an estimator for the population variance of \(X\) and \(Y\) whereas
the sample correlation

\[ r_{XY} = \frac{s_{XY}}{s_Xs_Y} \] can be used to estimate the
population correlation, a standardized measure for the strength of the
linear relationship between \(X\) and \(Y\). See chapter 3.7 in the book
for a more detailed treatment of these estimators.

As for variance and standard deviation, these estimators are implemented
as \texttt{R} functions in the \texttt{stats} package. We can use them
to estimate population covariance and population correlation of the
fictional data on age and earnings.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute sample covariance of X and Y}
\KeywordTok{cov}\NormalTok{(X, Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 213.934
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute sample correlation between X and Y}
\KeywordTok{cor}\NormalTok{(X, Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.706372
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# equivalent way to compute the sample correlation}
\KeywordTok{cov}\NormalTok{(X, Y)}\OperatorTok{/}\NormalTok{(}\KeywordTok{sd}\NormalTok{(X) }\OperatorTok{*}\StringTok{ }\KeywordTok{sd}\NormalTok{(Y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.706372
\end{verbatim}

The estimates indicate that \(X\) and \(Y\) are moderately correlated.

The next code chunk uses the function \texttt{mvnorm()} from package
\texttt{MASS} to generate bivariate sample data with different degree of
correlation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'MASS'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     select
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# positive correlation (0.81)}
\NormalTok{example1 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{mu =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{Sigma =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{),}
                    \DataTypeTok{empirical =} \OtherTok{TRUE}
\NormalTok{                    )}

\CommentTok{# negative correlation (-0.81)}
\NormalTok{example2 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{mu =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{Sigma =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\OperatorTok{-}\DecValTok{2}\NormalTok{, }\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{),}
                    \DataTypeTok{empirical =} \OtherTok{TRUE}
\NormalTok{                    )}

\CommentTok{# no correlation }
\NormalTok{example3 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{mu =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }
                    \DataTypeTok{Sigma =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{),}
                    \DataTypeTok{empirical =} \OtherTok{TRUE}
\NormalTok{                    )}

\CommentTok{# no correlation (quadratic relationship)}
\NormalTok{X <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\OperatorTok{-}\NormalTok{X}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(X))}

\NormalTok{example4 <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(X, Y)}

\CommentTok{# divide plot area as 2-by-2 array}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{# plot data sets}
\KeywordTok{plot}\NormalTok{(example1, }\DataTypeTok{col =} \StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{'X'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Y'}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Correlation = 0.81"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(example2, }\DataTypeTok{col =} \StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{'X'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Y'}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Correlation = -0.81"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(example3, }\DataTypeTok{col =} \StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{'X'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Y'}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Correlation = 0"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(example4, }\DataTypeTok{col =} \StringTok{'steelblue'}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{'X'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Y'}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Correlation = 0"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-96-1} \end{center}

\section{Exercises}\label{exercises-1}

\begin{center}\textit{This interactive part of URFITE is only available in the HTML version.}\end{center}

\hypertarget{lrwor}{\chapter{Linear Regression with One
Regressor}\label{lrwor}}

This chapter introduces the basics in linear regression and shows how to
perform regression analysis in \texttt{R}. In linear regression, the aim
is to model the relationship between a dependent variable \(Y\) and one
or more explanatory variables denoted by \(X_1, X_2, \dots, X_k\).
Following the book we will focus on the concept of simple linear
regression throughout the whole chapter. In simple linear regression,
there is just one explanatory variable \(X_1\). If for example a school
cuts the class sizes by hiring new teachers, that is the school lowers
\(X_1\), the student-teacher ratios of their classes, how would this
affect \(Y\), the performance of the students involved in a standardized
test? With linear regression we can not only examine whether the
student-teacher ratio \emph{does have} an impact on the test results but
we can also learn about the \emph{direction} and the \emph{strength} of
this effect.

The following packages are needed for reproducing the code presented in
this chapter:

\begin{itemize}
\item
  \texttt{AER} accompanies the Book \emph{Applied Econometrics with R}
  and provides useful functions and data sets.
\item
  \texttt{MASS} is a collection of functions for applied statistics.
\end{itemize}

\section{Simple Linear Regression}\label{simple-linear-regression}

To start with an easy example, consider the following combinations of
average test score and the average student-teacher ratio in some
fictional school districts.

1

2

3

4

5

6

7

TestScore

680

640

670

660

630

660.0

635

STR

15

17

19

20

22

23.5

25

To work with these data in \texttt{R} we begin by generating two
vectors: one for the student-teacher ratios (\texttt{STR}) and one for
test scores (\texttt{TestScore}), both containing the data from the
table above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create sample data}
\NormalTok{STR <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{22}\NormalTok{, }\FloatTok{23.5}\NormalTok{, }\DecValTok{25}\NormalTok{)}
\NormalTok{TestScore <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{680}\NormalTok{, }\DecValTok{640}\NormalTok{, }\DecValTok{670}\NormalTok{, }\DecValTok{660}\NormalTok{, }\DecValTok{630}\NormalTok{, }\DecValTok{660}\NormalTok{, }\DecValTok{635}\NormalTok{) }

\CommentTok{# Print out sample data}
\NormalTok{STR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 15.0 17.0 19.0 20.0 22.0 23.5 25.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TestScore}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 680 640 670 660 630 660 635
\end{verbatim}

If we use a simple linear regression model, we assume that the true
relationship between both variables can be represented by a straight
line, formally

\[ Y = b \cdot X + a. \]

For now, let us suppose that the true function which relates test score
and student-teacher ratio to each other is

\[TestScore = 713 - 3 \times STR.\]

If possible, it is always a good idea to visualize the data you work
with in an appropriate way. For our purpose it is suitable to use the
function \texttt{plot()} to produce a scatter plot with \texttt{STR} on
the \(X\)-axis and \texttt{TestScore} on the \(Y\) axis. An easy way to
do so is to call
\texttt{plot(y\_variable\ \textasciitilde{}\ x\_variable)} whereby
\texttt{y\_variable} and \texttt{x\_variable} are placeholders for the
vectors of observations we want to plot. Furthermore, we might want to
add the true relationship to the plot. To draw a straight line,
\texttt{R} provides the function \texttt{abline()}. We just have to call
this function with arguments \texttt{a} (representing the intercept) and
\texttt{b} (representing the slope) after executing \texttt{plot()} in
order to add the line to our scatter plot.

The following code reproduces figure 4.1 from the textbook.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a scatter plot of the data}
\KeywordTok{plot}\NormalTok{(TestScore }\OperatorTok{~}\StringTok{ }\NormalTok{STR)}

\CommentTok{# add the true relationship to the plot}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{a =} \DecValTok{713}\NormalTok{, }\DataTypeTok{b =} \OperatorTok{-}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-108-1} \end{center}

We find that the line does not touch any of the points although we
claimed that it represents the true relationship. The reason for this is
the core problem of statistics, \emph{randomness}. Most of the time
there are influences which cannot be explained in a purely deterministic
fashion and thus complicate finding the true relationship.

In order to account for these differences between observed data and the
true relationship, we extend our model from above by an \emph{error
term} \(u\) which covers these random effects. Put differently, \(u\)
accounts for all the differences between the true regression line and
the actual observed data. Beside pure randomness, these deviations could
also arise from measurement errors or, as will be discussed later, could
be the consequence of leaving out other factors that are relevant in
explaining the dependent variable. Which other factors are plausible in
our example? For one thing, the test scores might be driven by the
teachers quality and the background of the students. It is also
imaginable that in some classes, the students were lucky on the test
days and thus achieved higher scores. For now, we will summarize such
influences by an additive component:

\[ TestScore = \beta_0 + \beta_1 \times STR + \text{other factors} \]

Of course this idea is very general as it can be easily extended to
other situations that can be described with a linear model. The basic
linear regression function we will work with hence is

\[ Y_i = \beta_0 + \beta_1 X_i + u_i. \]

Key Concept 4.1 summarizes the terminology of the simple linear
regression model.

Key Concept 4.1

Terminology for the Linear Regression Model with a Single Regressor

The linear regression model is

\[Y_i = \beta_0 + \beta_1 X_1 + u_i \]

where

\begin{itemize}
\tightlist
\item
  the index \(i\) runs over the observations, \(i=1,\dots,n\)
\item
  \(Y_i\) is the \emph{dependent variable}, the \emph{regressand}, or
  simply the \emph{left-hand variable}
\item
  \(X_i\) is the \emph{independent variable}, the \emph{regressor}, or
  simply the \emph{right-hand variable}
\item
  \(Y = \beta_0 + \beta_1 X\) is the \emph{population regression line}
  also called the \emph{population regression function}
\item
  \(\beta_0\) is the \emph{intercept} of the population regression line
\item
  \(\beta_1\) is the \emph{slope} of the population regression line
\item
  \(u_i\) is the \emph{error term}.
\end{itemize}

\section{Estimating the Coefficients of the Linear Regression
Model}\label{estimating-the-coefficients-of-the-linear-regression-model}

In practice, the intercept \(\beta_0\) and slope \(\beta_1\) of the
population regression line are unknown. Therefore, we must employ data
to estimate both unknown parameters. In the following a real world
example will be used to demonstrate how this is achieved. We want to
relate test scores to student-teacher ratios measured in Californian
schools. The test score is the district-wide average of reading and math
scores for fifth graders. Again, the class size is measured as the
number of students divided by the number of teachers (the
student-teacher ratio). As for the data, the California School data set
(\texttt{CASchools}) comes with an \texttt{R} package called
\texttt{AER}, an acronym for
\href{https://cran.r-project.org/web/packages/AER/AER.pdf}{Applied
Econometrics with R}. After installing the package with
\texttt{install.packages("AER")} and attaching it with
\texttt{library("AER")} the data set can be loaded using the
\texttt{data()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install the AER package (once)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"AER"}\NormalTok{)}

\CommentTok{# load the AER package }
\KeywordTok{library}\NormalTok{(AER)   }

\CommentTok{# load the the data set in the workspace}
\KeywordTok{data}\NormalTok{(CASchools) }
\end{Highlighting}
\end{Shaded}

Note that once a package has been installed it is available for use at
further occasions when invoked with \texttt{library()} --- there is no
need to run \texttt{install.packages()} again!

For several reasons it is interesting to know what kind of object we are
dealing with. \texttt{class()} returns the class of an object. Depending
on the class of an object some functions (for example \texttt{plot()}
and \texttt{summary()}) behave differently.

Let us check the class of the object \texttt{CASchools}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(CASchools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

It turns out that \texttt{CASchools} is of class \texttt{data.frame}
which is a convenient format to work with, especially for performing
regression analysis.

With help of \texttt{head()} we get a first overview of our data. This
function shows only the first 6 rows of the data set which prevents an
overcrowded console output.

\BeginKnitrBlock{rmdnote}
Press ctrl + L to clear the console. This command deletes any code that
has been typed in and executed by you or printed to the console by R
functions. The Good news is that anything else is left untouched. You
neither loose defined variables and alike nor the code history. It is
still possible to recall previously executed R commands using the up and
down keys. If you are working in \emph{RStudio}, press ctrl + Up on your
keyboard (CMD + Up on a Mac) to review a list of previously entered
commands.
\EndKnitrBlock{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(CASchools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   district                          school  county grades students
## 1    75119              Sunol Glen Unified Alameda  KK-08      195
## 2    61499            Manzanita Elementary   Butte  KK-08      240
## 3    61549     Thermalito Union Elementary   Butte  KK-08     1550
## 4    61457 Golden Feather Union Elementary   Butte  KK-08      243
## 5    61523        Palermo Union Elementary   Butte  KK-08     1335
## 6    62042         Burrel Union Elementary  Fresno  KK-08      137
##   teachers calworks   lunch computer expenditure    income   english  read
## 1    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6
## 2    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5
## 3    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3
## 4    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9
## 5    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8
## 6     6.40  12.3188 86.9565       25    5580.147 10.415000 12.408759 605.7
##    math
## 1 690.0
## 2 661.9
## 3 650.9
## 4 643.5
## 5 639.9
## 6 605.4
\end{verbatim}

We find that the data set consists of plenty of variables and most of
them are numeric.

By the way: an alternative to \texttt{class()} and \texttt{head()} is
\texttt{str()} which is deduced from `structure' and gives a
comprehensive overview of the object. Try this!

Turning back to \texttt{CASchools}, the two variables we are interested
in (i.e.~average test score and the student-teacher ratio) are
\emph{not} included. However, it is possible to calculate both from the
provided data. To obtain the student-teacher ratios, we simply divide
the number of students by the number of teachers. The average test score
is the arithmetic mean of the test score for reading and the score of
the math test. The next code chunk shows how the two variables can be
constructed as vectors and how they are appended to \texttt{CASchools}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute STR and append it to CASchools}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{STR <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{students}\OperatorTok{/}\NormalTok{CASchools}\OperatorTok{$}\NormalTok{teachers }

\CommentTok{# compute TestScore and append it to CASchools}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{score <-}\StringTok{ }\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{read }\OperatorTok{+}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{math)}\OperatorTok{/}\DecValTok{2}     
\end{Highlighting}
\end{Shaded}

If we ran \texttt{head(CASchools)} again we would find the two variables
of interest as additional columns named \texttt{STR} and
``)\texttt{score")} (check this!).

Table 4.1 from the text book summarizes the distribution of test scores
and student-teacher ratios. Remember that there are several functions
which can be used to produce similar results, e.g.

\begin{itemize}
\item
  \texttt{mean()} (computes the arithmetic mean of the provided numbers)
\item
  \texttt{sd()} (computes the sample standard deviation)
\item
  \texttt{quantile()} (returns a vector of the specified sample
  quantiles for the data).
\end{itemize}

The next code chunk shows how to achieve this. First, we compute summary
statistics on the columns \texttt{STR} and \texttt{score} of
\texttt{CASchools}. In order to have a nice display format we gather the
computed measures in a \texttt{data.frame} named
\texttt{DistributionSummary}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute sample averages of STR and score}
\NormalTok{avg_STR <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR) }
\NormalTok{avg_score <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score)}

\CommentTok{# compute sample standard deviations of STR and score}
\NormalTok{sd_STR <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR) }
\NormalTok{sd_score <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score)}

\CommentTok{# set up a vector of percentiles and compute the quantiles }
\NormalTok{quantiles <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.10}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\FloatTok{0.9}\NormalTok{)}
\NormalTok{quant_STR <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR, quantiles)}
\NormalTok{quant_score <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{score, quantiles)}

\CommentTok{# gather everything in a data.frame }
\NormalTok{DistributionSummary <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
                                  \DataTypeTok{Average =} \KeywordTok{c}\NormalTok{(avg_STR, avg_score), }
                                  \DataTypeTok{StandardDeviation =} \KeywordTok{c}\NormalTok{(sd_STR, sd_score), }
                                  \DataTypeTok{quantile =} \KeywordTok{rbind}\NormalTok{(quant_STR, quant_score)}
\NormalTok{                                  )}

\CommentTok{# print the summary to the console}
\NormalTok{DistributionSummary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Average StandardDeviation quantile.10. quantile.25.
## quant_STR    19.64043          1.891812      17.3486     18.58236
## quant_score 654.15655         19.053347     630.3950    640.05000
##             quantile.40. quantile.50. quantile.60. quantile.75.
## quant_STR       19.26618     19.72321      20.0783     20.87181
## quant_score    649.06999    654.45000     659.4000    666.66249
##             quantile.90.
## quant_STR       21.86741
## quant_score    678.85999
\end{verbatim}

As done for the sample data, we use \texttt{plot()} for a visual survey.
This allows us to detect specific characteristics of our data, such as
outliers which are hard to discover by looking at mere numbers. This
time we add some additional arguments to the call of \texttt{plot()}.

The first argument in our call of \texttt{plot()},
\texttt{score \textasciitilde{} STR}, is again a formula that states the
dependent variable and the regressor. However, this time the two
variables are not saved in separate vectors but are columns of
\texttt{CASchools}. Therefore, \texttt{R} would not find them without
the argument \texttt{data} being correctly specified. \texttt{data} must
be in accordance with the name of the \texttt{data.frame} to which the
variables belong to, in this case \texttt{CASchools}. Further arguments
are used to change the appearance of the plot: while \texttt{main} adds
a title, \texttt{xlab} and \texttt{ylab} are adding custom labels to
both axes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }
     \DataTypeTok{data =}\NormalTok{ CASchools,}
     \DataTypeTok{main =} \StringTok{"Scatterplot of TestScore and STR"}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"STR (X)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test Score (Y)"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-114-1} \end{center}

The plot (figure 4.2 in the book) shows the scatter plot of all
observations on student-teacher ratio and Test score. We see that the
points are strongly scattered and an apparent relationship cannot be
detected by only looking at them. Yet it can be assumed that both
variables are negatively correlated, that is we expect to observe lower
test scores in bigger classes.

The function \texttt{cor()} (type and execute \texttt{?cor} for further
info) can be used to compute the correlation between two \emph{numeric}
vectors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR, CASchools}\OperatorTok{$}\NormalTok{score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.2263627
\end{verbatim}

As the scatter plot already suggests, the correlation is negative but
rather weak.

The task we are facing now is to find a line which fits best to the
data. Of course we could simply stick with graphical inspection and
correlation analysis and then select the best fitting line by
eyeballing. However, this is pretty unscientific and prone to subjective
perception: different students would draw different regression lines. On
this account, we are interested in techniques that are more
sophisticated. Such a technique is ordinary least squares (OLS)
estimation.

\subsection*{The Ordinary Least Squares
Estimator}\label{the-ordinary-least-squares-estimator}
\addcontentsline{toc}{subsection}{The Ordinary Least Squares Estimator}

The OLS estimator chooses the regression coefficients such that the
estimated regression line is as close as possible to the observed data
points. Therefore closeness is measured by the sum of the squared
mistakes made in predicting \(Y\) given \(X\). Let \(b_0\) and \(b_1\)
be some estimators of \(\beta_0\) and \(\beta_1\). Then the sum of
squared estimation mistakes can be expressed as

\[ \sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2. \]

The OLS estimator in the simple regression model is the pair of
estimators for intercept and slope which minimizes the expression above.
The derivation of the OLS estimators for both parameters are presented
in Appendix 4.1 of the book. The results are summarized in Key Concept
4.2.

Key Concept 4.2

The OLS Estimator, Predicted Values, and Residuals

The OLS estimators of the slope \(\beta_1\) and the intercept
\(\beta_0\) in the simple linear regression model are

\begin{align}
  \hat\beta_1 & = \frac{ \sum_{i = 1}^n (X_i - \overline{X})(Y_i - \overline{Y}) } { \sum_{i=1}^n (X_i - \overline{X})^2}  \\
  \\
  \hat\beta_0 & =  \overline{Y} - \hat\beta_1 \overline{X} 
\end{align}

The OLS predicted values \(\widehat{Y}_i\) and residuals \(\hat{u}_i\)
are

\begin{align}
  \widehat{Y}_i & =  \hat\beta_0 + \hat\beta_1 X_i,\\
  \\
  \hat{u}_i & =  Y_i - \widehat{Y}_i. 
\end{align}

The estimated intercept \(\hat{\beta}_0\), the slope parameter
\(\hat{\beta}_1\) and the residuals \(\left(\hat{u}_i\right)\) are
computed from a sample of \(n\) observations of \(X_i\) and \(Y_i\),
\(i\), \(...\), \(n\). These are \emph{estimates} of the unknown true
population intercept \(\left(\beta_0 \right)\), slope
\(\left(\beta_1\right)\), and error term \((u_i)\).

The formulas presented above may not be very intuitive at first glance.
The following interactive application aims to help you understand the
mechanics of OLS. You can add observations by clicking into the
coordinate system where the data are represented by points. If two or
more observations are available, the application computes a regression
line using OLS and some statistics which are displayed in the right
panel. The results are updated as you add further observations to the
left panel. A double-click resets the application i.e.~all data are
removed.

There are many possible ways to compute \(\hat{\beta_0}\) and
\(\hat{\beta_1}\) in \texttt{R}. For example, we could implement the
formulas presented in Key Concept 4.2 with two of \texttt{R}'s most
basic functions: \texttt{mean()} and \texttt{sum()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attach}\NormalTok{(CASchools) }\CommentTok{#allows to use the variables contained in CASchools directly}

\CommentTok{# compute beta_1 }
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((STR }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(STR))}\OperatorTok{*}\NormalTok{(score }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(score))) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{((STR }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(STR))}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\CommentTok{# compute beta_0}
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(score) }\OperatorTok{-}\StringTok{ }\NormalTok{beta_}\DecValTok{1} \OperatorTok{*}\StringTok{ }\KeywordTok{mean}\NormalTok{(STR)}

\CommentTok{# print the results to the console}
\NormalTok{beta_}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.279808
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta_}\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 698.9329
\end{verbatim}

Of course there are also other and even more manual ways to do the same
tasks. Luckily, OLS is one of the most widely-used estimation
techniques. Being a statistical programming language, R already contains
a built-in function named \texttt{lm()} (\textbf{l}inear \textbf{m}odel)
which can be used to carry out regression analysis.

The first argument of the function to be specified is, similar to
\texttt{plot()}, the regression formula with the basic syntax
\texttt{y \textasciitilde{} x} where ``)\texttt{y")} is the dependent
variable and \texttt{x} the explanatory variable. The argument
\texttt{data} determines the data set to be used in the regression. We
now revisit the example from the book where the relationship between the
test scores and the class sizes is analysed. The following code uses
\texttt{lm()} to replicate the results presented in figure 4.3 of the
book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the model and assign the result to linear_model}
\NormalTok{linear_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }\DataTypeTok{data =}\NormalTok{ CASchools)}

\CommentTok{# Print the standard output of the estimated lm object to the console }
\NormalTok{linear_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28
\end{verbatim}

Let us add the estimated regression line to the plot. This time we also
enlarge ranges of both axes by setting the arguments \texttt{xlim} and
\texttt{ylim}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the data}
\KeywordTok{plot}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{STR, }
     \DataTypeTok{data =}\NormalTok{ CASchools,}
     \DataTypeTok{main =} \StringTok{"Scatterplot of TestScore and STR"}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{"STR (X)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test Score (Y)"}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{30}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{600}\NormalTok{, }\DecValTok{720}\NormalTok{)}
\NormalTok{     )}

\CommentTok{# add the regression line}
\KeywordTok{abline}\NormalTok{(linear_model) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-118-1} \end{center}

Did you notice that this time, we did not pass the intercept and slope
parameters to \texttt{abline}? If you call \texttt{abline()} on an
object of class \texttt{lm} that only contains a single regressor
variable, \texttt{R} draws the regression line automatically!

\section{Measures of Fit}\label{measures-of-fit}

After estimating a linear regression model, the question occurs how well
the model describes the data. Visually this amounts to assess whether
the observations are tightly clustered around the regression line, or if
they are spread out. Both, the \emph{coefficient of determination} and
the \emph{standard error of the regression} measure how well the OLS
Regression line fits the data.

\subsection*{The Coefficient of
Determination}\label{the-coefficient-of-determination}
\addcontentsline{toc}{subsection}{The Coefficient of Determination}

\(R^2\), the \emph{coefficient of determination}, is the fraction of thr
sample variance of \(Y_i\) that is explained by \(X_i\). Mathematically,
the \(R^2\) can be written as the ratio of the explained sum of squares
to the total sum of squares. The \emph{explained sum of squares}
(\(ESS\)) is the sum of squared deviations of the predicted values
\(\hat{Y_i}\), from the average of the \(Y_i\). The \emph{total sum of
squares} (\(TSS\)) is the sum of squared deviations of the \(Y_i\) from
their average. Thus we have

\begin{align}
  ESS & =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2,   \\
  \\
  TSS & =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2,   \\
  \\
  R^2 & = \frac{ESS}{TSS}.
\end{align}

Since \(TSS = ESS + SSR\) we can also write

\[ R^2 = 1- \frac{SSR}{TSS} \]

where \(SSR\) is the sum of squared residuals, a measure for the errors
made when predicting the \(Y\) by \(X\). The \(SSR\) is defined as

\[ SSR = \sum_{i=1}^n \hat{u}_i^2. \]

\(R^2\) lies between \(0\) and \(1\). It is easy to see that a perfect
fit, i.e.~no errors made when fitting the regression line, implies
\(R^2 = 1\) since then we have \(SSR=0\). On the contrary, if our
estimated regression line does not explain any variation in the \(Y_i\),
we have \(ESS=0\) and consequently \(R^2=0\).

\subsection*{The Standard Error of the
Regression}\label{the-standard-error-of-the-regression}
\addcontentsline{toc}{subsection}{The Standard Error of the Regression}

The \emph{Standard Error of the Regression} (\(SER\)) is an estimator of
the standard deviation of the regression error \(\hat{u}_i\). As such it
measures the magnitude of a typical deviation from the regression line,
i.e.~the magnitude of a typical regression error.

\[ SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{where} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2} \]

Remember that the \(u_i\) are \emph{unobserved}. That is why we use
their estimated counterparts, the residuals \(\hat{u}_i\) instead. See
Chapter 4.3 of the book for a more detailed comment on the \(SER\).

\subsection*{Application to the Test Score
Data}\label{application-to-the-test-score-data}
\addcontentsline{toc}{subsection}{Application to the Test Score Data}

Both measures of fit can be obtained by using the function
\texttt{summary()} with an \texttt{lm} object provided as the only
argument. While the function \texttt{lm()} only prints out the estimated
coefficients to the console, \texttt{summary()} provides additional
predefined information such as the regression's \(R^2\) and the \(SER\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod_summary <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(linear_model)}
\NormalTok{mod_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.727 -14.251   0.483  12.822  48.540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 698.9329     9.4675  73.825  < 2e-16 ***
## STR          -2.2798     0.4798  -4.751 2.78e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 18.58 on 418 degrees of freedom
## Multiple R-squared:  0.05124,    Adjusted R-squared:  0.04897 
## F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06
\end{verbatim}

The \(R^2\) in the output is called \emph{Multiple R-squared} and has a
value of \(0.051\). Hence, \(5.1 \%\) of the variance of the dependent
variable \(score\) is explained by the explanatory variable \(STR\).
That is the regression explains little of the variance in \(score\) much
of the variation in test scores remains unexplained (cf.~Figure 4.3 of
the book).

The \(SER\) is called \emph{Residual standard error} and takes the value
\(18.58\). The unit of the \(SER\) is the same as the unit of the
dependent variable. In our context we can interpret the value as
follows: on average the deviation of the actual achieved test score and
the regression line is \(18.58\) points.

Now, let us check whether \texttt{summary()} uses the same definitions
for \(R^2\) and \(SER\) as we do by computing them manually.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute R^2 manually}
\NormalTok{SSR <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(mod_summary}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{TSS <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((score }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(score))}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{R2 <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{SSR}\OperatorTok{/}\NormalTok{TSS}

\CommentTok{# print the value to the console}
\NormalTok{R2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05124009
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute SER manually}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(CASchools)}
\NormalTok{SER <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(SSR }\OperatorTok{/}\StringTok{ }\NormalTok{(n}\OperatorTok{-}\DecValTok{2}\NormalTok{))}

\CommentTok{# print the value to the console}
\NormalTok{SER}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18.58097
\end{verbatim}

We find that the results coincide. Note that the values provided by
\texttt{summary()} are rounded to two decimal places. Can you do so
using \texttt{R}?

\section{The Least Squares
Assumptions}\label{the-least-squares-assumptions}

OLS performs well under a quite broad variety of different
circumstances. However, there are some assumptions which are posed on
the data which need to be satisfied in order to achieve reliable
results.

Key Concept 4.3

The Least Squares Assumptions

\[Y_i = \beta_0 + \beta_1 X_i + u_i \text{, } i = 1, ...,n\] where

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The error term \(u_i\) has conditional mean zero given \(X_i\):
  \(E(u_i|X_i) = 0\).
\item
  \((X_i,Y_i), i = 1,...,n\) are independent and identically distributed
  (i.i.d.) draws from their joint distribution.
\item
  Large outliers are unlikely: \(X_i\) and \(Y_i\) have nonzero finite
  fourth moments.
\end{enumerate}

\subsection*{Assumption \#1: The Error Term has Conditional Mean of
Zero}\label{assumption-1-the-error-term-has-conditional-mean-of-zero}
\addcontentsline{toc}{subsection}{Assumption \#1: The Error Term has
Conditional Mean of Zero}

This means that no matter which value we choose for \(X\), the error
term \(u\) must not show any systematic pattern and must have a mean of
\(0\). Consider the case that \(E(u) = 0\) but for low and high values
of \(X\), the error term tends to be positive and for midrange values of
\(X\) the error tends to be negative. We can use R to construct such an
example. To do so we generate our own data using R's build in random
number generators.

We will use the following functions you should be familiar with:

\begin{itemize}
\tightlist
\item
  \texttt{runif()} (generates uniformly distributed random numbers)
\item
  \texttt{rnorm()} (generates normally distributed random numbers)
\item
  \texttt{predict()} (does predictions based on the results of model
  fitting functions like \texttt{lm()})
\item
  \texttt{lines()} (adds line segments to an existing plot)
\end{itemize}

We start by creating a vector containing values that are randomly
scattered on the interval \([-5,5]\). For our example we decide to
generate uniformly distributed random numbers. This can be done with the
function \texttt{runif()}. We also need to simulate the error term. For
this we generate normally distributed random numbers with a mean equal
to \(0\) and a variance of \(1\) using \texttt{rnorm()}. The \(Y\)
values are obtained as a quadratic function of the \(X\) values and the
error. After generating the data we estimate both a simple regression
model and a quadratic model that also includes the regressor \(X^2\).
Finally, we plot the simulated data and add a the estimated regression
line of a simple regression model as well as the predictions made with a
quadratic model to compare the fit graphically.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set a random seed to make the results reproducible}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{321}\NormalTok{)}

\CommentTok{# simulate the data }
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DataTypeTok{min =} \OperatorTok{-}\DecValTok{5}\NormalTok{, }\DataTypeTok{max =} \DecValTok{5}\NormalTok{)}
\NormalTok{u <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{5}\NormalTok{)  }
\NormalTok{## the true relation  }
\NormalTok{Y <-}\StringTok{ }\NormalTok{X}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{X }\OperatorTok{+}\StringTok{ }\NormalTok{u                }

\CommentTok{# estimate a simple regression model }
\NormalTok{mod_simple <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}

\CommentTok{# predict using a quadratic model }
\NormalTok{prediction <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{  }\KeywordTok{I}\NormalTok{(X}\OperatorTok{^}\DecValTok{2}\NormalTok{)), }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X =} \KeywordTok{sort}\NormalTok{(X)))}

\CommentTok{# plot the results}
\KeywordTok{plot}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\KeywordTok{abline}\NormalTok{(mod_simple, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{sort}\NormalTok{(X), prediction)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-121-1} \end{center}

The plot shows what is meant by \(E(u_i|X_i) = 0\) and why it does not
hold for the linear model:

Using the quadratic model (represented by the black curve) we see that
there are no systematic deviations of the observation from the predicted
relation. It is credible that the assumption is not violated when such a
model is employed. However, using a simple linear regression model we
see that the assumption is probably violated as \(E(u_i|X_i)\) varies
with the \(X_i\).

\subsection*{Assumption \#2: Independently and Identically Distributed
Data}\label{assumption-2-independently-and-identically-distributed-data}
\addcontentsline{toc}{subsection}{Assumption \#2: Independently and
Identically Distributed Data}

Most sampling schemes used when collecting data from populations produce
i.i.d. samples. For example, we could use \texttt{R}'s random number
generator to randomly select student IDs from a university's enrollment
list and record age \(X\) and earnings \(Y\) of the corresponding
students. This is a typical example of simple random sampling and
ensures that all the \((X_i, Y_i)\) are drawn randomly from the same
population.

A prominent example where the i.i.d. assumption is not fulfilled is time
series data where we have observations on the same unit over time. For
example, take \(X\) as the number of workers employed by a production
company over the course of time. Due to technological change, the
company makes job cuts periodically but there are also some
non-deterministic influences that relate to economics, politics and
alike. Using \texttt{R} we can easily simulate such a process and plot
it.

We start the series with a total of 5000 workers and simulate the
reduction of employment with a simple autoregressive process that
exhibits a downward trend and has normally distributed errors:\footnote{See
  Chapter 14 of the book for more on autoregressive processes and time
  series analysis in general.}

\[ employment_t = 0.98 \cdot employment_{t-1} + u_t \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\CommentTok{# initialize the employment vector}
\NormalTok{X <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{5000}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{99}\NormalTok{))}

\CommentTok{# generate a date vector}
\NormalTok{Date <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(}\StringTok{"1951/1/1"}\NormalTok{), }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2050/1/1"}\NormalTok{), }\StringTok{"years"}\NormalTok{)}

\CommentTok{# generate time series observations with random influences}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{100}\NormalTok{) \{}
\NormalTok{    X[i] <-}\StringTok{ }\FloatTok{0.98} \OperatorTok{*}\StringTok{ }\NormalTok{X[i}\OperatorTok{-}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{200}\NormalTok{)}
\NormalTok{\}}

\CommentTok{#plot the results}
\KeywordTok{plot}\NormalTok{(Date, X, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Workers"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Time"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-122-1} \end{center}

It is evident that the observations on the number of employees cannot be
independent in this example: the level of today's employment is
correlated with tomorrows employment level. Thus, the i.i.d. assumption
is violated.

\subsection*{Assumption \#3: Large Outliers are
Unlikely}\label{assumption-3-large-outliers-are-unlikely}
\addcontentsline{toc}{subsection}{Assumption \#3: Large Outliers are
Unlikely}

It is easy to come up with situations where extreme observations,
i.e.~observations that deviate considerably from the usual range of the
data, may occur. Such observations are called outliers. Technically
speaking, assumption \#3 requires that \(X\) and \(Y\) have a finite
kurtosis.\footnote{See chapter 4.4 in the book.}

Common cases where we want to exclude or (if possible) correct such
outliers is when they are apparently typos, conversion errors or
measurement errors. Even if it seems like extreme observations have been
recorded correctly, it is advisable to exclude them before estimating a
model since OLS suffers from \emph{sensitivity to outliers}.

What does this mean? One can show that extreme observations receive
heavy weighting in the estimation of the unknown regression coefficients
when using OLS. Therefore, outliers can lead to strongly distorted
estimates of regression coefficients. To get a better impression of this
issue, consider the following application where we have placed some
sample data on \(X\) and \(Y\) which are highly correlated. The relation
between \(X\) and \(Y\) seems to be explained pretty good by the plotted
regression line: all of the blue dots lie close to the red line and we
have \(R^2=0.92\).

Now go ahead and add a further observation at, say, \((18,2)\). This
observations clearly is an outlier. The result is quite striking: the
estimated regression line differs greatly from the one we adjudged to
fit the data well. The slope is heavily downward biased and \(R^2\)
decreased to a mere \(29\%\)! Double-click inside the coordinate system
to reset the app. Feel free to experiment. Choose different coordinates
for the outlier or add additional ones.

The following code roughly reproduces what is shown in figure 4.5 in the
book. As done above we use sample data generated using \texttt{R}'s
random number functions \texttt{rnorm()} and \texttt{runif()}. We
estimate two simple regression models, one based on the original data
set and another using a modified set where one observation is change to
be an outlier and then plot the results. In order to understand the
complete code you should be familiar with the function \texttt{sort()}
which sorts the entries of a numeric vector in ascending order.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# generate the data}
\NormalTok{X <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DataTypeTok{min =} \DecValTok{30}\NormalTok{, }\DataTypeTok{max =} \DecValTok{70}\NormalTok{ ))}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{ , }\DataTypeTok{mean =} \DecValTok{200}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{50}\NormalTok{)}
\NormalTok{Y[}\DecValTok{9}\NormalTok{] <-}\StringTok{ }\DecValTok{2000}

\CommentTok{# fit model with outlier}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}

\CommentTok{# fit model without outlier}
\NormalTok{fitWithoutOutlier <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y[}\OperatorTok{-}\DecValTok{9}\NormalTok{] }\OperatorTok{~}\StringTok{ }\NormalTok{X[}\OperatorTok{-}\DecValTok{9}\NormalTok{])}

\CommentTok{# plot the results}
\KeywordTok{plot}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\KeywordTok{abline}\NormalTok{(fit)}
\KeywordTok{abline}\NormalTok{(fitWithoutOutlier, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-123-1} \end{center}

\hypertarget{tsdotoe}{\section{The Sampling Distribution of the OLS
Estimator}\label{tsdotoe}}

Because \(\hat{\beta_0}\) and \(\hat{\beta_1}\) are computed from a
randomly drawn sample, the estimators themselves are random variables
with a probability distribution --- the so-called sampling distribution
of the estimators --- which describes the values they could take on over
different random samples. Although the sampling distribution of
\(\hat{\beta_0}\) and \(\hat{\beta_1}\) can be complicated when the
sample size is small and generally differs with the number of
observation, \(n\), it is possible to make certain statements about it
that hold for all \(n\). In particular
\[ E(\hat{\beta_0}) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta_1}) = \beta_1,\]
that is, \(\hat\beta_0\) and \(\hat\beta_1\) are unbiased estimators of
\(\beta_0\) and \(\beta_1\), the true parameters. If the sample is
sufficiently large, by the central limit theorem the \emph{joint}
sampling distribution of the estimators is well approximated by the
bivariate normal distribution (2.1). This implies that the marginal
distributions are also normal in large samples. Core facts on the
large-sample distributions of \(\beta_0\) and \(\beta_1\) are presented
in Key Concept 4.4.

Key Concept 4.4

Large Sample Distribution of \(\hat\beta_0\) and \(\hat\beta_1\)

If the least squares assumptions in Key Concept 4.3 hold, then in large
samples \(\hat\beta_0\) and \(\hat\beta_1\) have a joint normal sampling
distribution. The large sample normal distribution of \(\hat\beta_1\) is
\(N(\beta_1, \sigma^2_{\hat\beta_1})\), where the variance of the
distribution, \(\sigma^2_{\hat\beta_1}\), is

\[ \sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2} \tag{4.1}. \]

The large sample normal distribution of \(\hat\beta_0\) is
\(N(\beta_0, \sigma^2_{\hat\beta_0})\) with

\[ \sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{4.2} \]

The interactive simulation below continuously generates random samples
\((X_i,Y_i)\) of \(200\) observations where \(E(Y\vert X) = 100 + 3X\),
estimates a simple regression model, stores the estimate of the slope
\(\beta_1\) and visualizes the distribution of the
\(\widehat{\beta}_1\)'s observed so far using a histogram. The idea here
is that for a large number of \(\widehat{\beta}_1\)'s, the histogram
gives a good approximation to the sampling distribution of the
estimator. By decreasing the time between two sampling iterations, it
becomes clear that the shape of the histogram approaches the
characteristic bell shape of a normal distribution centered at the true
slope of \(3\).

\subsection*{Simulation Study 1}\label{simulation-study-1}
\addcontentsline{toc}{subsection}{Simulation Study 1}

Whether the statements of Key Concept 4.4 really hold can also be
verified using \texttt{R}. For this we first we build our own population
of \(100000\) observations in total. To do this we need values for the
independent variable \(X\), for the error term \(u\), and for the
parameters \(\beta_0\) and \(\beta_1\). With these combined in a simple
regression model, we compute the dependent variable \(Y\). In our
example we generate the numbers \(X_i\), \(i = 1\), \ldots{} ,\(100000\)
by drawing a random sample from a uniform distribution on the interval
\([0,20]\). The realizations of the error terms \(u_i\) are drawn from a
standard normal distribution with parameters \(\mu = 0\) and
\(\sigma^2 = 100\) (note that \texttt{rnorm()} requires \(\sigma\) as
input for the argument \texttt{sd}, see \texttt{?rnorm}). Furthermore we
chose \(\beta_0 = -2\) and \(\beta_1 = 3.5\) so the true model is

\[ Y_i = -2 + 3.5 \cdot X_i. \]

Finally, we store the results in a data.frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate data}
\NormalTok{N <-}\StringTok{ }\DecValTok{100000}
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(N, }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{20}\NormalTok{)}
\NormalTok{u <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N, }\DataTypeTok{sd =} \DecValTok{10}\NormalTok{)}

\CommentTok{# population regression}
\NormalTok{Y <-}\StringTok{ }\OperatorTok{-}\DecValTok{2} \OperatorTok{+}\StringTok{ }\FloatTok{3.5} \OperatorTok{*}\StringTok{ }\NormalTok{X }\OperatorTok{+}\StringTok{ }\NormalTok{u}
\NormalTok{population <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(X, Y)}
\end{Highlighting}
\end{Shaded}

From now on we will consider the previously generated data as the true
population (which of course would be \emph{unknown} in a real world
application, otherwise there would be no reason to draw a random sample
in the first place). The knowledge about the true population and the
true relationship between \(Y\) and \(X\) can be used to verify the
statements made in Key Concept 4.4.

First, let us calculate the true variances \(\sigma^2_{\hat{\beta}_0}\)
and \(\sigma^2_{\hat{\beta}_1}\) for a randomly drawn sample of size
\(n = 100\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set sample size}
\NormalTok{n <-}\StringTok{ }\DecValTok{100}

\CommentTok{# compute the variance of beta_hat_0}
\NormalTok{H_i <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(X) }\OperatorTok{/}\StringTok{ }\KeywordTok{mean}\NormalTok{(X}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{X}
\NormalTok{var_b0 <-}\StringTok{ }\KeywordTok{var}\NormalTok{(H_i }\OperatorTok{*}\StringTok{ }\NormalTok{u) }\OperatorTok{/}\StringTok{ }\NormalTok{(n }\OperatorTok{*}\StringTok{ }\KeywordTok{mean}\NormalTok{(H_i}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\NormalTok{ )}

\CommentTok{# compute the variance of hat_beta_1}
\NormalTok{var_b1 <-}\StringTok{ }\KeywordTok{var}\NormalTok{( ( X }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(X) ) }\OperatorTok{*}\StringTok{ }\NormalTok{u ) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{100} \OperatorTok{*}\StringTok{ }\KeywordTok{var}\NormalTok{(X)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print variances to the console}
\NormalTok{var_b0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.045066
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var_b1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03018694
\end{verbatim}

Now let us assume that we do not know the true values of \(\beta_0\) and
\(\beta_1\) and that it is not possible to observe the whole population.
However, we can observe a random sample of \(n\) observations. Then, it
would not be possible to compute the true parameters but we could obtain
estimates of \(\beta_0\) and \(\beta_1\) from the sample data using OLS.
However, we know that these estimates are outcomes of random variables
themselves since the observations are randomly sampled from the
population. Key Concept 4.4. describes their distributions for large
\(n\). When drawing a single sample of size \(n\) it is not possible to
make any statement about these distributions. Things change if we repeat
the sampling scheme many times and compute the estimates for each
sample: using this procedure we simulate outcomes of the respective
distributions.

To achieve this in R, we employ the following approach:

\begin{itemize}
\tightlist
\item
  We assign the number of repetitions, say \(10000\), to \texttt{reps}.
  Then we initialize a matrix \texttt{fit} were the estimates obtained
  in each sampling iteration shall be stored row-wise. Thus \texttt{fit}
  has to be a matrix of dimensions \texttt{reps}\(\times2\).
\item
  In the next step we draw \texttt{reps} random samples of size
  \texttt{n} from the population and obtain the OLS estimates for each
  sample. The results are stored as row entries in the outcome matrix
  \texttt{fit}. This is done using a \texttt{for()} loop.
\item
  At last, we estimate variances of both estimators using the sampled
  outcomes and plot histograms of the latter. We also add a plot of the
  density functions belonging to the distributions that follow from Key
  Concept 4.4. The function \texttt{bquote()} is used to obtain math
  expressions in the titles and labels of both plots. See
  \texttt{?bquote}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set repetitions and sample size}
\NormalTok{n <-}\StringTok{ }\DecValTok{100}
\NormalTok{reps <-}\StringTok{ }\DecValTok{10000}

\CommentTok{# initialize the matrix of outcomes}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ reps)}

\CommentTok{# loop sampling and estimation of the coefficients}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{reps)\{}
\NormalTok{ sample <-}\StringTok{ }\NormalTok{population[}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N, n), ]}
\NormalTok{ fit[i, ] <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ sample)}\OperatorTok{$}\NormalTok{coefficients}
\NormalTok{\}}

\CommentTok{# compute variance estimates using outcomes}
\KeywordTok{var}\NormalTok{(fit[, }\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.057089
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(fit[, }\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03021784
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot histograms of beta_0 estimates}
\KeywordTok{hist}\NormalTok{(fit[ ,}\DecValTok{1}\NormalTok{], }
     \DataTypeTok{main =} \KeywordTok{bquote}\NormalTok{(The }\OperatorTok{~}\StringTok{ }\NormalTok{Distribution  }\OperatorTok{~}\StringTok{ }\NormalTok{of }\OperatorTok{~}\StringTok{ }\DecValTok{10000} \OperatorTok{~}\StringTok{ }\NormalTok{beta[}\DecValTok{0}\NormalTok{] }\OperatorTok{~}\StringTok{ }\NormalTok{Estimates), }
     \DataTypeTok{xlab =} \KeywordTok{bquote}\NormalTok{(}\KeywordTok{hat}\NormalTok{(beta)[}\DecValTok{0}\NormalTok{]), }
     \DataTypeTok{freq =}\NormalTok{ F)}

\CommentTok{# add true distribution to plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x, }\OperatorTok{-}\DecValTok{2}\NormalTok{, }\KeywordTok{sqrt}\NormalTok{(var_b0)), }\DataTypeTok{add =}\NormalTok{ T, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-128-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot histograms of beta_hat_1 }
\KeywordTok{hist}\NormalTok{(fit[ ,}\DecValTok{2}\NormalTok{], }
     \DataTypeTok{main =} \KeywordTok{bquote}\NormalTok{(The }\OperatorTok{~}\StringTok{ }\NormalTok{Distribution  }\OperatorTok{~}\StringTok{ }\NormalTok{of }\OperatorTok{~}\StringTok{ }\DecValTok{10000} \OperatorTok{~}\StringTok{ }\NormalTok{beta[}\DecValTok{1}\NormalTok{] }\OperatorTok{~}\StringTok{ }\NormalTok{Estimates), }
     \DataTypeTok{xlab =} \KeywordTok{bquote}\NormalTok{(}\KeywordTok{hat}\NormalTok{(beta)[}\DecValTok{1}\NormalTok{]), }
     \DataTypeTok{freq =}\NormalTok{ F)}

\CommentTok{# add true distribution to plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(x, }\FloatTok{3.5}\NormalTok{, }\KeywordTok{sqrt}\NormalTok{(var_b1)), }\DataTypeTok{add =}\NormalTok{ T, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-128-2} \end{center}

We are now able to say the following: first, our variance estimates are
in favor of the claims made in Key Concept 4.4 since they come close to
the computed theoretical values. Second, the histograms suggest that the
distributions of the estimators can be well approximated by the
respective theoretical normal distributions stated in Key Concept 4.4.

\subsection*{Simulation Study 2}\label{simulation-study-2}
\addcontentsline{toc}{subsection}{Simulation Study 2}

A further result implied by Key Concept 4.4 is that both estimators are
consistent i.e.~they converge in probability to the true parameters we
are interested in. This is since their variances converge to \(0\) as
\(n\) increases. We can check this by repeating the simulation above for
an increasing sequence of sample sizes. This means we no longer assign
the sample size but a \emph{vector} of sample sizes:
\texttt{n <- c(...)}. Let us look at the distributions of \(\beta_1\).
The idea here is to add an additional call of \texttt{for()} to the
code. This is done in order to loop over the vector of sample sizes
\texttt{n}. For each of the sample sizes we carry out the same
simulation as before but plot a density estimate for the outcomes of
each iteration over \texttt{n}. Notice that we have to change \texttt{n}
to ``)\texttt{n{[}j{]}")} in the inner loop to ensure that the
\texttt{j}\(^{th}\) element of \texttt{n} is used. In the simulation, we
use sample sizes of \(100, 250, 1000\) and \(3000\). Consequently we
have a total of four distinct simulations using different sample sizes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# set repetitions and the vector of sample sizes}
\NormalTok{reps <-}\StringTok{ }\DecValTok{1000}
\NormalTok{n <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{3000}\NormalTok{)}

\CommentTok{# initialize the matrix of outcomes}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ reps)}

\CommentTok{# devide the plot panel in a 2-by-2 array}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{# Loop sampling and plotting #}

\CommentTok{# outer loop over n}
\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(n)) \{}
  
  \CommentTok{# inner loop: sampling and estimating of the coefficients}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{reps)\{}
\NormalTok{    sample <-}\StringTok{ }\NormalTok{population[}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N, n[j]), ]}
\NormalTok{    fit[i, ] <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ sample)}\OperatorTok{$}\NormalTok{coefficients}
\NormalTok{  \}}
  
  \CommentTok{# draw density estimates}
  \KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(fit[ ,}\DecValTok{2}\NormalTok{]), }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\FloatTok{4.5}\NormalTok{), }\DataTypeTok{col =}\NormalTok{ j, }
       \DataTypeTok{main =} \KeywordTok{paste}\NormalTok{(}\StringTok{"n="}\NormalTok{, n[j]), }\DataTypeTok{xlab =} \KeywordTok{bquote}\NormalTok{(}\KeywordTok{hat}\NormalTok{(beta)[}\DecValTok{1}\NormalTok{]))}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-129-1} \end{center}

We find that, as \(n\) increases, the distribution of \(\hat\beta_1\)
concentrates around its mean, i.e.~its variance decreases. Put
differently, the likelihood of observering estimates close to the true
value of \(\beta_1 = 3.5\) grows as we increase the sample size. The
same behavior can be observed if we analyze the distribution of
\(\hat\beta_0\) instead.

\subsection*{Simulation Study 3}\label{simulation-study-3}
\addcontentsline{toc}{subsection}{Simulation Study 3}

Furthermore, (4.1) reveals that the variance of the OLS estimator for
\(\beta_1\) decreases as the variance of the \(X_i\) increases. In other
words, as we increase the amount of information provided by the
regressor, that is increasing \(Var(X)\), which is used to estimate
\(\beta_1\), we are more confident that the estimate is close to the
true value (i.e. \(Var(\hat\beta_1)\) decreases). We can visualize this
by reproducing Figure 4.6 from the book. To do this, we sample
observations \((X_i,Y_i)\), \(i=1,\dots,100\) from a bivariate normal
distribution with

\[E(X)=E(Y)=5,\] \[Var(X)=Var(Y)=5\] and \[Cov(X,Y)=4.\]

Formally, this is written down as

\begin{align}
  \begin{pmatrix}
    X \\
    Y \\
  \end{pmatrix}
  \overset{i.i.d.}{\sim} & \ \mathcal{N} 
  \left[
    \begin{pmatrix}
      5 \\
      5 \\
    \end{pmatrix}, \ 
    \begin{pmatrix}
      5 & 4 \\
      4 & 5 \\
    \end{pmatrix}
  \right]. \tag{4.3}
\end{align}

To carry out the random sampling, we make use of the function
\texttt{mvrnorm()} from the package \texttt{MASS} which allows to draw
random samples from multivariate normal distributions, see
\texttt{?mvtnorm}. Next, we use \texttt{subset()} to split the sample
into two subsets such that the first set, \texttt{set1}, consists of
observations that fulfill the condition
\(\lvert X - \overline{X} \rvert > 1\) and the second set,
\texttt{set2}, includes the remainder of the sample. We then plot both
sets and use different colors to make the observations distinguishable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the MASS package}
\KeywordTok{library}\NormalTok{(MASS)}

\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}

\CommentTok{# simulate bivarite normal data}
\NormalTok{bvndata <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }
                \DataTypeTok{mu =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), }
                \DataTypeTok{Sigma =} \KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{                ) }

\CommentTok{# assign column names / convert to data.frame}
\KeywordTok{colnames}\NormalTok{(bvndata) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{)}
\NormalTok{bvndata <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(bvndata)}

\CommentTok{# subset the data}
\NormalTok{set1 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(bvndata, }\KeywordTok{abs}\NormalTok{(}\KeywordTok{mean}\NormalTok{(X) }\OperatorTok{-}\StringTok{ }\NormalTok{X) }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{set2 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(bvndata, }\KeywordTok{abs}\NormalTok{(}\KeywordTok{mean}\NormalTok{(X) }\OperatorTok{-}\StringTok{ }\NormalTok{X) }\OperatorTok{<=}\StringTok{ }\DecValTok{1}\NormalTok{)}

\CommentTok{# plot both data sets}
\KeywordTok{plot}\NormalTok{(set1, }\DataTypeTok{xlab =} \StringTok{"X"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{)}
\KeywordTok{points}\NormalTok{(set2, }\DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-130-1} \end{center}

It is clear that observations that are close to the sample average of
the \(X_i\) have less variance than those that are farther away. Now, if
we were to draw a line as accurately as possible through either of the
two sets it is obvious that choosing the observations indicated by the
black dots, i.e.~using the set of observations which has larger variance
than the blue ones, would result in a more precise line. Now, let us use
OLS to estimate slope and intercept for both sets of observations. We
then plot the observations along with both regression lines.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate both regression lines}
\NormalTok{lm.set1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ set1)}
\NormalTok{lm.set2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X, }\DataTypeTok{data =}\NormalTok{ set2)}

\CommentTok{# plot observations}
\KeywordTok{plot}\NormalTok{(set1, }\DataTypeTok{xlab =} \StringTok{"X"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Y"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{)}
\KeywordTok{points}\NormalTok{(set2, }\DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{)}

\CommentTok{# add both lines to the plot}
\KeywordTok{abline}\NormalTok{(lm.set1, }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(lm.set2, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-131-1} \end{center}

Evidently, the green regression line does far better in describing data
sampled from the bivariate normal distribution stated in (4.3) than the
red line. This is a nice example for demonstrating why we are interested
in a high variance of the regressor \(X\): more variance in the \(X_i\)
means more information from which the precision of the estimation
benefits.

\section{Exercises}\label{exercises-2}

\begin{center}\textit{This interactive part of URFITE is only available in the HTML version.}\end{center}

\chapter{Hypothesis Tests and Confidence Intervals in the Simple Linear
Regression
Model}\label{hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model}

In this chapter, we continue our treatment of the simple linear
regression model. The following subsections discuss how we may use our
knowledge about the sampling distribution of the OLS estimator in order
to make statements regarding its uncertainty.

These subsections cover the following topics:

\begin{itemize}
\item
  Testing Hypotheses regarding regression coefficients
\item
  Confidence intervals for regression coefficients
\item
  Regression when \(X\) is a dummy variable
\item
  Heteroskedasticity and Homoskedasticity
\end{itemize}

The packages \texttt{AER} and \texttt{scales} are required for
reproduction of the code chunks presented throughout this chapter.

\section{Testing Two-Sided Hypotheses Concerning the Slope
Coefficient}\label{testing-two-sided-hypotheses-concerning-the-slope-coefficient}

Using the fact that \(\hat{\beta}_1\) is approximately normally
distributed in large samples (see \protect\hyperlink{tsdotoe}{Key
Concept 4.4}), testing hypotheses about the true value \(\beta_1\) can
be done with the same approach as discussed in chapter 3.2.

Key Concept 5.1

General Form of the \(t\)-Statistic

Remember from chapter 3 that a general \(t\)-statistic has the form

\[
  t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.
\]

Key Concept 5.2

Testing Hypotheses regarding \(\beta_1\)

For testing the hypothesis \(H_0: \beta_1 = \beta_{1,0}\), we need to
perform the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the standard error of \(\hat{\beta}_1\), \(SE(\hat{\beta}_1)\)
\end{enumerate}

\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Compute the \(t\)-statistic
\end{enumerate}

\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) }. \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Given a two sided alternative (\(H_1:\beta_1 \neq \beta_{1,0}\)) we
  reject at the \(5\%\) level if \(|t^{act}| > 1.96\) or, equivalently,
  if the \(p\)-value is less than \(0.05\).\\
  Recall the definition of the \(p\)-value:
\end{enumerate}

\begin{align}
    p \text{-value} =& \, \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| > \left|        \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] \\
    =& \, \text{Pr}_{H_0} (|t| > |t^{act}|) \\
    =& \, 2 \cdot \Phi(-|t^{act}|)
  \end{align}

~~~~~~ The last equality holds due to the normal approximation for large
samples.

Consider again the OLS regression stored in \texttt{linear\_model} from
Chapter 4 that gave us the regression line

\[ \widehat{TestScore} \ = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times STR \ , \ R^2=0.051 \ , \ SER=18.6. \]

For testing a hypothesis concerning the slope parameter (the coefficient
on \(STR\)), we need \(SE(\hat{\beta}_1)\), the standard error of the
respective point estimator. As common in the literature, standard errors
are presented in parentheses below the point estimates.

Key Concept 5.1 states that it is rather cumbersome to compute the
standard error and thereby the \(t\)-statistic by hand. The question you
should be asking yourself right now is: can we obtain these values with
minimum effort using \texttt{R}? Yes, we can. Let us first use
\texttt{summary()} to get a summary on the estimated coefficients in
\texttt{linear\_model}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# print the summary of the coefficients to the console}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Estimate Std. Error   t value      Pr(>|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## STR          -2.279808  0.4798255 -4.751327  2.783308e-06
\end{verbatim}

Looking at the second column of the coefficients' summary, we discover
values for \(SE(\hat\beta_0)\) and \(SE(\hat\beta_1)\). Also, in the
third column \texttt{t value}, we find \(t\)-statistics \(t^{act}\)
suitable for tests of the separate hypotheses \(H_0: \beta_0=0\) and
\(H_0: \beta_1=0\). Furthermore, the output provides us with
\(p\)-values corresponding to both tests against the two-sided
alternatives \(H_1:\beta_0\neq0\) respectively \(H_1:\beta_1\neq0\) in
the fourth column of the table.

Let us have a closer look at the test of

\[H_0: \beta_1=0 \ \ \ vs. \ \ \ H_1: \beta_1 \neq 0.\]

Using our revisited knowledge about \(t\)-statistics we find that

\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75. \]

What does this tell us about the significance of the estimated
coefficient? We reject the null hypothesis at the \(5\%\) level of
significance since \(|t^{act}| > 1.96\) that is the observed test
statistic falls into the region of rejection. Or, alternatively as
leading to the same result, we have
\(p\text{-value} = 2.78*10^{-6} < 0.05\). We conclude that the
coefficient is significantly different from zero. With other words, our
analysis provides evidence that the class size \emph{has an influence}
on the students test scores. We say that \(\beta_1\) is significantly
different from \(0\) at the level of \(5\%\).

Note that, although the difference is negligible in the present case as
we will see later, \texttt{summary()} does not perform the normal
approximation but calculates \(p\)-values using the \(t\)-distribution
instead. Generally, the degrees of freedom of the assumed
\(t\)-distribution are determined in the following manner:

\[ \text{DF} = n - k - 1 \]

where \(n\) is the number of observations used to estimate the model and
\(k\) is the number of regressors, excluding the intercept. In our case,
we have \(n=420\) observations and the only regressor is \(STR\) so
\(k=1\). The simplest way to determine the model degrees of freedom is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# determine residual degrees of freedom}
\NormalTok{linear_model}\OperatorTok{$}\NormalTok{df.residual}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 418
\end{verbatim}

Hence, for the assumed sampling distribution of \(\hat\beta_1\) we have

\[\hat\beta_1 \sim t_{418}\] such that the \(p\)-value for a two-sided
significance test can be obtained by executing the following code:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{pt}\NormalTok{(}\OperatorTok{-}\FloatTok{4.751327}\NormalTok{, }\DataTypeTok{df =} \DecValTok{418}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.78331e-06
\end{verbatim}

The result is very close to the value provided by \texttt{summary()}.
However since \(n\) is sufficiently large one could just as well use the
standard normal density to compute the \(p\)-value:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\FloatTok{4.751327}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.02086e-06
\end{verbatim}

The difference is indeed negligible. These findings tell us that, if
\(H_0: \beta_1 = 0\) is true and we were to repeat the whole process of
gathering observations and estimating the model, chances of observing a
\(\hat\beta_1 \geq |-4.75|\) are roughly \(1:359285\) --- so higher
chances than winning the lottery next Saturday but still very unlikely!

Using \texttt{R} we may visualize how such a statement is made when
using the normal approximation. This reflects the principles depicted in
figure 5.1 in the book. Do not let the following code chunk deter you:
the code is somewhat longer than the usual examples and looks
unappealing but there is \textbf{a lot} of repetition since color
shadings and annotations are added on both tails of the normal
distribution. We recommend to execute the code step by step in order to
see how the graph is augmented with the annotations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot the standard normal on the domain [-6,6]}
\NormalTok{t <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ t, }
     \DataTypeTok{y =} \KeywordTok{dnorm}\NormalTok{(t, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
     \DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{yaxs =} \StringTok{"i"}\NormalTok{, }
     \DataTypeTok{axes =}\NormalTok{ F, }
     \DataTypeTok{ylab =} \StringTok{""}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Calculating the p-Value of a Two-Sided Test When t^act = -4.75"}\NormalTok{, }
     \DataTypeTok{cex.lab =} \FloatTok{0.7}
\NormalTok{     )}

\NormalTok{tact <-}\StringTok{ }\OperatorTok{-}\FloatTok{4.75}

\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\OperatorTok{-}\FloatTok{1.96}\NormalTok{, }\FloatTok{1.96}\NormalTok{, }\OperatorTok{-}\NormalTok{tact, tact), }\DataTypeTok{cex.axis =} \FloatTok{0.7}\NormalTok{)}

\CommentTok{# Shade the critical regions using polygon():}

\CommentTok{# critical region in left tail}
\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\OperatorTok{-}\FloatTok{1.96}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\OperatorTok{-}\FloatTok{1.96}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\NormalTok{, }\OperatorTok{-}\FloatTok{1.96}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'orange'}
\NormalTok{        )}

\CommentTok{# critical region in right tail}

\KeywordTok{polygon}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.96}\NormalTok{, }\KeywordTok{seq}\NormalTok{(}\FloatTok{1.96}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{6}\NormalTok{),}
        \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\FloatTok{1.96}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.01}\NormalTok{)), }\DecValTok{0}\NormalTok{), }
        \DataTypeTok{col =} \StringTok{'orange'}
\NormalTok{        )}

\CommentTok{# Add arrows and texts indicating critical regions and the p-value}
\KeywordTok{arrows}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\OperatorTok{-}\FloatTok{2.5}\NormalTok{, }\FloatTok{0.02}\NormalTok{, }\DataTypeTok{length =} \FloatTok{0.1}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(}\FloatTok{3.5}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\FloatTok{0.02}\NormalTok{, }\DataTypeTok{length =} \FloatTok{0.1}\NormalTok{)}

\KeywordTok{arrows}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{, }\FloatTok{0.16}\NormalTok{, }\OperatorTok{-}\FloatTok{4.75}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DataTypeTok{length =} \FloatTok{0.1}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FloatTok{0.16}\NormalTok{, }\FloatTok{4.75}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DataTypeTok{length =} \FloatTok{0.1}\NormalTok{)}

\KeywordTok{text}\NormalTok{(}\OperatorTok{-}\FloatTok{3.5}\NormalTok{, }\FloatTok{0.22}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{paste}\NormalTok{(}\StringTok{"0.025="}\NormalTok{,}\KeywordTok{expression}\NormalTok{(alpha),}\StringTok{"/2"}\NormalTok{,}\DataTypeTok{sep =} \StringTok{""}\NormalTok{), }\DataTypeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{3.5}\NormalTok{, }\FloatTok{0.22}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{paste}\NormalTok{(}\StringTok{"0.025="}\NormalTok{,}\KeywordTok{expression}\NormalTok{(alpha),}\StringTok{"/2"}\NormalTok{,}\DataTypeTok{sep =} \StringTok{""}\NormalTok{), }\DataTypeTok{cex =} \FloatTok{0.7}\NormalTok{)}

\KeywordTok{text}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"-|"}\NormalTok{,t[act],}\StringTok{"|"}\NormalTok{)), }\DataTypeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\DataTypeTok{labels =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"|"}\NormalTok{,t[act],}\StringTok{"|"}\NormalTok{)), }\DataTypeTok{cex =} \FloatTok{0.7}\NormalTok{)}

\CommentTok{# Add ticks indicating critical values at the 0.05-level, t^act and -t^act }
\KeywordTok{rug}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.96}\NormalTok{, }\FloatTok{1.96}\NormalTok{), }\DataTypeTok{ticksize  =} \FloatTok{0.145}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}
\KeywordTok{rug}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{tact, tact), }\DataTypeTok{ticksize  =} \OperatorTok{-}\FloatTok{0.0451}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"darkgreen"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-150-1} \end{center}

The \(p\)-Value is the area under the curve to left of \(-4.75\) plus
the area under the curve to the right of \(4.75\). As we already know
from the calculations above, this value is very small.

\section{Confidence Intervals for Regression
Coefficients}\label{confidence-intervals-for-regression-coefficients}

As we already know, estimates of the regression coefficients \(\beta_0\)
and \(\beta_1\) are afflicted with sampling uncertainty, see
\protect\hyperlink{lrwor}{Chapter 4}. Therefore, we will \emph{never}
estimate the exact true value of these parameters from sample data in an
empirical application. However, we may construct confidence intervals
for the intercept and the slope parameter.

A \(95\%\) confidence interval for \(\beta_i\) has two equivalent
definitions:

\begin{itemize}
\tightlist
\item
  The interval is the set of values for which a hypothesis test to the
  level of \(5\%\) cannot be rejected.
\item
  The interval has a probability of \(95\%\) to contain the true value
  of \(\beta_i\). So in \(95\%\) of all samples that could be drawn, the
  confidence interval will cover the true value of \(\beta_i\).
\end{itemize}

We also say that the interval has a confidence level of \(95\%\). The
idea of the confidence interval is summarized in Key Concept 5.3.

Key Concept 5.3

A Confidence Interval for \(\beta_i\)

Imagine you could draw all possible random samples of given size. The
interval that contains the true value \(\beta_i\) in \(95\%\) of all
samples is given by the expression

\[ \text{KI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]

Equivalently, this interval can be seen as the set of null hypotheses
for which a \(5\%\) two-sided hypothesis test does not reject.

\subsection*{Simulation Study: Confidence
Intervals}\label{simulation-study-confidence-intervals}
\addcontentsline{toc}{subsection}{Simulation Study: Confidence
Intervals}

To get a better understanding of confidence intervals we will conduct
another simulation study. For now, assume that we are confronted with
the following sample of \(n=100\) observations on a single variable
\(Y\) where

\[ Y_i \overset{i.i.d}{\sim} N(5,25) \ \ \forall \ i = 1, \dots, 100.\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed for reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}

\CommentTok{# generate and plot the sample data}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }
           \DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }
           \DataTypeTok{sd =}\DecValTok{5}
\NormalTok{           )}

\KeywordTok{plot}\NormalTok{(Y, }
     \DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-151-1} \end{center}

We assume that the data is generated by the model

\[ Y_i = \mu + \epsilon_i \]

where \(\mu\) is an unknown constant and we know that
\(\epsilon_i \overset{i.i.d.}{\sim} N(0,25)\). In this model, the OLS
estimator for \(\mu\) is given by

\[ \hat\mu = \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i \]

(can you verify this?) i.e.~the sample average of the \(Y_i\). It
further holds that

\[ SE(\hat\mu) = \frac{\sigma_{\epsilon}}{\sqrt{n}} = \frac{5}{\sqrt{100}} \]

(see \protect\hyperlink{SVSSDASE}{Chapter 3.3}) A large sample \(95\%\)
confidence interval for \(\mu\) is then given by

\begin{equation} 
KI^{\mu}_{0.95} = \left[\hat\mu - 1.96 \times \frac{5}{\sqrt{100}} \ , \ \hat\mu + 1.96 \times \frac{5}{\sqrt{100}}  \right]. \label{eq:KI}
\end{equation}

It is fairly easy to compute this interval in \texttt{R} by hand. The
following code chunk generates a named vector containing the interval
bounds:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind}\NormalTok{(}
  \DataTypeTok{CIlower =} \KeywordTok{mean}\NormalTok{(Y) }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{10}\NormalTok{, }
  \DataTypeTok{CIupper =} \KeywordTok{mean}\NormalTok{(Y) }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{10} 
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       CIlower  CIupper
## [1,] 4.502625 6.462625
\end{verbatim}

Knowing that \(\mu = 5\) we see that our example covers the true value
for the present sample. As opposed to real world examples, we can use
\texttt{R} to get a better understanding of confidence intervals by
repeatedly sampling data, estimating \(\mu\) and computing the
confidence interval for \(\mu\) as in \eqref{eq:KI}.

The procedure is as follows:

\begin{itemize}
\tightlist
\item
  We initialize the vectors \texttt{lower} and \texttt{upper} in which
  the simulated interval limits are to be saved. We want to simulate
  \(10000\) intervals so both vectors are set to have this length.
\item
  We use a \texttt{for()} loop to sample \(100\) observations from the
  \(N(5,25)\) distribution and compute \(\hat\mu\) as well as the
  boundaries of the confidence interval in every iteration of the loop.
\item
  At last we join \texttt{lower} and \texttt{upper} in a matrix.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# initialize vectors of lower and upper interval boundaries}
\NormalTok{lower <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}
\NormalTok{upper <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10000}\NormalTok{)}

\CommentTok{# loop sampling / estimation / CI}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
\NormalTok{  Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =}\DecValTok{5}\NormalTok{)}
\NormalTok{  lower[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Y) }\OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{10}
\NormalTok{  upper[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Y) }\OperatorTok{+}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{10}
\NormalTok{\}}

\CommentTok{# join vectors of interval bounds in a matrix}
\NormalTok{CIs <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(lower, upper)}
\end{Highlighting}
\end{Shaded}

According to Key Concept 5.3 we expect that the fraction of the
\(10000\) simulated intervals saved in the matrix \texttt{CIs} that
contain the true value \(\mu=5\) should be roughly \(95\%\). We can
easily check this using logical operators.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(CIs[, }\DecValTok{1}\NormalTok{] }\OperatorTok{<=}\StringTok{ }\DecValTok{5} \OperatorTok{&}\StringTok{ }\DecValTok{5} \OperatorTok{<=}\StringTok{ }\NormalTok{CIs[, }\DecValTok{2}\NormalTok{])}\OperatorTok{/}\DecValTok{10000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9487
\end{verbatim}

The simulation shows that the fraction of intervals covering \(\mu=5\),
i.e.~those intervals for which \(H_0: \mu = 5\) cannot be rejected is
close to the theoretical value of \(95\%\).

Let us draw a plot of the first \(100\) simulated confidence intervals
and indicate those which \emph{do not} cover the true value of \(\mu\).
We do this by adding horizontal lines representing the confidence
intervals on top of each other.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# identify intervals not covering mu}
\CommentTok{# (4 intervals out of 100)}
\NormalTok{ID <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\OperatorTok{!}\NormalTok{(CIs[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OperatorTok{<=}\StringTok{ }\DecValTok{5} \OperatorTok{&}\StringTok{ }\DecValTok{5} \OperatorTok{<=}\StringTok{ }\NormalTok{CIs[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{]))}

\CommentTok{# initialize the plot}
\KeywordTok{plot}\NormalTok{(}\DecValTok{0}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{), }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{), }
     \DataTypeTok{ylab =} \StringTok{"Sample"}\NormalTok{, }
     \DataTypeTok{xlab =} \KeywordTok{expression}\NormalTok{(mu), }
     \DataTypeTok{main =} \StringTok{"Confidence Intervals: Correct H0"}\NormalTok{)}

\CommentTok{# set up color vector}
\NormalTok{colors <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{gray}\NormalTok{(}\FloatTok{0.6}\NormalTok{), }\DecValTok{100}\NormalTok{)}
\NormalTok{colors[ID] <-}\StringTok{ "red"}

\CommentTok{# draw reference line at mu=5}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \DecValTok{5}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{# add horizontal bars representing the CIs}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{) \{}
  \KeywordTok{lines}\NormalTok{(}\KeywordTok{c}\NormalTok{(CIs[j, }\DecValTok{1}\NormalTok{], CIs[j, }\DecValTok{2}\NormalTok{]), }
        \KeywordTok{c}\NormalTok{(j, j), }
        \DataTypeTok{col =}\NormalTok{ colors[j], }
        \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-155-1} \end{center}

We find that for the first \(100\) samples, the true null hypothesis is
rejected in four cases so these intervals do not cover \(\mu=5\). We
have indicated the intervals which lead to a rejection of the true null
hypothesis by red color.

Let us now turn back to the example of test scores and class sizes. The
regression model from Chapter 4 is stored in \texttt{linear\_model}. An
easy way to get \(95\%\) confidence intervals for \(\beta_0\) and
\(\beta_1\), the coefficients on \texttt{(intercept)} and \texttt{STR},
is to use the function \texttt{confint()}. We only have to provide a
fitted model object as an to this function. The confidence level is set
to \(95\%\) by default but can be modified by setting the argument
\texttt{level}, see \texttt{?confint}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(linear_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 2.5 %     97.5 %
## (Intercept) 680.32312 717.542775
## STR          -3.22298  -1.336636
\end{verbatim}

Let us check if the calculation is done as we expect it to be. For
\(\beta_1\), that is the coefficient on \texttt{STR}, according to the
formula presented above the interval borders are computed as

\[  -2.279808 \pm 1.96 \times 0.4798255 \, \Rightarrow \, \text{KI}_{0.95}^{\beta_1} = \left[ -3.22, -1.34 \right]  \]

so this actually leads to the same interval. Obviously, this interval
\emph{does not} contain the value zero which, as we have already seen in
the previous section, leads to the rejection of the null hypothesis
\(\beta_{1,0} = 0\).

\section{Regression when X is a Binary
Variable}\label{regression-when-x-is-a-binary-variable}

Instead of using a continuous regressor \(X\), we might be interested in
running the regression

\[ Y_i = \beta_0 + \beta_1 D_i + u_i \tag{5.2} \]

where \(D_i\) is a binary variable, a so-called \emph{dummy variable}.
For example, we may define \(D_i\) in the following way:

\[ D_i = \begin{cases}
        1 \ \ \text{if $STR$ in $i^{th}$ school district < 20} \\
        0 \ \ \text{if $STR$ in $i^{th}$ school district $\geq$ 20} \\
      \end{cases} \tag{5.3} \]

The regression model now is

\[ TestScore_i = \beta_0 + \beta_1 D_i + u_i. \tag{5.4} \]

Let us see how these data look like in a scatter plot:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create the dummy variable as defined above using a for loop}
\NormalTok{D <-}\StringTok{ }\NormalTok{CASchools}\OperatorTok{$}\NormalTok{STR }\OperatorTok{<}\StringTok{ }\DecValTok{20}
\NormalTok{CASchools}\OperatorTok{$}\NormalTok{D <-}\StringTok{ }\NormalTok{D}

\CommentTok{# Plot the data}
\KeywordTok{plot}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{D, CASchools}\OperatorTok{$}\NormalTok{score,            }\CommentTok{# provide the data to be ploted}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{,                                }\CommentTok{# use filled circles as plot symbols}
     \DataTypeTok{cex =} \FloatTok{0.5}\NormalTok{,                               }\CommentTok{# set size of plot symbols to 0.5}
     \DataTypeTok{col =} \StringTok{"Steelblue"}\NormalTok{,                       }\CommentTok{# set the symbols' color to "Steelblue"}
     \DataTypeTok{xlab =} \KeywordTok{expression}\NormalTok{(D[i]),                 }\CommentTok{# Set title and axis names}
     \DataTypeTok{ylab =} \StringTok{"Test Score"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Dummy Regression"}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-158-1} \end{center}

We see that with \(D\) as the regressor, it is not useful to think of
\(\beta_1\) as a slope parameter since \(D_i \in \{0,1\}\), i.e.~we only
observe two discrete values instead of a continuum of regressor values
lying in some range on the real line. Simply put, there is no continuous
line depicting the conditional expectation function
\(E(TestScore_i | D_i)\) since this function is solely defined for
\(X\)-positions \(0\) and \(1\).

Therefore, the interpretation of the coefficients in our regression
model is as follows:

\begin{itemize}
\item
  \(E(Y_i | D_i = 0) = \beta_0\) so \(\beta_0\) is the expected test
  score in districts where \(D_i=0\) i.e.~where \(STR\) is below \(20\).
\item
  \(E(Y_i | D_i = 1) = \beta_0 + \beta_1\) or, using the result above,
  \(\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)\). Thus, \(\beta_1\)
  is \emph{the difference in group specific expectations}, i.e.~the
  difference in expected test score between districts with \(STR < 20\)
  and those with \(STR \geq 20\).
\end{itemize}

We will now use \texttt{R} to estimate the dummy regression model as
defined by the equations (5.2) and (5.3) .

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the dummy regression model}
\NormalTok{dummy_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{D, }\DataTypeTok{data =}\NormalTok{ CASchools)}
\KeywordTok{summary}\NormalTok{(dummy_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = score ~ D, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -50.496 -14.029  -0.346  12.884  49.504 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  650.077      1.393 466.666  < 2e-16 ***
## DTRUE          7.169      1.847   3.882  0.00012 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 18.74 on 418 degrees of freedom
## Multiple R-squared:  0.0348, Adjusted R-squared:  0.0325 
## F-statistic: 15.07 on 1 and 418 DF,  p-value: 0.0001202
\end{verbatim}

One can see that the expected test score in districts with \(STR < 20\)
(\(D_i = 1\)) is predicted to be \(650.1 + 7.17 = 657.27\) while
districts with \(STR \geq 20\) (\(D_i = 0\)) are expected to have an
average test score of only \(650.1\).

Group specific predictions can be added to the plot by execution of the
following code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add group specific predictions to the plot}
\KeywordTok{points}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ CASchools}\OperatorTok{$}\NormalTok{D, }
       \DataTypeTok{y =} \KeywordTok{predict}\NormalTok{(dummy_model), }
       \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
       \DataTypeTok{pch =} \DecValTok{20}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

Here we use the function \texttt{predict()} to obtain estimates of the
group specific means. The red dots represent these sample group
averages. Accordingly, \(\hat{\beta}_1 = 7.17\) can be seen as the
difference in group averages.

By inspection of the output generated with
\texttt{summary(dummy\_model)} we may also find an answer to the
question whether there is a statistically significant difference in
group means. This in turn would support the hypothesis that students
perform differently when they are taught in small classes compared to
those taught in large classes. We can assess this by a two-tailed test
of the hypothesis \(H_0: \beta_1 = 0\). Conveniently the \(t\)-statistic
and the corresponding \(p\)-value for this test are computed by
\texttt{summary()}.

Since \texttt{t value} \(= 3.88 > 1.96\) we reject the null hypothesis
at the \(5\%\) level of significance. The same conclusion can be made
when using the \(p\)-value which reports significance to the
\(0.00012\%\) level.

As done with ')\texttt{linear\_model\textquotesingle{})}, we may
alternatively use the \texttt{confint()} function to compute a \(95\%\)
confidence interval for the true difference in means and see if the
hypothesized value is an element of this confidence set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confidence intervals for coefficients in the dummy regression model}
\KeywordTok{confint}\NormalTok{(dummy_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  2.5 %    97.5 %
## (Intercept) 647.338594 652.81500
## DTRUE         3.539562  10.79931
\end{verbatim}

We reject the hypothesis that there is no difference between group means
at the \(5\%\) significance level since \(\beta_{1,0} = 0\) lies outside
of \([3.54, 10.8]\), the \(95\%\) confidence interval for the
coefficient on \(D\).

\section{Heteroskedasticity and
Homoskedasticity}\label{heteroskedasticity-and-homoskedasticity}

All inference made in the previous chapters relies on the assumption
that the error variance does not vary as regressor values change. But
this will not necessarily be the case in empirical applications.

Key Concept 5.4

Heteroskedasticity and Homoskedasticity

\begin{itemize}
\item
  We say that the error term of our regression model is homoskedastic if
  the variance of the conditional distribution of \(u_i\) given \(X_i\),
  \(Var(u_i|X_i=x)\), is constant \emph{for all} observations in our
  sample \[ \text{Var}(u_i|X_i=x) = \sigma^2 \ \forall \ i=1,\dots,n. \]
\item
  If instead there is dependence of the conditional variance of \(u_i\)
  on \(X_i\), the error term is said to be heteroskedastic. We then
  write
  \[ \text{Var}(u_i|X_i=x) = \sigma_i^2 \ \forall \ i=1,\dots,n. \]
\item
  Homoskedasticity is a \emph{special case} of heteroskedasticity.
\end{itemize}

For a better understanding of heteroskedasticity, we generate some
bivariate heteroskedastic data, estimate a linear regression model and
then use box plots to depict the conditional distributions of the
residuals.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load scales package for adjusting color opacities}
\KeywordTok{library}\NormalTok{(scales)}

\CommentTok{# Genrate some heteroskedastic data}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
\NormalTok{x <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{,}\DecValTok{25}\NormalTok{), }\DataTypeTok{each =} \DecValTok{25}\NormalTok{)}
\NormalTok{e <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{12}\NormalTok{)                }
\NormalTok{i <-}\StringTok{ }\KeywordTok{order}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DataTypeTok{max =} \KeywordTok{dnorm}\NormalTok{(e, }\DataTypeTok{sd =} \DecValTok{12}\NormalTok{))) }
\NormalTok{y <-}\StringTok{ }\DecValTok{720} \OperatorTok{-}\StringTok{ }\FloatTok{3.3} \OperatorTok{*}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{e[}\KeywordTok{rev}\NormalTok{(i)]}

\CommentTok{# Estimate the model }
\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x)}

\CommentTok{# Plot the data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }
     \DataTypeTok{y =}\NormalTok{ y, }
     \DataTypeTok{main =} \StringTok{"An Example of Heteroskedasticity"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Student-Teacher Ratio"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Test Score"}\NormalTok{,}
     \DataTypeTok{cex =} \FloatTok{0.5}\NormalTok{, }
     \DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{27}\NormalTok{), }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{600}\NormalTok{, }\DecValTok{710}\NormalTok{)}
\NormalTok{     )}

\CommentTok{# Add the regression line to the plot}
\KeywordTok{abline}\NormalTok{(mod, }\DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{)}

\CommentTok{# Add boxplots to the plot}
\KeywordTok{boxplot}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }
        \DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }
        \DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{), }
        \DataTypeTok{col =} \KeywordTok{alpha}\NormalTok{(}\StringTok{"gray"}\NormalTok{, }\FloatTok{0.4}\NormalTok{), }
        \DataTypeTok{border =} \StringTok{"black"}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-162-1} \end{center}

For this artificial data it is clear to see that we face unequal
conditional error variances. Specifically, we observe that the variance
in test scores (and therefore the variance of the errors committed)
\emph{increases} with the student teacher ratio.

\subsection*{A Real-World Example for
Heteroskedasticity}\label{a-real-world-example-for-heteroskedasticity}
\addcontentsline{toc}{subsection}{A Real-World Example for
Heteroskedasticity}

Think about the economic value of education: if there would not be an
expected economic value-added to receiving education at university, you
probably would not be reading this script right now. A starting point to
empirical verification of such a relation is to have data on graduates.
More precisely, we need data on wages and education of graduates in
order to estimate a model like

\[ wage_i = \beta_0 + \beta_1 \cdot education_i + u_i. \]

What can be presumed about this relation? It is likely that, on average,
higher educated workers earn more money than workers with less
education, so we expect to estimate an upward sloping regression line.
Also it seems plausible that better educated workers are more likely to
meet the requirements for the well-paid jobs. However, workers with low
education will have no shot at those well-paid jobs. Therefore it seems
plausible that the distribution of earnings spreads out as education
increases. In other words: we expect that there is heteroskedasticity.

To verify this empirically we may use real data on hourly earnings and
the number of years of education of employees. Such data can be found in
\texttt{CPSSWEducation}. This data set is part of the package
\texttt{AER} and stems from the Current Population Survey (CPS) which is
conducted periodically by the \href{http://www.bls.gov/}{Bureau of Labor
Statistics} in the United States.

The subsequent code chunks demonstrate how to import the data into
\texttt{R} and how to produce a plot in the fashion of Figure 5.3 in the
book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package and attach data}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(}\StringTok{"CPSSWEducation"}\NormalTok{)}
\KeywordTok{attach}\NormalTok{(CPSSWEducation)}

\CommentTok{# get an overview}
\KeywordTok{summary}\NormalTok{(CPSSWEducation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age          gender        earnings        education    
##  Min.   :29.0   female:1202   Min.   : 2.137   Min.   : 6.00  
##  1st Qu.:29.0   male  :1748   1st Qu.:10.577   1st Qu.:12.00  
##  Median :29.0                 Median :14.615   Median :13.00  
##  Mean   :29.5                 Mean   :16.743   Mean   :13.55  
##  3rd Qu.:30.0                 3rd Qu.:20.192   3rd Qu.:16.00  
##  Max.   :30.0                 Max.   :97.500   Max.   :18.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate a simple regression model}
\NormalTok{labor_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(earnings }\OperatorTok{~}\StringTok{ }\NormalTok{education)}

\CommentTok{# plot observations and add the regression line}
\KeywordTok{plot}\NormalTok{(education, }
\NormalTok{     earnings, }
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{150}\NormalTok{)}
\NormalTok{     )}

\KeywordTok{abline}\NormalTok{(labor_model, }
       \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
       \DataTypeTok{lwd =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-163-1} \end{center}

From inspecting the plot we can tell that the mean of the distribution
of earnings increases with the level of education. This is also
suggested by formal analysis: the estimated regression model stored in
\texttt{labor\_mod} shows that there is a positive relation between
years of education and earnings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{labor_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = earnings ~ education)
## 
## Coefficients:
## (Intercept)    education  
##      -3.134        1.467
\end{verbatim}

The estimated regression equation states that, on average, an additional
year of education increases a workers hourly earnings by about
\(\$ 1.47\). Once more we use \texttt{confint()} to obtain a \(95\%\)
confidence interval for both regression coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(labor_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 2.5 %    97.5 %
## (Intercept) -5.015248 -1.253495
## education    1.330098  1.603753
\end{verbatim}

Since the interval is \([1.33, 1.60]\) we can reject the hypothesis that
the coefficient on \texttt{education} is zero at the \(5\%\) level.

Furthermore, the plot indicates that there is heteroskedasticity: if we
assume the regression line to be a reasonably good representation of the
conditional mean function \(E(earnings_i\vert education_i)\), the
dispersion of hourly earnings around that function clearly increases
with the level of education, i.e.~the variance of the distribution of
earnings increases. In other words: the variance of the residuals (the
errors made in explaining earnings by education) increases with
education so that the regression errors are heteroskedastic. This
example makes a case that the assumption of homoskedasticity is doubtful
in economic applications.

\subsection*{Should We Care About
Heteroskedasticity?}\label{should-we-care-about-heteroskedasticity}
\addcontentsline{toc}{subsection}{Should We Care About
Heteroskedasticity?}

To answer the question whether we should worry about heteroskedasticity
being present, let us see how the variance of \(\hat\beta_1\) is
computed under the assumption of homoskedasticity. In this case we have

\[ \sigma^2_{\hat\beta_1} = \frac{\sigma^2_u}{n \cdot \sigma^2_X} \tag{5.5} \]

which is a simplified version of the general equation (4.1) presented in
Key Concept 4.4. See Appendix 5.1 of the book for details on the
derivation. \texttt{summary()} estimates (5.5) by

\[ \overset{\sim}{\sigma}^2_{\hat\beta_1} = \frac{SER^2}{\sum_{i=1}^n (X_i - \overline{X})^2} \ \ \text{where} \ \ SER=\frac{1}{n-2} \sum_{i=1}^n \hat u_i^2. \]

Thus \texttt{summary()} estimates the \emph{homoskedasticity-only}
standard error

\[ \sqrt{ \overset{\sim}{\sigma}^2_{\hat\beta_1} } = \sqrt{ \frac{SER^2}{\sum_{i=1}^n(X_i - \overline{X})^2} }. \]

This is in fact an estimator for the standard deviation of the estimator
\(\hat{\beta}_1\) that is \emph{inconsistent} for the true value
\(\sigma^2_{\hat\beta_1}\) when there is heteroskedasticity. The
implication is that \(t\)-statistics computed in the manner of Key
Concept 5.1 do not have a standard normal distribution, even in large
samples. This issue may invalidate inference drawn when using the
previously treated tools for hypothesis testing: we should be cautious
when making statements about the significance of regression coefficients
on the basis of \(t\)-statistics as computed by \texttt{summary()} or
confidence intervals produced by \texttt{confint()} if it is doubtful
for the assumption of homoskedasticity to hold!

We will now use \texttt{R} to compute the homoskedasticity-only standard
error estimate for \(\hat{\beta}_1\) in the test score regression model
\texttt{linear\_model} by hand and see if it matches the value produced
by \texttt{summary()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Store model summary in 'model'}
\NormalTok{model <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(linear_model)}

\CommentTok{# Extract the standard error of the regression from model summary}
\NormalTok{SER <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{sigma}

\CommentTok{# Compute the variation in 'size'}
\NormalTok{V <-}\StringTok{ }\NormalTok{(}\KeywordTok{nrow}\NormalTok{(CASchools)}\OperatorTok{-}\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{var}\NormalTok{(CASchools}\OperatorTok{$}\NormalTok{STR)}

\CommentTok{# Compute the standard error of the slope parameter's estimator and print it}
\NormalTok{SE.beta_}\FloatTok{1.}\NormalTok{hat <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(SER}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{V)}
\NormalTok{SE.beta_}\FloatTok{1.}\NormalTok{hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4798255
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Use logical operators to see if the value computed by hand matches the one provided }
\CommentTok{# in mod$coefficients. Round estimates to four decimal places}
\KeywordTok{round}\NormalTok{(model}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{], }\DecValTok{4}\NormalTok{) }\OperatorTok{==}\StringTok{ }\KeywordTok{round}\NormalTok{(SE.beta_}\FloatTok{1.}\NormalTok{hat, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Indeed, the estimated values are equal.

\subsection*{Computation of Heteroskedasticity-Robust Standard
Errors}\label{computation-of-heteroskedasticity-robust-standard-errors}
\addcontentsline{toc}{subsection}{Computation of
Heteroskedasticity-Robust Standard Errors}

Consistent estimation of \(\sigma_{\hat{\beta}_1}\) under
heteroskedasticity is granted when the following \emph{robust} estimator
is used.

\[ SE(\hat{\beta}_1) = \sqrt{ \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \tag{5.6} \]

Standard error estimates computed this way are also referred to as
\href{https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors}{Eicker-Huber-White
standard errors}. It can be quite cumbersome to do this calculation by
hand. Luckily, there are \texttt{R} function for that purpose. A
convenient one, named \texttt{vcovHC()} is part of the package
')\texttt{sandwich\textquotesingle{})}.\footnote{The package
  \texttt{sandwich} is a dependency of the package \texttt{AER} meaning
  that it is attached automatically if you load \texttt{AER}} This
function can compute a variety of standard error estimators. The one
brought forward in (5.6) is computed when the argument \texttt{type} is
set to \texttt{"HC0"}.

Let us now compute robust standard error estimates for the coefficients
in \texttt{linear\_model}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the sandwich package}
\KeywordTok{library}\NormalTok{(sandwich)}

\CommentTok{# compute robust standard error estimates}
\NormalTok{vcov <-}\StringTok{ }\KeywordTok{vcovHC}\NormalTok{(linear_model, }\DataTypeTok{type =} \StringTok{"HC0"}\NormalTok{)}
\NormalTok{vcov}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             (Intercept)        STR
## (Intercept)  106.908469 -5.3383689
## STR           -5.338369  0.2685841
\end{verbatim}

The output of \texttt{vcovHC()} is the variance-covariance matrix of
coefficient estimates. We are interested in the square root of the
diagonal elements of this matrix since these values are the standard
error estimates we seek.

\BeginKnitrBlock{rmdknit}
When we have k \textgreater{} 1 regressors, writing down the equations
for a regression model becomes very messy. A more convinient way to
denote and estimate so-called multiple regression models is by using
matrix algebra. This is why functions like vcovHC() produce matrices. In
the simple linear regression model, the variances and covariances of the
coefficient estimators can be gathered in the variance-covariance matrix

\begin{equation}
\text{Var}
  \begin{pmatrix}
    \hat\beta_0 \\
    \hat\beta_1
  \end{pmatrix} = 
\begin{pmatrix}
  \text{Var}(\hat\beta_0) & \text{Cov}(\hat\beta_0,\hat\beta_1) \\
\text{Cov}(\hat\beta_0,\hat\beta_1) & \text{Var}(\hat\beta_1)
\end{pmatrix}
\end{equation}

which is a symmetric matrix. So vcovHC() gives us
\(\widehat{\text{Var}}(\hat\beta_0)\),
\(\widehat{\text{Var}}(\hat\beta_1)\) and
\(\widehat{\text{Cov}}(\hat\beta_0,\hat\beta_1)\) but most of the time
we are interested in the diagonal elements of the estimated matrix.
\EndKnitrBlock{rmdknit}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute the square root of the diagonal elements in vcov}
\NormalTok{robust_se <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(vcov))}
\NormalTok{robust_se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)         STR 
##   10.339655    0.518251
\end{verbatim}

Now assume we want to generate a coefficient summary as provided by
\texttt{summary()} but with \emph{robust} standard error estimates for
the coefficient estimators, robust \(t\)-statistics and corresponding
\(p\)-values for the regression model
')\texttt{linear\_model\textquotesingle{})}. This can be done using
\texttt{coeftest()} from the package \texttt{lmtest}, see
\texttt{?coeftest}. Further we specify in the argument \texttt{vcov.}
that \texttt{vcov}, the Eicker-Huber-White estimate of the variance
matrix we have computed before should be used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We invoke the function `coeftest()` on our model}
\KeywordTok{coeftest}\NormalTok{(linear_model, }\DataTypeTok{vcov. =}\NormalTok{ vcov)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 698.93295   10.33966  67.597 < 2.2e-16 ***
## STR          -2.27981    0.51825  -4.399 1.382e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that values reported in the column \texttt{Std. Error} are equal
to the ones received using \texttt{sqrt(diag(vcov))}.

How severe are the implications of using homoskedasticity-only standard
errors in the presence of heteroskedasticity? The answer is: it depends.
As mentioned above we may face the risk of drawing wrong conclusions
when conducting significance tests. Let us illustrate this by generating
another example of a heteroskedastic data set and use it to estimate a
simple regression model. We take

\[ Y_i = \beta_1 \cdot X_i + u_i \ \ , \ \ u_i \overset{i.i.d.}{\sim} N(0,0.36 \cdot X_i^2)  \]

with \(\beta_1=1\) as the data generating process. Clearly the
assumption of homoskedasticity is violated here since the variance of
the errors is a non-linear, increasing function of \(X_i\) but the
errors have zero mean and are i.i.d. such that the assumptions made in
Key Concept 4.3 are not violated. As before, the true conditional mean
function we are interested in estimating is

\[ E(Y_i\vert X_i) = \beta_1 X_i. \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{21}\NormalTok{)}

\CommentTok{# generate heteroskedastic data }
\NormalTok{X <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{500}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{500}\NormalTok{, }\DataTypeTok{mean =}\NormalTok{ X, }\DataTypeTok{sd =} \FloatTok{0.6}\OperatorTok{*}\NormalTok{X)}

\CommentTok{# estimate a simple regression model}
\NormalTok{reg <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\end{Highlighting}
\end{Shaded}

We plot the data and add the regression line.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the data}
\KeywordTok{plot}\NormalTok{(X, Y, }
     \DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
     \DataTypeTok{cex =} \FloatTok{0.8}
\NormalTok{     )}

\CommentTok{# add the regression line to the plot}
\KeywordTok{abline}\NormalTok{(reg, }
       \DataTypeTok{col =} \StringTok{"darkred"}\NormalTok{, }
       \DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-171-1} \end{center}

The plot shows that the data are heteroskedastic as the variance of
\(Y\) grows with \(X\). We continue by conducting a significance test of
the (true) null hypothesis \(H_0: \beta_1 = 1\) twice, once using the
homoskedasticity-only standard error formula and once with the robust
version (5.6). An easy way to do this in \texttt{R} is the function
\texttt{linearHypothesis()} from the package \texttt{car}, see
\texttt{?linearHypothesis}. It allows to test linear hypotheses about
parameters in linear models in a similar way as done with a
\(t\)-statistic and offers various robust covariance matrix estimators.
We test by comparing the tests' \(p\)-values to the significance level
of \(5\%\).

\BeginKnitrBlock{rmdknit}
linearHypothesis() computes a test statistic that follows an \(F\)
distribution under the null hypothesis. We will not loose too much words
on the theory behind it at this time. In general, the idea of the \(F\)
test is to compare the fit of different models. When testing a
hypothesis about a \emph{single} coefficient using an \(F\) test, one
can show that the test statistic is simply the square of the
corresponding \(t\)-statistic:

\[ F = t^2 = \frac{\hat\beta_i - \beta_{i,0}}{SE(\hat\beta_i)} \sim F_{1,n-k-1}  \]

In linearHypothesis(), the hypothesis must be provided as a
\emph{string}. The function returns an object of class anova which
contains further information on the test that can be accessed using the
\$ operator.
\EndKnitrBlock{rmdknit}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test hypthesis using default standard error}
\KeywordTok{linearHypothesis}\NormalTok{(reg, }\DataTypeTok{hypothesis.matrix =} \StringTok{"X = 1"}\NormalTok{)}\OperatorTok{$}\StringTok{'Pr(>F)'}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test hypothesis using robust standard error}
\KeywordTok{linearHypothesis}\NormalTok{(reg, }\DataTypeTok{hypothesis.matrix =} \StringTok{"X = 1"}\NormalTok{, }\DataTypeTok{white.adjust =} \StringTok{"hc0"}\NormalTok{)}\OperatorTok{$}\StringTok{'Pr(>F)'}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

This is a good example of what can go wrong if we ignore
heteroskedasticity: for the data set at hand the default method rejects
the null hypothesis \(\beta_1 = 1\) although it is true. When using the
robust standard error formula the test does not reject the null. Of
course we could argue that this is just a coincidence and both tests are
equally well in maintaining the type I error rate of \(5\%\). This can
be further investigated by computing \emph{Monte Carlo} estimates of the
rejection frequencies of both tests on the basis of a large number of
random samples. We proceed as follows:

\begin{itemize}
\tightlist
\item
  initialize vectors \texttt{t} and \texttt{t.rob}.
\item
  Using a \texttt{for()} loop, we generate \(10000\) heteroskedastic
  random samples of size \(1000\), estimate the regression model and
  check whether the tests falsly reject the null at the level of \(5\%\)
  using comparison operators. The results are stored in the respective
  vectors \texttt{t} and \texttt{t.rob}.
\item
  After the simulation, we compute the fraction of false rejections for
  both tests.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# initialize vectors t and t.rob}
\NormalTok{t <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\NormalTok{t.rob <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}

\CommentTok{# loop sampling and estimation}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}
  
  \CommentTok{# sample data}
\NormalTok{  X <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{1000}
\NormalTok{  Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{mean =}\NormalTok{ X, }\DataTypeTok{sd =} \FloatTok{0.6}\OperatorTok{*}\NormalTok{X)}

  \CommentTok{# estimate regression model}
\NormalTok{  reg <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}

  \CommentTok{# homoskedasdicity-only significance test}
\NormalTok{  t[i] <-}\StringTok{ }\KeywordTok{linearHypothesis}\NormalTok{(reg, }\StringTok{"X = 1"}\NormalTok{)}\OperatorTok{$}\StringTok{'Pr(>F)'}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}

  \CommentTok{# robust significance test}
\NormalTok{  t.rob[i] <-}\StringTok{ }\KeywordTok{linearHypothesis}\NormalTok{(reg, }\StringTok{"X = 1"}\NormalTok{, }\DataTypeTok{white.adjust =} \StringTok{"hc0"}\NormalTok{)}\OperatorTok{$}\StringTok{'Pr(>F)'}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}

\NormalTok{\}}

\CommentTok{# compute the fraction of false rejections}
\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{t =} \KeywordTok{sum}\NormalTok{(t), }\DataTypeTok{t.rob =} \KeywordTok{sum}\NormalTok{(t.rob)) }\OperatorTok{/}\StringTok{ }\DecValTok{10000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           t  t.rob
## [1,] 0.0781 0.0516
\end{verbatim}

The results show that we face an increased risk of falsely rejecting the
null using the homoskedasticity-only standard error for the testing
problem at hand: with the common standard error estimator, \(7.62\%\) of
all tests reject the null hypothesis falsely. In contrast, with the
robust test statistic we are very close to the nominal level of \(5\%\).

\section{The Gauss-Markov Theorem}\label{the-gauss-markov-theorem}

When estimating regression models, we know that the results of the
estimation procedure are outcomes of a random process. However, when
using unbiased estimators, at least on average, we estimate the true
parameter. When comparing different unbiased estimators, it is therefore
interesting to know which one has the highest precision: being aware
that the likelihood of estimating the \emph{exact} value of the
parameter of interest is \(0\) in an empirical application, we want to
make sure that the likelihood of obtaining an estimate very close to the
true value is as high as possible. This means we want to use the
estimator with the lowest variance of all unbiased estimators. The
Gauss-Markov theorem states that, in the class of conditionally unbiased
linear estimators, the OLS estimator has this property under certain
conditions.

Key Concept 5.5

The Gauss-Markov Theorem for \(\hat{\beta}_1\)

Suppose that the assumptions made in Key Concept 4.3 hold \emph{and}
that the errors are \emph{homoskedastic}. The OLS estimator is the best
(in the sense of smallest variance) linear conditionally unbiased
estimator (BLUE) in this setting.

Let us have a closer look at what this means:

\begin{itemize}
\item
  Estimators of \(\beta_1\) that are linear functions of the
  \(Y_1, \dots, Y_n\) and that are unbiased conditionally on the
  regressor \(X_1, \dots, X_n\) can be written as
  \[ \overset{\sim}{\beta}_1 = \sum_{i=1}^n a_i Y_i \] where the \(a_i\)
  are weights that are allowed to depend on the \(X_i\) but \emph{not}
  on the \(Y_i\).
\item
  We already know that \(\overset{\sim}{\beta}_1\) has a sampling
  distribution: \(\overset{\sim}{\beta}_1\) is a linear function of the
  \(Y_i\) which are random variables. If now
  \[ E(\overset{\sim}{\beta}_1 | X_1, \dots, X_n) = \beta_1 \] we say
  that \(\overset{\sim}{\beta}_1\) is a linear unbiased estimator of
  \(\beta_1\), conditionally on the \(X_1, \dots, X_n\).
\item
  We may ask if \(\overset{\sim}{\beta}_1\) is also the \emph{best}
  estimator in this class, i.e.~the most efficient one of all linear
  conditionally unbiased estimators where ``most efficient'' means
  smallest variance. The weights \(a_i\) play an important role here and
  it turns out that OLS uses just the right weights to have the BLUE
  property.
\end{itemize}

\subsection*{Simulation Study: BLUE
Estimator}\label{simulation-study-blue-estimator}
\addcontentsline{toc}{subsection}{Simulation Study: BLUE Estimator}

Consider the case of a regression of \(Y_i,\dots,Y_n\) only on a
constant. Here, the \(Y_i\) are assumed to be a random sample from a
population with mean \(\mu\) and variance \(\sigma^2\). We know that the
OLS estimator in this model is simply the sample mean:

\begin{equation}
\hat{\beta}_1 = \overline{\beta}_1 = \sum_{i=1}^n \underbrace{\frac{1}{n}}_{=a_i} Y_i \label{eq:bluemean}
\end{equation}

Clearly, each observation is weighted by

\[a_i = \frac{1}{n}.\]

and we also know that
\(\text{Var}(\hat{\beta}_1)=\text{Var}(\hat\beta_1)=\frac{\sigma^2}{n}\).

We will now use \texttt{R} to conduct a simulation study that
demonstrates what happens to the variance of \eqref{eq:bluemean} if
different weights \[ w_i = \frac{1 \pm \epsilon}{n} \] are assigned to
either half of the sample \(Y_1, \dots, Y_n\) instead of using
\(\frac{1}{n}\), the weights implied by OLS.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set sample size and number of repititions}
\NormalTok{n <-}\StringTok{ }\DecValTok{100}      
\NormalTok{reps <-}\StringTok{ }\FloatTok{1e5}

\CommentTok{# Choose epsilon and create a vector of weights as defined above}
\NormalTok{epsilon <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{w <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{((}\DecValTok{1}\OperatorTok{+}\NormalTok{epsilon)}\OperatorTok{/}\NormalTok{n, n}\OperatorTok{/}\DecValTok{2}\NormalTok{), }
       \KeywordTok{rep}\NormalTok{((}\DecValTok{1}\OperatorTok{-}\NormalTok{epsilon)}\OperatorTok{/}\NormalTok{n, n}\OperatorTok{/}\DecValTok{2}\NormalTok{) }
\NormalTok{      )}

\CommentTok{# Draw a random sample y_1,...,y_n from the standard normal distribution, }
\CommentTok{# use both estimators 1e5 times and store the result in the vectors 'ols' and }
\CommentTok{# 'weightedestimator'}

\NormalTok{ols <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, reps)}
\NormalTok{weightedestimator <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, reps)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{reps) \{}
\NormalTok{  y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{  ols[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(y)}
\NormalTok{  weightedestimator[i] <-}\StringTok{ }\KeywordTok{crossprod}\NormalTok{(w, y)}
\NormalTok{\}}

\CommentTok{# Plot kernel density estimates of the estimators' distributions }

\CommentTok{# OLS}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(ols), }
     \DataTypeTok{col =} \StringTok{"purple"}\NormalTok{, }
     \DataTypeTok{lwd =} \DecValTok{3}\NormalTok{, }
     \DataTypeTok{main =} \StringTok{"Density of OLS and Weighted Estimator"}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Estimates"}
\NormalTok{     )}

\CommentTok{# Weighted}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(weightedestimator), }
      \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{3}
\NormalTok{      ) }

\CommentTok{# Add a dashed line at 0 and add a legend to the plot}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }
       \KeywordTok{c}\NormalTok{(}\StringTok{"OLS"}\NormalTok{, }\StringTok{"Weighted"}\NormalTok{), }
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"purple"}\NormalTok{, }\StringTok{"steelblue"}\NormalTok{), }
       \DataTypeTok{lwd =} \DecValTok{3}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-174-1} \end{center}

What conclusion can we draw from the result?

\begin{itemize}
\tightlist
\item
  Both estimators seem to be unbiased: the means of their estimated
  distributions are zero.
\item
  The estimator using weights that deviate from those implied by OLS is
  less efficient than the OLS estimator: there is higher dispersion when
  weights are \(w_i = \frac{1 \pm 0.8}{100}\) instead of
  \(w_i=\frac{1}{100}\) as required by the OLS solution.
\end{itemize}

Hence, our simulation results confirm what is stated by the Gauss-Markov
Theorem.

\section{Using the t-Statistic in Regression When the Sample Size Is
Small}\label{using-the-t-statistic-in-regression-when-the-sample-size-is-small}

The three OLS assumptions discussed in \protect\hyperlink{lrwor}{Chapter
4} (see Key Concept 4.3) are the foundation for the results on the large
sample distribution of the OLS estimators in the simple regression
model. What can be said about the distribution of the estimators and
their \(t\)-statistics when the sample size is small and the population
distribution of the data is unknown? Provided that the three least
squares assumptions hold and the errors are normally distributed and
homoskedastic (we refer to these conditions as the homoskedastic normal
regression assumptions), we have normally distributed estimators and
\(t\)-distributed test statistics in small samples.

Recall the \protect\hyperlink{thetdist}{definition} on a
\(t\)-distributed variable

\[ \frac{Z}{\sqrt{W/m}} \sim t_m\]

where \(Z\) is a standard normal random variable, \(W\) is \(\chi^2\)
distributed with \(m\) degrees of freedom and \(Z\) and \(W\) are
independent. See section 5.6 in the book for a more detailed discussion
of the small sample distribution of \(t\)-statistics in regression.

Let us simulate the distribution of regression \(t\)-statistics based on
a large number of small random samples when the sample size is small,
say \(n=20\), and compare their simulated distributions to their
theoretical distribution which should be \(t_{18}\), the
\(t\)-distribution with \(18\) degrees of freedom (recall that
\(\text{DF}=n-k-1\)).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# initialize vectors}
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}

\CommentTok{# loop sampling / estimation / t statistics}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{) \{}

\NormalTok{  X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{  Y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{20}\NormalTok{, }\DataTypeTok{mean =}\NormalTok{ X)}
\NormalTok{  reg <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(Y }\OperatorTok{~}\StringTok{ }\NormalTok{X))}
\NormalTok{  beta_}\DecValTok{0}\NormalTok{[i] <-}\StringTok{ }\NormalTok{(reg}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{0}\NormalTok{)}\OperatorTok{/}\NormalTok{(reg}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{])}
\NormalTok{  beta_}\DecValTok{1}\NormalTok{[i] <-}\StringTok{ }\NormalTok{(reg}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{(reg}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{])}

\NormalTok{\}}

\CommentTok{# plot the distributions and compare with t_18 density function:}

\CommentTok{# divide plotting area}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{# plot the simulated density of beta_0}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(beta_}\DecValTok{0}\NormalTok{), }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{ , }
     \DataTypeTok{main =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{widehat}\NormalTok{(beta)[}\DecValTok{0}\NormalTok{]), }
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{    )}

\CommentTok{# add the t_18 density to the plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{18}\NormalTok{), }
      \DataTypeTok{add =}\NormalTok{ T, }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lty =} \DecValTok{2}
\NormalTok{      )}

\CommentTok{# plot the simulated density of beta_1}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(beta_}\DecValTok{1}\NormalTok{), }
     \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
     \DataTypeTok{main =} \KeywordTok{expression}\NormalTok{(}\KeywordTok{widehat}\NormalTok{(beta)[}\DecValTok{1}\NormalTok{]), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{ ,}\DecValTok{4}\NormalTok{)}
\NormalTok{     )}

\CommentTok{# add the t_18 density to the plot}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dt}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{18}\NormalTok{), }
      \DataTypeTok{add =}\NormalTok{ T, }
      \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
      \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \DataTypeTok{lty =} \DecValTok{2}
\NormalTok{      ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-175-1} \end{center}

The outcomes are consistent with our expectations: the empirical
distributions of both estimators seem to track the theoretical
\(t_{18}\) distribution quite closely.

\section{Exercises}\label{exercises-3}

\begin{center}\textit{This interactive part of URFITE is only available in the HTML version.}\end{center}

\chapter{Regression with Panel Data}\label{regression-with-panel-data}

Regression using panel data may mititgate omitted variable bias when
there is no information on variables available that correlate with both
the regressors of interest and the independent variable and if these
variables are constant in the time dimension or across entities.
Provided that panel data is availabe, panel regression methods may
improve upon multiple regression models which, as discussed in Chapter
9, produce results that are not internally valid in such a setting.

This chapter covers the following topics:

\begin{itemize}
\tightlist
\item
  Notation for panel data
\item
  Fixed effects regression using time and/or entity fixed effects
\item
  Computation of standard errors in fixed effects regression models
\end{itemize}

Following the book, for applications we make use of the data set
\texttt{Fatalities} from the \texttt{AER} package which is a panel data
set reporting annually state level observations on U.S. traffic
fatalities for the period 1982 through 1988. The applications are
concered with the question if there are effects of alcohol taxes and
drunk driving laws on road fatalities and, if present, \emph{how strong}
these effects are.

For this purpose we will introduce a convenient \texttt{R} function that
enables us to estimate linear panel regression models, the function
\texttt{plm()} which comes with the package \texttt{plm}. Usage of
\texttt{plm()} is very similar as for the \texttt{lm()} function which
we have used throughout the previous chapters for estimation of simple
and multiple regression models.

\section{Panel Data}\label{panel-data}

Key Concept 10.1

Notation for Panel Data

In contrast to cross-section data where we have observations on \(n\)
subjects (entities), panel data has observations on \(n\) entities at
\(T\geq2\) time periods. This is denoted

\[(X_{it},Y_{it}), \ i=1,\dots,n \ \ \ \text{and} \ \ \ t=1,\dots,T \]
where the index \(i\) refers to the entity beeing observed while \(t\)
refers to the time period.

Sometimes panel data is also called longitudinal data as it adds a
temporal dimension to cross-sectional data. Let us have a glimpse at the
data set \texttt{Fatalities} by checking its structure and listing of
the first few observations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load package and data}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(Fatalities)}

\CommentTok{# obtain dimension and inspect structure}
\KeywordTok{is.data.frame}\NormalTok{(Fatalities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(Fatalities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 336  34
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(Fatalities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    336 obs. of  34 variables:
##  $ state       : Factor w/ 48 levels "al","az","ar",..: 1 1 1 1 1 1 1 2 2 2 ...
##  $ year        : Factor w/ 7 levels "1982","1983",..: 1 2 3 4 5 6 7 1 2 3 ...
##  $ spirits     : num  1.37 1.36 1.32 1.28 1.23 ...
##  $ unemp       : num  14.4 13.7 11.1 8.9 9.8 ...
##  $ income      : num  10544 10733 11109 11333 11662 ...
##  $ emppop      : num  50.7 52.1 54.2 55.3 56.5 ...
##  $ beertax     : num  1.54 1.79 1.71 1.65 1.61 ...
##  $ baptist     : num  30.4 30.3 30.3 30.3 30.3 ...
##  $ mormon      : num  0.328 0.343 0.359 0.376 0.393 ...
##  $ drinkage    : num  19 19 19 19.7 21 ...
##  $ dry         : num  25 23 24 23.6 23.5 ...
##  $ youngdrivers: num  0.212 0.211 0.211 0.211 0.213 ...
##  $ miles       : num  7234 7836 8263 8727 8953 ...
##  $ breath      : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 1 1 1 ...
##  $ jail        : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 2 2 2 ...
##  $ service     : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 2 2 2 ...
##  $ fatal       : int  839 930 932 882 1081 1110 1023 724 675 869 ...
##  $ nfatal      : int  146 154 165 146 172 181 139 131 112 149 ...
##  $ sfatal      : int  99 98 94 98 119 114 89 76 60 81 ...
##  $ fatal1517   : int  53 71 49 66 82 94 66 40 40 51 ...
##  $ nfatal1517  : int  9 8 7 9 10 11 8 7 7 8 ...
##  $ fatal1820   : int  99 108 103 100 120 127 105 81 83 118 ...
##  $ nfatal1820  : int  34 26 25 23 23 31 24 16 19 34 ...
##  $ fatal2124   : int  120 124 118 114 119 138 123 96 80 123 ...
##  $ nfatal2124  : int  32 35 34 45 29 30 25 36 17 33 ...
##  $ afatal      : num  309 342 305 277 361 ...
##  $ pop         : num  3942002 3960008 3988992 4021008 4049994 ...
##  $ pop1517     : num  209000 202000 197000 195000 204000 ...
##  $ pop1820     : num  221553 219125 216724 214349 212000 ...
##  $ pop2124     : num  290000 290000 288000 284000 263000 ...
##  $ milestot    : num  28516 31032 32961 35091 36259 ...
##  $ unempus     : num  9.7 9.6 7.5 7.2 7 ...
##  $ emppopus    : num  57.8 57.9 59.5 60.1 60.7 ...
##  $ gsp         : num  -0.0221 0.0466 0.0628 0.0275 0.0321 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list first few observations}
\KeywordTok{head}\NormalTok{(Fatalities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   state year spirits unemp   income   emppop  beertax baptist  mormon
## 1    al 1982    1.37  14.4 10544.15 50.69204 1.539379 30.3557 0.32829
## 2    al 1983    1.36  13.7 10732.80 52.14703 1.788991 30.3336 0.34341
## 3    al 1984    1.32  11.1 11108.79 54.16809 1.714286 30.3115 0.35924
## 4    al 1985    1.28   8.9 11332.63 55.27114 1.652542 30.2895 0.37579
## 5    al 1986    1.23   9.8 11661.51 56.51450 1.609907 30.2674 0.39311
## 6    al 1987    1.18   7.8 11944.00 57.50988 1.560000 30.2453 0.41123
##   drinkage     dry youngdrivers    miles breath jail service fatal nfatal
## 1    19.00 25.0063     0.211572 7233.887     no   no      no   839    146
## 2    19.00 22.9942     0.210768 7836.348     no   no      no   930    154
## 3    19.00 24.0426     0.211484 8262.990     no   no      no   932    165
## 4    19.67 23.6339     0.211140 8726.917     no   no      no   882    146
## 5    21.00 23.4647     0.213400 8952.854     no   no      no  1081    172
## 6    21.00 23.7924     0.215527 9166.302     no   no      no  1110    181
##   sfatal fatal1517 nfatal1517 fatal1820 nfatal1820 fatal2124 nfatal2124
## 1     99        53          9        99         34       120         32
## 2     98        71          8       108         26       124         35
## 3     94        49          7       103         25       118         34
## 4     98        66          9       100         23       114         45
## 5    119        82         10       120         23       119         29
## 6    114        94         11       127         31       138         30
##    afatal     pop  pop1517  pop1820  pop2124 milestot unempus emppopus
## 1 309.438 3942002 208999.6 221553.4 290000.1    28516     9.7     57.8
## 2 341.834 3960008 202000.1 219125.5 290000.2    31032     9.6     57.9
## 3 304.872 3988992 197000.0 216724.1 288000.2    32961     7.5     59.5
## 4 276.742 4021008 194999.7 214349.0 284000.3    35091     7.2     60.1
## 5 360.716 4049994 203999.9 212000.0 263000.3    36259     7.0     60.7
## 6 368.421 4082999 204999.8 208998.5 258999.8    37426     6.2     61.5
##           gsp
## 1 -0.02212476
## 2  0.04655825
## 3  0.06279784
## 4  0.02748997
## 5  0.03214295
## 6  0.04897637
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# summarize variables 'state' and 'year'}
\KeywordTok{summary}\NormalTok{(Fatalities[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      state       year   
##  al     :  7   1982:48  
##  az     :  7   1983:48  
##  ar     :  7   1984:48  
##  ca     :  7   1985:48  
##  co     :  7   1986:48  
##  ct     :  7   1987:48  
##  (Other):294   1988:48
\end{verbatim}

We find that the data set consists of 336 observations on 34 variables.
Notice that the variable \texttt{state} is a factor variable with 48
levels (one for each of the 48 contigous US states). The varaible
\texttt{year} is also a factor variable that has 7 levels identifying
the time period when the observation was made which gives us
\(7\times48 = 336\) observations in total. Since all variables are
observed for all entities and over all time periods we say the the panel
is \textbf{balanced}. If there were missing data for at least one
entities in at least one time period we would call the panel
\textbf{unbalanced}.

\subsubsection*{Example: Traffic Deaths and Alcohol
Taxes}\label{example-traffic-deaths-and-alcohol-taxes}
\addcontentsline{toc}{subsubsection}{Example: Traffic Deaths and Alcohol
Taxes}

Coming to the question of how the alcohol taxes and traffic fatalities
are related, we start by reproducing Figure 10.1 of the book. For this
we estimate simple regressions using data for years 1982 and 1988 that
model the relationship between beer tax (adjusted for 1988 dollars) and
the traffic fatility rate. Beforehand we define the latter as the number
of fatalitites per 10000 inhabitants and choose subsets of the
observations made for years 1982 and 1988. Afterwards, we plot both
subsets and add the corresponding estimated regression functions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define fatality rate}
\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{fatal_rate <-}\StringTok{ }\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{fatal }\OperatorTok{/}\StringTok{ }\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{pop }\OperatorTok{*}\StringTok{ }\DecValTok{10000}

\CommentTok{# subset data}
\NormalTok{Fatalities1982 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(Fatalities, year }\OperatorTok{==}\StringTok{ "1982"}\NormalTok{)}
\NormalTok{Fatalities1988 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(Fatalities, year }\OperatorTok{==}\StringTok{ "1988"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate simple regression models using 1982 and 1988 data}
\KeywordTok{library}\NormalTok{(lmtest)}
\KeywordTok{library}\NormalTok{(sandwich)}

\NormalTok{fatal1982_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }\DataTypeTok{data =}\NormalTok{ Fatalities1982)}
\NormalTok{fatal1988_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }\DataTypeTok{data =}\NormalTok{ Fatalities1988)}

\KeywordTok{coeftest}\NormalTok{(fatal1982_mod, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatal1982_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  2.01038    0.14957 13.4408   <2e-16 ***
## beertax      0.14846    0.13261  1.1196   0.2687    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coeftest}\NormalTok{(fatal1988_mod, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatal1988_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(>|t|)    
## (Intercept)  1.85907    0.11461 16.2205 < 2.2e-16 ***
## beertax      0.43875    0.12786  3.4314  0.001279 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The estimated regression functions are:

\begin{align*}
  \widehat{FatalityRate} =& \, \underset{(0.15)}{2.01} + \underset{(0.13)}{0.15} \times BeerTax \quad (1982 \text{ data}), \\
  \widehat{FatalityRate} =& \, \underset{(0.11)}{1.86} + \underset{(0.13)}{0.44} \times BeerTax \quad (1988 \text{ data})
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot observations of interest and add estimated regression line for 1982 data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Fatalities1982}\OperatorTok{$}\NormalTok{beertax, }
     \DataTypeTok{y =}\NormalTok{ Fatalities1982}\OperatorTok{$}\NormalTok{fatal_rate, }
     \DataTypeTok{xlab =} \StringTok{"Beer tax (in 1988 dollars)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Fatality rate (fatalities per 10000)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Traffic Fatality Rates and Beer Taxes in 1982"}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{4.5}\NormalTok{),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}

\KeywordTok{abline}\NormalTok{(fatal1982_mod, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-194-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot observations of interest and add estimated regression line for 1988 data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Fatalities1988}\OperatorTok{$}\NormalTok{beertax, }
     \DataTypeTok{y =}\NormalTok{ Fatalities1988}\OperatorTok{$}\NormalTok{fatal_rate, }
     \DataTypeTok{xlab =} \StringTok{"Beer tax (in 1988 dollars)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Fatality rate (fatalities per 10000)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Traffic Fatality Rates and Beer Taxes in 1988"}\NormalTok{,}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{4.5}\NormalTok{),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}

\KeywordTok{abline}\NormalTok{(fatal1988_mod, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{URFITE_files/figure-latex/unnamed-chunk-194-2.pdf}

In both plots, each point represents observations of beer tax and
fatality rate for a given state in the respective year. The regression
results indicate a postive relationship between the beer tax and the
fatality rate for both years, wherby the estimated coefficient on beer
tax for the 1988 data is almost three times as large as for the 1988
data set. This is contrary to our expectations: alcohol taxes are
supposed to \emph{lower} the rate of traffic fatalities. As known from
Chapter 6, this is possibly due to omitted variable bias, since both
models do not include any covariates, e.g.~economic conditions. This
could be corrected for using a multiple regression approach. However,
both models cannot account for omitted \emph{unobservable} factors that
differ from state to state but can be assumed to be constant over the
observation span, e.g.~the populations attitude towas drunk driving. As
shown in the next section, panel data allow us to hold such factors
constant.

\section{\texorpdfstring{Panel Data with Two Time Periods: ``Before and
After''
Comparisons}{Panel Data with Two Time Periods: Before and After Comparisons}}\label{PDWTTP}

Suppose there are only \(T=2\) time periods \(t=1982,1988\). This allows
us to analyze differences in changes of the the fatality rate from year
1982 to 1988. We start by considering the population regression model
\[FatalityRate_{it} = \beta_0 + \beta_1 BeerTax_{it} + \beta_2 Z_{i} + u_{it}\]
where the \(Z_i\) are state specific characteristics that differ between
states but are \emph{constant over time}. For \(t=1982\) and \(t=1988\)
we have

\begin{align*}
  FatalityRate_{i1982} =&\, \beta_0 + \beta_1 BeerTax_{i1982} + \beta_2 Z_i + u_{i1982}, \\
  FatalityRate_{i1988} =&\, \beta_0 + \beta_1 BeerTax_{i1988} + \beta_2 Z_i + u_{i1988}.
\end{align*}

We can eliminate the \(Z_i\) by regressing the difference in the
fatality rate between 1988 and 1982 on the difference in beer tax
between those years:
\[FatalityRate_{i1988} - FatalityRate_{i1982} = \beta_1 (BeerTax_{i1988} - BeerTax_{i1982}) + u_{i1988} - u_{i1982}\]
Using this regression model we can obtain an estimate for \(\beta_1\)
without worrying about a possible bias due to omission of the \(Z_i\)
since these influences are eliminated from the model. Next we use use
\texttt{R} to estimate a regression based on the differenced data and
plot the estimated regression function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# differences }
\NormalTok{diff_fatal_rate <-}\StringTok{ }\NormalTok{Fatalities1988}\OperatorTok{$}\NormalTok{fatal_rate }\OperatorTok{-}\StringTok{ }\NormalTok{Fatalities1982}\OperatorTok{$}\NormalTok{fatal_rate}
\NormalTok{diff_beertax <-}\StringTok{ }\NormalTok{Fatalities1988}\OperatorTok{$}\NormalTok{beertax }\OperatorTok{-}\StringTok{ }\NormalTok{Fatalities1982}\OperatorTok{$}\NormalTok{beertax}

\CommentTok{# estimate regression on differenced data}
\NormalTok{fatal_diff_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(diff_fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{diff_beertax)}

\KeywordTok{coeftest}\NormalTok{(fatal_diff_mod, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fatal_diff_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##               Estimate Std. Error t value Pr(>|t|)   
## (Intercept)  -0.072037   0.065355 -1.1022 0.276091   
## diff_beertax -1.040973   0.355006 -2.9323 0.005229 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Note that including the intercept allows for a change in the mean
fatality rate in the time between 1982 and 1988 in the absence of a
change in the beer tax.

We obtain the OLS estimated regression function
\[\widehat{FatalityRate_{i1988} - FatalityRate_{i1982}} = \underset{(0.065)}{-0.072} \underset{(0.36)}{-1.04} \times (BeerTax_{i1988}-BeerTax_{i1982}).\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot differenced data}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ diff_beertax, }
     \DataTypeTok{y =}\NormalTok{ diff_fatal_rate, }
     \DataTypeTok{xlab =} \StringTok{"Change in beer tax (in 1988 dollars)"}\NormalTok{,}
     \DataTypeTok{ylab =} \StringTok{"Change in fatality rate (fatalities per 10000)"}\NormalTok{,}
     \DataTypeTok{main =} \StringTok{"Changes in Traffic Fatality Rates and Beer Taxes in 1982-1988"}\NormalTok{,}
     \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.6}\NormalTok{),}
     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.5}\NormalTok{, }\DecValTok{1}\NormalTok{),}
     \DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"steelblue"}\NormalTok{)}

\CommentTok{# add regression line to plot}
\KeywordTok{abline}\NormalTok{(fatal_diff_mod, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{URFITE_files/figure-latex/unnamed-chunk-196-1} \end{center}

We observe that the estimated coefficient on beer tax is now negative
and significantly different from zero at the level of \(5\%\). Its
interpretation is that raising the beer tax by \(\$1\) causes trafic
fatalities to decrease by \(1.04\) per \(10000\) people. This is rather
large as the average fatality rate is approximately \(2\) persons per
\(10000\) people.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute mean fatality rate over all states for all time periods}
\KeywordTok{mean}\NormalTok{(Fatalities}\OperatorTok{$}\NormalTok{fatal_rate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.040444
\end{verbatim}

Once more this outcome is likely to be a consequence of omitting factors
that influence the fatality rate and are correlated with the beer tax
\emph{and} change over time. The message is that we need to be more
careful and control for such factors before drawing conclusion about the
effect of a raise in beer taxes.

Furthermore, note that the approach presented in this section discards
information for years \(1983\) to \(1987\). A method that allows to use
data for more than \(T=2\) time periods and allows us to add control
variables is the fixed effects regression approach.

\section{Fixed Effects Regression}\label{fixed-effects-regression}

Consider the panel regression model

\[Y_{it} = \beta_0 + \beta_1 X_{it} + \beta_2 Z_i +  u_{it}\]

where the \(Z_i\) are unobserved time-invariant heterogeneities across
the entities \(i=1,\dots,n\). We are interested in estimating
\(\beta_1\), the effect on \(Y_i\) of a change in \(X_i\) holding
constant \(Z_i\). Letting \(\alpha_i = \beta_0 + \beta_2 Z_i\) we obtain
the model

\begin{align}
Y_{it} = \alpha_i + \beta_1 X_{it} + u_{it} \label{eq:femodel}.
\end{align}

Having invidual specific intercepts \(\alpha_i\), \(i=1,\dots,n\), where
each of these can be understood as the fixed effect of entity \(i\),
this model is called the \emph{fixed effects regression model}. The
variation in the \(\alpha_i\), \(i=1,\dots,n\) comes from the \(Z_i\).
\eqref{eq:femodel} can be rewritten as a regression model containing
\(n-1\) dummy regressors and a constant:

\begin{align}
Y_{it} = \beta_i + \beta_1 X_{it} + \gamma_2 D2_i + \gamma_3 D3_i + \cdots + \gamma_n Dn_i + u_{it} \label{eq:drmodel}.
\end{align}

Model \eqref{eq:drmodel} has \(n\) different intercepts --- one for every
entity. Both \eqref{eq:femodel} and \eqref{eq:drmodel} are equivalnt
representations of the fixed effects regression model.

The fixed effects regression model can be generalized to contain more
than just one determinant of \(Y\) that is correlated with \(X\) and
change over time. Key Concept 10.2 presents the generalized fixed
effects regression model.

Key Concept 10.2

The Fixed Effects Regression Model

The fixed effects regression model is

\begin{align}
Y_{it} = \beta_1 X_{1,it} + \cdots + \beta_k X_{k,it} + \alpha_i + u_{it} \label{eq:gfemodel}
\end{align}

with \(i=1,\dots,n\) and \(t=1,\dots,T\). The \(\alpha_i\) are
entity-specific intercepts that capture heterogeneities across entities.
An equivalent representation of this model is given by

\begin{align}
Y_{it} = \beta_0 + \beta_1 X_{1,it} + \cdots + \beta_k X_{k,it} + \gamma_2 D2_i + \gamma_3 D3_i + \cdots + \gamma_n Dn_i  + u_{it} \label{eq:gdrmodel}
\end{align}

where the \(D2_i,D3_i,\dots,Dn_i\) are dummy variables.

\subsection*{Estimation and Inference}\label{estimation-and-inference}
\addcontentsline{toc}{subsection}{Estimation and Inference}

Software packages use a so-called ``entity-demeaned'' OLS algorithm that
is computationally more efficient than estimating regression models with
\(k+n\) regressors as needed for models \eqref{eq:gfemodel} and
\eqref{eq:gdrmodel}.

Taking averages on both sides of \eqref{eq:femodel} we obtain

\begin{align*}
\frac{1}{n} \sum_{i=1}^n Y_{it} =& \, \beta_1 \frac{1}{n} \sum_{i=1}^n X_{it} + \frac{1}{n} \sum_{i=1}^n a_i + \frac{1}{n} \sum_{i=1}^n u_{it} \\
\overline{Y} =& \, \beta_1 \overline{X}_i + \alpha_i + \overline{u}_i. 
\end{align*}

Substraction from \eqref{eq:femodel} gives

\begin{align}
Y_{it} - \overline{Y}_i =& \, \beta_1(X_{it}-\overline{X}_i) + (u_{it} - \overline{u}_i) \\
\overset{\sim}{Y}_{it} =& \, \beta_1 \overset{\sim}{X}_{it} + \overset{\sim}{u}_{it}. \label{eq:edols}
\end{align}

In this model, the OLS estimate of the parameter of interest \(\beta_1\)
is equal to the estimate obtained using \eqref{eq:drmodel} --- without the
need to estimate \(n-1\) dummies and an intercept.

We conclude that there are two ways of estimating \(\beta_1\) in the
fixed effects regression:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  OLS regression of the dummy regression model \eqref{eq:drmodel}
\item
  OLS regression using the entity demeaned data \eqref{eq:edols}
\end{enumerate}

Provided the fixed effects regression assumptions stated in Key Concept
10.3 hold, the sampling distribution of the OLS estimator in the fixed
effects regression model has a normal distribution in large samples. The
variance of the estimates can be estimated and we can compute standard
errors, \(t\)-statistics and confidence intervals for model
coefficients. In the next section, we will see how to estimate a fixed
effects model using \texttt{R} and how to obtain a model summary that
makes use of heteroskedasticity robust standard errors. Thereby we leave
aside complicated formulas of the estimators. See Chapter 10.5 and
Appendix 10.2 of the book for a discussion of the theoretical aspects.

\subsection*{Application to Traffic
Deaths}\label{application-to-traffic-deaths}
\addcontentsline{toc}{subsection}{Application to Traffic Deaths}

Following Key Concept 10.2, the simple fixed effects model for
estimation of the relation between traffic fatality rates and the beer
taxes is

\begin{align}
\widehat{FatalityRate}_{it} = \beta_1 BeerTax_{it} + StateFixedEffects + u_{it}, \label{eq:fatsemod}
\end{align}

the regression of the traffic fatalaty rate on beer tax and 48 binary
regressor --- one for each state.

We can simply use the function \texttt{lm()} to obtain an estimate of
\(\beta_1\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fatal_fe_lm_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Fatalities)}
\NormalTok{fatal_fe_lm_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = fatal_rate ~ beertax + state - 1, data = Fatalities)
## 
## Coefficients:
## beertax  stateal  stateaz  statear  stateca  stateco  statect  statede  
## -0.6559   3.4776   2.9099   2.8227   1.9682   1.9933   1.6154   2.1700  
## statefl  statega  stateid  stateil  statein  stateia  stateks  stateky  
##  3.2095   4.0022   2.8086   1.5160   2.0161   1.9337   2.2544   2.2601  
## statela  stateme  statemd  statema  statemi  statemn  statems  statemo  
##  2.6305   2.3697   1.7712   1.3679   1.9931   1.5804   3.4486   2.1814  
## statemt  statene  statenv  statenh  statenj  statenm  stateny  statenc  
##  3.1172   1.9555   2.8769   2.2232   1.3719   3.9040   1.2910   3.1872  
## statend  stateoh  stateok  stateor  statepa  stateri  statesc  statesd  
##  1.8542   1.8032   2.9326   2.3096   1.7102   1.2126   4.0348   2.4739  
## statetn  statetx  stateut  statevt  stateva  statewa  statewv  statewi  
##  2.6020   2.5602   2.3137   2.5116   2.1874   1.8181   2.5809   1.7184  
## statewy  
##  3.2491
\end{verbatim}

As discussed in the previous section, it is also possible to estimate
\(\beta_1\) by applying OLS to the demeaned data,

\[\overset{\sim}{FatalityRate} = \beta_1 \overset{\sim}{BeerTax}_{it} + u_{it}. \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# demeaned data}
\NormalTok{Fatalities_demeaned <-}\StringTok{ }\KeywordTok{with}\NormalTok{(Fatalities,}
            \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{fatal_rate =}\NormalTok{ fatal_rate }\OperatorTok{-}\StringTok{ }\KeywordTok{ave}\NormalTok{(fatal_rate, state),}
            \DataTypeTok{beertaxs =}\NormalTok{ beertax }\OperatorTok{-}\StringTok{ }\KeywordTok{ave}\NormalTok{(beertax, state)))}

\CommentTok{# estimate regression}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Fatalities_demeaned))}
\end{Highlighting}
\end{Shaded}

Equivalently it is possible to use \texttt{plm()} from the package with
the same name.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install and load the 'plm' package}
\NormalTok{## install.packages("plm")}
\KeywordTok{library}\NormalTok{(plm)}
\end{Highlighting}
\end{Shaded}

As for \texttt{lm()} we have to specify the regression formula and the
data to be used in our call of \texttt{plm()}. Additionally, it is
required to pass a vector of names of entity and time id variables to
the \texttt{index} argument. For \texttt{Fatalities}, the id variable
for entities is named \texttt{state} and the time id variable is
\texttt{year}. Since the fixed effects estimator is also called the
\emph{within} estimator, we set \texttt{model\ =\ "within"}. Finally,
the \texttt{coeftest()} function allows to obtain significance based on
robust standard errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate the fixed effects regression with plm()}
\NormalTok{fatal_fe_mod <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }
                    \DataTypeTok{data =}\NormalTok{ Fatalities,}
                    \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }
                    \DataTypeTok{model =} \StringTok{"within"}\NormalTok{)}

\CommentTok{# summary using robust standard errors}
\KeywordTok{coeftest}\NormalTok{(fatal_fe_mod, }\DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatal_fe_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##         Estimate Std. Error t value Pr(>|t|)  
## beertax -0.65587    0.28880  -2.271  0.02388 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Agian, we find that the estimated coefficient is \(-0.6559\). Notice
that \texttt{plm()} uses the entity-demeaned OLS algorithm and does not
report dummy coefficients. The estimated regression function is

\begin{align}
\widehat{FatalityRate} = \underset{(0.29)}{-0.66} \times BeerTax + StateFixedEffects. \label{eq:efemod}
\end{align}

We conclude that the coefficient on \(BeerTax\) is nagative and highly
significant. The interpretation is that the estimated reduction in
traffic fatalities due to a \(\$1\) increase in the real beer tax is
\(0.66\) per \(10000\) people which is still pretty high. Though
including state fixed effects eliminates the risk of a bias due to
omitted factors that vary across states but not over time, we suspect
that there are other omitted varibales that vary over time and thus
cause a bias.

\section{Regression with Time Fixed
Effects}\label{regression-with-time-fixed-effects}

Controlling for variables that are constant across entities but vary
over time can be done by including time fixed effects. If there are
\emph{only} time fixed effects, the fixed effects regression model
becomes
\[Y_{it} = \beta_0 + \beta_1 X_{it} + \delta_2 B2_t + \cdots + \delta_T BT_t + u_{it},\]
where only \(T-1\) dummies are included (\(B1\) is omitted) since the
model includes an intercept. This model eliminates omitted variables
bias caused by exluding unobserved varaibles that evolve over time but
are constant across entities only.

In some applications it is meaningful to include entity and time fixed
effects. The \textbf{entity and time fixed effects} regression model is
\[Y_{it} = \beta_0 + \beta_1 X_{it} + \gamma_2 D2_i + \cdots + \gamma_n DT_i + \delta_2 B2_t + \cdots + \delta_T BT_t + u_{it} .\]
The combined model allows to eliminate bias from unobservables that
change over time but are constant over entities and it controls for
factors that differ across entities but are constant over time. Such
models can be estimated using the OLS algorithm that is implemented in
\texttt{R}.

The following code chunk shows how to estimate the combined entity and
time fixed effects model of the relation between fatalities and beer
tax,
\[FatalityRate_{it} = \beta_1 BeerTax_{it} + StateEffects + TimeFixedEffects + u_{it}\]
using both, \texttt{lm()} and \texttt{plm()}. It is straightforward to
estimate this regression with \texttt{lm()} since it is just an
extension of \eqref{eq:fatsemod} so we only have to adjust the
\texttt{formula} argument by adding the additional regressor
\texttt{year} for time fixed effects. In our call of \texttt{plm()} we
set another argument \texttt{effect\ =\ "twoways"} for inclusion of
entity \emph{and} time dummies.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Estimate combined time and entity fixed effects regression model}

\CommentTok{# via lm()}
\NormalTok{fatal_tefe_lm_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Fatalities)}
\NormalTok{fatal_tefe_lm_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = fatal_rate ~ beertax + state + year - 1, data = Fatalities)
## 
## Coefficients:
##  beertax   stateal   stateaz   statear   stateca   stateco   statect  
## -0.63998   3.51137   2.96451   2.87284   2.02618   2.04984   1.67125  
##  statede   statefl   statega   stateid   stateil   statein   stateia  
##  2.22711   3.25132   4.02300   2.86242   1.57287   2.07123   1.98709  
##  stateks   stateky   statela   stateme   statemd   statema   statemi  
##  2.30707   2.31659   2.67772   2.41713   1.82731   1.42335   2.04488  
##  statemn   statems   statemo   statemt   statene   statenv   statenh  
##  1.63488   3.49146   2.23598   3.17160   2.00846   2.93322   2.27245  
##  statenj   statenm   stateny   statenc   statend   stateoh   stateok  
##  1.43016   3.95748   1.34849   3.22630   1.90762   1.85664   2.97776  
##  stateor   statepa   stateri   statesc   statesd   statetn   statetx  
##  2.36597   1.76563   1.26964   4.06496   2.52317   2.65670   2.61282  
##  stateut   statevt   stateva   statewa   statewv   statewi   statewy  
##  2.36165   2.56100   2.23618   1.87424   2.63364   1.77545   3.30791  
## year1983  year1984  year1985  year1986  year1987  year1988  
## -0.07990  -0.07242  -0.12398  -0.03786  -0.05090  -0.05180
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# via plm()}
\NormalTok{fatal_tefe_mod <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }
                      \DataTypeTok{data =}\NormalTok{ Fatalities,}
                      \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }
                      \DataTypeTok{model =} \StringTok{"within"}\NormalTok{, }
                      \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{)}

\KeywordTok{coeftest}\NormalTok{(fatal_tefe_mod, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fatal_tefe_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##         Estimate Std. Error t value Pr(>|t|)  
## beertax -0.63998    0.35015 -1.8277  0.06865 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Notice that since we exclude the intercept, \texttt{lm()} does estimate
coefficients for \((n-1) + (T-1) = 47 + 6 = 53\) binary variables!
Again, \texttt{plm()} does only report the estimated coefficient on
\(BeerTax\).

The estimated regression function is

\begin{align}
\widehat{FatalityRate} =  \underset{(0.35)}{-0.64} \times BeerTax + StateEffects + TimeFixedEffects. \label{eq:cbnfemod}
\end{align}

The value of \(-0.66\), the result is close to the estimated coefficient
for the regression model including only entity fixed effects.
Unsurprisingly, the coefficient is less precisely estimated as before
but significantly different from zero at the level of \(10\%\).

In view of \eqref{eq:efemod} and \eqref{eq:cbnfemod} we conclude that the
estimated relationship between traffic fatalities and the real beer tax
is not affected by omitted variable bias due to factors that are
constant over time.

\section{The Fixed Effects Regression Assumptions and Standard Errors
for Fixed Effects
Regression}\label{the-fixed-effects-regression-assumptions-and-standard-errors-for-fixed-effects-regression}

This section focusses on the entity fixed effects regression model and
presents model assumptions that need to hold in order for OLS to produce
unbiased estimates that are normally distributed for large \(n\). These
assumptions are an extension of the assumptions made for the multiple
regression model (see Key Concept 6.4) and are given in Key Concept
10.3. We will also briefly discuss standard errors in fixed effects
regression models which differ from standard errors in multiple
regression as the regression error can exhibit serial correlation in
panel models.

Key Concept 10.3

The Fixed Effects Regression Assumptions

In the fixed effects regression model
\[ Y_{it} = \beta_1 X_{it} + \alpha_i + u_{it} \ \ , \ \ i=1,\dots,n, \ t=1,\dots,T, \]
we assume the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The error term \(u_{it}\) has conditional mean zero, that is
  \(E(u_{it}|u_{i1}, u_{i2},\dots, u_{iT})\).
\item
  \((X_{i1}, X_{i2}, \dots, X_{i3}, u_{i1}, \dots, u_{iT})\),
  \(i=1,\dots,n\) are i.i.d. draws from their joint distribution.
\item
  Large outliers are unlikely i.e. \((X_{it}, u_{it})\) have nonzero
  finite fourth moments.
\item
  There is no perfect multicollinearity.
\end{enumerate}

When there are multiple regressors, \(X_{it}\) is replaced by
\(X_{1,it}, X_{2,it}, \dots, X_{k,it}\).

The first assumption is that the error is uncorrelated with \emph{all}
observations of the variable \(X\) for the entity \(i\) over time. If
this assumption is violated, we face omitted variables bias. The second
assumption ensures that variables are i.i.d. \emph{across} entities
\(i=1,\dots,n\). Notice that this does not require the observations to
be uncorrelated \emph{within} an entity. We say that the \(X_{it}\) are
allowed to be \textbf{autocorrelated} or \textbf{serially correlated}
within entities. This is a common property of time series data. The same
is allowed for errors \(u_{it}\). Consult Chapter 10.5 of the book for a
detailed explanation for why autocorrelation is a plausible
characteristic in panel applications. The second assumption is granted
if the entities are selected by simple random sampling. The third and
fourth assumptions are analagous to the multiple regression assumptions
made in Key Concept 6.4.

\subsubsection*{Standard Errors for Fixed Effects
Regression}\label{standard-errors-for-fixed-effects-regression}
\addcontentsline{toc}{subsubsection}{Standard Errors for Fixed Effects
Regression}

Similar as for heteroskedasticity, autocorrelation invalidates the usual
standard error fomulas and also alters the way heteroskedasticity-robust
standard errors must be computed since these are derived under the
assumption that there is no autocorrelation. In the context of
heteroskedasticity \emph{and} autocorrelation so-called
\emph{heteroskedasticity and autocorrelation-consistent (HAC) standard
errors} need to be used. \emph{Clustered standard errors} belong to
these type of standard errors. They allow for heteroskedasticity and
autocorrelated errors within an entity but \emph{not} for any kind of
correlation across entities.

As shown in the examples throughout this chapter, it is fairly easy to
specify usage of clustered standard errors in regression summaries
procuced by function like \texttt{coeftest()} in conjunction with
\texttt{vcovHC()} from the \texttt{sandwich} package. Conveniently,
\texttt{vcovHC()} recognizes panel model objects (objects of class
\texttt{plm}) and computes clusterd standard errors by default.

The regressions conducted during this chapter are a good examples for
why usage of clustered standard errors is crucial in fixed effects
models. For example, consider the entity and time fixed effects
regression model for fatalities. Since \texttt{fatal\_tefe\_lm\_mod} is
an object of class \texttt{lm}, \texttt{coeftest()} does not compute
clustered standard errros but uses robust standard errors that are only
valid in the absence of autocorrelated errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check class of the model object}
\KeywordTok{class}\NormalTok{(fatal_tefe_lm_mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "lm"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Summary based on heteroskedasticity-robust standard errors (no adjustment for autocorrelation)}
\KeywordTok{coeftest}\NormalTok{(fatal_tefe_lm_mod, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fatal_tefe_lm_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))[}\DecValTok{1}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Estimate Std. Error    t value   Pr(>|t|) 
## -0.6399800  0.2547149 -2.5125346  0.0125470
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check class of the (plm) model object}
\KeywordTok{class}\NormalTok{(fatal_tefe_mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "plm"        "panelmodel"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Summary based on clusterd standard errors (adjustment for autocorrelation + heteroskedasticity)}
\KeywordTok{coeftest}\NormalTok{(fatal_tefe_mod, }\DataTypeTok{vcov =} \KeywordTok{vcovHC}\NormalTok{(fatal_tefe_mod, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##         Estimate Std. Error t value Pr(>|t|)  
## beertax -0.63998    0.35015 -1.8277  0.06865 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The outcomes deviate rather strongly: imposing no autocorrelation we
obtain a standard error of \(0.25\) which implyies significance of
\(\beta_1\), the coefficient on \(BeerTax\) at the level of \(5\%\). On
the contrary, using the clusterd standard error (\(0.35\)) leads to
acceptance of the hypothesis \(H_0: \beta_1 = 0\) at the same level of
significance, see equation \eqref{eq:cbnfemod}. Consult Appendix 10.2 of
the book for insights on the computation of clustered standard errors.

\section{Drunk Driving Laws and Traffic
Deaths}\label{drunk-driving-laws-and-traffic-deaths}

There are two major sources of omitted variable bias that are not
accounted for by all of the models of the relation between trafic
fatalitites and beer taxes that we have considered so far: economic
conditions and driving laws. Fortunately, \texttt{Fatalities} has
recordings on state specific legal drinking age (\texttt{drinkage}),
punishment (\texttt{jail}, \texttt{service}) and various economic
indicators like unemployment rate (\texttt{unemp}) and per capita income
(\texttt{income}). We may use this data to extend the preceeding
analysis.

At first, we define the variables according to the regression results
presented in Table 10.1 of the book.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Discretize the minimum legal drinking age}
\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{drinkagec <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(Fatalities}\OperatorTok{$}\NormalTok{drinkage,}
                            \DataTypeTok{breaks =} \DecValTok{18}\OperatorTok{:}\DecValTok{22}\NormalTok{, }
                            \DataTypeTok{include.lowest =} \OtherTok{TRUE}\NormalTok{, }
                            \DataTypeTok{right =} \OtherTok{FALSE}
\NormalTok{                            )}

\CommentTok{# Set minimum drinking age [21,22] to be the baseline level}
\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{drinkagec <-}\StringTok{ }\KeywordTok{relevel}\NormalTok{(Fatalities}\OperatorTok{$}\NormalTok{drinkagec, }\StringTok{"[21,22]"}\NormalTok{)}

\CommentTok{# Mandadory jail or community service?}
\NormalTok{Fatalities}\OperatorTok{$}\NormalTok{punish <-}\StringTok{ }\KeywordTok{with}\NormalTok{(Fatalities, }\KeywordTok{factor}\NormalTok{(jail }\OperatorTok{==}\StringTok{ "yes"} \OperatorTok{|}\StringTok{ }\NormalTok{service }\OperatorTok{==}\StringTok{ "yes"}\NormalTok{, }
                                             \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{)}
\NormalTok{                                             )}
\NormalTok{                          )}

\CommentTok{# Observations on all variables for 1982 and 1988 only}
\NormalTok{Fatalities_1982_}\DecValTok{1988}\NormalTok{ <-}\StringTok{ }\NormalTok{Fatalities[}\KeywordTok{with}\NormalTok{(Fatalities, year }\OperatorTok{==}\StringTok{ }\DecValTok{1982} \OperatorTok{|}\StringTok{ }\NormalTok{year }\OperatorTok{==}\StringTok{ }\DecValTok{1988}\NormalTok{), ]}
\end{Highlighting}
\end{Shaded}

Next, we estimate all seven models using \texttt{plm()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fatalities_mod1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax, }\DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod2 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state, }\DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod3 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year,}
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{,}\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{, }
                       \DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod4 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{drinkagec }
                       \OperatorTok{+}\StringTok{ }\NormalTok{punish }\OperatorTok{+}\StringTok{ }\NormalTok{miles }\OperatorTok{+}\StringTok{ }\NormalTok{unemp }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
                       \DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod5 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{drinkagec }
                       \OperatorTok{+}\StringTok{ }\NormalTok{punish }\OperatorTok{+}\StringTok{ }\NormalTok{miles,}
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
                       \DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod6 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{drinkage }
                       \OperatorTok{+}\StringTok{ }\NormalTok{punish }\OperatorTok{+}\StringTok{ }\NormalTok{miles }\OperatorTok{+}\StringTok{ }\NormalTok{unemp }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
                       \DataTypeTok{data =}\NormalTok{ Fatalities)}

\NormalTok{fatalities_mod7 <-}\StringTok{ }\KeywordTok{plm}\NormalTok{(fatal_rate }\OperatorTok{~}\StringTok{ }\NormalTok{beertax }\OperatorTok{+}\StringTok{ }\NormalTok{state }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{drinkagec }
                       \OperatorTok{+}\StringTok{ }\NormalTok{punish }\OperatorTok{+}\StringTok{ }\NormalTok{miles }\OperatorTok{+}\StringTok{ }\NormalTok{unemp }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(income), }
                       \DataTypeTok{index =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
                       \DataTypeTok{model =} \StringTok{"within"}\NormalTok{,}
                       \DataTypeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
                       \DataTypeTok{data =}\NormalTok{ Fatalities_1982_}\DecValTok{1988}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Yet again we may use \texttt{stargazer()} to generate a comprehensive
tabular presentation of the results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stargazer)}

\NormalTok{rob_se <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod1, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod2, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod3, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod4, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod5, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod6, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))),}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{vcovHC}\NormalTok{(fatalities_mod7, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{)))}
\NormalTok{)}

\KeywordTok{stargazer}\NormalTok{(fatalities_mod1, fatalities_mod2, fatalities_mod3, }
\NormalTok{          fatalities_mod4, fatalities_mod5, fatalities_mod6, fatalities_mod7, }
          \DataTypeTok{digits =} \DecValTok{3}\NormalTok{,}
          \DataTypeTok{type =} \StringTok{"latex"}\NormalTok{, }
          \DataTypeTok{se =}\NormalTok{ rob_se,}
          \DataTypeTok{model.numbers =} \OtherTok{FALSE}\NormalTok{,}
          \DataTypeTok{column.labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"(1)"}\NormalTok{, }\StringTok{"(2)"}\NormalTok{, }\StringTok{"(3)"}\NormalTok{, }\StringTok{"(4)"}\NormalTok{, }\StringTok{"(5)"}\NormalTok{, }\StringTok{"(6)"}\NormalTok{, }\StringTok{"(7)"}\NormalTok{)}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

\% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University.
E-mail: hlavac at fas.harvard.edu \% Date and time: Tue, May 29, 2018 -
14:12:29 \% Requires LaTeX packages: rotating

\begin{sidewaystable}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{-5pt}}lccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{7}{c}{\textit{Dependent variable:}} \\ 
\cline{2-8} 
\\[-1.8ex] & \multicolumn{7}{c}{fatal\_rate} \\ 
 & OLS &  &  & Linear Panel Regression &  &  &  \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7)\\ 
\hline \\[-1.8ex] 
 beertax & 0.365$^{***}$ & $-$0.656$^{**}$ & $-$0.640$^{*}$ & $-$0.445 & $-$0.690$^{**}$ & $-$0.456 & $-$0.926$^{***}$ \\ 
  & (0.053) & (0.289) & (0.350) & (0.291) & (0.345) & (0.301) & (0.337) \\ 
  & & & & & & & \\ 
 drinkagec[18,19) &  &  &  & 0.028 & $-$0.010 &  & 0.037 \\ 
  &  &  &  & (0.068) & (0.081) &  & (0.101) \\ 
  & & & & & & & \\ 
 drinkagec[19,20) &  &  &  & $-$0.018 & $-$0.076 &  & $-$0.065 \\ 
  &  &  &  & (0.049) & (0.066) &  & (0.097) \\ 
  & & & & & & & \\ 
 drinkagec[20,21) &  &  &  & 0.032 & $-$0.100$^{*}$ &  & $-$0.113 \\ 
  &  &  &  & (0.050) & (0.055) &  & (0.123) \\ 
  & & & & & & & \\ 
 drinkage &  &  &  &  &  & $-$0.002 &  \\ 
  &  &  &  &  &  & (0.021) &  \\ 
  & & & & & & & \\ 
 punishyes &  &  &  & 0.038 & 0.085 & 0.039 & 0.089 \\ 
  &  &  &  & (0.101) & (0.109) & (0.101) & (0.161) \\ 
  & & & & & & & \\ 
 miles &  &  &  & 0.00001 & 0.00002$^{*}$ & 0.00001 & 0.0001$^{***}$ \\ 
  &  &  &  & (0.00001) & (0.00001) & (0.00001) & (0.00005) \\ 
  & & & & & & & \\ 
 unemp &  &  &  & $-$0.063$^{***}$ &  & $-$0.063$^{***}$ & $-$0.091$^{***}$ \\ 
  &  &  &  & (0.013) &  & (0.013) & (0.021) \\ 
  & & & & & & & \\ 
 log(income) &  &  &  & 1.816$^{***}$ &  & 1.786$^{***}$ & 0.996 \\ 
  &  &  &  & (0.624) &  & (0.631) & (0.666) \\ 
  & & & & & & & \\ 
 Constant & 1.853$^{***}$ &  &  &  &  &  &  \\ 
  & (0.047) &  &  &  &  &  &  \\ 
  & & & & & & & \\ 
\hline \\[-1.8ex] 
Observations & 336 & 336 & 336 & 335 & 335 & 335 & 95 \\ 
R$^{2}$ & 0.093 & 0.041 & 0.036 & 0.360 & 0.066 & 0.357 & 0.659 \\ 
Adjusted R$^{2}$ & 0.091 & $-$0.120 & $-$0.149 & 0.217 & $-$0.134 & 0.219 & 0.157 \\ 
Residual Std. Error & 0.544 (df = 334) &  &  &  &  &  &  \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{7}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{sidewaystable}

While columns (2) and (3) recap the results \eqref{eq:efemod} and
\eqref{eq:cbnfemod}, column (1) presents an estimate of the coefficient of
interest in the naive OLS regression of the fatality rate on beer tax
without any fixed effects. We obtain a \emph{positive} estimate for the
coefficient on beer tax that is likely to be upward biased. Notice that
the model fit is rather bad, too (\(\overline{R^2} = 0.091\)). The sign
of the estimate changes as we extend the model by both entity and time
fixed effects in models (2) and (3). Furthermore \(\overline{R^2}\)
increases substantially as fixed effects are included in the model
equation. Nonetheless, as discussed before, the magnitudes of both
estimates are by far too large.

Thus, model specifications (4) to (7) include covariates that shall
capture the effect of overall state economic conditions as well as the
legal framework. These covariates are as follows:

\begin{itemize}
\tightlist
\item
  \texttt{unemp}: a numeric variable stating the state specific
  unemployment rate.
\item
  \texttt{log(income)}: the logarithm of real per capita income (in
  prices of 1988).
\item
  \texttt{miles}: average miles per driver.
\item
  \texttt{drinkage}: state specifiy minimum legal drinking age.
\item
  \texttt{drinkagc}: a discretized version of \texttt{drinkage} that
  classifies states into four categories of minimal drinking age;
  \(18\), \(19\), \(20\), \(21\) and older. R denotes this as
  \texttt{{[}18,19)}, \texttt{{[}19,20)}, \texttt{{[}20,21)} and
  \texttt{{[}21,22{]}}. These categories are included as dummy
  regressors whereby \texttt{{[}21,22{]}} is chosen to be the reference
  category.
\item
  \texttt{punish}: a dummy variable with levels \texttt{yes} and
  \texttt{no} that measures if drunk driving is severely punished by
  mandatory jail time or mandatory community service (first conviction).
\end{itemize}

Considering (4) as the base specification, we observe four interesting
results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Including the covariates does not lead to a major reduction of the
  estimated effect of the beer tax. Plus, the coefficient is not
  significantly different from zero at the level of \(5\%\) as the
  estimate is rather imprecise.
\item
  The minimum legal drinking age \emph{does not} have an effect on
  traffic fatalities: none of the three dummy variables are
  significantly different from zero at any level of significance common
  in practice. Moreover, an \(F\)-Test of the joint hypothesis that all
  three coefficients are zero does not reject. The next code chunk shows
  how to test this hypothesis.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test if legal drinking age has no explanatory power}
\KeywordTok{linearHypothesis}\NormalTok{(fatalities_mod4,}
                 \DataTypeTok{test =} \StringTok{"F"}\NormalTok{,}
                 \KeywordTok{c}\NormalTok{(}\StringTok{"drinkagec[18,19)=0"}\NormalTok{, }\StringTok{"drinkagec[19,20)=0"}\NormalTok{, }\StringTok{"drinkagec[20,21)"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatalities_mod4, }\DataTypeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## drinkagec[18,19) = 0
## drinkagec[19,20) = 0
## drinkagec[20,21) = 0
## 
## Model 1: restricted model
## Model 2: fatal_rate ~ beertax + state + year + drinkagec + punish + miles + 
##     unemp + log(income)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(>F)
## 1    276                 
## 2    273  3 0.3691 0.7753
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  There is no evidence that punishment for first offenders has a
  deterring effects on drunk driving: the corresponding coefficient is
  not significant at the \(10\%\) level.
\item
  The economic variables have power in explaning traffic fatalities. We
  can check that the employment rate and per capita income are jointly
  significant at the level of \(0.1\%\).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test if economic indicators have no explanatory power}
\KeywordTok{linearHypothesis}\NormalTok{(fatalities_mod4, }
                 \DataTypeTok{test =} \StringTok{"F"}\NormalTok{,}
                 \KeywordTok{c}\NormalTok{(}\StringTok{"log(income)"}\NormalTok{, }\StringTok{"unemp"}\NormalTok{), }
                 \DataTypeTok{vcov. =} \KeywordTok{vcovHC}\NormalTok{(fatalities_mod4, }\DataTypeTok{type =} \StringTok{"HC2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## log(income) = 0
## unemp = 0
## 
## Model 1: restricted model
## Model 2: fatal_rate ~ beertax + state + year + drinkagec + punish + miles + 
##     unemp + log(income)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(>F)    
## 1    275                        
## 2    273  2 30.482 1.125e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Model (5) omitts the economic factors. The result supports the
presumption that economic indicators should remain part of the model as
the coefficient on beer tax is sensitive to the inclusion of the latter.

Results for model (6) are in favour of the presumption that legal
drinking age has little explanatory power and that the coefficient of
interest is not senstive to changes in the functional form of the
relation between drinking age and traffic fatalities.

Estimation outcomes for spefcification (7) reveals that reducing the
amount of available information inflates standard errors but does not
lead to drastic changes in coefficient estimates.

\subsubsection*{Summary}\label{summary}
\addcontentsline{toc}{subsubsection}{Summary}

We have not found evidence that menacing severe punishments and
increasing the minimum drinking ages are appropriate measures for
reducing traffic fatalities due to drunk driving. Nonetheless we
concluded that there is some effect of alcohol taxes on traffic
fatalities which, however, is estimated imprecisely and cannot be
interpreted as the causal effect of interest as there seems to be
omitted variable bias. This bias persists even though we used a panel
approach that controls for entity specific and time invariant
unobservables. A possible source for this bias are omitted variables
that change across entities \emph{and} over time.

A powerful method that can be used if common panel regression approaches
fail is instrumental variables regression. We will return to this
concept in Chapter 12.

\bibliography{book.bib,packages.bib}


\end{document}
