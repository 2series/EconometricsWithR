# Estimation of Dynamic Causal Effects

For answering some questions in economic research it interesting to know how large the effect on $Y$ now and in the future of a change in $X$ is. This called the *dynamic causal effect* on $Y$ of a change in $X$. In this section we will see how to estimate dynamic causal effects using several <tt>R</tt> applications where we investigate the dynamic effect of cold weather in Florida on the price of orange juice concentrate.

The discussion includes:

+ Estimation of distributed lag models
+ Heteroskedasticity- and autocorrelation-consistent (HAC) standard errors
+ Generalized least squares (GLS) of ADL models

```{r, warning=FALSE, message=FALSE}
library(AER)
library(quantmod)
library(PerformanceAnalytics)
library(dynlm)
library(orcutt)
library(nlme)
```

## The Orange Juice Data 

The largest cultivation region for oranges in the U.S. is located in state of Florida which usually has ideal climate for the fruit growth and thus makes Florida the source of almost all frozen juice contentrate produced in the country However, from time to time and depening on their severeness, cold snaps cause loss of harvests such that the supply of oranges decreases and consequently the price of frozen juice conzentrate rises. The timing of the price increases is complicated: a cut in today's supply of concentrate influences not today's price but also future prices because supply in future periods will be lower, too. Clearly, the magnitude of today's and future price increases due to a freeze is an empirical question that can be investigated using a distributed lag model --- a model that relates price changes to weather conditions.

To begin with the analysis, we reproduce Figure 15.1 of the book which displays plots of the price index for frozen concentrated orange juice, percent changes in the price as well as monthly freezing degree days in Orlando, the center of Florida's orange-growing region.

```{r}
# Load the frozen orange juice data set
data("FrozenJuice")

# price index for frozen concentrated juice
FOJCPI <- 100 * FrozenJuice[,1]/FrozenJuice[,2]
FOJC_pctc <- 100 * diff(log(FOJCPI))
FDD <- FrozenJuice[,3]
```

```{r, fig.align='center'}
FOJCPI_xts <- as.xts(FOJCPI)
FDD_xts <- as.xts(FrozenJuice[,3])

# Plot orange juice price index
chart.TimeSeries(FOJCPI, 
                 main = "Frozen Concentrated Orange Juice",
                 ylab = "Price index", 
                 col = "steelblue", 
                 lwd = 2,
                 date.format = "%Y", 
                 minor.ticks = F)

# Plot percent changes in prices
chart.TimeSeries(FOJC_pctc, 
     main = "Monthly Changes in the Price of Frozen Conentrated Orange Juice",
     ylab = "Percent",
     col = "steelblue", 
     lwd = 2,
     date.format = "%Y", 
     minor.ticks = F)

# plot freezing degree days 
chart.TimeSeries(FrozenJuice[, 3],
     main = "Monthly Freezing Degree Days in Orlando, FL",
     ylab = "Freezing degree days",
     col = "steelblue", 
     lwd = 2,
     date.format = "%Y", 
     minor.ticks = F
     )
```

It is conspicious that periods with a high amount of freezing degree days are followed by large month-to-month price changes. These coindciding movements give rise to run a simple regression of price changes $\%ChgOJC_t$ on freezing degree days ($FDD_t$) to estimate the effect of an additional freezing degree day one the price in the current month. For this, as for all other regressions in this chapter, we use $T=611$ observations (January 1950 to December 200).

```{r}
# simple regression of percent changes on freezing degree days
orange_SR <- dynlm(FOJC_pctc ~ FDD)
coeftest(orange_SR, vcov. = vcovHAC)
```

Notice that the standard errors are computed using a HAC estimator of the model variance-covariance matrix, a matter that will not be further commented at this point, see Chapter 14.5.

\begin{align*}
  \widehat{\%ChgOJC_t} = \underset{(0.19)}{-0.42} + \underset{(0.13)}{0.47} FDD_t
\end{align*}

The estimated coefficient on $FDD_t$ has the following interpretation: an additional freezing degree day in month $t$ leads to a price increase 0f $0.47\%$ in the same month.  

To consider effects of cold snaps on the orange juice price over following, we include lagged values of $FDD_t$ in our model which leads to a so-called *distributed lag regression*. We estimate a specification of a contemporeneous and six lagged values of $FDD_t$.

```{r}
# distributed lag model with 6 lags of freezing degree days
orange_DLM <- dynlm(FOJC_pctc ~ FDD + L(FDD, 1:6))
coeftest(orange_DLM, vcov. = vcovHAC)
```

As a result we obtain

\begin{align*}
  \widehat{\%ChgOJC_t} =& \, \underset{(0.21)}{-0.69} + \underset{(0.14)}{0.47} FDD_t + \underset{(0.08)}{0.15} FDD_{t-1} + \underset{(0.06)}{0.06} FDD_{t-2} + \underset{(0.05)}{0.07} FDD_{t-3} \\ &\,+ \underset{(0.03)}{0.04} FDD_{t-4} + \underset{(0.03)}{0.05} FDD_{t-5} + \underset{(0.05)}{0.05} FDD_{t-6} (\#eq:orangemod1)
\end{align*}

where the coefficient on $FDD_{t-1}$ estimates the price increase in period $t$ caused by an additional freezing degree day in the preceding month, the coefficient on $FDD_{t-2}$ estimates the effect of an additional freezing degree day two month ago and so on. Consequently, the coefficients in \@ref(eq:orangemod1) can be interpreted as price changes in current and future periods due to a unit increase in the current month' freezing degree days.

## Dynamic Causal Effects

This section of the book describes the general idea of a dynamic causal effect and how the concept of a randomized controlled experiment which (as has been discussed in Chapter 13) constitutes and ideal study design can be translated to time series applications, using several examples. For brevity we will not go into the details but note once again that the distributed lag model mentionted above can often be used to estimate a dynamic causal relationship. 

In general, for empirical attempts to measure a dynamic causal effect, the assumptions of stationarity (see Key Concept 14.5) and exogeneity must hold. In the time series appliaction up until here we have assumed that the model error term has conditional mean zero given current and past values of the regressors. For estimation of a dynamic causal effect using a distributed lag model, assuming a stronger form termed *strict exogeneity* may be useful. Strict exogeneity states that the error term has mean zero conditional on past, present *and future* values of the independent variables. 

The two concepts of exogeneity and the distributive lag model are summarized in Key Concept 15.1

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 15.1 </h3>
<h3 class = "left"> The Distributed Lag Model and Exogeneity </h3>

<p>

The general distributed lag model is
\begin{align*}
  Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + \beta_3 X_{t-2} + \dots + \beta_{r+1} X_{t-r} + u_t (\#eq:dlm)
\end{align*}
where it is assumed that

1. $X$ is an exogenous variable, $$E(u_t\vert X_t, X_{t-1}, X_{t-2},\dots) = 0.$$

2.
     (a) $X_t,Y_t$ have a stationary distribution.
     (b) $(Y_t,X_t)$ and $(Y_{t-j},X_{t-j})$ become independently distributed as $j$ gets large.

3. Large outliers are unlikely. In particular, we need that the all variables have more than eight nonzero and finite moments --- a stronger assumption than before (four finite nonzero moments) that is required for computation of the HAC covariance matrix estimator.

4. There is no perfect multicollinearity.

 The distributed lag model may be extended to include contempoarneous and past values of additional regressors.

**On the assumption of exogeneity**

+ There is another form of exogeneity termed *strict exogeneity* which assumes $$E(u_t\vert \dots X_{t+2},X_{t+1},X_t,X_{t-1},X_{t-2},\dots)=0,$$ that is the error term is has mean zero conditional on past, present and future values of $X$. Strict exogeneity implies exogeneity (as defined in 1.) but not the other way around. From this point we will therefore distinguish between exogeneity and strict exogeneity. 

+ Exogeneity as in 1. suffices for OLS estimators of the coefficient in distributed lag models to be consistent. However, if the the assumption of strict exogeneity can be made, more efficient estimators can be applied.

</p>

</div>


## Dynamic Multipliers and Cumulative Dynamic Multipliers

The following terminoligy regarding the coefficients in the distributed lag model \@ref(eq:dlm) are useful for upcoming applications:

+ The dynamic causal effect is also termed the *dynamic multiplier*. We say that $\beta_{h+1}$ in \@ref(eq:dlm) is the $h$-period dynamic multiplier. 

+ The contemporaneous effect of $X$ ond $Y$, $\beta_1$, is termed the *impact effect*. 

+ The $h$-period *cumulative dynamic multiplier* of a unit change in $X$ and $Y$ is defined as the cumulative sum of the dynamic multipliers. In particualr, $\beta_1$ is the zero-period cumulative dynamic multiplier, $\beta_1 + \beta_2$ is the one-period cumulative dynamic multiplier and so forth.

      Cumulative dynamic multipliers of \@ref(eq:dlm) are the coefficients $\delta_1,\delta_2,\dots,\delta_r,\delta_{r+1}$ in the modified regression
\begin{align*}
  Y_t =& \, \delta_0 + \delta_1 \Delta X_t + \delta_2 \Delta X_{t-1} + \dots + \delta_r \Delta X_{t-r+1} + \delta_{r+1} X_{t-r} + u_t. (\#eq:DCMreg)
\end{align*} and thus can be directly estimated using OLS wich makes it convenient to compute their HAC standard errors. $\delta_{r+1}$ is called the *long-run dynamic multiplier*.

It is straightforward to compute the cumulative dynamic multipliers for \@ref(eq:orangemod1), the estimated distributed lag regression of changes in orange juice concentrate prices on freezing degree days, using the corresponding model object `orange_DLM` and the function `cumsum()`.

```{r}
# compute cumulative multipliers
cum_mult <-cumsum(orange_DLM$coefficients[-1])

# rename entries
names(cum_mult) <- paste(0:6, sep = "-", "period CDM")

cum_mult
```

Translating the distributed lag model with six lags of $FDD$ to \@ref(eq:DCMreg), we see that the OLS coefficient estimates in this model coincide with the multipliers stored in `cum_mult`.

```{r}
# estimate cumulative dynamic multipliers using the modified regression
cum_mult_reg <-dynlm(FOJC_pctc ~ d(FDD) + d(L(FDD,1:5)) + L(FDD,6))
coef(cum_mult_reg)[-1]
```

As noted above, this spcification of model allows to obtain standard errors for the estimated dynamic cumulative multipliers.

```{r}
coeftest(cum_mult_reg, vcov. = vcovHAC)
```

## HAC Standard Errors

The error term $u_t$ in the distributed lag model \@ref(eq:dlm) may be serially correlated due to serially correlated determinants of $Y_t$ that are not included as regressors. When these factors are not correlated with the regressors included in the model, serially correlated errors do not violate the assumption of exogeneity such that the OLS estimator remains unbiased and consistent.

Instead, autocorrelated standard errors render the usual homoskedasticity-only *and* heteroskedasticity-robust standard errrors invalid and may lead to misleading statistical inference. HAC errors are a remedy for this issue. Key Concept 15.2

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 15.2 </h3>
<h3 class = "left"> HAC Standard errors </h3>

<p>

**Problem**:

If the error term $u_t$ in the distributed lag model \@ref(eq:dlm) is serially correlated, satistical inference that rests on usual (heteroskedasticity-robust) standard errors can be strongly misleading.

**Solution**:

Heteroskedasticity- and autocorrelation-consistent (HAC) estimators of the variance-covariance matrix circumvent this issue. There are <tt>R</tt> functions like `vcovHAC()` from the package <tt>sandwich</tt> which are convinient for computation of such estimators.

The package <tt>sandwich</tt> also contains the function `NeweyWest()`, an implementation of the HAC variace-covariance estimator proposed by Newey and West (1987).

</p>

</div>

Consider the distributed lag regression model with no lags and the single regressor $X_t$ 
\begin{align*}
  Y_t = \beta_0 + \beta_1 X_t + u_t.
\end{align*}
with autocorrelated errors. A brief derivation of 
\begin{align*}
  \overset{\sim}{\sigma}^2_{\widehat{\beta}_1} = \widehat{\sigma}^2_{\widehat{\beta}_1} \widehat{f}_t, (\#eq:nwhac)
\end{align*}
the so-called *Newey-West variance estimator* for the variance of the OLS estimator of $\beta_1$ is presented in Chapter 15.4 of the book. $\widehat{\sigma}^2_{\widehat{\beta}_1}$ in \@ref(eq:nwhac) is the heteroskedasticity-robust variance estimate of $\widehat{\beta}_1$ and 
\begin{align*}
  \widehat{f}_t = 1 + 2 \sum_{j=1}^{m-1} \left(\frac{m-j}{m}\right) \overset{\sim}{\rho}_j (\#eq:nwhacf)
\end{align*}
is a correction factor that adjusts for serially correlated errors and involves estimates of $m-1$ autocovariances $\overset{\sim}{\rho}_j$, whereby $m$ is a truncation parameter to be chosen. A rule-of-thumb for choosing $m$ is
\begin{align*}
  m = \left \lfloor{0.75 \cdot T^{1/3}}\right\rfloor.
\end{align*}

In the following we simulate a time series that, as stated above, follows a distributed lag model with autocorrelated errors and then show how to compute the Newey West HAC estimate of $SD(\widehat{\beta}_1)$ using <tt>R</tt>. This is done in two seperat but, as we will see, identical approches: at first we follow the derivation presented in the book step-by-step and compute the estimate \glqq by hand \grqq. Then we prove that the result is exactly the estimate obtained when using the function `NeweyWest()`. 

```{r}
# function that computes rho tilde
acf_c <- function(x,j) {
  return(
    t(x[-c(1:j)]) %*% na.omit(Lag(x, j)) / t(x) %*% x
  )
}

# simulate time series with serially correlated errors
set.seed(1)

eps <- arima.sim(n = 100, model = list(ma = 0.5))
X <- runif(100, 1, 10)
Y <- 0.5 * X + eps

# compute OLS residuals
res <- lm(Y ~ X)$res

# compute v
v <- (X - mean(X)) * res

# compute robust estimate of beta_1 variance
var_beta_hat <- 1/100 * (1/(100-2) * sum((X - mean(X))^2 * res^2) ) / 
                        (1/100 * sum((X-mean(X))^2))^2

# rule of thumb truncation parameter
m <- floor(0.75 * 100^(1/3))

# compute correction factor
f_hat_T <- 1 + 2 * sum(
  (m - 1:(m-1))/m * sapply(1:(m-1), function(i) acf_c(x=v, j=i))
  ) 

# compute Newey West HAC estimate of the standard error 
sqrt(var_beta_hat * f_hat_T)
```

By choosing `lag = m-1` it is ensured that the maximum order of autocorrelations used is $m-1$ --- just as in equation \@ref(eq:nwhacf). Notice that we further set the arguments `prewhite = F` and `adjust = T` to ensure that the formula \@ref(eq:nwhac) is used and finite sample adjustments are made.

```{r}
# Using NeweyWest():
NW_VCOV <- NeweyWest(lm(Y ~ X), 
              lag = m - 1, prewhite = F, 
              adjust = T
              )

# compute standard error
sqrt(diag(NW_VCOV))[2]
```

We find that the computed standard errors coincide. Of course, a variance-covariance matrix estimate as computed by `NeweyWest()` can be supplied as the argument `vcov` in `coeftest()` such that HAC $t$-statistics and $p$-values are provided.

```{r}
example_mod <- lm(Y ~ X)
coeftest(example_mod, vcov = NW_VCOV)
```

## Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors

In general, the errors in a distributed lag model are correlated which necessitates usage of HAC standard errors for valid inference. If, however, the assumption of exogeneity (the frist assumption stated in Key Concept 15.1) is replaced by strict exogeneity, that is $$E(u_t\vert \dots, X_{t+1}, X_{t}, X_{t-1}, \dots),$$ more efficient approaches than OLS estimation of the coefficients may be available. For a general distributed lag model with $r$ lags and AR($p$) errors, these approaches are summarized in Key Concept 15.4.

<div class = "keyconcept"> 
<h3 class = "right"> Key Concept 15.4 </h3>
<h3 class = "left"> Estimation of Dynamic Multipliers Under Strict Exogeneity </h3>
<p>

Consider the general distributed lag model with $r$ lags and assume that the errors follow an $AR(p)$ process,
\begin{align*}
  Y_t =& \, \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + \dots + \beta_{r+1} X_{t-r} + u_t \\
  u_t =& \, \phi_1 u_{t-1} + \phi u_{t-2} + \dots + \phi_p u_{t-p} + \overset{\sim}{u}_t. (\#eq:dlmarerrors)
\end{align*}
Assuming strict exogeneity of $X_t$, one may rewrite the above model in the ADL specification
\begin{align*}
  Y_t =& \, \alpha_0 + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} \\
      &+ \, \delta_0 X_t + \delta_1 X_{t-1} + \dots + \delta_q X_{t-q} + \overset{\sim}{u}_t
\end{align*}
where $q=r+p$ and compute estimates of the dynamic multipliers $\beta_1, \beta_2, \dots, \beta_{r+1}$ using OLS estimates of $\phi_1, \phi_2, \dots, \phi_p, \delta_0, \delta_1, \dots, \delta_q$. 

An alternative is to estimate the dynamic multipliers using feasible GLS, that is to apply the OLS estimator to a quasi-differenced specification of \@ref(eq:dlmarerrors). Under strict exogeneity, the feasible GLS approach is the BLUE estimator for the dynamic multipliers in large samples.

</p>
</div>

In what follows we shortly review how different representations of a distribted lag model can be obtained when the causal effect in $Y$ of a change in $X$ lasts for only two periods and show how these specification can be estimated by OLS and GLS using <tt>R</tt>.

The model is
\begin{align*}
  Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + u_t (\#eq:dldynamic)
\end{align*}
so a change in $X$ has a contemporaneous effect on $Y$ ($\beta_1$) and an effect in the next period ($\beta_2$). The error term $u_t$ follows an AR($1$) process,$$u_t = \phi_1 u_{t-1} + \overset{\sim}{u_t}$$ where $\overset{\sim}{u_t}$ is serially uncorrelated.

One can show that the ADL representation of this model is
\begin{align*}
  Y_t = \alpha_0 + \phi_1 Y_{t-1} + \delta_0 X_t + \delta_1 X_{t-1} + \delta_2 X_{t-2} + \overset{\sim}{u}_t (\#eq:adl21dynamic)
\end{align*}
with the restrictions
\begin{align*}
  \beta_1 =& \, \delta_0, \\
  \beta_2 =& \, \delta_1 + \phi_1 \delta_0.
\end{align*}
Another way of writing this ADL($1$,$2$) representation is the *quasi-difference* model
\begin{align*}
  \overset{\sim}{Y}_t = \alpha_0 + \beta_1 \overset{\sim}{X}_t + \beta_2   \overset{\sim}{X}_{t-1} + \overset{\sim}{u}_t (\#eq:qdm)
\end{align*}
where $\overset{\sim}{Y}_t = Y_t - \phi_1 Y_{t-1}$ and $\overset{\sim}{X}_t = X_t - \phi_1 X_{t-1}$. Notice that the error term $\overset{\sim}{u}_t$ is uncorrelated in both models and, as shown in Chapter 15.5 of the book, $$E(u_t\vert X_{t+1}, X_t, X_{t-1}, \dots) = 0$$ which is implied by the assumption of strict exogeneity.

We continue by simulation a time series of $500$ observations using the model \@ref(eq:dldynamic) where $\beta_1 = 0.1$, $\beta_2 = 0.25$, $\phi = 0.5$ and $\overset{\sim}{u}_t \sim N(0,1)$ and start by estimating the distributed lag model.

```{r}
# simulate time series with serially correlated errors
set.seed(1)

obs <- 501

eps <- arima.sim(n = obs-1 , model = list(ar = 0.5))
X <- arima.sim(n = obs, model = list(ar = 0.25))
Y <- 0.1 * X[-1] + 0.25 * X[-obs] + eps
X <- ts(X[-1])

# estimate the distributed lag model
dlm <- dynlm(Y ~ X + L(X))
```

It is straightforward to check whether the residuals of this model exhibit autocorrelation by using `acf()`.

```{r, fig.align='center'}
# check that the residuals are serially correlated
acf(
  residuals(dlm)
)
```

The plot indicates that the residuals are autocorrelated. In particular, the pattern reveals that the residuals follow an autoregressive process, as the sample autocorrelation function decays quickly for the first few lags and is probably zero for higher lag orders. At any case, HAC standard errors should.

```{r}
# coefficient summary using NeweyWest estimator
coeftest(dlm, vcov = NeweyWest(dlm, prewhite = F, adjust = T))
```

##### OLS Estimation of the ADL Model {-}

Next, we estimate the ADL($2$,$1$) model \@ref(eq:adl21dynamic) using OLS. Notice that the errors are uncorrelated in this representation of the model. This statement is supported by a plot of the sample autocorrelation function of the residual series.

```{r, fig.align='center'}
# Estimate ADL(2, 1) representation
adl21_dynamic <- dynlm(Y ~ L(Y) + X + L(X, 1:2))

# plot sample autocorrelaltions of residuals
acf(adl21_dynamic$residuals)
```

The estimated coefficients in `adl21_dynamic$coefficients` *are not* the dynamic multiplers we are interested in but can be computed according to the restrictions in \@ref(eq:adl21dynamic) where the true coefficients are replaced by their OLS estimates.

```{r}
t <- adl21_dynamic$coefficients

# compute estimated dynamic effects using coefficient restrictions
# in the ADL(2,1) representation
c(
  "hat_beta_1" = t[3],
  "hat_beta_2" = t[4] + t[3] * t[2]
  )
```

##### GLS Estimation {-}

Strict exogeneity allows OLS estimation of the quasi-difference model \@ref(eq:qdm). The idea of applying the OLS estimator to a model where the variables are linearly transformed such that the model errors are uncorrelated and homoskedastic is called *generalized least squares* (GLS)

The OLS estimator in \@ref(eq:qdm) is called the *infeasible GLS* estimator because we $\overset{\sim}{Y}$ and $\overset{\sim}{X}$ cannot be computed without $\phi_1$, the autoregressive coefficient in the error AR($1$) model, which is generally unknown in practice.

Suppose $\phi = 0.5$ was known. The infeasible GLS estimates of the dynamic multipliers in \@ref(eq:dldynamic) are obtained by applying OLS to the transformed data. 

```{r}
# GLS: estimate quasi-differenced specification by OLS
iGLS_dynamic <- dynlm(I(Y- 0.5 * L(Y)) ~ I(X - 0.5* L(X)) + I(L(X) - 0.5* L(X,2)) )

summary(iGLS_dynamic)
```

The *feasible GLS* estimator uses preliminary estimation of the coefficients in the presumed error term model, computes the quasi-differenced data and then estimates the model using OLS. This idea was introduced by Cochrance and Orcutt (1949) and can be extended by continuing this process iteratively. Such a procedure is implemented in the function <tt>cochrane.orcutt()</tt> from the package <tt>orcutt</tt>.

```{r}
X_t <- c(X[-1])
X_l1 <- c(X[-500])
Y_t <- c(Y[-1])

# iterated cochrance-orcutt procedure
cochrane.orcutt(lm(Y_t ~ X_t + X_l1))
```

Some more sophisticated methods are provided with the package <tt>nlme</tt>.

```{r}
# feasible GLS maximum likelihood estimation preocedure
gls(Y_t ~ X_t + X_l1, correlation = corAR1())
```


