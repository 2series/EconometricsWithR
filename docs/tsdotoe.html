<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Using R for Introduction to Econometrics</title>
  <meta name="description" content="Using R for Introduction to Econometrics">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Using R for Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Using R for Introduction to Econometrics" />
  
  
  

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2017-11-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="the-least-squares-assumptions.html">
<link rel="next" href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="MathJax-2.7.2/MathJax.js?config=default"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/SVG"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- <script src="js/d3.v3.min.js"></script> 
<script src="js/leastsquares.js"></script>
<script src="js/d3functions.js"></script>
<script src="d3.slider.js"></script>
<script src="slrm.js"></script> -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">URFITE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#expected-values-mean-and-variance"><i class="fa fa-check"></i>Expected Values, Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html"><i class="fa fa-check"></i><b>2.2</b> Probability Distributions of Continuous Random Variables</a><ul>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li><a href="probability-distributions-of-continuous-random-variables.html#thetdist">The Student <span class="math inline">\(t\)</span> Distribution</a></li>
<li><a href="probability-distributions-of-continuous-random-variables.html#the-f-distribution">The <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html"><i class="fa fa-check"></i><b>2.3</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="random-sampling-and-the-distribution-of-sample-averages.html"><a href="random-sampling-and-the-distribution-of-sample-averages.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="estimation-of-the-population-mean.html"><a href="estimation-of-the-population-mean.html"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="properties-of-the-sample-mean.html"><a href="properties-of-the-sample-mean.html"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#p-value"><span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-sigma_y-is-known">Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#sample-variance-sample-standard-deviation-and-standard-error"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-sigma_y-is-unknown">Calculating the <span class="math inline">\(p\)</span>-value When <span class="math inline">\(\sigma_Y\)</span> is Unknown</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#the-t-statistic">The <span class="math inline">\(t\)</span>-statistic</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confidence-intervals-for-the-population-mean.html"><a href="confidence-intervals-for-the-population-mean.html"><i class="fa fa-check"></i><b>3.4</b> Confidence intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="comparing-means-from-different-populations.html"><a href="comparing-means-from-different-populations.html"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="an-application-to-the-gender-gap-of-earnings.html"><a href="an-application-to-the-gender-gap-of-earnings.html"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="scatterplots-sample-covariance-and-sample-correlation.html"><a href="scatterplots-sample-covariance-and-sample-correlation.html"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="estimating-the-coefficients-of-the-linear-regression-model.html"><a href="estimating-the-coefficients-of-the-linear-regression-model.html"><i class="fa fa-check"></i><b>4.1</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="estimating-the-coefficients-of-the-linear-regression-model.html"><a href="estimating-the-coefficients-of-the-linear-regression-model.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="measures-of-fit.html"><a href="measures-of-fit.html"><i class="fa fa-check"></i><b>4.2</b> Measures of Fit</a><ul>
<li><a href="measures-of-fit.html#the-r2">The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#standard-error-of-the-regression"><i class="fa fa-check"></i>Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html"><i class="fa fa-check"></i><b>4.3</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption #1: The Error Term has Conditional Mean of Zero</a></li>
<li><a href="the-least-squares-assumptions.html#assumption-2-all-x_i-y_i-are-independently-and-identically-distributed">Assumption #2: All <span class="math inline">\((X_i, Y_i)\)</span> are Independently and Identically Distributed</a></li>
<li class="chapter" data-level="" data-path="the-least-squares-assumptions.html"><a href="the-least-squares-assumptions.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption #3: Large outliers are unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tsdotoe.html"><a href="tsdotoe.html"><i class="fa fa-check"></i><b>4.4</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-1"><i class="fa fa-check"></i>R Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-2"><i class="fa fa-check"></i>R Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#r-simulation-study-3"><i class="fa fa-check"></i>R Simulation Study 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="testing-two-sided-hypotheses-concerning-beta-1.html"><a href="testing-two-sided-hypotheses-concerning-beta-1.html"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="5.2" data-path="confidence-intervals-for-regression-coefficients.html"><a href="confidence-intervals-for-regression-coefficients.html"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="confidence-intervals-for-regression-coefficients.html"><a href="confidence-intervals-for-regression-coefficients.html#r-simulation-study-5.1"><i class="fa fa-check"></i>R Simulation Study 5.1</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regression-when-x-is-a-binary-variable.html"><a href="regression-when-x-is-a-binary-variable.html"><i class="fa fa-check"></i><b>5.3</b> Regression when <span class="math inline">\(X\)</span> is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html#r-simulation-study-blue-estimator"><i class="fa fa-check"></i>R Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><a href="using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><i class="fa fa-check"></i><b>5.6</b> Using the <span class="math inline">\(t\)</span>-Statistic in Regression When the Sample Size Is Small</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="the-multiple-regression-model.html"><a href="the-multiple-regression-model.html"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="measures-of-fit-in-multiple-regression.html"><a href="measures-of-fit-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="the-distribution-of-the-ols-estimators-in-multiple-regression.html"><a href="the-distribution-of-the-ols-estimators-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><a href="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="an-application-to-test-scores-and-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="an-application-to-test-scores-and-the-student-teacher-ratio.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="joint-hypothesis-testing-using-the-f-statistic.html"><a href="joint-hypothesis-testing-using-the-f-statistic.html"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the <span class="math inline">\(F\)</span>-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="confidence-sets-for-multiple-coefficients.html"><a href="confidence-sets-for-multiple-coefficients.html"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="model-specification-for-multiple-regression.html"><a href="model-specification-for-multiple-regression.html"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="model-specification-for-multiple-regression.html"><a href="model-specification-for-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="analysis-of-the-test-score-data-set.html"><a href="analysis-of-the-test-score-data-set.html"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="a-general-strategy-for-modeling-nonlinear-regression-functions.html"><a href="a-general-strategy-for-modeling-nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modeling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nonlinear-functions-of-a-single-independent-variable.html"><a href="nonlinear-functions-of-a-single-independent-variable.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="interactions-between-independent-variables.html"><a href="interactions-between-independent-variables.html"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><a href="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="assessing-studies-based-on-multiple-regression.html"><a href="assessing-studies-based-on-multiple-regression.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using R for Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tsdotoe" class="section level2">
<h2><span class="header-section-number">4.4</span> The Sampling Distribution of the OLS Estimator</h2>
<p>Because the OLS estimators <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are computed from a randomly drawn sample, the estimators themselves are random variables with a probability distribution — the so-called sampling distribution of the estimators — which describes the values they could take over different random samples. Although the sampling distribution of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> can be complicated when the sample size is small and generally differs with the number of observation, <span class="math inline">\(n\)</span>, it is possible to make certain statements about it that hold for all <span class="math inline">\(n\)</span>. In particular <span class="math display">\[ E(\hat{\beta_0}) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta_1}) = \beta_1,\]</span> that is, <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the true parameters. If the sample is sufficiently large, by the central limit theorem the <em>joint</em> sampling distribution of the estimators is well approximated by the bivariate normal distribution (<a href="#mjx-eqn-2.1">2.1</a>). This implies that the marginal distributions are also normal in large samples. Core facts on the large-sample distribution of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are presented in Key Concept 4.4.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.4
</h3>
<h3 class="left">
Large Sample Distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>
</h3>
<p>
<p>If the least squares assumptions in Key Concept 4.3 hold, then in large samples <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> have a jointly normal sampling distribution. The large sample normal distribution of <span class="math inline">\(\hat\beta_1\)</span> is <span class="math inline">\(N(\beta_1, \sigma^2_{\hat\beta_1})\)</span>, where the variance of the distribution, <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span>, is</p>
<p><span class="math display">\[ \sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2} \tag{4.1}. \]</span></p>
<p>The large sample normal distribution of <span class="math inline">\(\hat\beta_0\)</span> is <span class="math inline">\(N(\beta_0, \sigma^2_{\hat\beta_0})\)</span>, where</p>
<p><span class="math display">\[ \sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{4.2} \]</span></p>
</p>
</div>
<iframe height="470" width="800" frameborder="0" scrolling="no" src="SmallSampleDIstReg.html">
</iframe>
<div id="r-simulation-study-1" class="section level3 unnumbered">
<h3>R Simulation Study 1</h3>
<p>Whether Key Koncept 4.4 really holds can be verified using R. First we build our own population of <span class="math inline">\(100000\)</span> observations in total. To do this we need values for our independent variable <span class="math inline">\(X\)</span>, for the error term <span class="math inline">\(u\)</span>, and the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. With all this combined in a simple regression model, we can compute our dependent variable <span class="math inline">\(Y\)</span>. <br> In our example we generate the numbers <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i = 1\)</span>, … ,<span class="math inline">\(100000\)</span> by drawing a random sample from a uniform distribution on the interval <span class="math inline">\([0,20]\)</span>. The realisations of the error terms <span class="math inline">\(u_i\)</span> are drawn from a standard normal distribution with parameters <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 100\)</span> (note that <code>rnorm()</code> requires <span class="math inline">\(\sigma\)</span> as input for the argument <code>sd</code>, see <code>?rnorm</code>). Furthermore we chose <span class="math inline">\(\beta_0 = -2\)</span> and <span class="math inline">\(\beta_1 = 3.5\)</span> so the true model is</p>
<p><span class="math display">\[ Y_i = -2 + 3.5 \cdot X_i. \]</span></p>
<p>Finally, we store the results in a data.frame.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate data</span>
N &lt;-<span class="st"> </span><span class="dv">100000</span>
X &lt;-<span class="st"> </span><span class="kw">runif</span>(N, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">20</span>)
u &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">sd =</span> <span class="dv">10</span>)

<span class="co"># population regression</span>
Y &lt;-<span class="st"> </span><span class="op">-</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">3.5</span> <span class="op">*</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>u
population &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X, Y)</code></pre></div>
</div>
<p>From now on we will consider the previously generated data as the true population (which of course would be <em>unknown</em> in a real world application, otherwise there would not be a reason to do draw a random sample in the first place). The knowledge about the true population and the true relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> can be used to verify the statements made in Key Concept 4.4.</p>
<p>First, let us calculate the true variances <span class="math inline">\(\sigma^2_{\hat{\beta_0}}\)</span> and <span class="math inline">\(\sigma^2_{\hat{\beta_1}}\)</span> for a randomly drawn sample of size <span class="math inline">\(n = 100\)</span>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set sample size</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>

<span class="co"># compute the variance of hat_beta_0</span>
H_i &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X) <span class="op">/</span><span class="st"> </span><span class="kw">mean</span>(X<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>X
var_b0 &lt;-<span class="st"> </span><span class="kw">var</span>(H_i <span class="op">*</span><span class="st"> </span>u) <span class="op">/</span><span class="st"> </span>(n <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(H_i<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span> )

<span class="co"># compute the variance of hat_beta_1</span>
var_b1 &lt;-<span class="st"> </span><span class="kw">var</span>( ( X <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X) ) <span class="op">*</span><span class="st"> </span>u ) <span class="op">/</span><span class="st"> </span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="kw">var</span>(X)<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print variances to the console</span>
var_b0</code></pre></div>
<pre><code>## [1] 4.045066</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">var_b1</code></pre></div>
<pre><code>## [1] 0.03018694</code></pre>
<p>Now let us assume that we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and that it is not possible to observe the whole population. However, we can observe a random sample of <span class="math inline">\(n\)</span> observations. Then, it would not be possible to compute the true parameters but we could obtain estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> from the sample data using OLS. However, we know that these estimates are outcomes of random variables themselves since the observations are randomly sampled from the population. Key Concept 4.4. describes their distributions for large <span class="math inline">\(n\)</span>. When drawing a single sample of size <span class="math inline">\(n\)</span> it is not possible to make any statement about these distributions. Things change if we repeat the sampling scheme many times and compute the estimates for each sample: using such a procedure we simulate outcomes of the respective distributions.</p>
<p>To achieve this in R, we employ the following approach:</p>
<ul>
<li>We assign the number of repetitions, say <span class="math inline">\(10000\)</span>, to <code>reps</code>. Then we initialize a matrix <code>fit</code> were the estimates obtained in each sampling iteration shall be stored row-wise. Thus <code>fit</code> has to be an array of dimensions <code>reps</code><span class="math inline">\(\times2\)</span>.</li>
<li>In the next step we draw <code>reps</code> random sample of size <code>n</code> from the population and obtain the OLS estimates for each sample. The results are stored as row entries in the outcome matrix <code>fit</code>. This is done using a <code>for()</code> loop.</li>
<li>At last, we estimate variances of both coefficient estimators using the sampled outcomes and plot histograms of the latter. We also add plot of the density functions belonging to the distributions that follow from Key Concept 4.4. The function <code>bquote()</code> is used to obtain math expressions in the titels and labels of both plots. See <code>?bquote</code>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set repetitions and sample size</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>
reps &lt;-<span class="st"> </span><span class="dv">10000</span>

<span class="co"># initialize the matrix of outcomes</span>
fit &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> reps)

<span class="co"># loop sampling and estimating of the coefficients</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps){
 sample &lt;-<span class="st"> </span>population[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, n),]
 fit[i, ] &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> sample)<span class="op">$</span>coefficients
}

<span class="co"># compute variance estimates using outcomes</span>
<span class="kw">var</span>(fit[ ,<span class="dv">1</span>])</code></pre></div>
<pre><code>## [1] 4.057089</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(fit[ ,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 0.03021784</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot histograms of beta_0 estimates</span>
<span class="kw">hist</span>(fit[ ,<span class="dv">1</span>], 
     <span class="dt">main =</span> <span class="kw">bquote</span>(The <span class="op">~</span><span class="st"> </span>Distribution  <span class="op">~</span><span class="st"> </span>of <span class="op">~</span><span class="st"> </span><span class="dv">10000</span> <span class="op">~</span><span class="st"> </span>beta[<span class="dv">0</span>] <span class="op">~</span><span class="st"> </span>Estimates), 
     <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">0</span>]), 
     <span class="dt">freq =</span> F)
<span class="co"># add true distribution to plot</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x,<span class="op">-</span><span class="dv">2</span>,<span class="kw">sqrt</span>(var_b0)), <span class="dt">add =</span> T, <span class="dt">col=</span><span class="st">&quot;darkred&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-92-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot histograms of beta_1 estimates</span>
<span class="kw">hist</span>(fit[ ,<span class="dv">2</span>], 
     <span class="dt">main =</span> <span class="kw">bquote</span>(The <span class="op">~</span><span class="st"> </span>Distribution  <span class="op">~</span><span class="st"> </span>of <span class="op">~</span><span class="st"> </span><span class="dv">10000</span> <span class="op">~</span><span class="st"> </span>beta[<span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>Estimates), 
     <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]), 
     <span class="dt">freq =</span> F)

<span class="co"># add true distribution to plot</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x,<span class="fl">3.5</span>,<span class="kw">sqrt</span>(var_b1)), <span class="dt">add =</span> T, <span class="dt">col=</span><span class="st">&quot;darkred&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-92-2.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We are now able to say the following: first, our variance estimates are in favour of the claims made in Key Concept 4.4 since they come close to the computed theoretical values. Second, the histograms suggest that the estimators distributions indeed follow normal distributions which can be fairly approximated by the respective normal distributions stated in Key Concept 4.4.</p>
</div>
<div id="r-simulation-study-2" class="section level3 unnumbered">
<h3>R Simulation Study 2</h3>
<p>A further result implied by Key Concept 4.4 is that both estimators are consistent i.e. they converge in probability to their true value. This is since their variances converge to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\)</span> increases. We can check this by repeating the simulation above for an increasing sequence of sample sizes. This means we no langer assign the sample size but a <em>vector</em> of sample sizes: <code>n &lt;- c(...)</code>. <br> Let us look at the distributions of <span class="math inline">\(\beta_1\)</span>. The idea here is to add an additional call of <code>for()</code> to the code. This is done in order to loop over the vector of sample sizes <code>n</code>. For each of the sample sizes we carry out the same simulation as before but plot a density estimate for the outcomes of each iteration over <code>n</code>. Notice that we have to change <code>n</code> to <code>n[j]</code> in the inner loop to ensure that the <code>j</code><span class="math inline">\(^{th}\)</span> element of <code>n</code> is used. In the simulation, we use sample sizes <span class="math inline">\(100, 250, 1000\)</span> and <span class="math inline">\(3000\)</span>. Consequently we have a total of four distinct simulations using different sample sizes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># set repetitions and the vector of sample sizes</span>
reps &lt;-<span class="st"> </span><span class="dv">1000</span>
n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>, <span class="dv">250</span>, <span class="dv">1000</span>, <span class="dv">3000</span>)

<span class="co"># initialize the matrix of outcomes</span>
fit &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> reps)

<span class="co"># devide the plot panel in a 2-by-2 array</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))

#### Loop sampling and plotting ####

<span class="co"># outer loop over n</span>
<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n)) {
  
  <span class="co"># inner loop: sampling and estimating of the coefficients</span>
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps){
    sample &lt;-<span class="st"> </span>population[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, n[j]), ]
    fit[i, ] &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> sample)<span class="op">$</span>coefficients
  }
  
  <span class="co"># draw density estimates</span>
  <span class="kw">plot</span>(<span class="kw">density</span>(fit[,<span class="dv">2</span>]), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">2.5</span>,<span class="fl">4.5</span>), <span class="dt">col=</span>j, 
       <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;n=&quot;</span>, n[j]), <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]))
  
}</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-93-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We find that, as <span class="math inline">\(n\)</span> increases, the distribution of <span class="math inline">\(\hat\beta_1\)</span> concentrates around its mean, i.e. its variance decreases. Put differently, the likelihood of observering estimates close to the true value of <span class="math inline">\(\beta_1 = 3.5\)</span> grows as we increase the sample size. The same behaviour could be observed if we would analyze the distribution of <span class="math inline">\(\hat\beta_0\)</span> instead.</p>
</div>
<div id="r-simulation-study-3" class="section level3 unnumbered">
<h3>R Simulation Study 3</h3>
<p>Furthermore, (<a href="#mjx-eqn-4.1">4.1</a>) reveals that the variance of the OLS estimator for <span class="math inline">\(\beta_1\)</span> decreases as the variance of the <span class="math inline">\(X_i\)</span> increases. In other words, as we increase the amount of information provided by the regressor, that is increasing <span class="math inline">\(Var(X)\)</span>, which is used to estimate <span class="math inline">\(\beta_1\)</span>, we are more confident that the estimate is close to the true value (i.e. <span class="math inline">\(Var(\hat\beta_1)\)</span> decreases).<br> We can visualize this by reproducing figure 4.6 from the book. To do this, we sample <span class="math inline">\(100\)</span> observations <span class="math inline">\((X,Y)\)</span> from a bivariate normal distribution with</p>
<p><span class="math display">\[E(X)=E(Y)=5,\]</span> <span class="math display">\[Var(X)=Var(Y)=5\]</span> and <span class="math display">\[Cov(X,Y)=4.\]</span></p>
<p>Formally, this is written down as</p>
<span class="math display">\[\begin{align}
  \begin{pmatrix}
    X \\
    Y \\
  \end{pmatrix}
  \overset{i.i.d.}{\sim} &amp; \ \mathcal{N} 
  \left[
    \begin{pmatrix}
      5 \\
      5 \\
    \end{pmatrix}, \ 
    \begin{pmatrix}
      5 &amp; 4 \\
      4 &amp; 5 \\
    \end{pmatrix}
  \right]. \tag{4.3}
\end{align}\]</span>
<p>To carry out the random sampling, we make use of the function <code>mvtnorm()</code> from the package <code>MASS</code> which allows to draw random samples from multivariate normal distributions, see <code>?mvtnorm</code>. Next, we use the <code>subset()</code> function to split the sample into two subsets such that the first set, <code>set1</code>, consists of observations that fulfill the condition <span class="math inline">\(\lvert X - \overline{X} \rvert &gt; 1\)</span> and the second set, <code>set2</code>, includes the remainder of the sample. We then plot both sets and use different colors to make them distinguishable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the MASS package</span>
<span class="kw">library</span>(MASS)

<span class="co"># set random seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">4</span>)

<span class="co"># simulate bivarite normal data</span>
bvndata &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">100</span>, 
                <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>), 
                <span class="dt">Sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">4</span>),<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">5</span>))
                ) 

<span class="co"># assign column names / convert to data.frame</span>
<span class="kw">colnames</span>(bvndata) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>,<span class="st">&quot;Y&quot;</span>)
bvndata &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(bvndata)

<span class="co"># subset the data</span>
set1 &lt;-<span class="st"> </span><span class="kw">subset</span>(bvndata, <span class="kw">abs</span>(<span class="kw">mean</span>(X) <span class="op">-</span><span class="st"> </span>X) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>)
set2 &lt;-<span class="st"> </span><span class="kw">subset</span>(bvndata, <span class="kw">abs</span>(<span class="kw">mean</span>(X) <span class="op">-</span><span class="st"> </span>X) <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>)

<span class="co"># plot both data sets</span>
<span class="kw">plot</span>(set1, <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>)
<span class="kw">points</span>(set2, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-94-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It is clear that observations that are close to the sample average of the <span class="math inline">\(X_i\)</span> have less variance than those that are farther away. Now, if we were to draw a line as accurately as possible through either of the two sets it is obvious that choosing the observations indicated by the black dots, i.e. using the set of observations which has larger variance than the blue ones, would result in a more precise line. Now, let us use OLS to estimate and draw the regression lines for both sets of observations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate both regression lines</span>
lm.set1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> set1)
lm.set2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> set2)

<span class="co"># add both lines to the plot</span>
<span class="kw">abline</span>(lm.set1, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>)
<span class="kw">abline</span>(lm.set2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-95-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Evidently, the green regression line does far better in describing data sampled from the bivariate normal distribution stated in (<a href="#mjx-eqn-4.3">4.3</a>) than the red line. This is a nice example why we are interested in a high variance of the regressor <span class="math inline">\(X\)</span>: more variance in the <span class="math inline">\(X_i\)</span> means more information from which the precision of the estimation benefits.</p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="the-least-squares-assumptions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-ch4.Rmd",
"text": "Edit"
},
"download": ["URFITE.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
