<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Using R for Introduction to Econometrics</title>
  <meta name="description" content="Using R for Introduction to Econometrics">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Using R for Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Using R for Introduction to Econometrics" />
  
  
  

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2017-11-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="lrwor.html">
<link rel="next" href="regression-models-with-multiple-regressors.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="MathJax-2.7.2/MathJax.js?config=default"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/SVG"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- <script src="js/d3.v3.min.js"></script> 
<script src="js/leastsquares.js"></script>
<script src="js/d3functions.js"></script>
<script src="d3.slider.js"></script>
<script src="slrm.js"></script> -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">URFITE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i>Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#expected-values-mean-and-variance"><i class="fa fa-check"></i>Expected Values, Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li><a href="probability-theory.html#thetdist">The Student <span class="math inline">\(t\)</span> Distribution</a></li>
<li><a href="probability-theory.html#the-f-distribution">The <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#random-sampling-and-the-distribution-of-sample-averages"><i class="fa fa-check"></i>Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#estimation-of-the-population-mean"><i class="fa fa-check"></i>Estimation of the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#properties-of-the-population-mean"><i class="fa fa-check"></i>Properties of the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i>Hypothesis Tests Concerning the Population Mean</a><ul>
<li><a href="a-review-of-statistics-using-r.html#p-value"><span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="a-review-of-statistics-using-r.html#calculating-the-p-value-when-sigma_y-is-known">Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#sample-variance-sample-standard-deviation-and-standard-error"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li><a href="a-review-of-statistics-using-r.html#calculating-the-p-value-when-sigma_y-is-unknown">Calculating the <span class="math inline">\(p\)</span>-value When <span class="math inline">\(\sigma_Y\)</span> is Unknown</a></li>
<li><a href="a-review-of-statistics-using-r.html#the-t-statistic">The <span class="math inline">\(t\)</span>-statistic</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i>Confidence intervals for the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#comparing-means-from-different-populations"><i class="fa fa-check"></i>Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#an-application-to-the-gender-gap-of-earnings"><i class="fa fa-check"></i>An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i>Scatterplots, Sample Covariance and Sample Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i>Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i>Measures of fit</a><ul>
<li><a href="lrwor.html#the-r2">The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#standard-error-of-the-regression"><i class="fa fa-check"></i>Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-least-squares-assumptions"><i class="fa fa-check"></i>The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption #1: The Error Term has Conditional Mean of Zero</a></li>
<li><a href="lrwor.html#assumption-2-all-x_i-y_i-are-independently-and-identically-distributed">Assumption #2: All <span class="math inline">\((X_i, Y_i)\)</span> are Independently and Identically Distributed</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption #3: Large outliers are unlikely</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i>The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-1"><i class="fa fa-check"></i>R Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-2"><i class="fa fa-check"></i>R Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-3"><i class="fa fa-check"></i>R Simulation Study 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#testing-two-sided-hypotheses-concerning-beta_1">Testing Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#confidence-intervals-for-regression-coefficients"><i class="fa fa-check"></i>Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#r-simulation-study-5.1"><i class="fa fa-check"></i>R Simulation Study 5.1</a></li>
</ul></li>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#regression-when-x-is-a-binary-variable">Regression when <span class="math inline">\(X\)</span> is a Binary Variable</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#heteroskedasticity-and-homoskedasticity"><i class="fa fa-check"></i>Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#the-gauss-markov-theorem"><i class="fa fa-check"></i>The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#r-simulation-study-blue-estimator"><i class="fa fa-check"></i>R Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small">Using the <span class="math inline">\(t\)</span>-Statistic in Regression When the Sample Size Is Small</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#omitted-variable-bias"><i class="fa fa-check"></i>Omitted Variable Bias</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#the-multiple-regression-model"><i class="fa fa-check"></i>The Multiple Regression Model</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#measures-of-fit-in-multiple-regression"><i class="fa fa-check"></i>Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i>OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i>The Distribution of the OLS Estimators in Multiple Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the <span class="math inline">\(F\)</span>-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="8.1" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.1</b> Interactions Between Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using R for Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model" class="section level1">
<h1><span class="header-section-number">5</span> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</h1>
<p>In this chapter, we continue with the treatment of the simple linear regression model. The following subsction discuss how we may use our knowledge about the sampling distribution of the OLS estimator in order to make statements regarding its uncertainty. <br> These subsections cover the following topics:</p>
<ul>
<li><p>Testing Hypotheses about regression coefficients</p></li>
<li><p>Confidence intervals for regression coefficients</p></li>
<li><p>Regression when <span class="math inline">\(X\)</span> is a dummy variable</p></li>
<li><p>Heteroskedasticity and Homoskedasticity</p></li>
</ul>
<div id="testing-two-sided-hypotheses-concerning-beta_1" class="section level2 unnumbered">
<h2>Testing Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></h2>
<p>Using the fact that <span class="math inline">\(\hat{\beta}_1\)</span> is approximately normal distributed in large samples (see <a href="lrwor.html#tsdotoe">Key Concept 4.4</a>), testing hypothesis about the true value <span class="math inline">\(\beta_1\)</span> can be done with the same approach as discussed in chapter 3.2.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.1
</h3>
<h3 class="left">
General Form of the <span class="math inline">\(t\)</span>-Statistic
</h3>
<p>Remember from chapter 3 that a general <span class="math inline">\(t\)</span>-statistic has the form</p>
<p><span class="math display">\[
  t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.
\]</span></p>
</div>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.2
</h3>
<h3 class="left">
Testing Hyothesis about <span class="math inline">\(\beta_1\)</span>
</h3>
<p>For testing the hypothesis <span class="math inline">\(H_0: \beta_1 = \beta_{1,0}\)</span>, we need to perform the following steps:</p>
<ol style="list-style-type: decimal">
<li>Compute the standard error of <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(SE(\hat{\beta}_1)\)</span></li>
</ol>
<p><span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Compute the <span class="math inline">\(t\)</span>-statistic</li>
</ol>
<p><span class="math display">\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) }. \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Now, given a two sided alternative (<span class="math inline">\(H_1:\beta_1 \neq \beta_{1,0}\)</span>) we reject at the <span class="math inline">\(5\%\)</span> level if <span class="math inline">\(|t^{act}| &gt; 1.96\)</span> or, equivalently, if the <span class="math inline">\(p\)</span>-value is less than <span class="math inline">\(0.05\)</span>.<br><br />
Recall the definition of the <span class="math inline">\(p\)</span>-value:</li>
</ol>
<span class="math display">\[\begin{align}
    p \text{-value} =&amp; \, \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| &gt; \left|        \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] \\
    =&amp; \, \text{Pr}_{H_0} (|t| &gt; |t^{act}|) \\
    =&amp; \, 2 \cdot \Phi(-|t^{act}|)
  \end{align}\]</span>
<p>       The last equality holds due to the normal approximation for large samples.</p>
</div>
<p>Consider again the OLS regression stored in <code>linear_model</code> from Chapter 4 that gave us the regression line</p>
<p><span class="math display">\[ \widehat{TestScore} \ = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times STR \ , \ R^2=0.051 \ , \ SER=18.6. \]</span></p>
<p>For testing a hypothesis about the slope parameter (the coefficient on <span class="math inline">\(STR\)</span>), we need <span class="math inline">\(SE(\hat{\beta}_1)\)</span>, the standard error of the respective point estimator. As common in the literature, standard errors are presented in parantheses below the point estimates.</p>
<p>As can be witnessed in Key Concept 5.1 it is rather cumbersome to compute the standard error and thus the <span class="math inline">\(t\)</span>-statistic by hand. The question You should be asking Yourself right now is obvious: can we obtain these values with minimum effort using R? Yes, we can. Let us first use <code>summary()</code> to get a summary on the estimated coefficients in <code>linear_model</code>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print the summary of coefficients to the console</span>
<span class="kw">summary</span>(linear_model)<span class="op">$</span>coefficients</code></pre></div>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## STR          -2.279808  0.4798255 -4.751327  2.783308e-06</code></pre>
</div>
<p>When looking at the second column of the coefficients’ summary, we discover values for <span class="math inline">\(SE(\hat\beta_0)\)</span> and <span class="math inline">\(SE(\hat\beta_1)\)</span>. Also, in the third column, named <code>t value</code>, we find <span class="math inline">\(t\)</span>-statistics <span class="math inline">\(t^{act}\)</span> suitable for tests of the individual hypotheses <span class="math inline">\(H_0: \beta_0=0\)</span> and <span class="math inline">\(H_0: \beta_1=0\)</span>. Furthermore, the output provides us with <span class="math inline">\(p\)</span>-values corresponding to both tests against the two-sided alternatives <span class="math inline">\(H_1:\beta_0\neq0\)</span> respectively <span class="math inline">\(H_1:\beta_1\neq0\)</span> in the fourth column of the table.</p>
<p>Let us have a closer look at the test of</p>
<p><span class="math display">\[H_0: \beta_1=0 \ \ \ vs. \ \ \ H_1: \beta_1 \neq 0.\]</span></p>
<p>Using our revisited knowledge about <span class="math inline">\(t\)</span>-statistics we find that</p>
<p><span class="math display">\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75. \]</span></p>
<p>What does this tell us about the significance of the estimated coefficient? We reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance since <span class="math inline">\(|t^{act}| &gt; 1.96\)</span> that is the observed test statistic falls into the region of rejection. Or, alternatively and leading to the same result, we have <span class="math inline">\(p\text{-value} = 2.78*10^{-6} &lt; 0.05\)</span>. We conclude that the coefficient is significantly different from zero. With other words, our analysis provides evidence that the clase size <em>has an influence</em> on the students test scores. We say that <span class="math inline">\(\beta_1\)</span> is significantly different from <span class="math inline">\(0\)</span> at the level of <span class="math inline">\(5\%\)</span>.</p>
<p>Note that, although the difference is negligible in the present case as we will see later, <code>summary()</code> does not perform the normal approximation but calculates <span class="math inline">\(p\)</span>-values using the appropriate <span class="math inline">\(t\)</span>-distribution instead. Generally, the degrees of freedom are determined in the following manner:</p>
<p><span class="math display">\[ \text{DF} = n - k - 1 \]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations used to estimate the model and <span class="math inline">\(k\)</span> is the number of regressors, excluding the intercept. In our case, we have <span class="math inline">\(n=420\)</span> observations and the only regressor is <span class="math inline">\(STR\)</span> so <span class="math inline">\(k=1\)</span>. A sleek way to determine the model degress of freedom using R is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># determine degrees of freedom</span>
linear_model<span class="op">$</span>df.residual</code></pre></div>
<pre><code>## [1] 418</code></pre>
<p>Hence, for the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> we have</p>
<p><span class="math display">\[ \hat\beta_1 \sim t_{418}\]</span> such that the <span class="math inline">\(p\)</span>-value for a two-sided significance test can be obtained by executing the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pt</span>(<span class="op">-</span><span class="fl">4.751327</span>, <span class="dt">df =</span> <span class="dv">418</span>)</code></pre></div>
<pre><code>## [1] 2.78331e-06</code></pre>
<p>The result is very close to the value provided by <code>summary</code>. However since <span class="math inline">\(n\)</span> is sufficiently large one could just as well use the standard normal density to compute the <span class="math inline">\(p\)</span>-value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="fl">4.751327</span>)</code></pre></div>
<pre><code>## [1] 2.02086e-06</code></pre>
<p>The difference is indeed negligible. These findings tell us that, if <span class="math inline">\(H_0: \beta_1 = 0\)</span> is true and we were to repeat the whole process of gathering observations and estimating the model, chances of observing a <span class="math inline">\(\hat\beta_1 \geq |-4.75|\)</span> are roughly <span class="math inline">\(1:359285\)</span> — so higher chances than winning the lottory next saturday but still very unlikely!</p>
<p>Using R we may visualise how such a statement is made when using the normal approximation. This reflects the principles depicted in figure 5.1 in the book. Do not let the following code chunk deter You: the code is somewhat longer than the usual examples and looks unappealing but there is <strong>a lot</strong> of repetition since color shadings and annotations are added on both tails of the normal distribution. We recommend You to execute the code step by step in order to see how the graph is augmented with the annotations.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the standard normal on the domain [-6,6]</span>
t &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>,<span class="dv">6</span>,<span class="fl">0.01</span>)

<span class="kw">plot</span>(<span class="dt">x =</span> t, 
     <span class="dt">y =</span> <span class="kw">dnorm</span>(t, <span class="dv">0</span>, <span class="dv">1</span>), 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">yaxs =</span> <span class="st">&quot;i&quot;</span>, 
     <span class="dt">axes =</span> F, 
     <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, 
     <span class="dt">main =</span> <span class="st">&quot;Calculating the p-Value of a Two-Sided Test When t^act = -4.75&quot;</span>, 
     <span class="dt">cex.lab =</span> <span class="fl">0.7</span>
     )

tact &lt;-<span class="st"> </span><span class="op">-</span><span class="fl">4.75</span>

<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="op">-</span><span class="fl">1.96</span>,<span class="fl">1.96</span>,<span class="op">-</span>tact,tact), <span class="dt">cex.axis=</span><span class="fl">0.7</span>)

<span class="co"># Shade the critical regions using polygon()</span>

## critical region in left tail
<span class="kw">polygon</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">6</span>, <span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>,<span class="op">-</span><span class="fl">1.96</span>,<span class="fl">0.01</span>),<span class="op">-</span><span class="fl">1.96</span>),
        <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>,<span class="op">-</span><span class="fl">1.96</span>,<span class="fl">0.01</span>)),<span class="dv">0</span>), 
        <span class="dt">col =</span> <span class="st">&#39;orange&#39;</span>
        )

## critical region in right tail

<span class="kw">polygon</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">1.96</span>, <span class="kw">seq</span>(<span class="fl">1.96</span>, <span class="dv">6</span>, <span class="fl">0.01</span>), <span class="dv">6</span>),
        <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="fl">1.96</span>, <span class="dv">6</span>, <span class="fl">0.01</span>)), <span class="dv">0</span>), 
        <span class="dt">col =</span> <span class="st">&#39;orange&#39;</span>
        )

<span class="co"># Add arrows and texts indicating critical regions and the p-value</span>
<span class="kw">arrows</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">0.2</span>, <span class="op">-</span><span class="fl">2.5</span>, <span class="fl">0.02</span>, <span class="dt">length =</span> <span class="fl">0.1</span>)
<span class="kw">arrows</span>(<span class="fl">3.5</span>, <span class="fl">0.2</span>, <span class="fl">2.5</span>, <span class="fl">0.02</span>, <span class="dt">length =</span> <span class="fl">0.1</span>)

<span class="kw">arrows</span>(<span class="op">-</span><span class="dv">5</span>, <span class="fl">0.16</span>, <span class="op">-</span><span class="fl">4.75</span>, <span class="dv">0</span>, <span class="dt">length =</span> <span class="fl">0.1</span>)
<span class="kw">arrows</span>(<span class="dv">5</span>, <span class="fl">0.16</span>, <span class="fl">4.75</span>, <span class="dv">0</span>, <span class="dt">length =</span> <span class="fl">0.1</span>)

<span class="kw">text</span>(<span class="op">-</span><span class="fl">3.5</span>,<span class="fl">0.22</span>, <span class="dt">labels =</span> <span class="kw">paste</span>(<span class="st">&quot;0.025=&quot;</span>,<span class="kw">expression</span>(alpha),<span class="st">&quot;/2&quot;</span>,<span class="dt">sep =</span> <span class="st">&quot;&quot;</span>), <span class="dt">cex =</span> <span class="fl">0.7</span>)
<span class="kw">text</span>(<span class="fl">3.5</span>,<span class="fl">0.22</span>, <span class="dt">labels =</span> <span class="kw">paste</span>(<span class="st">&quot;0.025=&quot;</span>,<span class="kw">expression</span>(alpha),<span class="st">&quot;/2&quot;</span>,<span class="dt">sep =</span> <span class="st">&quot;&quot;</span>), <span class="dt">cex =</span> <span class="fl">0.7</span>)

<span class="kw">text</span>(<span class="op">-</span><span class="dv">5</span>,<span class="fl">0.18</span>, <span class="dt">labels =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;-|&quot;</span>,t[act],<span class="st">&quot;|&quot;</span>)), <span class="dt">cex =</span> <span class="fl">0.7</span>)
<span class="kw">text</span>(<span class="dv">5</span>,<span class="fl">0.18</span>, <span class="dt">labels =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;|&quot;</span>,t[act],<span class="st">&quot;|&quot;</span>)), <span class="dt">cex =</span> <span class="fl">0.7</span>)

<span class="co"># Add ticks indicating critical values at the 0.05-level, t^act and -t^act </span>
<span class="kw">rug</span>(<span class="kw">c</span>(<span class="op">-</span><span class="fl">1.96</span>,<span class="fl">1.96</span>), <span class="dt">ticksize  =</span> <span class="fl">0.145</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">rug</span>(<span class="kw">c</span>(<span class="op">-</span>tact,tact), <span class="dt">ticksize  =</span> <span class="op">-</span><span class="fl">0.0451</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-100-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The <span class="math inline">\(p\)</span>-Value is the area under the curve to left of <span class="math inline">\(-4.75\)</span> plus the area under the curve to the right of <span class="math inline">\(4.75\)</span>. As we already know from the calculations above, this value is very small.</p>
</div>
<div id="confidence-intervals-for-regression-coefficients" class="section level2 unnumbered">
<h2>Confidence Intervals for Regression Coefficients</h2>
<p>As we already know, estimates of the regression coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are afflicted with sampling uncertainty, see chapter 4. Therefore, we will <em>never</em> estimate the exact true value of these parameters from sample data in an empirical application. However, we may construct confidence intervals for the intercept and the slope parameter.</p>
<p>A <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(\beta_i\)</span> has two equivalent definitions:</p>
<ul>
<li>The interval is the set of values for which a hypothesis test to the level of <span class="math inline">\(5\%\)</span> cannot be rejected.</li>
<li>The interval has a probability of <span class="math inline">\(95\%\)</span> to contain the true value of <span class="math inline">\(\beta_i\)</span>. So in <span class="math inline">\(95\%\)</span> of all samples that could be drawn, the confidence interval will cover the true value of <span class="math inline">\(\beta_i\)</span>.</li>
</ul>
<p>We also say that the interval has a confidence level of <span class="math inline">\(95\%\)</span>. The idea is summarized in Key Concept 5.3.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.3
</h3>
<h3 class="left">
A Confidence Interval for <span class="math inline">\(\beta_i\)</span>
</h3>
<p>Imagine You could draw all possible random samples of given size. The interval that contains the true value <span class="math inline">\(\beta_i\)</span> in <span class="math inline">\(95\%\)</span> of all samples is given by the expression</p>
<p><span class="math display">\[ \text{KI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]</span></p>
<p>Equivalently, this interval can be seen as the set of null hypotheses for which a <span class="math inline">\(5\%\)</span> two-sided hypothesis test does not reject.</p>
</div>
<div id="r-simulation-study-5.1" class="section level3 unnumbered">
<h3>R Simulation Study 5.1</h3>
<p>To get a better understanding of confidence intervalls we will conduct another simulation study. For now, assume that we are confronted with the following sample of <span class="math inline">\(n=100\)</span> observations on a single variable <span class="math inline">\(Y\)</span> where</p>
<p><span class="math display">\[ Y_i \overset{i.i.d}{\sim} N(5,25) \ \ \forall \ i = 1, \dots, 100.\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">4</span>)

<span class="co"># generate and plot the sample data</span>
Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, 
           <span class="dt">mean =</span> <span class="dv">5</span>, 
           <span class="dt">sd =</span><span class="dv">5</span>
           )

<span class="kw">plot</span>(Y, 
     <span class="dt">pch=</span><span class="dv">19</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>
     )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-101-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We assume that the data is generated by the model</p>
<p><span class="math display">\[ Y_i = \mu + \epsilon_i \]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the unknown constant and we know that <span class="math inline">\(\epsilon_i \overset{i.i.d.}{\sim} N(0,25)\)</span>. In this model, the OLS estimator for <span class="math inline">\(\mu\)</span> is given by</p>
<p><span class="math display">\[ \hat\mu = \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i \]</span></p>
<p>(try to verify this!) i.e. the sample average of the <span class="math inline">\(Y_i\)</span>. It further holds that</p>
<p><span class="math display">\[ SE(\hat\mu) = \frac{\sigma_{\epsilon}}{\sqrt{n}} = \frac{5}{\sqrt{100}}. \]</span></p>
<p>A large sample <span class="math inline">\(95\%\)</span> confidence intervall for <span class="math inline">\(\mu\)</span> is then given by</p>
<span class="math display" id="eq:KI">\[\begin{equation} 
KI^{\mu}_{0.95} = \left[\hat\mu - 1.96 \times \frac{5}{\sqrt{100}} \ , \ \hat\mu + 1.96 \times \frac{5}{\sqrt{100}}  \right]. \tag{5.1}
\end{equation}\]</span>
<p>It is fairly easy to compute this interval in R by hand. The following code chunck generates a named vector containing the interval bounds:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cbind</span>(
  <span class="dt">CIlower =</span> <span class="kw">mean</span>(Y) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span><span class="op">/</span><span class="dv">10</span>, 
  <span class="dt">CIupper =</span> <span class="kw">mean</span>(Y) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span><span class="op">/</span><span class="dv">10</span> 
)</code></pre></div>
<pre><code>##       CIlower  CIupper
## [1,] 4.502625 6.462625</code></pre>
<p>Nowing that <span class="math inline">\(\mu = 5\)</span> we see that our example covers the true value for the present sample. <br> As opposed to real world examples, we can use R to get a better understanding of confidence intervals by repeatedly sampling data, estimating <span class="math inline">\(\mu\)</span> and computing the confidence interval for <span class="math inline">\(\mu\)</span> as in <a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#eq:KI">(5.1)</a>.</p>
<p>The procedure is as follows:</p>
<ul>
<li>We initialize the vectors <code>lower</code> and <code>upper</code> in which the simulated interval boundaries are to be saved. We want to simulate <span class="math inline">\(10000\)</span> intervals so both vectors are set to have this length.</li>
<li>We use a <code>for()</code> loop to sample <span class="math inline">\(100\)</span> observations from the <span class="math inline">\(N(5,25)\)</span> distribution and compute <span class="math inline">\(\hat\mu\)</span> as well as the boundaries of the confidence interval in every iteration of the loop.</li>
<li>At last we join <code>lower</code> and <code>upper</code> in an array.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># initialize vectors of lower and upper interval boundaries</span>
lower &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)
upper &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)

<span class="co"># loop sampling / estimation / CI</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {
  Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">5</span>, <span class="dt">sd =</span><span class="dv">5</span>)
  lower[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(Y) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span><span class="op">/</span><span class="dv">10</span>
  upper[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(Y) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span><span class="op">/</span><span class="dv">10</span>
}

<span class="co"># join vectors of interval boundaries</span>
CIs &lt;-<span class="st"> </span><span class="kw">cbind</span>(lower, upper)</code></pre></div>
<p>According to Key Concept 5.3 we expect that the fraction of the <span class="math inline">\(10000\)</span> simulated intervals saved in the array <code>CIs</code> that contain the true value <span class="math inline">\(\mu=5\)</span> should be roughly <span class="math inline">\(95\%\)</span>. We can check this using logical operators.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(CIs[,<span class="dv">1</span>] <span class="op">&lt;=</span><span class="st"> </span><span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span><span class="dv">5</span> <span class="op">&lt;=</span><span class="st"> </span>CIs[,<span class="dv">2</span>])<span class="op">/</span><span class="dv">10000</span></code></pre></div>
<pre><code>## [1] 0.9487</code></pre>
<p>The simulation shows that the fraction of intervals covering <span class="math inline">\(\mu=5\)</span>, i.e. those intervals for which <span class="math inline">\(H_0: \mu = 5\)</span> cannot be rejected is close to the theoretical value of <span class="math inline">\(95%\)</span>.</p>
<p>Let us draw a plot of the first <span class="math inline">\(100\)</span> simulated confidence intervals and indicate those which <em>do not</em> cover the true value of <span class="math inline">\(\mu\)</span>. We do this by adding horizonal lines representing the confidence intervals on top of each other.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># identify intervals not covering mu</span>
<span class="co"># (4 intervals out of 100)</span>
ID &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="op">!</span>(CIs[<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>,<span class="dv">1</span>] <span class="op">&lt;=</span><span class="st"> </span><span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span><span class="dv">5</span> <span class="op">&lt;=</span><span class="st"> </span>CIs[<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>,<span class="dv">2</span>]))

<span class="co"># initialize the plot</span>
<span class="kw">plot</span>(<span class="dv">0</span>, 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">7</span>), 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">100</span>), 
     <span class="dt">ylab =</span> <span class="st">&quot;Sample&quot;</span>, 
     <span class="dt">xlab =</span> <span class="kw">expression</span>(mu), 
     <span class="dt">main =</span> <span class="st">&quot;Confidence Intervals: Correct H0&quot;</span>)

<span class="co"># setup color vector</span>
colors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">gray</span>(<span class="fl">0.6</span>), <span class="dv">100</span>)
colors[ID] &lt;-<span class="st"> &quot;red&quot;</span>

<span class="co"># draw reference line at mu=5</span>
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">5</span>, <span class="dt">lty=</span><span class="dv">2</span>)

<span class="co"># add horizontal bars representing the CIs</span>
<span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  <span class="kw">lines</span>(<span class="kw">c</span>(CIs[j,<span class="dv">1</span>], CIs[j,<span class="dv">2</span>]), 
        <span class="kw">c</span>(j,j), 
        <span class="dt">col =</span> colors[j], 
        <span class="dt">lwd=</span><span class="dv">2</span>)
}</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-105-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>We find that for the first 100 samples, the true null hypthesis is rejected in four cases so these intervals do not cover <span class="math inline">\(\mu=5\)</span>. We have indicated the intervals which lead to a rejection of the true null hypothesis by red color.</p>
<p>Let us now turn back to the example of test scores and class sizes.The regression model from chapter 4 is stored in <code>linear_model</code>. An easy way to get <span class="math inline">\(95\%\)</span> confidence intervals for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the coefficients on <code>(intercept)</code> and <code>STR</code>, is to use the function <code>confint()</code>. We only have to provide a fitted model object as the argument <code>object</code> to this function. The confidence level is set to <span class="math inline">\(95\%\)</span> by default but can be modified by setting the argument <code>level</code>, see <code>?confint</code>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(<span class="dt">object =</span> linear_model)</code></pre></div>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) 680.32312 717.542775
## STR          -3.22298  -1.336636</code></pre>
</div>
<p>Let us check if the calculation is done as we expect it to be. For <span class="math inline">\(\beta_1\)</span>, that is the coefficient on <code>STR</code>, according to the formula presented above the interval borders are computed as</p>
<p><span class="math display">\[  -2.279808 \pm 1.96 \times 0.4798255 \, \Rightarrow \, \text{KI}_{0.95}^{\beta_1} = \left[ -3.22, -1.34 \right]  \]</span></p>
<p>so this actually leads to the same interval. Obviously, this interval <em>does not</em> contain the value zero what, as we have already seen in the previous section, leads to rejection of the null hypothesis <span class="math inline">\(\beta_{1,0} = 0\)</span>.</p>
</div>
</div>
<div id="regression-when-x-is-a-binary-variable" class="section level2 unnumbered">
<h2>Regression when <span class="math inline">\(X\)</span> is a Binary Variable</h2>
<p>Instead of using a continuous regressor <span class="math inline">\(X\)</span>, we might be interested in running the regression</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 D_i + u_i \tag{5.2} \]</span></p>
<p>where <span class="math inline">\(D_i\)</span> is binary variable, a so-called <em>dummy variable</em>. For example, we may define <span class="math inline">\(D_i\)</span> in the following way:</p>
<p><span class="math display">\[ D_i = \begin{cases}
        1 \ \ \text{if $STR$ in $i^{th}$ school district &lt; 20} \\
        0 \ \ \text{if $STR$ in $i^{th}$ school district $\geq$ 20} \\
      \end{cases} \tag{5.3} \]</span></p>
<p>The regression model now is</p>
<p><span class="math display">\[ TestScore_i = \beta_0 + \beta_1 D_i + u_i. \tag{5.4} \]</span></p>
<p>Let us see how these data look like in a scatter plot:</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the dummy variable as defined above using a for loop</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(CASchools)) {
  <span class="cf">if</span> (CASchools<span class="op">$</span>STR[i] <span class="op">&lt;</span><span class="st"> </span><span class="dv">20</span>) { 
    CASchools<span class="op">$</span>D[i] &lt;-<span class="st"> </span><span class="dv">1</span>
    } <span class="cf">else</span> {
      CASchools<span class="op">$</span>D[i] &lt;-<span class="st"> </span><span class="dv">0</span>
    }
  }

<span class="co"># Plot the data</span>
<span class="kw">plot</span>(CASchools<span class="op">$</span>D, CASchools<span class="op">$</span>score,          <span class="co"># provide the data to be ploted</span>
     <span class="dt">pch=</span><span class="dv">20</span>,                                <span class="co"># use filled circles as plot symbols</span>
     <span class="dt">cex=</span><span class="fl">0.5</span>,                               <span class="co"># set size of plot symbols to 0.5</span>
     <span class="dt">col=</span><span class="st">&quot;Steelblue&quot;</span>,                       <span class="co"># set the symbols&#39; color to &quot;Steelblue&quot;</span>
     <span class="dt">xlab=</span><span class="kw">expression</span>(D[i]),                 <span class="co"># Set title and axis names</span>
     <span class="dt">ylab=</span><span class="st">&quot;Test Score&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Dummy Regression&quot;</span>
     )</code></pre></div>
</div>
<div class="unfolded">
<p><img src="URFITE_files/figure-html/unnamed-chunk-108-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We see that with <span class="math inline">\(D\)</span> as the regressor, it is not useful to think of <span class="math inline">\(\beta_1\)</span> as a slope parameter since <span class="math inline">\(D_i \in \{0,1\}\)</span>, i.e. we only observe two discrete values instead of a continuoum of regressor values lying (in some range) on the real line. Simply put, there is no continuous line depicting the conditional expectation function <span class="math inline">\(E(TestScore_i | D_i)\)</span> since this function is solely defined for <span class="math inline">\(X\)</span>-positions <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>Therefore, the interpretation of the coefficients in our regression model is as follows:</p>
<ul>
<li><p><span class="math inline">\(E(Y_i | D_i = 0) = \beta_0\)</span> so <span class="math inline">\(\beta_0\)</span> is the expected test score in districts where <span class="math inline">\(D_i=0\)</span> i.e. where <span class="math inline">\(STR\)</span> is below <span class="math inline">\(20\)</span>.</p></li>
<li><p><span class="math inline">\(E(Y_i | D_i = 1) = \beta_0 + \beta_1\)</span> or, using the result above, <span class="math inline">\(\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)\)</span>. Thus, <span class="math inline">\(\beta_1\)</span> is the difference in group specific expectations, i.e. the difference in expected test score between districts with <span class="math inline">\(STR &lt; 20\)</span> and those with <span class="math inline">\(STR \geq 20\)</span>.</p></li>
</ul>
<p>We will now use R to estimate the dummy regression model as defined by equations (<a href="#mjx-eqn-5.2">5.2</a>) - (<a href="#mjx-eqn-5.3">5.3</a>) .</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the dummy regression model</span>
dummy_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>D, <span class="dt">data =</span> CASchools)
<span class="kw">summary</span>(dummy_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ D, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -50.496 -14.029  -0.346  12.884  49.504 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  650.077      1.393 466.666  &lt; 2e-16 ***
## D              7.169      1.847   3.882  0.00012 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.74 on 418 degrees of freedom
## Multiple R-squared:  0.0348, Adjusted R-squared:  0.0325 
## F-statistic: 15.07 on 1 and 418 DF,  p-value: 0.0001202</code></pre>
</div>
<p>One can see that the expected test score in districts with <span class="math inline">\(STR &lt; 20\)</span> (<span class="math inline">\(D_i = 1\)</span>) is predicted to be <span class="math inline">\(650.1 + 7.17 = 657.27\)</span> while districs with <span class="math inline">\(STR \geq 20\)</span> (<span class="math inline">\(D_i = 0\)</span>) are expected to have an average test score of only <span class="math inline">\(650.1\)</span>.</p>
<p>Group specific predictions can be added to the plot by execution of the following code chunk:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># add group specific predictions to the plot</span>
<span class="kw">points</span>(<span class="dt">x =</span> CASchools<span class="op">$</span>D, 
       <span class="dt">y =</span> <span class="kw">predict</span>(dummy_model), 
       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
       <span class="dt">pch =</span> <span class="dv">20</span>
       )</code></pre></div>
<p>Here we use the function <code>predict()</code> to obtain estimates of the group specific means. The red dots represent these sample group averages. Accordingly, <span class="math inline">\(\hat{\beta}_1 = 7.17\)</span> can be seen as the difference in group averages.</p>
<p>By inspection of the output generated with <code>summary(dummy_model)</code> we may also find an answer to the question whether there is a statistically significant difference in group means. This in turn would support the hypothesis that students perform differently when they are taught in small classes rather than in large groups. We can assess this by a two-tailed test of the hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. Conviniently the <span class="math inline">\(t\)</span>-statistic and the corresponding <span class="math inline">\(p\)</span>-value for this test are computed defaultly by <code>summary()</code>!</p>
<p>Since <code>t value</code> <span class="math inline">\(= 3.88 &gt; 1.96\)</span> we reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance. The same conclusion can be made when using the <span class="math inline">\(p\)</span>-value which reports significance to the <span class="math inline">\(0.00012\%\)</span> level.</p>
<p>As done with <code>linear_model</code>, we may alternatively use the <code>confint()</code> function to compute a <span class="math inline">\(95\%\)</span> confidence interval for the true difference in means and see if the hypothesised value is an element of this confidence set.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># confidence intervals for coefficients in the dummy regression</span>
<span class="kw">confint</span>(dummy_model)</code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 647.338594 652.81500
## D             3.539562  10.79931</code></pre>
</div>
<p>We reject the hypothesis that there is no difference between group means at the <span class="math inline">\(5\%\)</span> significance level since <span class="math inline">\(\beta_{1,0} = 0\)</span> lies outside of <span class="math inline">\([3.54, 10.8]\)</span>, the <span class="math inline">\(95\%\)</span> confidence interval for the coefficient on <span class="math inline">\(D\)</span>.</p>
</div>
<div id="heteroskedasticity-and-homoskedasticity" class="section level2 unnumbered">
<h2>Heteroskedasticity and Homoskedasticity</h2>
<p>All inference made in the previous chapters relies on the assumption that the error variance does not vary as regressor values change. But this will not necessarily be the case in most empirical applications.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.4
</h3>
<h3 class="left">
Heteroskedasticity and Homoskedasticity
</h3>
<ul>
<li><p>We say that the error term of our regression model is homoskedastic if the variance of the conditional distribution of <span class="math inline">\(u_i\)</span> given <span class="math inline">\(X_i\)</span>, <span class="math inline">\(Var(u_i|X_i=x)\)</span>, is constant <em>for all</em> observations in our sample <span class="math display">\[ \text{Var}(u_i|X_i=x) = \sigma^2 \ \forall \ i=1,\dots,n. \]</span></p></li>
<li><p>If instead there is dependence of the conditional variance of <span class="math inline">\(u_i\)</span> on <span class="math inline">\(X_i\)</span>, the error term is said to be heteroskedastic. We then write <span class="math display">\[ \text{Var}(u_i|X_i=x) = \sigma_i^2 \ \forall \ i=1,\dots,n. \]</span></p></li>
<li>Homoskedasticity is a <em>special case</em> of heteroskedasticity.</li>
</ul>
</div>
<p>For a better understanding of heteroskedasticity, we generate some bivariate heteroskedastic data, estimate a linear regression model and then use boxplots to depict the conditional distributions of the residuals.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load scales package for custom color opacities</span>
<span class="kw">library</span>(scales)

<span class="co"># Genrate some heteroskedastic data</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>) 
x &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">15</span>,<span class="dv">20</span>,<span class="dv">25</span>),<span class="dt">each=</span><span class="dv">25</span>)
e &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">sd=</span><span class="dv">12</span>)                
i &lt;-<span class="st"> </span><span class="kw">order</span>(<span class="kw">runif</span>(<span class="dv">100</span>, <span class="dt">max=</span><span class="kw">dnorm</span>(e, <span class="dt">sd=</span><span class="dv">12</span>))) 
y &lt;-<span class="st"> </span><span class="dv">720</span> <span class="op">-</span><span class="st"> </span><span class="fl">3.3</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>e[<span class="kw">rev</span>(i)]

<span class="co"># Estimate the model </span>
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x)

<span class="co"># Plot the data</span>
<span class="kw">plot</span>(<span class="dt">x=</span>x, 
     <span class="dt">y=</span>y, 
     <span class="dt">main=</span><span class="st">&quot;An Example of Heteroskedasticity&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Student-Teacher Ratio&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score&quot;</span>,
     <span class="dt">cex =</span> <span class="fl">0.5</span>, 
     <span class="dt">pch =</span> <span class="dv">19</span>, 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">8</span>,<span class="dv">27</span>), 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">600</span>,<span class="dv">710</span>)
     )

<span class="co"># Add the regression line to the plot</span>
<span class="kw">abline</span>(mod, <span class="dt">col=</span><span class="st">&quot;darkred&quot;</span>)

<span class="co"># Add boxplots to the plot</span>
<span class="kw">boxplot</span>(y <span class="op">~</span><span class="st"> </span>x, 
        <span class="dt">add =</span> <span class="ot">TRUE</span>, 
        <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">15</span>,<span class="dv">20</span>,<span class="dv">25</span>), 
        <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;gray&quot;</span>, <span class="fl">0.4</span>), 
        <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>
        )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-112-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>For this artificial data it is straightforward to see that we face unequal conditional error variances. Specifically, we observe that the variance in test scores (and therefore the variance of the errors committed) <em>increases</em> with the student teacher ratio.</p>
<div id="a-real-world-example-for-heteroskedasticity" class="section level3 unnumbered">
<h3>A Real-World Example for Heteroskedasticity</h3>
<p>Think about the economic value of education: if there would not be an expected economic value-added to receiving education at university, You probably would not be reading this script right now. A starting point to empirical verification of such a relation exists is to have data on individuals that are in an employment relationship. More precisely, we need data on wages and education in order to work with a model like</p>
<p><span class="math display">\[ wage_i = \beta_0 + \beta_1 \cdot education_i + u_i. \]</span></p>
<p>What can be presumed about this relation? It is likely that, on average, higher educated workers earn more money than workers with less education so we expect to estimate an upward sloping regression line. <br> Also it seems plausible that workers with better education are more likely to meet the requirements for the well-paid jobs. However, workers with low education will have no shot at those well-paid jobs. Therefore it seems plausible that the distribution of earnings spreads out as education increases. In other words: we expect that there is heteroskedasticity!</p>
<p>To verify this empirically we may use real data on hourly earnings and the number of years of education of employees. Such data can be found in <code>CPSSWEducation</code>. This data set is part of the package <code>AER</code> and stems from the Current Population Survey (CPS) which is conducted periodically by the <a href="http://www.bls.gov/">Bureau of Labor Statistics</a> in the US.</p>
<p>The subsequent code chunks demonstrate how to load the data into R and how to produce a plot in the fashion of figure 5.3 in the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load package and attach data</span>
<span class="kw">library</span>(AER)
<span class="kw">data</span>(<span class="st">&quot;CPSSWEducation&quot;</span>)
<span class="kw">attach</span>(CPSSWEducation)

<span class="co"># get an overview</span>
<span class="kw">summary</span>(CPSSWEducation)</code></pre></div>
<pre><code>##       age          gender        earnings        education    
##  Min.   :29.0   female:1202   Min.   : 2.137   Min.   : 6.00  
##  1st Qu.:29.0   male  :1748   1st Qu.:10.577   1st Qu.:12.00  
##  Median :29.0                 Median :14.615   Median :13.00  
##  Mean   :29.5                 Mean   :16.743   Mean   :13.55  
##  3rd Qu.:30.0                 3rd Qu.:20.192   3rd Qu.:16.00  
##  Max.   :30.0                 Max.   :97.500   Max.   :18.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate a simple regression model</span>
labor_model &lt;-<span class="st"> </span><span class="kw">lm</span>(earnings <span class="op">~</span><span class="st"> </span>education)

<span class="co"># plot observations and add the regression line</span>
<span class="kw">plot</span>(education, 
     earnings, 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">150</span>)
     )

<span class="kw">abline</span>(labor_model, <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-113-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>From inspecting the plot we can tell that the mean of the distribution of earnings increases with the level of education. This is also suggested by formal analysis: the estimated regression model stored in <code>labor_mod</code> asserts that there is a positive relation between years of education and earnings.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">labor_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = earnings ~ education)
## 
## Coefficients:
## (Intercept)    education  
##      -3.134        1.467</code></pre>
<p>The estimated regression equation states that, on average, an additional year of education increases a workers hourly earnings by about <span class="math inline">\(\$ 1.47\)</span>. Once more we use <code>confint()</code> to obtain a <span class="math inline">\(95\%\)</span> confidence interval for both regression coefficients.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(labor_model)</code></pre></div>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) -5.015248 -1.253495
## education    1.330098  1.603753</code></pre>
<p>Since the intervall is <span class="math inline">\([1.33, 1.60]\)</span> we can reject the hypothesis that the coefficient on <code>education</code> is zero at the <span class="math inline">\(5\%\)</span> level.</p>
<p>What is more, the plot indicates that there is heteroskedasticity: if we assume the regression line to be a reasonably good representation of the conditional mean function <span class="math inline">\(E(earnings_i\vert education_i)\)</span>, the dispersion of hourly earnings around that function cleary increases with the level of education, i.e. the variance of the distribution of earnings increases. In other words: the variance of the residuals increases with the years of education so that the regression errors are heteroskedastic.<br> This example makes a case that it is doubtful to assume homoskedasticity in many economic applications.</p>
</div>
<div id="should-we-care-about-heteroskedasticity" class="section level3 unnumbered">
<h3>Should We Care About Heteroskedasticity?</h3>
<p>To answer this question, let us see how the variance of <span class="math inline">\(\hat\beta_1\)</span> is computed under the assumption of homoskedasticity. In this case we have</p>
<p><span class="math display">\[ \sigma^2_{\hat\beta_1} = \frac{\sigma^2_u}{n \cdot \sigma^2_X} \tag{5.5} \]</span></p>
<p>which is a simplified version of the general equation (<a href="#mjx-eqn-4.1">4.1</a>) presented in Key Concept 4.4. See Appendix 5.1 of the book for details on the derivation. The <code>summary()</code> function in R estimates (<a href="#mjx-eqn-5.5">5.5</a>) by</p>
<p><span class="math display">\[ \overset{\sim}{\sigma}^2_{\hat\beta_1} = \frac{SER^2}{\sum_{i=1}^n (X_i - \overline{X})^2} \ \ \text{where} \ \ SER=\frac{1}{n-2} \sum_{i=1}^n \hat u_i^2. \]</span></p>
<p>Thus <code>summary()</code> estimates the <em>homoskedasticity-only</em> standard error</p>
<p><span class="math display">\[ \sqrt{ \overset{\sim}{\sigma}^2_{\hat\beta_1} } = \sqrt{ \frac{SER^2}{\sum(X_i - \overline{X})^2} }. \]</span></p>
<p>This in fact is an estimator for the standard deviation of the estimator <span class="math inline">\(\hat{\beta}_1\)</span> that is <em>inconsistent</em> for the true value <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span> when there is heteroskedasticity. The implication is that <span class="math inline">\(t\)</span>-statistics computed in the manner of Key Concept 5.1 do not have a standard normal distribution, even in large samples. This issue may invalidate inference drawn when using the previously treated tools for hypothesis testing: we should be cautious when making statements about the significance of regression coefficients on the basis of <span class="math inline">\(t\)</span>-statistics as computed by <code>summary()</code> or confidence intervals produced by <code>confint()</code> if it is doubtful for the assumption of homoskedasticity to hold!</p>
<p>We will now use R to compute the homoskedasticity-only standard error estimate for <span class="math inline">\(\hat{\beta}_1\)</span> in the test score regression model <code>linear_model</code> by hand and see if it matches the value produced by <code>summary()</code>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Store model summary in &#39;mod&#39;</span>
model &lt;-<span class="st"> </span><span class="kw">summary</span>(linear_model)

<span class="co"># Extract the standard error of the regression from model summary</span>
SER &lt;-<span class="st"> </span>model<span class="op">$</span>sigma

<span class="co"># Compute the variation in &#39;size&#39;</span>
V &lt;-<span class="st"> </span>(<span class="kw">nrow</span>(CASchools)<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">var</span>(CASchools<span class="op">$</span>STR)

<span class="co"># Compute the standard error of the slope parameter&#39;s estimator and print it</span>
SE.beta_<span class="fl">1.</span>hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(SER<span class="op">^</span><span class="dv">2</span><span class="op">/</span>V)
SE.beta_<span class="fl">1.</span>hat</code></pre></div>
<pre><code>## [1] 0.4798255</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use logical operators to see if the value computed by hand matches the one provided # in mod$coefficients. Round estimates to four decimal places</span>

<span class="kw">round</span>(model<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">2</span>], <span class="dv">4</span>) <span class="op">==</span><span class="st"> </span><span class="kw">round</span>(SE.beta_<span class="fl">1.</span>hat, <span class="dv">4</span>)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<p>Indeed, the estimated values are equal.</p>
</div>
<div id="computation-of-heteroskedasticity-robust-standard-errors" class="section level3 unnumbered">
<h3>Computation of Heteroskedasticity-Robust Standard Errors</h3>
<p>Cosistent estimation of <span class="math inline">\(\sigma_{\hat{\beta}_1}\)</span> under heteroskedasticity is granted when the following <em>robust</em> estimator is used.</p>
<p><span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \tag{5.6} \]</span></p>
<p>Standard error estimates computed this way are also referred to as <a href="https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors">Eicker-Huber-White standard errors</a>. It can be quite cumbersome to do this calculation by hand. Luckily, there are R function for that purpose. A convenient one, named <code>vcovHC()</code> is part of the <code>sandwich</code> package. This function can compute a variety of standard error estimators. The one brought forward in (<a href="#mjx-eqn-5.6">5.6</a>) is computed when the argument <code>type</code> is set to <code>&quot;HC0&quot;</code>.</p>
<p>Let us now compute robust standard error estimates for the coefficients in <code>linear_model</code>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the sandwich package</span>
<span class="kw">library</span>(sandwich)

<span class="co"># compute robust standard error estimates</span>
vcov &lt;-<span class="st"> </span><span class="kw">vcovHC</span>(linear_model, <span class="dt">type =</span> <span class="st">&quot;HC0&quot;</span>)
vcov</code></pre></div>
<pre><code>##             (Intercept)        STR
## (Intercept)  106.908469 -5.3383689
## STR           -5.338369  0.2685841</code></pre>
</div>
<p>The output of <code>vcovHC()</code> is the variance-covariance matrix of coefficient estimates. We are interested in the square root of the diagonal elements of this matrix since these values are the standard error estimates we seek.</p>

<div class="rmdknit">
<p>When we have k &gt; 1 regressors, writing down the equations for a regression model becomes very messy. A more convinient way to denote and estimate so-called multiple regression models is matrix algebra. This is why functions like <tt>vcovHC()</tt> produce matrices. In the simple linear regression model, the variances and covariances of the coefficient estimators can be gathered in the variance-covariance matrix</p>
<span class="math display">\[\begin{equation}
\text{Var}
  \begin{pmatrix}
    \hat\beta_0 \\
    \hat\beta_1
  \end{pmatrix} = 
\begin{pmatrix}
  \text{Var}(\hat\beta_0) &amp; \text{Cov}(\hat\beta_0,\hat\beta_1) \\
\text{Cov}(\hat\beta_0,\hat\beta_1) &amp; \text{Var}(\hat\beta_1)
\end{pmatrix}
\end{equation}\]</span>
<p>which is a symmetric matrix. So <tt>vcovHC()</tt> gives us <span class="math inline">\(\widehat{\text{Var}}(\hat\beta_0)\)</span>, <span class="math inline">\(\widehat{\text{Var}}(\hat\beta_1)\)</span> and <span class="math inline">\(\widehat{\text{Cov}}(\hat\beta_0,\hat\beta_1)\)</span> but most of the time we are interested in the diagonal elements of the estimated matrix.</p>
</div>

<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the square root of the diagonal elements in vcov</span>
robust_se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(vcov))
robust_se</code></pre></div>
<pre><code>## (Intercept)         STR 
##   10.339655    0.518251</code></pre>
</div>
<p>Now assume we want to generate a coefficient summary as provided by <code>summary()</code> but with <em>robust</em> standard error estimates for the coefficient estimators, robust <span class="math inline">\(t\)</span>-statistics and corresponding <span class="math inline">\(p\)</span>-values for the regression model <code>linear_model</code>. This can be done using <code>coeftest()</code> from the package <code>lmtest</code>, see <code>?coeftest</code>. Further we specify in the argument <code>vcov.</code> that <code>vcov</code>, the Eicker-Huber-White estimate of the variance matrix we have computed before should be used.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We invoke the function `coeftest()` on our model</span>
<span class="kw">coeftest</span>(linear_model, <span class="dt">vcov. =</span> vcov)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 698.93295   10.33966  67.597 &lt; 2.2e-16 ***
## STR          -2.27981    0.51825  -4.399 1.382e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<p>We see that values reported in the column <code>Std. Error</code> equal the ones received using <code>sqrt(diag(vcov))</code>.</p>
<p>How severe are the implications of using homoskedasticity-only standard errors in the presence of heteroskedasticity? The answer is: it depends. As mentioned above we may face the risk of drawing wrong conclusions when conducting significance tests. <br> Let us illustrate this by generating another example of a heteroskedastic data set and use it to estimate a simple regression model. We take</p>
<p><span class="math display">\[ Y_i = \beta_1 \cdot X_i + u_i \ \ , \ \ u_i \overset{i.i.d.}{\sim} N(0,0.36 \cdot X_i^2)  \]</span></p>
<p>with <span class="math inline">\(\beta_1=1\)</span> as the data generating process. The assumption of homoskedasticity is violated since the variance of the errors is a non-linear increasing function of <span class="math inline">\(X_i\)</span> but the errors have zero mean and are i.i.d. such that the assumptions made in Key Concept 4.3 are not violated. As before, the true conditional mean function we are interested in estimating is</p>
<p><span class="math display">\[ E(Y_i\vert X_i) = X_i. \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">21</span>)

<span class="co"># generate heteroskedastic data </span>
X &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>
Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">mean =</span> X, <span class="dt">sd =</span> <span class="fl">0.6</span><span class="op">*</span>X)

<span class="co"># estimate a simple regression model</span>
reg &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)</code></pre></div>
<p>We plot the data and add the regression line.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data</span>
<span class="kw">plot</span>(X, Y, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span>)

<span class="co"># add the regression line to the plot</span>
<span class="kw">abline</span>(reg, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>, <span class="dt">lwd =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-121-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot clearly shows that the data are heteroskedastic as the variance of <span class="math inline">\(Y\)</span> grows with <span class="math inline">\(X\)</span>. We continue by conducting a significance test of the (true) null hypothesis <span class="math inline">\(H_0: \beta_1 = 1\)</span> twice, once using the homoskedasticity-only standard error formula and once with the robust version (<a href="#mjx-eqn-5.6">5.6</a>). An idiomatic way to do this in R is the function <code>linearHypothesis()</code> from the package <code>car</code>, see <code>?linearHypothesis</code>. It allows to test linear hypotheses about parameters in linear models in a similar way as done with a <span class="math inline">\(t\)</span>-statistic and offers various robust covariance matrix estimators. We test by comparing the tests’ <span class="math inline">\(p\)</span>-values to the significance level of <span class="math inline">\(5\%\)</span>.</p>

<div class="rmdknit">
<p><tt>linearHypothesis()</tt> computes a test statistic that follows an <span class="math inline">\(F\)</span> distribution under the null hypothesis. We will not loose too much words on the theory behind it at this time. In general, the core idea of the <span class="math inline">\(F\)</span> test is to compare the fit of different models. When testing a hypothesis about a <em>single</em> coefficient using a <span class="math inline">\(F\)</span> test, one can show that the test statistic is simply the square of the corresponding <span class="math inline">\(t\)</span>-statistic:</p>
<p><span class="math display">\[ F = t^2 = \frac{\hat\beta_i - \beta_{i,0}}{SE(\hat\beta_i)} \sim F_{1,n-k-1}  \]</span></p>
In <tt>linearHypothesis()</tt>, the hypothesis must be provided as a <em>string</em>. The function returns an object of class <tt>anova</tt> which contains further information on the test that can be accessed using the <tt>$</tt> operator. For example, we can obtain the test’s <span class="math inline">\(p\)</span>-value by adding <tt>$‘Pr(&gt;F)’</tt> right behind the function call.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test using default standard error</span>
<span class="kw">linearHypothesis</span>(reg, <span class="dt">hypothesis.matrix =</span> <span class="st">&quot;X = 1&quot;</span>)<span class="op">$</span><span class="st">&#39;Pr(&gt;F)&#39;</span>[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test using robust standard error</span>
<span class="kw">linearHypothesis</span>(reg, <span class="dt">hypothesis.matrix =</span> <span class="st">&quot;X = 1&quot;</span>, <span class="dt">white.adjust =</span> <span class="st">&quot;hc0&quot;</span>)<span class="op">$</span><span class="st">&#39;Pr(&gt;F)&#39;</span>[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>This is a good example of what can go wrong if we do not care for heteroskedasticity: for the data set at hand the default method rejects the null hypothesis <span class="math inline">\(\beta_1 = 1\)</span> although it is true. Using the robust standard error though the test does not reject the null. Of course we could argue that this is just a coincidence and both tests are equally well in maintaining the type I error rate of <span class="math inline">\(5\%\)</span>. This can be further investigated by computing Monte Carlo estimates of the rejection frequencies of both tests on the basis of a large number of random samples. We proceed as follows:</p>
<ul>
<li>initialize vectors <code>t</code> and <code>t.rob</code> as type <code>numeric</code> with length <span class="math inline">\(10000\)</span>.</li>
<li>Using a <code>for()</code> loop, we generate <span class="math inline">\(10000\)</span> heteroskedastic random samples of size <span class="math inline">\(1000\)</span>, estimate the regression model and check whether the tests wrongly reject the null at the level of <span class="math inline">\(5\%\)</span> using comparison operators. The results are stored in the respective vectors <code>t</code> and <code>t.rob</code>.</li>
<li>After the simulation, we compute the fraction of rejections for both tests.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize vectors t and t.rob</span>
t &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)
t.rob &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)

<span class="co"># loop sampling and estimation</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {
  
  <span class="co"># sample data</span>
  X &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>
  Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">mean =</span> X, <span class="dt">sd =</span> <span class="fl">0.6</span><span class="op">*</span>X)

  <span class="co"># estimate regression model</span>
  reg &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)

  <span class="co"># homoskedasdicity-only significance test</span>
  t[i] &lt;-<span class="st"> </span><span class="kw">linearHypothesis</span>(reg, <span class="st">&quot;X = 1&quot;</span>)<span class="op">$</span><span class="st">&#39;Pr(&gt;F)&#39;</span>[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>

  <span class="co"># robust significance test</span>
  t.rob[i] &lt;-<span class="st"> </span><span class="kw">linearHypothesis</span>(reg, <span class="st">&quot;X = 1&quot;</span>, <span class="dt">white.adjust =</span> <span class="st">&quot;hc0&quot;</span>)<span class="op">$</span><span class="st">&#39;Pr(&gt;F)&#39;</span>[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>

}

<span class="co"># compute fraction of rejections</span>
<span class="kw">cbind</span>(<span class="dt">t =</span> <span class="kw">sum</span>(t), <span class="dt">t.rob =</span> <span class="kw">sum</span>(t.rob)) <span class="op">/</span><span class="st"> </span><span class="dv">10000</span></code></pre></div>
<pre><code>##           t  t.rob
## [1,] 0.0762 0.0524</code></pre>
<p>The results show that we face an increased risk of falsely rejecting the null using the homoskedasticity-only standard error for the testing problem at hand: with the common standard error estimator, <span class="math inline">\(7.62\%\)</span> of all tests reject the null hypothesis falsely. In contrast, with the robust test statistic we are close to the nominal level of <span class="math inline">\(5\%\)</span>.</p>
</div>
</div>
<div id="the-gauss-markov-theorem" class="section level2 unnumbered">
<h2>The Gauss-Markov Theorem</h2>
<p>When estimating regression models, we know that the results of the estimation procedure are outcomes of a random process. However, when using unbiased estimators, at least on average, we estimate the true parameter. When comparing different unbiased estimators, it is therefore interesting to know which one has the highest precision: being aware that the likelihood of estimating the <em>exact</em> value of the parameter of interest is <span class="math inline">\(0\)</span> in an empirical application, we want to make sure that the likelihood of obtaining an estimate very close to the true value is as high as possible. We want to use the estimator with the lowest variance of all unbiased estimators. The Gauss-Markov theorem states that the OLS estimator has this property under certain conditions.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.5
</h3>
<h3 class="left">
The Gauss-Markov Theorem for <span class="math inline">\(\hat{\beta}_1\)</span>
</h3>
<p>Suppose that the assumptions made in Key Concept 4.3 hold <em>and</em> that the errors are <em>homoskedastic</em>. The OLS estimator is the best (in the sense of smallest variance) linear conditionally unbiased estimator (BLUE) in this setting.</p>
<p>Let us have a closer look at what this means:</p>
<ul>
<li><p>Estimators of <span class="math inline">\(\beta_1\)</span> that are linear functions of the <span class="math inline">\(Y_1, \dots, Y_n\)</span> and that are unbiased conditionally on the regressor <span class="math inline">\(X_1, \dots, X_n\)</span> can be written as <span class="math display">\[ \overset{\sim}{\beta}_1 = \sum_{i=1}^n a_i Y_i \]</span> where the <span class="math inline">\(a_i\)</span> are weights that are allowed to depend on the <span class="math inline">\(X_i\)</span> but <em>not</em> on the <span class="math inline">\(Y_i\)</span>.</p></li>
<li><p>We already know that <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> has a sampling distribution: <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is a linear function of the <span class="math inline">\(Y_i\)</span> which are random variables. If now <span class="math display">\[ E(\overset{\sim}{\beta}_1 | X_1, \dots, X_n) = \beta_1 \]</span> we say that <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is a linear unbiased estimator of <span class="math inline">\(\beta_1\)</span>, conditionally on the <span class="math inline">\(X_1, \dots, X_n\)</span>.</p></li>
<li>We may ask if <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is also the <em>best</em> estimator in this class, i.e. the most efficient one of all linear unbiased estimators where “most efficient” means smallest variance. The weights <span class="math inline">\(a_i\)</span> play an important role here and it turns out that OLS uses just the right weights to have the BLUE property.</li>
</ul>
</div>
<div id="r-simulation-study-blue-estimator" class="section level3 unnumbered">
<h3>R Simulation Study: BLUE Estimator</h3>
<p>Consider the case of a regression of <span class="math inline">\(Y_i,\dots,Y_n\)</span> only on a constant. Here, the <span class="math inline">\(Y_i\)</span> are assumed to be a random sample from a population with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. We know that the OLS estimator in this model is simply the sample mean:</p>
<span class="math display" id="eq:bluemean">\[\begin{equation}
\hat{\beta}_1 = \overline{\beta}_1 = \sum_{i=1}^n \underbrace{\frac{1}{n}}_{=a_i} Y_i \tag{5.2}
\end{equation}\]</span>
<p>Clearly, each observation is weighted by</p>
<p><span class="math display">\[a_i = \frac{1}{n}.\]</span></p>
<p>and We also know that <span class="math inline">\(\text{Var}(\hat{\beta}_1)=\text{Var}(\hat\beta_1)=\frac{\sigma^2}{n}\)</span>.</p>
<p>We will now use R for a simulation study that illustrates what happens to the variance of <a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#eq:bluemean">(5.2)</a> if different weights <span class="math display">\[ w_i = \frac{1 \pm \epsilon}{n} \]</span> are assigned to either half of the sample <span class="math inline">\(Y_1, \dots, Y_n\)</span> instead of using <span class="math inline">\(\frac{1}{n}\)</span>, the weights implied by OLS.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Set sample size and number of repititions</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>      
reps &lt;-<span class="st"> </span><span class="fl">1e5</span>

<span class="co"># Choose epsilon and create a vector of weights as defined above</span>
epsilon &lt;-<span class="st"> </span><span class="fl">0.8</span>
w &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>((<span class="dv">1</span><span class="op">+</span>epsilon)<span class="op">/</span>n,n<span class="op">/</span><span class="dv">2</span>), 
       <span class="kw">rep</span>((<span class="dv">1</span><span class="op">-</span>epsilon)<span class="op">/</span>n,n<span class="op">/</span><span class="dv">2</span>) 
      )

<span class="co"># Draw a random sample y_1,...,y_n from the standard normal distribution, </span>
<span class="co"># use both estimators 1e5 times and store the result in thr vectors ols and </span>
<span class="co"># weightedestimator</span>

ols &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, reps)
weightedestimator &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, reps)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps) {
  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n)
  ols[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(y)
  weightedestimator[i] &lt;-<span class="st"> </span><span class="kw">crossprod</span>(w,y)
}

<span class="co"># Plot kernel density estimates of the estimators&#39; distributions </span>

## OLS
<span class="kw">plot</span>(<span class="kw">density</span>(ols), 
     <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">3</span>, 
     <span class="dt">main =</span> <span class="st">&quot;Density of OLS and Weighted Estimator&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Estimates&quot;</span>
     )

## Weighted
<span class="kw">lines</span>(<span class="kw">density</span>(weightedestimator), 
      <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">3</span>
      ) 

## Add a dashed line at 0 and a legend to the plot
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&#39;topright&#39;</span>, 
       <span class="kw">c</span>(<span class="st">&quot;OLS&quot;</span>,<span class="st">&quot;Weighted&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;steelblue&quot;</span>), 
       <span class="dt">lwd =</span> <span class="dv">3</span>
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-124-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>What conclusion can we draw from the result?</p>
<ul>
<li>Both estimators seem to be unbiased: the means of their estimated distributions are zero.</li>
<li>The <code>weightedestimator</code> is less efficient than the <code>ols</code> estimator: there is higher dispersion when weights are <span class="math inline">\(w_i = \frac{1 \pm 0.8}{100}\)</span> instead of <span class="math inline">\(w_i=\frac{1}{100}\)</span> as required by the OLS solution.</li>
</ul>
<p>Hence, our simulation results confirm what is stated by the Gauss-Markov Theorem.</p>
</div>
</div>
<div id="using-the-t-statistic-in-regression-when-the-sample-size-is-small" class="section level2 unnumbered">
<h2>Using the <span class="math inline">\(t\)</span>-Statistic in Regression When the Sample Size Is Small</h2>
<p>The three OLS assumptions discussed in <a href="lrwor.html#lrwor">chapter 4</a> (see Key Concept 4.3) are the foundation results on the large sample distribution of the OLS estimators in the simple regression model. What can be said about the distribution of the estimators and their <span class="math inline">\(t\)</span>-statistics when the sample size is small and the population distribution of the data is unkown? Provided that the three least squares assumptions hold and the errors are normally distributed and homoskedastic (we refer to these conditions as the homoskedastic normal regression assumptions), we have normally distributed estimators and <span class="math inline">\(t\)</span>-distributed test tatistics. Recall the <a href="probability-theory.html#thetdist">definition</a> of a <span class="math inline">\(t\)</span>-distributed variable</p>
<p><span class="math display">\[ \frac{Z}{\sqrt{W/m}} \sim t_m\]</span></p>
<p>where <span class="math inline">\(Z\)</span> is a standard normal random variable, <span class="math inline">\(W\)</span> is <span class="math inline">\(\chi^2\)</span> distributed with <span class="math inline">\(m\)</span> degrees of freedom and <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> are independent. See section 5.6 in the book for a more detailed discussion of the small sample distribution of <span class="math inline">\(t\)</span>-statistics in regression.</p>
<p>Let us simulate the distribution of regression <span class="math inline">\(t\)</span>-statistics based on a large number of small random samples (<span class="math inline">\(n=20\)</span>) and compare their simulated distributions to their theoretical distribution which sould be <span class="math inline">\(t_{18}\)</span>, the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(18\)</span> degrees of freedom (recall that <span class="math inline">\(\text{DF}=n-k-1\)</span>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize vectors</span>
beta_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)
beta_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)

<span class="co"># loop sampling / estimation / t statistics</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {

  X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">20</span>)
  Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">20</span>, <span class="dt">mean =</span> X)
  reg &lt;-<span class="st"> </span><span class="kw">summary</span>(<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X))
  beta_<span class="dv">0</span>[i] &lt;-<span class="st"> </span>(reg<span class="op">$</span>coefficients[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">0</span>)<span class="op">/</span>(reg<span class="op">$</span>coefficients[<span class="dv">1</span>,<span class="dv">2</span>])
  beta_<span class="dv">1</span>[i] &lt;-<span class="st"> </span>(reg<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(reg<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">2</span>])

}

<span class="co"># plot distributions and compare with t_18 density function</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))

<span class="co"># plot simulated density of beta_0</span>
<span class="kw">plot</span>(<span class="kw">density</span>(beta_<span class="dv">0</span>), 
     <span class="dt">lwd=</span> <span class="dv">2</span> , 
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="kw">widehat</span>(beta)[<span class="dv">0</span>]), 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>)
    )

<span class="co"># add t_18 density to the plot</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df =</span> <span class="dv">18</span>), 
      <span class="dt">add =</span> T, 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>, 
      <span class="dt">lty =</span> <span class="dv">2</span>
      )

<span class="co"># plot simulated density of beta_1</span>
<span class="kw">plot</span>(<span class="kw">density</span>(beta_<span class="dv">1</span>), 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="kw">widehat</span>(beta)[<span class="dv">1</span>]), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>)
     )

<span class="co"># add t_18 density to the plot</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df =</span> <span class="dv">18</span>), 
      <span class="dt">add =</span> T, 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>, 
      <span class="dt">lty =</span> <span class="dv">2</span>
      ) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-125-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The outcomes are consistent with our expectations: the empirical distributions of both estimators seem to track the <span class="math inline">\(t_{18}\)</span> distribution quite closely.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lrwor.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-models-with-multiple-regressors.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-ch5.Rmd",
"text": "Edit"
},
"download": ["URFITE.pdf", "URFITE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
