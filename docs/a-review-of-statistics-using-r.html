<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Using R for Introduction to Econometrics</title>
  <meta name="description" content="Using R for Introduction to Econometrics">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Using R for Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Using R for Introduction to Econometrics" />
  
  
  

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2017-11-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="probability-theory.html">
<link rel="next" href="lrwor.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="MathJax-2.7.2/MathJax.js?config=default"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/SVG"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- <script src="js/d3.v3.min.js"></script> 
<script src="js/leastsquares.js"></script>
<script src="js/d3functions.js"></script>
<script src="d3.slider.js"></script>
<script src="slrm.js"></script> -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">URFITE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i>Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#expected-values-mean-and-variance"><i class="fa fa-check"></i>Expected Values, Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li><a href="probability-theory.html#thetdist">The Student <span class="math inline">\(t\)</span> Distribution</a></li>
<li><a href="probability-theory.html#the-f-distribution">The <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#random-sampling-and-the-distribution-of-sample-averages"><i class="fa fa-check"></i>Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="probability-theory.html"><a href="probability-theory.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#estimation-of-the-population-mean"><i class="fa fa-check"></i>Estimation of the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#properties-of-the-population-mean"><i class="fa fa-check"></i>Properties of the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i>Hypothesis Tests Concerning the Population Mean</a><ul>
<li><a href="a-review-of-statistics-using-r.html#p-value"><span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="a-review-of-statistics-using-r.html#calculating-the-p-value-when-sigma_y-is-known">Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#sample-variance-sample-standard-deviation-and-standard-error"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li><a href="a-review-of-statistics-using-r.html#calculating-the-p-value-when-sigma_y-is-unknown">Calculating the <span class="math inline">\(p\)</span>-value When <span class="math inline">\(\sigma_Y\)</span> is Unknown</a></li>
<li><a href="a-review-of-statistics-using-r.html#the-t-statistic">The <span class="math inline">\(t\)</span>-statistic</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i>Confidence intervals for the Population Mean</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#comparing-means-from-different-populations"><i class="fa fa-check"></i>Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#an-application-to-the-gender-gap-of-earnings"><i class="fa fa-check"></i>An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="" data-path="a-review-of-statistics-using-r.html"><a href="a-review-of-statistics-using-r.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i>Scatterplots, Sample Covariance and Sample Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i>Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i>Measures of fit</a><ul>
<li><a href="lrwor.html#the-r2">The <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#standard-error-of-the-regression"><i class="fa fa-check"></i>Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-least-squares-assumptions"><i class="fa fa-check"></i>The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption #1: The Error Term has Conditional Mean of Zero</a></li>
<li><a href="lrwor.html#assumption-2-all-x_i-y_i-are-independently-and-identically-distributed">Assumption #2: All <span class="math inline">\((X_i, Y_i)\)</span> are Independently and Identically Distributed</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption #3: Large outliers are unlikely</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i>The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-1"><i class="fa fa-check"></i>R Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-2"><i class="fa fa-check"></i>R Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#r-simulation-study-3"><i class="fa fa-check"></i>R Simulation Study 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#testing-two-sided-hypotheses-concerning-beta_1">Testing Two-Sided Hypotheses Concerning <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#confidence-intervals-for-regression-coefficients"><i class="fa fa-check"></i>Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#r-simulation-study-5.1"><i class="fa fa-check"></i>R Simulation Study 5.1</a></li>
</ul></li>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#regression-when-x-is-a-binary-variable">Regression when <span class="math inline">\(X\)</span> is a Binary Variable</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#heteroskedasticity-and-homoskedasticity"><i class="fa fa-check"></i>Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#the-gauss-markov-theorem"><i class="fa fa-check"></i>The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html"><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#r-simulation-study-blue-estimator"><i class="fa fa-check"></i>R Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li><a href="hypothesis-tests-and-confidence-intervals-in-the-simple-linear-regression-model.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small">Using the <span class="math inline">\(t\)</span>-Statistic in Regression When the Sample Size Is Small</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#omitted-variable-bias"><i class="fa fa-check"></i>Omitted Variable Bias</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#the-multiple-regression-model"><i class="fa fa-check"></i>The Multiple Regression Model</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#measures-of-fit-in-multiple-regression"><i class="fa fa-check"></i>Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i>OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regression-models-with-multiple-regressors.html"><a href="regression-models-with-multiple-regressors.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i>The Distribution of the OLS Estimators in Multiple Regression</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the <span class="math inline">\(F\)</span>-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html"><a href="hypothesis-tests-and-confidence-intervals-in-multiple-regression.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="8.1" data-path="nonlinear-regression-functions.html"><a href="nonlinear-regression-functions.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.1</b> Interactions Between Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using R for Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-review-of-statistics-using-r" class="section level1">
<h1><span class="header-section-number">3</span> A Review of Statistics using R</h1>
<p>This section reviews important statistical concepts:</p>
<ul>
<li><p>Estimation</p></li>
<li><p>Hypothesis testing</p></li>
<li><p>Confidence intervals</p></li>
</ul>
<p>Since these types of statistical methods are heavily used in econometrics, we will discuss them in the context of inference about an unknown population mean and discuss several applications in R.</p>
<div id="estimation-of-the-population-mean" class="section level2 unnumbered">
<h2>Estimation of the Population Mean</h2>
<div class="keyconcept">
<h3 class="right">
Key Concept 3.1
</h3>
<h3 class="left">
Estimators and Estimates
</h3>
<p><font color="#004A93"><strong>Estimators</strong></font> are functions of sample data that are drawn randomly from an unknown population. <font color="#004A93"><strong>Estimates</strong></font> are numerical values computed by estimators based on the sample data. Estimators are random variables because they are functions of <em>random</em> data. Estimates are nonrandom numbers.</p>
</div>
<p>Think of some economic variable, for example hourly earnings of college graduates, denoted by <span class="math inline">\(Y\)</span>. Suppose we are interested in <span class="math inline">\(\mu_Y\)</span> the mean of <span class="math inline">\(Y\)</span>. In order to exactly calculate <span class="math inline">\(\mu_Y\)</span> we would have to interview every graduated member of the working population in the economy. We simply cannot do this for time and cost reasons. However, we could draw a random sample from <span class="math inline">\(n\)</span> i.i.d. observations <span class="math inline">\(Y_1, \dots, Y_n\)</span> and estimate <span class="math inline">\(\mu_Y\)</span> using one of the simplest estimators in the sense of Key Concept 3.1 one can think of:</p>
<p><span class="math display">\[ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i, \]</span></p>
<p>the sample mean of <span class="math inline">\(Y\)</span>. Then again, we could use an even simpler estimator for <span class="math inline">\(\mu_Y\)</span>: the very first observation in the sample, <span class="math inline">\(Y_1\)</span>. Is <span class="math inline">\(Y_1\)</span> a good estimator? For now, assume that</p>
<p><span class="math display">\[ Y \sim \chi_{12}^2 \]</span></p>
<p>which is not too unreasonable as the measure is nonnegative and we expect many hourly earnings to be in a range of <span class="math inline">\(5€\)</span> to <span class="math inline">\(15€\)</span>. Moreover, it is common for income distributions to be skewed to the right.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the chi_12^2 distribution</span>
<span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df=</span><span class="dv">12</span>), 
      <span class="dt">from =</span> <span class="dv">0</span>, 
      <span class="dt">to =</span> <span class="dv">40</span>, 
      <span class="dt">ylab =</span> <span class="st">&quot;density&quot;</span>, 
      <span class="dt">xlab =</span> <span class="st">&quot;hourly earnings in Euro&quot;</span>
      )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-41-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We draw a sample of <span class="math inline">\(n=100\)</span> observations and take the first observation <span class="math inline">\(Y_1\)</span> as an estimate for <span class="math inline">\(\mu_Y\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># sample from the chi_12^2 distribution, keep only the first observation</span>
<span class="kw">rchisq</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">df =</span> <span class="dv">12</span>)[<span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 8.257893</code></pre>
<p>The estimate <span class="math inline">\(8.26\)</span> is not too far away from <span class="math inline">\(\mu_Y = 12\)</span> but it is somewhat intuitive that we could do better: the estimator <span class="math inline">\(Y_1\)</span> discards a lot of information and its variance is the population variance:</p>
<p><span class="math display">\[ \text{Var}(Y_1) = \text{Var}(Y) = 2 \cdot 12 = 24 \]</span></p>
<p>This brings us to the following question: What is a ‘good’ estimator in the first place? This question is tackled in Key Concepts 3.2 and 3.3</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 3.2
</h3>
<h3 class="left">
Bias, Consistency and Efficiency
</h3>
<p>Disirable characteristics of an estimator are unbiasedness, consitency and Efficiency.</p>
<p><font color="#004A93"><strong>Unbiasedness</strong></font>:<br />
If the mean of the sampling distribution of some estimator <span class="math inline">\(\hat\mu_Y\)</span> for the population mean <span class="math inline">\(\mu_Y\)</span> equals <span class="math inline">\(\mu_Y\)</span> <span class="math display">\[ E(\hat\mu_Y) = \mu_Y \]</span> we say that the estimator is unbiased for <span class="math inline">\(\mu_Y\)</span>. The <em>bias</em> of <span class="math inline">\(\hat\mu_Y\)</span> is <span class="math inline">\(0\)</span>:</p>
<p><span class="math display">\[ E(\hat\mu_Y) - \mu_Y = 0\]</span></p>
<p><font color="#004A93"><strong>Consistency</strong></font>:</p>
<p>We want the uncertainty of the estimator <span class="math inline">\(\mu_Y\)</span> to decrease as the number of observations in the sample grows. More precisely, we want the proabability that the estimate <span class="math inline">\(\hat\mu_Y\)</span> falls within a small interval of the true value <span class="math inline">\(\mu_Y\)</span> to get increasingly closer to <span class="math inline">\(1\)</span> as <span class="math inline">\(n\)</span> grows. We write this as</p>
<p><span class="math display">\[ \hat\mu_Y \xrightarrow{p} \mu_Y. \]</span></p>
<p><font color="#004A93"><strong>Variance and efficiency</strong></font>:</p>
<p>We want the estimator to be efficient. Suppose we have two estimators, <span class="math inline">\(\hat\mu_Y\)</span> and <span class="math inline">\(\overset{\sim}{\mu}_Y\)</span> and for some given sample size <span class="math inline">\(n\)</span> it holds that</p>
<p><span class="math display">\[ E(\hat\mu_Y) = E(\overset{\sim}{\mu}_Y) = \mu_Y \]</span> but <span class="math display">\[\text{Var}(\hat\mu_Y) &lt; \text{Var}(\overset{\sim}{\mu}_Y).\]</span></p>
<p>We then would prefer to use <span class="math inline">\(\hat\mu_Y\)</span> as it has a lower variance than <span class="math inline">\(\overset{\sim}{\mu}_Y\)</span>, meaning that <span class="math inline">\(\hat\mu_Y\)</span> is more <em>efficient</em> in using the information provided by the observations in the sample.</p>
</div>
<div class="keyconcept">
<h3 class="right">
Key Concept 3.3
</h3>
<h3 class="left">
Efficiency of <span class="math inline">\(\overline{Y}\)</span>: The BLUE property
</h3>
<p>Let <span class="math inline">\(\hat\mu_Y\)</span> be a linear and unbiased estimator of <span class="math inline">\(\mu_Y\)</span> in the fashion of</p>
<p><span class="math display">\[ \hat\mu_Y = \frac{1}{n} \sum_{i=1}^n a_i Y_i\]</span></p>
<p>with nonrandom constants <span class="math inline">\(a_i\)</span>. We see that <span class="math inline">\(\hat\mu_Y\)</span> is a weighted average of the <span class="math inline">\(Y_i\)</span> and the <span class="math inline">\(a_i\)</span> are weights. For these type of estimators, <span class="math inline">\(\overline{Y}\)</span> with <span class="math inline">\(a_i = 1\)</span> for all <span class="math inline">\(i = 1, \dots, n\)</span> is the most efficient estimator. We say that <span class="math inline">\(\overline{Y}\)</span> is the <font color="#004A93"><strong>B</strong></font>est<font color="#004A93"><strong>L</strong></font>inear <font color="#004A93"><strong>U</strong></font>nbiased <font color="#004A93"><strong>E</strong></font>stimator (BLUE).</p>
</div>
</div>
<div id="properties-of-the-population-mean" class="section level2 unnumbered">
<h2>Properties of the Population Mean</h2>
<p>To examine properties of the sample mean as an estimator for the corresponding population mean, consider the following R example.</p>
<p>We generate a population <code>pop</code> which consists observations <span class="math inline">\(Y_i \ , \ i=1,\dots,10000\)</span> that stem from a normal distribution with mean <span class="math inline">\(\mu = 10\)</span> and variance <span class="math inline">\(\sigma^2 = 1\)</span>.<br> To investigate how the estimator <span class="math inline">\(\hat{\mu} = \bar{Y}\)</span> behaves we can draw random samples from this population and calculate <span class="math inline">\(\bar{Y}\)</span> for each of them. This is easily done by making use of the function <code>replicate()</code>. Its argument <code>expr</code> is evaluated <code>n</code> times. In this case we draw samples of sizes <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=25\)</span>, compute the sample means and repeat this exactly <span class="math inline">\(n=25000\)</span> times.</p>
<p>For comparison purposes we store results for the estimator <span class="math inline">\(Y_1\)</span>, the first observation in a sample for a sample of size <span class="math inline">\(5\)</span> separately.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate a fictive population</span>
pop &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10000</span>, <span class="dv">10</span>, <span class="dv">1</span>)

<span class="co"># sample form pop and estimate the mean</span>
est1 &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dt">expr =</span> <span class="kw">mean</span>(<span class="kw">sample</span>(<span class="dt">x =</span> pop, <span class="dt">size =</span> <span class="dv">5</span>)), <span class="dt">n =</span> <span class="dv">25000</span>)

est2 &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dt">expr =</span> <span class="kw">mean</span>(<span class="kw">sample</span>(<span class="dt">x =</span> pop, <span class="dt">size =</span> <span class="dv">25</span>)), <span class="dt">n =</span> <span class="dv">25000</span>)

fo &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dt">expr =</span> <span class="kw">sample</span>(<span class="dt">x =</span> pop, <span class="dt">size =</span> <span class="dv">5</span>)[<span class="dv">1</span>], <span class="dt">n =</span> <span class="dv">25000</span>)</code></pre></div>
<p>Check that <code>est1</code> and <code>est2</code> are vectors of length <span class="math inline">\(25000\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check if object type is vector</span>
<span class="kw">is.vector</span>(est1)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">is.vector</span>(est2)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check lengths</span>
<span class="kw">length</span>(est1)</code></pre></div>
<pre><code>## [1] 25000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(est2)</code></pre></div>
<pre><code>## [1] 25000</code></pre>
<p>The code chunk below produces a plot of the sampling distributions of the estimators <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(Y_1\)</span> on the basis of the <span class="math inline">\(25000\)</span> samples in each case. We also plot a curve depicting the density function of the <span class="math inline">\(N(10,1)\)</span> distribution.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot density estimate Y_1</span>
<span class="kw">plot</span>(<span class="kw">density</span>(fo), 
      <span class="dt">col =</span> <span class="st">&#39;green&#39;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>,
      <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">2</span>),
      <span class="dt">xlab =</span> <span class="st">&#39;estimates&#39;</span>,
      <span class="dt">main =</span> <span class="st">&#39;Sampling Distributions of Unbiased Estimators&#39;</span>
      )

<span class="co"># add density estimate for the distribution of the sample mean with n=5 to the plot</span>
<span class="kw">lines</span>(<span class="kw">density</span>(est1), 
     <span class="dt">col =</span> <span class="st">&#39;steelblue&#39;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">bty =</span> <span class="st">&#39;l&#39;</span>
     )

<span class="co"># add density estimate for the distribution of the sample mean with n=25 to the plot</span>
<span class="kw">lines</span>(<span class="kw">density</span>(est2), 
      <span class="dt">col =</span> <span class="st">&#39;red2&#39;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>
      )

<span class="co"># add a vertical line marking the true parameter</span>
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">10</span>, <span class="dt">lty =</span> <span class="dv">2</span>)

<span class="co"># add N(10,1) density to the plot</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x, <span class="dt">mean=</span><span class="dv">10</span>), 
     <span class="dt">lwd =</span> <span class="dv">2</span>,
     <span class="dt">lty =</span> <span class="dv">2</span>,
     <span class="dt">add =</span> T
     )

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;N(10,1)&quot;</span>,
                  <span class="kw">expression</span>(Y[<span class="dv">1</span>]),
                  <span class="kw">expression</span>(<span class="kw">bar</span>(Y) <span class="op">~</span><span class="st"> </span>n<span class="op">==</span><span class="dv">5</span>),
                  <span class="kw">expression</span>(<span class="kw">bar</span>(Y) <span class="op">~</span><span class="st"> </span>n<span class="op">==</span><span class="dv">25</span>)
                  ), 
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&#39;black&#39;</span>,<span class="st">&#39;green&#39;</span>, <span class="st">&#39;steelblue&#39;</span>, <span class="st">&#39;red2&#39;</span>),
       <span class="dt">lwd =</span> <span class="dv">2</span>
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-45-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>At first, notice how <em>all</em> sampling distributions (represented by the solid lines) are centered around <span class="math inline">\(\mu = 10\)</span>. This is evidence for the <em>unbiasedness</em> of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(\overline{Y}\)</span>. Of course, the theoretical density the <span class="math inline">\(N(10,1)\)</span> distribution is centered at <span class="math inline">\(10\)</span>, too.</p>
<p>Next, have a look add the spread of the sampling distributions. Several things are remarkable:</p>
<ul>
<li><p>First, the sampling distribution of <span class="math inline">\(Y_1\)</span> (green curve) tracks the density of the <span class="math inline">\(N(10,1)\)</span> distribution (black dashed line) pretty closely In fact, the sampling distribution of <span class="math inline">\(Y_1\)</span> <em>is</em> the <span class="math inline">\(N(10,1)\)</span> distribution. This is less surprising if You keep in mind that <span class="math inline">\(Y_1\)</span> estimator does nothing but reporting an observation that is randomly selected from a population with <span class="math inline">\(N(10,1)\)</span> distribution. Hence, <span class="math inline">\(Y_1 \sim N(10,1)\)</span>. Note that this result is invariant to the sample size <span class="math inline">\(n\)</span>: the sampling distribution of <span class="math inline">\(Y_1\)</span> is always the population distribution, no how large the sample is.</p></li>
<li><p>Second, both sampling distributions of <span class="math inline">\(\overline{Y}\)</span> show less dispersion than the sampling distribution of <span class="math inline">\(Y_1\)</span>. This means that <span class="math inline">\(\overline{Y}\)</span> has a lower variance than <span class="math inline">\(Y_1\)</span>. In view of Key Concepts 3.2 and 3.3, we find that <span class="math inline">\(\overline{Y}\)</span> is a more efficient estimator than <span class="math inline">\(Y_1\)</span>. In fact, one can show that this holds for all <span class="math inline">\(n&gt;1\)</span>.</p></li>
<li><p>Third, <span class="math inline">\(\overline{Y}\)</span> shows a behaviour that is termed consistency (see Key Concept 3.2). Notice that the blue and the red density curves are much more concentrated around <span class="math inline">\(\mu=10\)</span> then the green one. As the number of observations is increased from <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span>, the sampling distribution tightens around the true parameter. This effect is more dominant as the sample size is increased to <span class="math inline">\(25\)</span>. This implies that the probability of obtaining estimates that are close to the true value increases with <span class="math inline">\(n\)</span>.</p></li>
</ul>

<div class="rmdknit">
<p>A more precise way to express consitency of an estimator <span class="math inline">\(\hat\mu\)</span> for a parameter <span class="math inline">\(\mu\)</span> is</p>
<p><span class="math display">\[ P(|\hat{\mu} - \mu|&lt;\epsilon) \xrightarrow[n \rightarrow \infty]{p} 1 \quad \text{for any}\quad\epsilon&gt;0.\]</span></p>
This expression says that the probability of observing a deviation from the true value <span class="math inline">\(\mu\)</span> that is smaller than some arbitrary <span class="math inline">\(\epsilon &gt; 0\)</span> converges to <span class="math inline">\(1\)</span> as <span class="math inline">\(n\)</span> grows.<br> Note that consistency does not require unbiasedness:
</div>

<p>We encourage You to go ahead and modify the code. Try out different values for the sample size and see how the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> changes!</p>
<div id="overliney-is-the-least-squares-estimator-of-mu_y" class="section level4 unnumbered">
<h4><span class="math inline">\(\overline{Y}\)</span> is the least squares estimator of <span class="math inline">\(\mu_Y\)</span></h4>
<p>Assume You have some observations <span class="math inline">\(Y_1,\dots,Y_n\)</span> on <span class="math inline">\(Y \sim N(10,1)\)</span> (which is unknown) and would like to find an estimator <span class="math inline">\(m\)</span> that predicts the observations as good as possible where good means to choose <span class="math inline">\(m\)</span> such that the total deviation between the predicted value and the observed values is small. Mathematically this means we want to find an <span class="math inline">\(m\)</span> that minimizes</p>
<span class="math display" id="eq:sqm">\[\begin{equation}
  \sum_{i=1}^n (Y_i - m)^2. \tag{3.1}
\end{equation}\]</span>
<p>Think of <span class="math inline">\(Y_i - m\)</span> as the comitted mistake when predicting <span class="math inline">\(Y_i\)</span> by <span class="math inline">\(m\)</span>. We could just as well minimize the sum of absolute deviations from <span class="math inline">\(m\)</span> but minimizing the sum of squared deviations is mathematically more convenient and leads, roughly speaking, to the same result. That is why the estimator we are looking for is called the <em>least squares estimator</em>. As It turns out <span class="math inline">\(m = \overline{Y}\)</span>, the estimator of <span class="math inline">\(\mu_Y=10\)</span> is this wanted estimator.</p>
<p>We can show this by generating a random sample of fair size and plotting <a href="a-review-of-statistics-using-r.html#eq:sqm">(3.1)</a> as a function of <span class="math inline">\(m\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define the function and vectorize it</span>
sqm &lt;-<span class="st"> </span><span class="cf">function</span>(m) {
 <span class="kw">sum</span>((y<span class="op">-</span>m)<span class="op">^</span><span class="dv">2</span>)
}
sqm &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(sqm)

<span class="co"># draw random sample and compute the mean</span>
y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">10</span>, <span class="dv">1</span>)
<span class="kw">mean</span>(y)</code></pre></div>
<pre><code>## [1] 10.00543</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the objective function</span>
<span class="kw">curve</span>(<span class="kw">sqm</span>(x), 
      <span class="dt">from =</span> <span class="op">-</span><span class="dv">50</span>, 
      <span class="dt">to =</span> <span class="dv">70</span>,
      <span class="dt">xlab =</span> <span class="st">&quot;m&quot;</span>,
      <span class="dt">ylab =</span><span class="st">&quot;sqm(m)&quot;</span>
      )

<span class="co"># add vertical line at mean(y)</span>
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">mean</span>(y), 
       <span class="dt">lty =</span> <span class="dv">2</span>, 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-46-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice that <a href="a-review-of-statistics-using-r.html#eq:sqm">(3.1)</a> is a quadratic function so there is only one minimum. The plot shows that this minimum lies exactly at the sample mean of the sample data.</p>

<div class="rmdknit">
<p>Some R functions can only interact with functions that take a vector as input evaluate the function body on every values of the vector, for example <tt>curve()</tt>. We call such functions vectorized functions and it is often a good idea to write vectorized functions although this is cumbersome in some cases. Having a vectorized function in R is never a drawback since these functions work on both single values and vectors.</p>
<p>Let us look at the function <tt>sqm()</tt> which is nonvectorized</p>
<p><tt> sqm &lt;- function(m) {<br />
     sum((y-m)^2) #body of the function<br />
} </tt></p>
<p>Providing e.g. <tt>c(1,2,3)</tt> as the argument <tt>m</tt> would cause an error since then the operation <tt>y-m</tt> is invalid: the vecors <tt>y</tt> and <tt>m</tt> are of incompatible dimensions. This is why we cannot use <tt>sqm()</tt> in conjunction with <tt>curve()</tt>.</p>
Here comes <tt>Vectorize()</tt> into play. It generates a vectorized version of a non-vectorized function.
</div>

</div>
<div id="why-random-sampling-is-important" class="section level4 unnumbered">
<h4>Why Random Sampling is important</h4>
<p>So far, we assumed (somtimes implicitly) that observed data <span class="math inline">\(Y_1, \dots, Y_n\)</span> are the result of a sampling process that satisfies the assumption of i.i.d. random sampling. It is very important that this assumption is fulfilled when estimating a population mean using <span class="math inline">\(\overline{Y}\)</span>. If this is not the case, estimates are biased.</p>
<p>Let us fall back to <code>pop</code>, the fictive population of <span class="math inline">\(10000\)</span> observations and compute the population mean <span class="math inline">\(\mu_{\texttt{pop}}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the population mean of pop</span>
<span class="kw">mean</span>(pop)</code></pre></div>
<pre><code>## [1] 9.992604</code></pre>
<p>Next we sample <span class="math inline">\(10\)</span> observations from <code>pop</code> with <code>sample()</code> and estimate <span class="math inline">\(\mu_{\texttt{pop}}\)</span> using <span class="math inline">\(\overline{Y}\)</span> repeatedly. However this time we use a sampling scheme that deviates from simple random sampling: instead of ensuring that each member of the population has the same chance to end up in a sample, we assign a higher probability of beeing sampled to the <span class="math inline">\(2500\)</span> smallest observations of the population by setting the argument <code>prop</code> to a suitable vector of probability weights:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate outcome for the sample mean when the i.i.d. assumption fails</span>
est3 &lt;-<span class="st">  </span><span class="kw">replicate</span>(<span class="dt">n =</span> <span class="dv">25000</span>, 
                    <span class="dt">expr =</span> <span class="kw">mean</span>(<span class="kw">sample</span>(<span class="dt">x =</span> <span class="kw">sort</span>(pop), 
                                  <span class="dt">size =</span> <span class="dv">10</span>, 
                                  <span class="dt">prob =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">4</span>,<span class="dv">2500</span>),<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">7500</span>))
                                      )
                                )
                   )

<span class="co"># compute the sample mean of the outcomes</span>
<span class="kw">mean</span>(est3)</code></pre></div>
<pre><code>## [1] 9.443454</code></pre>
<p>Next we plot the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> for this non-i.i.d. case an compare it to the sampling distribution when the i.i.d. assumption holds.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sampling distribution of sample mean, i.i.d. holds, n=25</span>
<span class="kw">plot</span>(<span class="kw">density</span>(est2), 
      <span class="dt">col =</span> <span class="st">&#39;red2&#39;</span>,
      <span class="dt">lwd =</span> <span class="dv">2</span>,
      <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">8</span>,<span class="dv">11</span>),
      <span class="dt">xlab =</span> <span class="st">&#39;estimates&#39;</span>,
      <span class="dt">main =</span> <span class="st">&#39;When the i.i.d. Assumption Fails&#39;</span>
     )

<span class="co"># sampling distribution of sample mean, i.i.d. fails, n=25</span>
<span class="kw">lines</span>(<span class="kw">density</span>(est3),
      <span class="dt">col =</span> <span class="st">&#39;steelblue&#39;</span>,
      <span class="dt">lwd =</span> <span class="dv">2</span>
      )

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="kw">expression</span>(<span class="kw">bar</span>(Y) <span class="op">~</span><span class="st"> &quot;,&quot;</span> <span class="op">~</span><span class="st"> </span>n<span class="op">==</span><span class="dv">25</span> <span class="op">~</span><span class="st"> &quot;, i.i.d. fails&quot;</span>),
                  <span class="kw">expression</span>(<span class="kw">bar</span>(Y) <span class="op">~</span><span class="st"> &quot;,&quot;</span> <span class="op">~</span><span class="st"> </span>n<span class="op">==</span><span class="dv">25</span> <span class="op">~</span><span class="st"> &quot;, i.i.d. holds&quot;</span>)
                  ), 
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&#39;red2&#39;</span>, <span class="st">&#39;steelblue&#39;</span>),
       <span class="dt">lwd =</span> <span class="dv">2</span>
       )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>We find that in this case failure of the i.i.d. assumption implies that, on average, we underestimate <span class="math inline">\(\mu_Y\)</span> using <span class="math inline">\(\overline{Y}\)</span>: the corresponding distribution of <span class="math inline">\(\overline{Y}\)</span> is shifted to the left. In other words, <span class="math inline">\(\overline{Y}\)</span> is a <em>biased</em> estimator for <span class="math inline">\(\mu_Y\)</span> if the i.i.d. assumption does not hold.</p>
</div>
</div>
<div id="hypothesis-tests-concerning-the-population-mean" class="section level2 unnumbered">
<h2>Hypothesis Tests Concerning the Population Mean</h2>
<p>In this section we briefly review concepts in hypothesis testing and discuss how to conduct hypothesis tests in R. We focus on drawing inference about an unkown population mean.</p>
<div id="about-hypotheses-and-hypothesis-testing" class="section level4 unnumbered">
<h4>About Hypotheses and Hypothesis Testing</h4>
<p>In a significance test we want to exploit the information contained in a random sample as evidence in favour or against a hypothesis. Essentially, hypotheses are simple question that can be answered by ‘yes’ or ‘no’. When conducting a hypothesis test we always deal with two different hypotheses:</p>
<ul>
<li><p>The <font color="#004A93"><strong>null hypothesis</strong></font>, denoted <span class="math inline">\(H_0\)</span> is the hypothesis we are interested in testing</p></li>
<li><p>The <font color="#004A93"><strong>alternative hypothesis</strong></font>, denoted <span class="math inline">\(H_1\)</span>, is the hypothesis that holds if the null hypothesis is false</p></li>
</ul>
<p>The null hypothesis that the population mean of <span class="math inline">\(Y\)</span> equals the value <span class="math inline">\(\mu_{Y,0}\)</span> is written down as</p>
<p><span class="math display">\[ H_0: E(Y) = \mu_{Y,0}. \]</span></p>
<p>The alternative hypothesis states what holds if the null hypothesis is false. Often the alternative hypothesis chosen is the most general one,</p>
<p><span class="math display">\[ H_1: E(Y) \neq \mu_{Y,0}, \]</span></p>
<p>meaning that <span class="math inline">\(E(Y)\)</span> may be anything else but the value as the null hypothesis. This is called a two-sided alternative.</p>
<p>For brevity, we will only consider the case of a two-sided alternative in the subsequent sections of this chapter.</p>
</div>
<div id="p-value" class="section level3 unnumbered">
<h3><span class="math inline">\(p\)</span>-Value</h3>
<p>Assume that the null hypothesis is <em>true</em>. The <span class="math inline">\(p\)</span>-value is the probability of drawing data and observing a corresponding test statistics that is at least as adverse to what is stated under the null hypothesis as the test statistic actually computed using the sample data.</p>
<p>In context of population mean and sample mean, this definition can be stated mathematically in the following way:</p>
<span class="math display" id="eq:pvalue">\[\begin{equation}
p \text{-value} = P_{H_0}\left[ \lvert \overline{Y} - \mu_{Y,0} \rvert &gt; \lvert \overline{Y}^{act} - \mu_{Y,0} \rvert \right] \tag{3.2}
\end{equation}\]</span>
<p>In <a href="a-review-of-statistics-using-r.html#eq:pvalue">(3.2)</a>, <span class="math inline">\(\overline{Y}^{act}\)</span> is the acutally computed mean of the random sample.<br> Visualized, the <span class="math inline">\(p\)</span>-value is the area in the part of tails of the distribution of <span class="math inline">\(\overline{Y}\)</span> that lies beyond</p>
<p><span class="math display">\[ \mu_{Y,0} \pm \lvert \overline{Y}^{act} - \mu_{Y,0} \rvert. \]</span></p>
<p>Consequently, in order to compute the <span class="math inline">\(p\)</span>-value as in <a href="a-review-of-statistics-using-r.html#eq:pvalue">(3.2)</a>, knowledge about the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> when the null hypothesis is true is required. However in most cases the sampling distribution of <span class="math inline">\(\overline{Y}\)</span> is unkown. Furtunately, due to the large-sample normal approximation (see chapter 3) we know that under the null hypothesis</p>
<p><span class="math display">\[ \overline{Y} \sim N(\mu_{Y,0}, \, \sigma^2_{\overline{Y}}) \ \ , \ \ \sigma^2_{\overline{Y}} = \frac{\sigma_Y^2}{n} \]</span></p>
<p>and thus</p>
<p><span class="math display">\[ \frac{\overline{Y} - \mu_{Y,0}}{\sigma_Y/\sqrt{n}} \sim N(0,1). \]</span></p>
<p>So in large samples, the <span class="math inline">\(p\)</span>-value can be computed <em>without</em> knowledge about the sampling distribution of <span class="math inline">\(\overline{Y}\)</span>.</p>
</div>
<div id="calculating-the-p-value-when-sigma_y-is-known" class="section level3 unnumbered">
<h3>Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</h3>
<p>For now, let us assume that <span class="math inline">\(\sigma_\overline{Y}\)</span> is known. Then we can rewrite <a href="a-review-of-statistics-using-r.html#eq:pvalue">(3.2)</a> as</p>
<span class="math display" id="eq:pvaluenorm1">\[\begin{align}
p \text{-value} =&amp; \, P_{H_0}\left[ \left\lvert \frac{\overline{Y} - \mu_{Y,0}}{\sigma_\overline{Y}} \right\rvert &gt; \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_\overline{Y}} \right\rvert \right] \\
=&amp; \, 2 \cdot \Phi \left[ - \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_\overline{Y}}  \right\rvert\right].  \tag{3.3}
\end{align}\]</span>
<p>so the <span class="math inline">\(p\)</span>-value can be seen as the area in the tails of the <span class="math inline">\(N(0,1)\)</span> distribution that lies beyond</p>
<span class="math display" id="eq:pvaluenorm2">\[\begin{equation}
\pm \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_\overline{Y}} \right\rvert \tag{3.4}
\end{equation}\]</span>
<p>Whew, that was a lot of theory. Now we use R to visualize what is stated in <a href="a-review-of-statistics-using-r.html#eq:pvaluenorm1">(3.3)</a> and <a href="a-review-of-statistics-using-r.html#eq:pvaluenorm2">(3.4)</a>. The next code chunck replicates figure 3.1 of the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the standard normal density on the domain [-4,4]</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x),
      <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>),
      <span class="dt">main =</span> <span class="st">&#39;Calculating a p-value&#39;</span>,
      <span class="dt">yaxs =</span> <span class="st">&#39;i&#39;</span>,
      <span class="dt">xlab =</span> <span class="st">&#39;z&#39;</span>,
      <span class="dt">ylab =</span> <span class="st">&#39;&#39;</span>,
      <span class="dt">lwd =</span> <span class="dv">2</span>,
      <span class="dt">axes =</span> <span class="st">&#39;F&#39;</span>
      )

<span class="co"># add x-axis</span>
<span class="kw">axis</span>(<span class="dv">1</span>, 
     <span class="dt">at =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">0</span>,<span class="fl">1.5</span>), 
     <span class="dt">padj =</span> <span class="fl">0.75</span>,
     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="kw">expression</span>(<span class="op">-</span><span class="kw">frac</span>(<span class="kw">bar</span>(Y)<span class="op">^</span><span class="st">&quot;act&quot;</span><span class="op">~-</span><span class="er">~</span><span class="kw">bar</span>(mu)[Y,<span class="dv">0</span>],sigma[<span class="kw">bar</span>(Y)])),
                <span class="dv">0</span>,
                <span class="kw">expression</span>(<span class="kw">frac</span>(<span class="kw">bar</span>(Y)<span class="op">^</span><span class="st">&quot;act&quot;</span><span class="op">~-</span><span class="er">~</span><span class="kw">bar</span>(mu)[Y,<span class="dv">0</span>],sigma[<span class="kw">bar</span>(Y)])))
     )

<span class="co"># shade p-value/2 region in left tail</span>
<span class="kw">polygon</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">6</span>, <span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>,<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">0.01</span>),<span class="op">-</span><span class="fl">1.5</span>),
        <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>,<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">0.01</span>)),<span class="dv">0</span>), 
        <span class="dt">col =</span> <span class="st">&#39;steelblue&#39;</span>
        )

## shade p-value/2 region in right tail
<span class="kw">polygon</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">1.5</span>, <span class="kw">seq</span>(<span class="fl">1.5</span>, <span class="dv">6</span>, <span class="fl">0.01</span>), <span class="dv">6</span>),
        <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="fl">1.5</span>, <span class="dv">6</span>, <span class="fl">0.01</span>)), <span class="dv">0</span>), 
        <span class="dt">col =</span> <span class="st">&#39;steelblue&#39;</span>
        )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
</div>
<div id="sample-variance-sample-standard-deviation-and-standard-error" class="section level3 unnumbered">
<h3>Sample Variance, Sample Standard Deviation and Standard Error</h3>
<p>If <span class="math inline">\(\sigma^2_Y\)</span> is unknown, it must be estimated. This can be done efficiently using the sample variance</p>
<span class="math display">\[\begin{equation}
s_y^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2.
\end{equation}\]</span>
<p>Furthermore</p>
<span class="math display">\[\begin{equation}
s_y = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2}.
\end{equation}\]</span>
<p>is a suitable estimator for the standard deviation of <span class="math inline">\(Y\)</span>. In R, <span class="math inline">\(s_y\)</span> is implemented in the function <code>sd()</code>, see <code>?sd</code>.</p>
<p>Using R we can get a notion that <span class="math inline">\(s_y\)</span> is a consistent estimator for <span class="math inline">\(\sigma_Y\)</span>, that is</p>
<p><span class="math display">\[ s_Y \overset{p}{\longrightarrow} \sigma_Y. \]</span></p>
<p>The idea here is to generate a large number of samples <span class="math inline">\(Y_1,\dots,Y_n\)</span> where <span class="math inline">\(Y\sim N(10,10)\)</span>, estimate <span class="math inline">\(\sigma_Y\)</span> using <span class="math inline">\(s_y\)</span> and investigate how the distribution of <span class="math inline">\(s_Y\)</span> changes as <span class="math inline">\(n\)</span> grows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># vector of sample sizes</span>
n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10000</span>, <span class="dv">5000</span>, <span class="dv">2000</span>, <span class="dv">1000</span>, <span class="dv">500</span>)

<span class="co"># sample observations, estimate using sd() and plot estimated distributions</span>
s2_y &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="dt">expr =</span> <span class="kw">sd</span>(<span class="kw">rnorm</span>(n[<span class="dv">1</span>], <span class="dv">10</span>, <span class="dv">10</span>)))
<span class="kw">plot</span>(<span class="kw">density</span>(s2_y),
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="st">&#39;Sampling Distributions of&#39;</span> <span class="op">~</span><span class="st"> </span>s[y]),
     <span class="dt">xlab =</span> <span class="kw">expression</span>(s[y]),
     <span class="dt">lwd =</span> <span class="dv">2</span>
     )

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(n)) {
  s2_y &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="dt">expr =</span> <span class="kw">sd</span>(<span class="kw">rnorm</span>(n[i],<span class="dv">10</span>,<span class="dv">10</span>)))
  <span class="kw">lines</span>(<span class="kw">density</span>(s2_y), 
        <span class="dt">col=</span>i, 
        <span class="dt">lwd=</span><span class="dv">2</span>)
}

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="kw">expression</span>(n<span class="op">==</span><span class="dv">10000</span>),
                  <span class="kw">expression</span>(n<span class="op">==</span><span class="dv">5000</span>),
                  <span class="kw">expression</span>(n<span class="op">==</span><span class="dv">2000</span>),
                  <span class="kw">expression</span>(n<span class="op">==</span><span class="dv">1000</span>),
                  <span class="kw">expression</span>(n<span class="op">==</span><span class="dv">500</span>)
       ), 
       <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,
       <span class="dt">lwd =</span> <span class="dv">2</span>
)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-51-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot shows that the distribution of <span class="math inline">\(s_Y\)</span> tightens around the true value <span class="math inline">\(\sigma_Y = 10\)</span> as <span class="math inline">\(n\)</span> increases.</p>
<p>The function that estimates the standard deviation of an estimator is called the <em>standard error of the estimator</em>. Key Concept 3.4 summarizes the terminology in the context of the sample mean.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 3.4
</h3>
<h3 class="left">
The Standard Error of <span class="math inline">\(\overline{Y}\)</span>
</h3>
<p>Take an i.i.d. sample <span class="math inline">\(Y_1, \dots, Y_n\)</span>. The mean of <span class="math inline">\(Y\)</span> can be consistently estimated using <span class="math inline">\(\overline{Y}\)</span>, the sample mean of the <span class="math inline">\(Y_i\)</span>. Since <span class="math inline">\(\overline{Y}\)</span> is a random variable, it has a sampling distribution with variance <span class="math inline">\(\frac{\sigma_Y^2}{n}\)</span>.</p>
<p>The standard error of <span class="math inline">\(\overline{Y}\)</span>, denoted <span class="math inline">\(SE(\overline{Y})\)</span> is an estimator of the standard deviation <span class="math inline">\(\overline{Y}\)</span>:</p>
<p><span class="math display">\[ SE(\overline{Y}) = \hat\sigma_\overline{Y} = \frac{s_Y}{\sqrt{n}} \]</span></p>
<p>The caret (^) over <span class="math inline">\(\sigma\)</span> indicates that <span class="math inline">\(\hat\sigma_\overline{Y}\)</span> is an estimator for <span class="math inline">\(\sigma_\overline{Y}\)</span>.</p>
</div>
<p>As an example to underpin Key Concept 3.4, consider a sample of <span class="math inline">\(n=100\)</span> i.i.d. observations of the bernoulli distributed variable <span class="math inline">\(Y\)</span> with success probability <span class="math inline">\(p=0.1\)</span> and thus <span class="math inline">\(E(Y)=p=0.1\)</span> and <span class="math inline">\(\text{Var}(Y)=p(1-p)\)</span>. <span class="math inline">\(E(Y)\)</span> can be estimated by <span class="math inline">\(\overline{Y}\)</span> which then has variance</p>
<p><span class="math display">\[ \sigma^2_\overline{Y} = p(1-p)/n = 0.0009 \]</span></p>
<p>and standard deviation</p>
<p><span class="math display">\[ \sigma_\overline{Y} = \sqrt{p(1-p)/n} = 0.03. \]</span></p>
<p>In this case the standard error of <span class="math inline">\(\overline{Y}\)</span> is given as</p>
<p><span class="math display">\[ SE(\overline{Y}) = \sqrt{\overline{Y}(1-\overline{Y})/n} \]</span></p>
<p>Let verify whether <span class="math inline">\(\overline{Y}\)</span> and <span class="math inline">\(SE(\overline{Y})\)</span> estimate the respective true values on average.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw 10000 samples of size 100 and estimate the mean of Y and</span>
<span class="co"># estimate the standard error of the sample mean</span>

mean_estimates &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)
se_estimates &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {
  s &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, 
              <span class="dt">size =</span> <span class="dv">100</span>,  
              <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.9</span>, <span class="fl">0.1</span>),
              <span class="dt">replace =</span> T
              )
  mean_estimates[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(s)
  se_estimates[i] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>(s)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">mean</span>(s))<span class="op">/</span><span class="dv">100</span>)
}

<span class="kw">mean</span>(mean_estimates)</code></pre></div>
<pre><code>## [1] 0.099693</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(se_estimates)</code></pre></div>
<pre><code>## [1] 0.02953467</code></pre>
<p>Both estimators seem to be unbiased for the true parameters.</p>
</div>
<div id="calculating-the-p-value-when-sigma_y-is-unknown" class="section level3 unnumbered">
<h3>Calculating the <span class="math inline">\(p\)</span>-value When <span class="math inline">\(\sigma_Y\)</span> is Unknown</h3>
<p>When <span class="math inline">\(\sigma_Y\)</span> is unkown, the <span class="math inline">\(p\)</span>-value for a hypothesis test about <span class="math inline">\(\mu_Y\)</span> using <span class="math inline">\(\overline{Y}\)</span> can be computed by replacing <span class="math inline">\(\sigma_\overline{Y}\)</span> in <a href="a-review-of-statistics-using-r.html#eq:pvaluenorm1">(3.3)</a> by the standard error <span class="math inline">\(SE(\overline{Y}) = \hat\sigma_Y\)</span>. Then,</p>
<p><span class="math display">\[ p\text{-value} = 2\cdot\Phi\left(-\left\lvert \frac{\overline{Y}^{act}-\mu_{Y,0}}{SE(\overline{Y})} \right\rvert \right). \]</span></p>
<p>This is easily done in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sample and estimate, compute standard error and make a hypothesis</span>
samplemean_act &lt;-<span class="st"> </span><span class="kw">mean</span>(
  <span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, 
         <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.9</span>,<span class="fl">0.1</span>), 
         <span class="dt">replace =</span> T, 
         <span class="dt">size =</span> <span class="dv">100</span>
         )
  )

SE_samplemean &lt;-<span class="st"> </span><span class="kw">sqrt</span>(samplemean_act <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>samplemean_act)<span class="op">/</span><span class="dv">100</span>)

mean_h0 &lt;-<span class="st"> </span><span class="fl">0.1</span> <span class="co">#true null hypothesis</span>

<span class="co"># compute the pvalue</span>
pvalue &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="kw">abs</span>(samplemean_act<span class="op">-</span>mean_h0)<span class="op">/</span>SE_samplemean)
pvalue</code></pre></div>
<pre><code>## [1] 0.5382527</code></pre>
</div>
<div id="the-t-statistic" class="section level3 unnumbered">
<h3>The <span class="math inline">\(t\)</span>-statistic</h3>
<p>In hypothesis testing, the standardized sample average</p>
<span class="math display" id="eq:tstat">\[\begin{equation}
t = \frac{\overline{Y} - \mu_{Y,0}}{SE(\overline{Y})} \tag{3.5}
\end{equation}\]</span>
<p>is called <span class="math inline">\(t\)</span>-statistic. This <span class="math inline">\(t\)</span>-statistic has an important role when testing hypothesis about <span class="math inline">\(\mu_Y\)</span>. It is a prominent example of a test statistic.</p>
<p>Implicitly, we already have computed a <span class="math inline">\(t\)</span>-statistic for <span class="math inline">\(\overline{Y}\)</span> in the previous code chunk.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute a t-statistic for the sample mean</span>
tstatistic &lt;-<span class="st"> </span>(samplemean_act <span class="op">-</span><span class="st"> </span>mean_h0) <span class="op">/</span><span class="st"> </span>SE_samplemean
tstatistic</code></pre></div>
<pre><code>## [1] 0.6154575</code></pre>
<p>Using R we can show that if <span class="math inline">\(\mu_{Y,0}\)</span> equals the true value, that is the null hypothesis is true, <a href="a-review-of-statistics-using-r.html#eq:tstat">(3.5)</a> is approximately distributed <span class="math inline">\(N(0,1)\)</span> when <span class="math inline">\(n\)</span> is large.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize empty vector for t-statistics</span>
tstatistics &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)

<span class="co"># set sample size</span>
n &lt;-<span class="st"> </span><span class="dv">300</span>

<span class="co"># simulate 10000 t-statistics</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {
  s &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, 
              <span class="dt">size =</span> n,  
              <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.9</span>, <span class="fl">0.1</span>),
              <span class="dt">replace =</span> T
              )
  tstatistics[i] &lt;-<span class="st"> </span>(<span class="kw">mean</span>(s)<span class="op">-</span><span class="fl">0.1</span>)<span class="op">/</span>(<span class="kw">sqrt</span>(<span class="kw">mean</span>(s)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">mean</span>(s))<span class="op">/</span>n))
}

<span class="co"># plot density and compare to N(0,1) density</span>
<span class="kw">plot</span>(<span class="kw">density</span>(tstatistics),
     <span class="dt">xlab =</span> <span class="st">&#39;t-statistic&#39;</span>,
     <span class="dt">main =</span> <span class="st">&#39;Distribution of the t-statistic when n=300&#39;</span>,
     <span class="dt">lwd =</span> <span class="dv">2</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>),
     <span class="dt">col =</span> <span class="st">&#39;steelblue&#39;</span>
     )

<span class="co"># N(0,1) density (dashed)</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x), 
      <span class="dt">add =</span> T, 
      <span class="dt">lty =</span> <span class="dv">2</span>, 
      <span class="dt">lwd=</span> <span class="dv">2</span>
      )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>Judging from the plot, the normal approximation works reasonably well for the chosen sample size. This normal approximation has already been used in the definition of the <span class="math inline">\(p\)</span>-value, see <a href="a-review-of-statistics-using-r.html#eq:tstat">(3.5)</a>.</p>
</div>
<div id="hypothesis-testing-with-a-prespecified-significance-level" class="section level3 unnumbered">
<h3>Hypothesis Testing with a Prespecified Significance Level</h3>
<div class="keyconcept">
<h3 class="right">
Key Concept 3.5
</h3>
<h3 class="left">
The Terminology of Hypothesis Testing
</h3>
<p>In hypothesis testing, two types of mistakes are possible:</p>
<ol style="list-style-type: decimal">
<li><p>The null hypothesis <em>is</em> rejected although it is true (<span class="math inline">\(\alpha\)</span>-error / type-I-error)</p></li>
<li><p>The null hypothesis <em>is not</em> rejected although it is false (<span class="math inline">\(\beta\)</span>-error / type-II-error)</p></li>
</ol>
<p>The <font color="#004A93"><strong>significance level</strong></font> of the test is the probability to commit a type-I-error we are willing to accept in advance. E.g. using a prespecified significance level of <span class="math inline">\(0.05\)</span>, we reject the null hypothesis if and only if the <span class="math inline">\(p\)</span>-value is less than <span class="math inline">\(0.05\)</span>. The significance level is chosen before the test is conducted.</p>
<p>An equivalent procedure is to reject the null hypothesis if the test statistic observed is, in absolute value terms, larger than the <font color="#004A93"><strong>critical value</strong></font> of the test statistic. The critical value is determined by the significance level chosen and defines two disjoint sets of values which are called <font color="#004A93"><strong>acceptance region</strong></font> and <font color="#004A93"><strong>rejection region</strong></font>. The acceptance region contains all values of the test statistic for which the test does not reject while the rejection region contains all the values for which the test does reject.</p>
<p>The <font color="#004A93"><strong><span class="math inline">\(p\)</span>-value</strong></font> is the probability that, in repeated sampling under the same conditions, meaning i.i.d. sampling, the same null hypothesis and the same sample size, a test statistic is observed that provides just as much evidence against the null hypothesis as the test statistic actually observed.</p>
<p>The actual probability that the test rejects the true null hypothesis is called the <font color="#004A93"><strong>size of the test</strong></font>. In an ideal setting, the size does not exceed the significance level.</p>
<p>The probability that the test correctly rejects a false null hypothesis is called <font color="#004A93"><strong>power</strong></font>.</p>
</div>
<p>Reconsider <code>pvalue</code> computed further above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check whether p-value &lt; 0.05</span>
pvalue <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>The condition is not fulfilled so we do not reject the null hypotheis (remember that the null hypothesis is true in this example).</p>
<p>When working with a <span class="math inline">\(t\)</span>-statistic instead, it is equivalent to apply the following rule:</p>
<p><span class="math display">\[ \text{Reject } H_0 \text{ if } \lvert t^{act} \rvert &gt; 1.96 \]</span></p>
<p>We reject the null hypothesis at the significance level of <span class="math inline">\(5\%\)</span> if the computed <span class="math inline">\(t\)</span>-statistic lies beyond the critical value of 1.96 in absolute value terms. <span class="math inline">\(1.96\)</span> is the <span class="math inline">\(0.05\)</span>-quantile of the standard normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check the critical value</span>
<span class="kw">qnorm</span>(<span class="dt">p =</span> <span class="fl">0.05</span>)</code></pre></div>
<pre><code>## [1] -1.644854</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check whether the null is rejected using the t-statistic computed further above</span>
<span class="kw">abs</span>(tstatistic) <span class="op">&gt;</span><span class="st"> </span><span class="fl">1.96</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>As when using the <span class="math inline">\(p\)</span>-value, we cannot reject the null hypothesis using the corresponding <span class="math inline">\(t\)</span>-statistic. Key Concept 3.6 summarizes the procedure of performing a two-sided hypothesis about the population mean <span class="math inline">\(E(Y)\)</span>.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 3.6
</h3>
<h3 class="left">
Testing the Hypothesis <span class="math inline">\(E(Y) = \mu_{Y,0}\)</span> Against the Alternative <span class="math inline">\(E(Y) \neq \mu_{Y,0}\)</span>
</h3>
<ol style="list-style-type: decimal">
<li><p>Estimate <span class="math inline">\(\mu_{Y}\)</span> using <span class="math inline">\(\overline{Y}\)</span> and compute the standard error of <span class="math inline">\(\overline{Y}\)</span>, <span class="math inline">\(SE(\overline{Y})\)</span>.</p></li>
<li><p>Compute the <span class="math inline">\(t\)</span>-statistic.</p></li>
<li><p>Compute the <span class="math inline">\(p\)</span>-value and reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance if the <span class="math inline">\(p\)</span>-value is smaller than <span class="math inline">\(0.05\)</span> or equivalently, if</p></li>
</ol>
<p><span class="math display">\[ \left\lvert t^{act} \right\rvert &gt; 1.96. \]</span></p>
</div>
</div>
<div id="one-sided-alternatives" class="section level3 unnumbered">
<h3>One-sided Alternatives</h3>
<p>Sometimes we are interested in finding evidence that the mean is bigger or smaller than the some value hypothesized under the null. One can come up with many examples here but, to stick to the book, take the presumed wage differential between good and less educated working individuals. Since we hope that this differential exists, a relevant alternative (to the null hypothesis that there is no wage differential) is that good educated individuals earn more, i.e. that the average hourly wage for this group, <span class="math inline">\(\mu_Y\)</span> is <em>bigger</em> than <span class="math inline">\(\mu_{Y,0}\)</span> the know average wage of less educated workers.</p>
<p>This is an example of a <em>right-sided test</em> and the hypotheses pair is chosen as</p>
<p><span class="math display">\[ H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs} \ \ H_1: \mu_Y &gt; \mu_{Y,0}. \]</span></p>
<p>We reject the null hypothesis if the computed test-statistic is larger than the critical value <span class="math inline">\(1.64\)</span>, the <span class="math inline">\(0.95\)</span>-quantile of the <span class="math inline">\(N(0,1)\)</span> distribution. This ensures that <span class="math inline">\(1-0.95=5\%\)</span> probability mass remains in the area to the right of the critical value. Similar as before we can visualize this in R using the function <code>polygon()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the standard normal density on the domain [-4,4]</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x),
      <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>),
      <span class="dt">main =</span> <span class="st">&#39;Rejection Region of a Right-Sided Test&#39;</span>,
      <span class="dt">yaxs =</span> <span class="st">&#39;i&#39;</span>,
      <span class="dt">xlab =</span> <span class="st">&#39;t-statistic&#39;</span>,
      <span class="dt">ylab =</span> <span class="st">&#39;&#39;</span>,
      <span class="dt">lwd =</span> <span class="dv">2</span>,
      <span class="dt">axes =</span> <span class="st">&#39;F&#39;</span>
)

<span class="co"># add x-axis</span>
<span class="kw">axis</span>(<span class="dv">1</span>, 
     <span class="dt">at =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">0</span>,<span class="fl">1.64</span>,<span class="dv">4</span>), 
     <span class="dt">padj =</span> <span class="fl">0.5</span>,
     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&#39;&#39;</span>,<span class="dv">0</span>,<span class="kw">expression</span>(Phi<span class="op">^-</span><span class="dv">1</span><span class="op">~</span>(.<span class="dv">95</span>)<span class="op">==</span><span class="fl">1.64</span>),<span class="st">&#39;&#39;</span>)
)

<span class="co"># shade rejection region in right tail</span>
<span class="kw">polygon</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">1.64</span>, <span class="kw">seq</span>(<span class="fl">1.64</span>, <span class="dv">4</span>, <span class="fl">0.01</span>), <span class="dv">4</span>),
        <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="fl">1.64</span>, <span class="dv">4</span>, <span class="fl">0.01</span>)), <span class="dv">0</span>), 
        <span class="dt">col =</span> <span class="st">&#39;darkred&#39;</span>
)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>In an analogously manner for the <em>left-sided test</em> we have</p>
<p><span class="math display">\[ H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs.} \ \ H_1: \mu_Y &lt; \mu_{Y,0}. \]</span></p>
<p>The null is rejected if the observed test statistic falls short of the critical value which, for a test at the <span class="math inline">\(0.05\)</span> level of significance, is given by <span class="math inline">\(-1.64\)</span>, the <span class="math inline">\(0.05\)</span>-quantile of the <span class="math inline">\(N(0,1)\)</span> distribution. <span class="math inline">\(5\%\)</span> probability mass lies to the left of the critical value.</p>
<p>It is straight forward to adapt the code chunk above to the case of a left-sided test. We only have to fiddle with the color shading and the tick marks.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the standard normal density on the domain [-4,4]</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x),
      <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>),
      <span class="dt">main =</span> <span class="st">&#39;Rejection Region of a Left-Sided Test&#39;</span>,
      <span class="dt">yaxs =</span> <span class="st">&#39;i&#39;</span>,
      <span class="dt">xlab =</span> <span class="st">&#39;t-statistic&#39;</span>,
      <span class="dt">ylab =</span> <span class="st">&#39;&#39;</span>,
      <span class="dt">lwd =</span> <span class="dv">2</span>,
      <span class="dt">axes =</span> <span class="st">&#39;F&#39;</span>
)

<span class="co"># add x-axis</span>
<span class="kw">axis</span>(<span class="dv">1</span>, 
     <span class="dt">at =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">0</span>,<span class="op">-</span><span class="fl">1.64</span>,<span class="dv">4</span>), 
     <span class="dt">padj =</span> <span class="fl">0.5</span>,
     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&#39;&#39;</span>,<span class="dv">0</span>,<span class="kw">expression</span>(Phi<span class="op">^-</span><span class="dv">1</span><span class="op">~</span>(.<span class="dv">05</span>)<span class="op">==-</span><span class="fl">1.64</span>),<span class="st">&#39;&#39;</span>)
)

<span class="co"># shade rejection region in right tail</span>
<span class="kw">polygon</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="fl">1.64</span>, <span class="fl">0.01</span>), <span class="op">-</span><span class="fl">1.64</span>),
        <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="fl">1.64</span>, <span class="fl">0.01</span>)), <span class="dv">0</span>), 
        <span class="dt">col =</span> <span class="st">&#39;darkred&#39;</span>
)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
</div>
</div>
<div id="confidence-intervals-for-the-population-mean" class="section level2 unnumbered">
<h2>Confidence intervals for the Population Mean</h2>
<p>As stressed before, we will never estimate the exact value of a the population mean of <span class="math inline">\(Y\)</span> using a random sample. However, we can compute confidence intervals for the population mean. In general, a confidence interval for a unkown parameter is a set of values that contains the true parameter with a prespecified probability, the confidence level. Confidence intervals are computed using the information available in the sample. Since this information is the result of a random process, confidence intervals are random variables themselves.</p>
<p>Key Concept 3.7 shows how to compute confidence intervals for the unknown population mean <span class="math inline">\(E(Y)\)</span>.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 3.6
</h3>
<h3 class="left">
Confidence Intervals for the Population Mean
</h3>
<p>A <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(\mu_Y\)</span> is a <font color="#004A93"><strong>random variable</strong></font> that contains the true <span class="math inline">\(\mu_Y\)</span> in <span class="math inline">\(95\%\)</span> of all possible random samples. When <span class="math inline">\(n\)</span> is large we can use the normal approximation. Then, <span class="math inline">\(99\%\)</span>, <span class="math inline">\(95\%\)</span>, <span class="math inline">\(90\%\)</span> confidence intervals are</p>
<span class="math display">\[\begin{align}
&amp;99\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 2.58 \times SE(\overline{Y}) \right\}. \\
&amp;95\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 1.96 \times SE(\overline{Y}) \right\}. \\
&amp;90\%\text{ confidence interval for } \mu_Y = \left\{ \overline{Y} \pm 1.64 \times SE(\overline{Y}) \right\}.
\end{align}\]</span>
<p>These confidence intervals are sets of null hypotheses we cannot reject in a two-sided hypothesis test at the given level of confidence.</p>
<p>Now consider the following statements.</p>
<ol style="list-style-type: decimal">
<li><p>The interval <span class="math display">\[ \left\{ \overline{Y} \pm 1.96 \times SE(\overline{Y}) \right\} \]</span> covers the true value of <span class="math inline">\(\mu_Y\)</span> with a probability of <span class="math inline">\(95\%\)</span>.</p></li>
<li><p>We have computed <span class="math inline">\(\overline{Y} = 5.1\)</span> and <span class="math inline">\(SE(\overline{Y}=2.5\)</span> so the interval <span class="math display">\[ \left\{ 5.1  \pm 1.96 \times 2.5 \right\} = \left[0.2,10\right] \]</span> covers the true value of <span class="math inline">\(\mu_Y\)</span> with a probability of <span class="math inline">\(95\%\)</span>.</p></li>
</ol>
<p>While 1. is right (this is exactly in line with the definition above), 2. is completely wrong and none of Your lecturers wants to read such a sentence in a term paper, written exam or similar, believe us. The difference is that, while 1. is the definition of a random variable, 2. is one possible <em>outcome</em> of this random variable so there is no meaning in making any probabilistic statement about it. Either the computed interval <em>does cover</em> <span class="math inline">\(\mu_Y\)</span> or it <em>does not</em>!</p>
</div>
<p>In R, testing hypothesis about the mean of a population on the basis of a random sample is very easy due to functions like <code>t.test()</code> from the <tt>stats</tt> package. It procudes an object of type <tt>list</tt>. Luckily, one of the most simple ways to use <code>t.test()</code> is when You want to obtain a <span class="math inline">\(95\%\)</span> confidence interval for some population mean. We start by generating some random data and calling <code>t.test()</code> in conjunction with <code>ls()</code> to obtain a breakdown of the output components.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># generate some sample data</span>
sampledata &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>,<span class="dv">10</span>,<span class="dv">10</span>)

<span class="co"># checke type</span>
<span class="kw">typeof</span>(<span class="kw">t.test</span>(sampledata))</code></pre></div>
<pre><code>## [1] &quot;list&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># display list elements produced by t.test</span>
<span class="kw">ls</span>(
  <span class="kw">t.test</span>(sampledata)
)</code></pre></div>
<pre><code>## [1] &quot;alternative&quot; &quot;conf.int&quot;    &quot;data.name&quot;   &quot;estimate&quot;    &quot;method&quot;     
## [6] &quot;null.value&quot;  &quot;p.value&quot;     &quot;parameter&quot;   &quot;statistic&quot;</code></pre>
<p>Though we find that many items are reported, at the moment we are interested in computing a <span class="math inline">\(95\%\)</span> confidence set for the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(sampledata)<span class="op">$</span><span class="st">&quot;conf.int&quot;</span></code></pre></div>
<pre><code>## [1]  9.306651 12.871096
## attr(,&quot;conf.level&quot;)
## [1] 0.95</code></pre>
<p>This tells us that the <span class="math inline">\(95\%\)</span> confidence interval is</p>
<p><span class="math display">\[ \left[9.31, 12.87\right]. \]</span></p>
<p>In this example, the computed interval does cover the true <span class="math inline">\(\mu_Y\)</span> which we know to be <span class="math inline">\(10\)</span>.</p>
<p>Let us have a look at the whole standard output produced by <code>t.test()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(sampledata)</code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  sampledata
## t = 12.346, df = 99, p-value &lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##   9.306651 12.871096
## sample estimates:
## mean of x 
##  11.08887</code></pre>
<p>We see that <code>t.test()</code> not only computes a <span class="math inline">\(95\%\)</span> confidence interval but automatically conducts a two-sided significance test of the hypothesis <span class="math inline">\(H_0: \mu_Y = 0\)</span> at the level of <span class="math inline">\(5\%\)</span> and reports relevant parameters thereof: the alternative hypothesis, the estimated mean, the resulting <span class="math inline">\(t\)</span>-statistic, the degrees of freedom of the underlying <span class="math inline">\(t\)</span> distribution (<code>t.test()</code> does not perform the normal approximation) and the corresponding <span class="math inline">\(p\)</span>-value. Very convenient!</p>
<p>In this example, we come to the conclusion that the population mean <em>is not</em> significantly different from <span class="math inline">\(0\)</span> at the level of <span class="math inline">\(5\%\)</span> (which is correct), since <span class="math inline">\(\mu_Y = 0\)</span> is element of the <span class="math inline">\(95\%\)</span> confidence interval</p>
<p><span class="math display">\[ 0 \in \left[-0.27,0.12\right]. \]</span> We come to an equivalent result when using the <span class="math inline">\(p\)</span>-value rejection rule:</p>
<p><span class="math display">\[ p = 0.456 &gt; 0.05 \]</span></p>
</div>
<div id="comparing-means-from-different-populations" class="section level2 unnumbered">
<h2>Comparing Means from Different Populations</h2>
<p>Suppose You are interested in the means of two different populations, denote them <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span>. More specifically You are interested whether these population means are different from each other and plan an using a hypothesis test to verifiy this on the basis of independent sample data from both populations. A suitable pair of hypotheses then is</p>
<span class="math display" id="eq:hypmeans">\[\begin{equation}
H_0: \mu_1 - \mu_2 = d_0 \ \ \text{vs.} \ \ H_1: \mu_1 - \mu_2 \neq d_0 \tag{3.6}
\end{equation}\]</span>
<p>where <span class="math inline">\(d_0\)</span> denotes the hypothesized difference in means. The book teaches us that <span class="math inline">\(H_0\)</span> can be tested with the <span class="math inline">\(t\)</span>-statistic</p>
<span class="math display" id="eq:tstatmeans">\[\begin{equation}
t=\frac{(\overline{Y}_1 - \overline{Y}_2) - d_0}{SE(\overline{Y}_1 - \overline{Y}_2)} \tag{3.7}
\end{equation}\]</span>
<p>where</p>
<span class="math display">\[\begin{equation}
SE(\overline{Y}_1 - \overline{Y}_2) = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}.
\end{equation}\]</span>
<p>This is called a two sample <span class="math inline">\(t\)</span>-test. For large <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>, <a href="a-review-of-statistics-using-r.html#eq:tstatmeans">(3.7)</a> is standard normal distributed under the null hypothesis. Anlog to the simple <span class="math inline">\(t\)</span>-test we can compute confidence intervals for the true difference in population means:</p>
<p><span class="math display">\[ (\overline{Y}_1 - \overline{Y}_2) \pm 1.96 \times SE(\overline{Y}_1 - \overline{Y}_2) \]</span></p>
<p>is a <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(d\)</span>. <br> In R, Hypotheses as in <a href="a-review-of-statistics-using-r.html#eq:hypmeans">(3.6)</a> can be tested with <code>t.test()</code>, too. Note that <code>t.test()</code> chooses <span class="math inline">\(d_0 = 0\)</span> by default. This can be changed by setting the argument <code>mu</code> accordingly.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># draw data from two different populations with equal mean</span>
sample_pop1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">10</span>, <span class="dv">10</span>)
sample_pop2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">10</span>, <span class="dv">20</span>)

<span class="co"># perform a two sample t-test</span>
<span class="kw">t.test</span>(sample_pop1, sample_pop2)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  sample_pop1 and sample_pop2
## t = 0.872, df = 140.52, p-value = 0.3847
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.338012  6.028083
## sample estimates:
## mean of x mean of y 
## 11.088874  9.243838</code></pre>
<p>We find that the two sample <span class="math inline">\(t\)</span>-test does not reject the (true) null hypothesis that <span class="math inline">\(d_0 = 0\)</span>.</p>
</div>
<div id="an-application-to-the-gender-gap-of-earnings" class="section level2 unnumbered">
<h2>An Application to the Gender Gap of Earnings</h2>
<p>In this section discusses how to reproduce the results presented in the box ‘<em>The Gender Gap of Earnings of College Graduates in the United States</em>’ in the book.</p>
<p>In order to reproduce table 3.1 You need to download the replication data which is hosted by Pearson and can be found and downloaded <a href="http://wps.aw.com/aw_stock_ie_3/178/45691/11696965.cw/index.html">here</a>. Download the data for chapter three as an excel spreadsheet (<tt>cps_ch3.xlsx</tt>). This data set contains data that ranges from <span class="math inline">\(1992\)</span> to <span class="math inline">\(2008\)</span> and earnings are reported in prices of <span class="math inline">\(2008\)</span>.<br> There are several ways to import the <tt>.xlsx</tt>-files into R. Our suggestion is the function <code>read_excel()</code> from the <tt>readxl</tt> package. The package is not part of R’s standard distribution and has to be installed manually.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install and load the readxl package</span>
## install.packages(&#39;readxl&#39;)
<span class="kw">library</span>(readxl)</code></pre></div>
<p>You are now ready to import the data set. Make sure You use the correct path to the downloaded file! In our example, the file is saved in a subfolder (<tt>data</tt>) of the working directory. If You are not sure what Your current working directory is, use <code>getwd()</code>, see also <code>?getwd()</code>. This will give You the path that points to the place R is currently looking for files.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># import the data into R</span>
cps &lt;-<span class="st"> </span><span class="kw">read_excel</span>(<span class="dt">path =</span> <span class="st">&#39;data/cps_ch3.xlsx&#39;</span>)</code></pre></div>
<p>Next, install and load the package <tt>dyplr</tt>. This package provides some handy functions that simplify data wrangling a lot. It makes use of the <code>%&gt;%</code> operator.</p>

<div class="rmdknit">
<p>In general, the aim of pipe operators is to increase readability of written code. The pipe operator <tt>%&gt;%</tt>, also known as <tt>magrittr</tt>, is relatively new to R. It was originally introduced with the package <tt>magrittr</tt> but is available for several R packages. The most prominent ones are <tt>plotly</tt> and <tt>dplyr</tt>. See the following <a href="https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html">link</a> for more on the <tt>magrittr</tt> package.</p>
The basic idea is to simplify a sequece of function calls by chaining them.<br> <br> <br> 
<tt>
1:10 %>% mean<br>  
# [1] 5.5<br>  
<br>
# is equivalent to<br>
mean(1:10)<br>  
# [1] 5.5
</tt>

</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install and load the dplyr package</span>
## install.packages(&#39;dplyr&#39;)
<span class="kw">library</span>(<span class="st">&#39;dplyr&#39;</span>)</code></pre></div>
<p>First, get an overview over the data set. Next, use <code>%&gt;%</code> and some functions from the <code>dplyr</code> package to group the observations by gender and year and compute descriptive statistics for both groups.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get an overview of the data structure</span>
<span class="kw">head</span>(cps)</code></pre></div>
<pre><code>## # A tibble: 6 x 3
##   a_sex  year    ahe08
##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1     1  1992 17.16203
## 2     1  1992 15.33856
## 3     1  1992 22.94229
## 4     2  1992 13.28334
## 5     1  1992 22.12292
## 6     2  1992 12.16761</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># group data by gender and year and compute the mean, standard deviation</span>
<span class="co"># and number of observations for each group</span>
avgs &lt;-<span class="st"> </span>cps <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">group_by</span>(a_sex, year) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">summarise</span>(<span class="kw">mean</span>(ahe08), 
                  <span class="kw">sd</span>(ahe08), 
                  <span class="kw">n</span>()
                  )

<span class="co"># print results to the console</span>
<span class="kw">print</span>(avgs)</code></pre></div>
<pre><code>## # A tibble: 10 x 5
## # Groups:   a_sex [?]
##    a_sex  year `mean(ahe08)` `sd(ahe08)` `n()`
##    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;
##  1     1  1992      23.27382   10.172081  1594
##  2     1  1996      22.47544   10.103141  1379
##  3     1  2000      24.88314   11.599727  1303
##  4     1  2004      25.12169   12.008435  1894
##  5     1  2008      24.97840   11.778632  1838
##  6     2  1992      20.04629    7.868418  1368
##  7     2  1996      18.98048    7.951608  1230
##  8     2  2000      20.73938    9.359327  1181
##  9     2  2004      21.02373    9.363071  1735
## 10     2  2008      20.87478    9.657140  1871</code></pre>
</div>
<p>With the pipe operator <code>%&gt;%</code> we simply chain different R functions that produce compatible input and ouput. In the code above, we take the dataset <code>cps</code> and use it as an input for the function <code>group_by()</code>. The output of <code>group_by</code> is subsequently used as an input for <code>summarise()</code> and so forth.</p>
<p>Now that we have computed the statistics of interest for both genders, we can investigate how the gap in earnings between both groups evolves over time.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># split the data set by gender</span>
male &lt;-<span class="st"> </span>avgs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(a_sex <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) 
female &lt;-<span class="st"> </span>avgs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(a_sex <span class="op">==</span><span class="st"> </span><span class="dv">2</span>)

<span class="co"># Rename columns of both splits</span>
<span class="kw">colnames</span>(male)   &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sex&quot;</span>, <span class="st">&quot;Year&quot;</span>, <span class="st">&quot;Y_bar_m&quot;</span>, <span class="st">&quot;s_m&quot;</span>, <span class="st">&quot;n_m&quot;</span>)
<span class="kw">colnames</span>(female) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sex&quot;</span>, <span class="st">&quot;Year&quot;</span>, <span class="st">&quot;Y_bar_f&quot;</span>, <span class="st">&quot;s_f&quot;</span>, <span class="st">&quot;n_f&quot;</span>)

<span class="co"># Estimate Gender gaps, compute standard errors and confidence intervals for all dates</span>
gap &lt;-<span class="st"> </span>male<span class="op">$</span>Y_bar_m <span class="op">-</span><span class="st"> </span>female<span class="op">$</span>Y_bar_f

gap_se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(male<span class="op">$</span>s_m<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>male<span class="op">$</span>n_m <span class="op">+</span><span class="st"> </span>female<span class="op">$</span>s_f<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>female<span class="op">$</span>n_f)

gap_ci_l &lt;-<span class="st"> </span>gap <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>gap_se

gap_ci_u &lt;-<span class="st"> </span>gap <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>gap_se

result &lt;-<span class="st"> </span><span class="kw">cbind</span>(male[,<span class="op">-</span><span class="dv">1</span>], female[,<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)], gap, gap_se, gap_ci_l, gap_ci_u)

<span class="co"># print results to the console</span>
<span class="kw">print</span>(result, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre></div>
<pre><code>##   Year Y_bar_m  s_m  n_m Y_bar_f  s_f  n_f  gap gap_se gap_ci_l gap_ci_u
## 1 1992    23.3 10.2 1594    20.0 7.87 1368 3.23  0.332     2.58     3.88
## 2 1996    22.5 10.1 1379    19.0 7.95 1230 3.49  0.354     2.80     4.19
## 3 2000    24.9 11.6 1303    20.7 9.36 1181 4.14  0.421     3.32     4.97
## 4 2004    25.1 12.0 1894    21.0 9.36 1735 4.10  0.356     3.40     4.80
## 5 2008    25.0 11.8 1838    20.9 9.66 1871 4.10  0.354     3.41     4.80</code></pre>
</div>
<p>We observe virtually the same results as the ones presented in the book. the computed statistics suggest that there <em>is</em> a gender gap in earnings. Note that we can reject the null hypothesis that the gap is zero for all periodes. Further, estimates of the gap and bounds of the 95% confidence intervals indicate that the gap has been quite stable over the recent past.</p>
</div>
<div id="scatterplots-sample-covariance-and-sample-correlation" class="section level2 unnumbered">
<h2>Scatterplots, Sample Covariance and Sample Correlation</h2>
<p>A scatterplot represents two dimensional data, for example <span class="math inline">\(n\)</span> observation on <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>, by points in a cartesian coordinate system. It is very easy to generate scatterplots using the <code>plot()</code> function in R. Let’s generate some fictional data on age and earnings of workers and plot it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># generate data set</span>
X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> <span class="dv">100</span>, 
           <span class="dt">min =</span> <span class="dv">18</span>, 
           <span class="dt">max =</span> <span class="dv">70</span>
           )
Y &lt;-<span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">100</span>, <span class="dv">50</span>, <span class="dv">15</span>)

<span class="co"># plot observations</span>
<span class="kw">plot</span>(X, 
     Y, 
     <span class="dt">type =</span> <span class="st">&quot;p&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;A Scatterplot of X and Y&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Age&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Earnings&quot;</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">19</span>
     )</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-69-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot shows positive correlation between age and earnings. This in line with the assumption that older workers earn more than those that the joined the working population recently.</p>
<div id="sample-covariance-and-correlation" class="section level4 unnumbered">
<h4>Sample Covariance and Correlation</h4>
<p>By now You should be familiar with the concepts of variance and covariance. If not, we recommend You to work Your way through chapter 2 of the book (again).</p>
<p>As for the variance, covariance and correlation of two variables are properties that relate to the (unknown) joint probability distribution of these variable. Just as the individual population variances of both variables, we can estimate covariance and correlation by means of suitable estimators using a random sample <span class="math inline">\((X_i,Y_i)\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p>The sample covariance</p>
<p><span class="math display">\[ s_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) \]</span></p>
<p>is an estimator for the population variance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> whereas the sample correlation</p>
<p><span class="math display">\[ r_{XY} = \frac{s_{XY}}{s_Xs_Y} \]</span> can be used to estimate the population correlation, a standardized measure for the strength of the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. See chapter 3.7 in the book for a more detailed treatment of these estimators.</p>
<p>As for variance and standard deviation, these estimators are implemented as R functions in the <tt>stats</tt> package. We can use them to estimate population covariance and population correlation the fictional data on age and earnings.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute sample covariance of X and Y</span>
<span class="kw">cov</span>(X,Y)</code></pre></div>
<pre><code>## [1] 213.934</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute sample correlation between X and Y</span>
<span class="kw">cor</span>(X,Y)</code></pre></div>
<pre><code>## [1] 0.706372</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># equivalent way to compute the sample correlation</span>
<span class="kw">cov</span>(X,Y)<span class="op">/</span>(<span class="kw">sd</span>(X) <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(Y))</code></pre></div>
<pre><code>## [1] 0.706372</code></pre>
<p>The estimates indicate that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are moderately correlated.</p>
<p>The next code chunk uses the function <code>mvnorm()</code> from package <tt>MASS</tt> to generate bivariate example data with different degree of correlation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)

<span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># positive correlation (0.81)</span>
example1 &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">100</span>,
                    <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), 
                    <span class="dt">Sigma =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">ncol =</span> <span class="dv">2</span>),
                    <span class="dt">empirical =</span> <span class="ot">TRUE</span>
                    )

<span class="co"># negative correlation (-0.81)</span>
example2 &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">100</span>,
                    <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), 
                    <span class="dt">Sigma =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">ncol =</span> <span class="dv">2</span>),
                    <span class="dt">empirical =</span> <span class="ot">TRUE</span>
                    )

<span class="co"># no correlation </span>
example3 &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">100</span>,
                    <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), 
                    <span class="dt">Sigma =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">2</span>),
                    <span class="dt">empirical =</span> <span class="ot">TRUE</span>
                    )

<span class="co"># no correlation (quadratic relationship)</span>
X &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="fl">0.01</span>)
Y &lt;-<span class="st"> </span><span class="op">-</span>X<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(X))

example4 &lt;-<span class="st"> </span><span class="kw">cbind</span>(X,Y)

<span class="co"># estimate</span>

## Plots

<span class="co"># divide plot area as 2-by-2 array</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))

<span class="kw">plot</span>(example1, <span class="dt">col=</span><span class="st">&#39;steelblue&#39;</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">xlab =</span> <span class="st">&#39;X&#39;</span>, <span class="dt">ylab =</span> <span class="st">&#39;Y&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Correlation = 0.81&quot;</span>)
<span class="kw">plot</span>(example2, <span class="dt">col=</span><span class="st">&#39;steelblue&#39;</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">xlab =</span> <span class="st">&#39;X&#39;</span>, <span class="dt">ylab =</span> <span class="st">&#39;Y&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Correlation = -0.81&quot;</span>)
<span class="kw">plot</span>(example3, <span class="dt">col=</span><span class="st">&#39;steelblue&#39;</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">xlab =</span> <span class="st">&#39;X&#39;</span>, <span class="dt">ylab =</span> <span class="st">&#39;Y&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Correlation = 0&quot;</span>)
<span class="kw">plot</span>(example4, <span class="dt">col=</span><span class="st">&#39;steelblue&#39;</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">xlab =</span> <span class="st">&#39;X&#39;</span>, <span class="dt">ylab =</span> <span class="st">&#39;Y&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Correlation = 0&quot;</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-71-1.png" width="672" style="display: block; margin: auto;" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability-theory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lrwor.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-ch3.Rmd",
"text": "Edit"
},
"download": ["URFITE.pdf", "URFITE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
